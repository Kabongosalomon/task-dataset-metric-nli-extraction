<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danila</forename><surname>Rukhovich</surname></persName>
							<email>d.rukhovich@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center Moscow</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Vorontsova</surname></persName>
							<email>a.vorontsova@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center Moscow</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Konushin</surname></persName>
							<email>a.konushin@samsung.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Samsung AI Center Moscow</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Lomonosov Moscow State University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce the task of multi-view RGBbased 3D object detection as an end-to-end optimization problem. To address this problem, we propose ImVoxel-Net, a novel fully convolutional method of 3D object detection based on posed monocular or multi-view RGB images. The number of monocular images in each multiview input can variate during training and inference; actually, this number might be unique for each multi-view input. ImVoxelNet successfully handles both indoor and outdoor scenes, which makes it general-purpose. Specifically, it achieves state-of-the-art results in car detection on KITTI (monocular) and nuScenes (multi-view) benchmarks among all methods that accept RGB images. Moreover, it surpasses existing RGB-based 3D object detection methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark for multi-view 3D object detection. The source code and the trained models are available at https://github.com/saic-vul/imvoxelnet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>RGB images are an affordable and universal data source; therefore, RGB-based 3D object detection has been actively investigated in recent years. RGB images provide visual clues about the scene and its objects, yet they do not contain explicit information about the scene geometry and the absolute scale of the data. By virtue of that, detecting 3D objects from the RGB images is an ill-posed task. Given a monocular image, deep learning-based 3D object detection methods can only deduce the scale of the data. Moreover, the scene geometry cannot be unambiguously derived from the RGB images since some areas may be invisible. However, using several posed images might help obtain more information about the scene than a monocular RGB image. Accordingly, some 3D object detection methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref> run multi-view inference. These methods obtain predictions on each monocular RGB image independently, then aggre-gate these predictions.</p><p>In contrast, we use multi-view inputs not only for inference but also for training. During both training and inference, the proposed method accepts posed multi-view inputs with an arbitrary number of views; this number might be unique for each multi-view input. Besides, our method can accept posed monocular inputs (treated as a special case of multi-view inputs). Furthermore, it works surprisingly well on monocular benchmarks.</p><p>All RGB-based 3D object detection methods are designed to be indoor or outdoor and work under certain assumptions about the scene and the objects. For instance, outdoor methods are typically evaluated on cars. In general, cars are of similar size, they are located on the ground, and their projections onto the Bird's Eye View (BEV) do not intersect. Accordingly, a BEV-plane projection contains much information on the 3D location of a car. So, a common approach in outdoor 3D object detection is to reduce a 3D object detection in a point cloud to a 2D object detection in the BEV plane. At the same time, indoor objects might have different heights and be randomly located in space, so their projections onto the floor plane provide little information about their 3D positions. Overall, the design of RGB-based 3D object detection methods tends to be domain-specific.</p><p>To accumulate information from multiple inputs, we construct a voxel representation of the 3D space. We use this unified approach to detect objects in both indoor and outdoor scenes: we only choose between an indoor and outdoor head, while the meta-architecture remains the same.</p><p>In the proposed method, final predictions are obtained from 3D feature maps, which corresponds to the formulation of the point cloud-based detection problem. On this basis, we use off-the-shelf necks and heads from point cloudbased object detectors with no modifications.</p><p>Our contribution is three-fold:</p><p>? As far as we know, we are the first to formulate a task of end-to-end training for multi-view 3D object detection based on posed RGB images only.</p><p>? We propose a novel fully convolutional 3D object detector that works in both monocular and multi-view settings.</p><p>? With domain-specific heads, the proposed method achieves state-of-the-art results for both indoor and outdoor datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-view Scene Understanding</head><p>Many scene understanding methods accept multi-view inputs. For instance, some scene understanding sub-tasks can only be solved given multi-view inputs. For example, the SLAM task implies reconstructing 3D scene geometry and estimating camera poses given a sequence of frames. Structure-from-Motion (SfM) approaches are designed to estimate camera poses and intrinsics from an unordered set of images, whereas Multi-View Stereo (MVS) methods use SfM outputs to build a 3D point cloud.</p><p>Other scene understanding sub-tasks might be reformulated to be multi-view. Several methods that use multi-view inputs to address these tasks have been proposed recently. For instance, 3D-SIS <ref type="bibr" target="#b12">[13]</ref> performs 3D instance segmentation based on a set of RGB-D inputs. MVPointNet <ref type="bibr" target="#b16">[17]</ref> uses multi-view RGB-D inputs for 3D semantic segmentation. Atlas <ref type="bibr" target="#b25">[26]</ref> processes several monocular RGB images to perform 3D semantic segmentation and TSDF reconstruction jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">3D Object Detection.</head><p>Point cloud-based. Point clouds are three-dimensional, so it seems natural to employ a 3D convolutional network for detection. However, this approach requires exhaustive computation that causes slow inference on large outdoor scenes. Recent outdoor methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19]</ref> decrease the runtime by projecting the 3D point cloud to the BEV plane. The common practice in point cloud processing is to subdivide a point cloud into voxels. The projection onto the BEV plane implies that all voxels in each vertical column should be encoded into a fixed-length feature map. Then, this pseudo-image can be passed to a 2D object detection network to obtain final predictions.</p><p>Indoor object detection methods generate object proposals for each point in a point cloud. However, some indoor objects are not convex, so the geometrical center of an indoor object may not belong to this object (e.g., the center of a table or a chair might be in between legs). Accordingly, an object proposal given by a single center point might be irrelevant, so indoor methods use deep Hough voting to generate proposals <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>Stereo-based. Despite accepting more than one image, stereo-based methods cannot be considered multi-view as they use two images. In contrast, multi-view methods can process an arbitrary amount of inputs. Moreover, camera poses might be arbitrary for multi-view inputs, and for stereo inputs, the relative transformation between two cameras is known precisely and remains fixed while recording. This makes it possible to perform stereo reconstruction by estimating optical flow between the left and right images. Stereo-based methods rely heavily on the stereo assumptions, e. g., 3DOP <ref type="bibr" target="#b5">[6]</ref> uses stereo reconstruction to generate object proposals, while TLNet <ref type="bibr" target="#b31">[32]</ref> runs triangulation to merge proposals obtained for left and right images independently. Stereo R-CNN <ref type="bibr" target="#b20">[21]</ref> generates object proposals given both left and right images, then estimates object location by triangulating keypoints.</p><p>Monocular-based. Mono3D <ref type="bibr" target="#b6">[7]</ref> generates 3D anchors by aggregating clues from semantic maps, visible contours of the objects, and location priors via a complex energy function. Deep3DBox <ref type="bibr" target="#b24">[25]</ref> uses discretization to estimate the orientation of each object and derives its 3D pose from constraints between 2D and 3D bounding boxes. MonoGR-Net <ref type="bibr" target="#b30">[31]</ref> decomposes the 3D object detection problem into sub-tasks, namely object distance estimation, object location estimation, and object corners estimation. These subtasks are solved by separate networks, trained first stagewise then altogether to refine 3D bounding boxes.</p><p>Other methods, e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>, exploit 2D detection and lift information from 2D to 3D. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> extend 2D detection network with a 3D branch that regresses object pose. Some methods make use of external data sources, e.g., DeepMANTA <ref type="bibr" target="#b3">[4]</ref> uses an iterative coarse-to-fine algorithm of generating 2D object proposals, which are used to select a CAD model. 3D-RCNN <ref type="bibr" target="#b17">[18]</ref> also performs 2D detection and matches the outputs to 3D models. Then, it uses a render-and-compare approach to recover the shape and pose of an object.</p><p>Monocular indoor 3D object detection is a less explored problem, with only SUN RGB-D <ref type="bibr" target="#b36">[37]</ref> benchmark existing. This benchmark implies that indoor 3D object detection is a sub-task of total scene understanding. Beside detecting 3D objects, <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref> estimate camera poses and room layouts. The most recent Total3DUnderstanding <ref type="bibr" target="#b26">[27]</ref> reconstructs object meshes using an attention mechanism to consider relationships between objects.</p><p>Some outdoor 3D object detection methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b32">33]</ref> are evaluated on the nuScenes <ref type="bibr" target="#b2">[3]</ref> dataset on multi-view inputs. Specifically, these methods infer on each monocular RGB image, then aggregate the outputs. Aggregation is an inevitable part of the pipeline; however, doing this on the latest stage is controversial, as spatial information might not be exploited as effectively as possible.</p><p>So, none of the existing methods formulate 3D object detection given multiple RGB images as an end-to-end optimization problem. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>Our method accepts an arbitrary-sized set of RGB inputs along with camera poses. First, we extract features from the given images using a 2D convolutional backbone. Then, we project the obtained image features to a 3D voxel volume. For each voxel, the projected features from several images are aggregated via a simple element-wise averaging. Next, the voxel volume with assigned features is passed to a 3D convolutional network referred to as neck. The outputs of the neck serve as inputs to the last few convolutional layers (head) that predict bounding box features for each anchor. The resulting bounding boxes are parameterized as (x, y, z, w, h, l, ?), where (x, y, z) are the coordinates of the center, w, h, l are for width, height, and length, and ? is the rotation angle around z-axis. The general scheme of the proposed method is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>2D features projection and 3D neck network have been proposed in <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref>. First, we briefly outline these steps. Then, we introduce a novel multi-scale 3D head designed for indoor detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Volume Construction</head><p>Let I t ? R W ?H?3 be the t-th image in a set of T images. Here, T &gt; 1 in case of multi-view inputs and T = 1 for single-view inputs. Following <ref type="bibr" target="#b25">[26]</ref>, we first extract 2D features from passed inputs using a pretrained 2D backbone. It outputs four feature maps of shapes</p><formula xml:id="formula_0">W 4 ? H 4 ? c 0 , W 8 ? H 8 ?2c 0 , W 16 ? H 16 ?4c 0 , and W 32 ? H 32 ?8c 0 .</formula><p>We aggregate the obtained feature maps via Feature Pyramid Network (FPN), which outputs one tensor F t of shape W 4 ? H 4 ? c 1 . c 0 and c 1 are backbone-specific; actual values are present in 4.2.</p><p>For t-th input, the extracted 2D features F t are then projected into a 3D voxel volume V t ? R Nx?Ny?Nz?c1 . We set the z-axis to be perpendicular to the floor plane, with the x-axis pointing forward and the y-axis being orthogonal to both x and z-axes. For each dataset, there are known spatial limits for all three axes, estimated empirically in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b25">26]</ref>. Let us denote these limits as x min , x max , y min , y max , z min , z max . For a fixed voxel size s, spatial constraints can be formulated as N x s = x max ?x min , N y s = y max ? y min , and N z s = z max ? z min . We use a pinhole camera model, which determines the correspondence between 2D coordinates (u, v) in feature map F t and 3D coordinates (x, y, z) in volume V t :</p><formula xml:id="formula_1">u v = ? ? ? 1 4 0 0 0 1 4 0 0 0 1 ? ? KR t ? ? ? ? x y z 1 ? ? ? ? ,</formula><p>where K and R t are the intrinsic and extrinsic matrices, and ? is a perspective mapping. After projecting 2D features, all voxels along a camera ray get filled with the same features. We also define a binary mask M t of the same shape as V t , which indicates whether each voxel is inside the camera frustum. Thus, for each image I t , the mask M t is defined as:</p><formula xml:id="formula_2">M t (x, y, z) = 1, if 0 ? u &lt; W 4 and 0 ? v &lt; H 4 0, otherwise.</formula><p>Then, we project F t for each valid voxel in a volume V t :</p><formula xml:id="formula_3">V t (x, y, z) = F t (u, v), if M t (x, y, z) = 1 0, otherwise.</formula><p>The aggregated binary mask M is a sum of M 1 , . . . , M t :</p><formula xml:id="formula_4">M (x, y, z) = t M t (x, y, z), if t M t (x, y, z) &gt; 0 1,</formula><p>otherwise.</p><p>Finally, we obtain the 3D volume V by averaging projected features in volumes V 1 , . . . , V t across valid voxels:</p><formula xml:id="formula_5">V = 1 M t M t V t .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Feature Extraction</head><p>Indoor. Following <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref>, we pass the voxel volume V through a 3D convolutional encoder-decoder network to refine the features. For indoor scenes, we use an encoderdecoder architecture from <ref type="bibr" target="#b25">[26]</ref>. However, with over 48 3D convolutional layers, the original network is computationally heavy and slow on inference. For a better performance, we simplify the network by reducing the number of time-consuming 3D convolutional layers. The simplified encoder has only three downsampling residual blocks, each with three 3D convolutional layers. The simplified decoder consists of three upsampling blocks, and each upsampling block is made up with a transposed 3D convolutional layer with stride 2 followed by another 3D convolutional layer. The decoder branch outputs three feature maps of the following shapes:</p><formula xml:id="formula_6">Nx 4 ? Ny 4 ? Nz 4 ? c 2 , Nx 2 ? Ny 2 ? Nz 2 ? c 2 , and N x ? N y ? N z ? c 2 . For the actual value of c 2 , see 4.2.</formula><p>Outdoor. Outdoor methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39]</ref> reduce 3D object detection in 3D space to 2D object detection in the BEV plane. In these methods, both the neck and head are composed of 2D convolutions. The outdoor head accepts a 2D feature map, so we should obtain a 2D representation of a constructed 3D voxel volume to use in our method. In order to do that, we use the encoder part of the encoder-decoder architecture from <ref type="bibr" target="#b25">[26]</ref>. After passing through several 3D convolutional and downsampling layers of this encoder, a voxel volume V of shape N x ? N y ? N z ? c 1 is mapped to the tensor of shape N x ? N y ? c 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Detection Heads</head><p>ImVoxelNet constructs a 3D voxel representation of the space; thus, it can use the head from point cloudbased 3D object detection methods. Therefore, instead of time-consuming custom architecture implementation, one can employ state-of-the-art methods with no modifications. However, the design of heads significantly differs for outdoor <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> and indoor <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Outdoor Head</head><p>We reformulate outdoor 3D object detection as 2D object detection in the BEV plane following the common practice. We use the 2D anchor head that appeared to be efficient <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref> on KITTI <ref type="bibr" target="#b10">[11]</ref> and nuScenes <ref type="bibr" target="#b2">[3]</ref> datasets. Since outdoor 3D detection methods are evaluated on cars, all objects are of a similar scale and belong to the same category. For single-scale and single-class detection, the head consists of two parallel 2D convolutional layers. One layer estimates class probability, while the other regresses seven parameters of the bounding box.</p><p>Input. The input is a tensor of shape N x ? N y ? c 2 .</p><p>Output. For each 2D BEV anchor, the head returns a class probability p and a 3D bounding box as a 7-tuple:</p><formula xml:id="formula_7">?x = x gt ? x a d a , ?y = y gt ? y a d a , ?z = z gt ? z a d a , ?w = log w gt w a , ?l = log l gt l a , ?h = log h gt h a , ?? = sin(? gt ? ? a ).</formula><p>Here ? gt and ? a are the ground truth and anchor boxes, respectively. The length of the bounding box diagonal d a = (w a ) 2 + (l a ) 2 . z a is constant for all anchors since they are located in the BEV plane.</p><p>Loss. We use the loss function introduced in SECOND <ref type="bibr" target="#b38">[39]</ref>. The total outdoor loss consists of several loss terms, namely smooth mean absolute error as a location loss L loc , focal loss for classification L cls , and cross-entropy loss for direction L dir . Overall, we can formulate the outdoor loss as</p><formula xml:id="formula_8">L outdoor = 1 n pos (? loc L loc + ? cls L cls + ? dir L dir ),</formula><p>where n pos is the number of positive anchors, ? loc = 2, ? cls = 1, ? dir = 0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Indoor Head</head><p>All modern indoor 3D object detection methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41]</ref> perform deep Hough voting for sparse point cloud representation. In contrast, we follow <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b12">13]</ref> and use dense voxel representation of intermediate features.</p><p>To the best of our knowledge, there is no dense 3D multi-scale head for 3D object detection. We construct such a head inspired by a 2D detection method FCOS <ref type="bibr" target="#b37">[38]</ref>. An original FCOS head accepts 2D features from FPN and estimates 2D bounding boxes via 2D convolutional layers. To adapt FCOS for 3D detection, we replace 2D convolutions with 3D convolutions to process 3D inputs. Following FCOS and ATSS <ref type="bibr" target="#b39">[40]</ref>, we apply center sampling to select candidate object locations. In these works, 9 (3 ? 3) candidates were chosen; since we operate in 3D space, we set a limit of 27 candidate locations per object (3 ? 3 ? 3). The resulting head consists of three 3D convolutional layers for classification, location, and centerness, respectively, with weights shared across all object scales.</p><formula xml:id="formula_9">Input. A multi-scale input is composed of three tensors of shapes Nx 4 ? Ny 4 ? Nz 4 ? c 2 , Nx 2 ? Ny 2 ? Nz 2 ? c 2 , and N x ? N y ? N z ? c 2 .</formula><p>Output. For each 3D location (x a , y a , z a ) and each of three scales, the head estimates a class probability p, a centerness c, and a 3D bounding box as a 7-tuple:</p><formula xml:id="formula_10">?x min = x gt min ? x a , ?x max = x gt max ? x a , ?y min = y gt min ? y a , ?y max = y gt max ? y a , ?z min = z gt min ? z a , ?z max = z gt max ? z a , ?.</formula><p>Here, x gt min , x gt max , y gt min , y gt max , z gt min , z gt max denote the minimum and maximum coordinates along axes of a ground truth bounding box.</p><p>Loss. We adapt the loss function used in the original FCOS <ref type="bibr" target="#b37">[38]</ref>. It consists of focal loss for classification L cls , cross-entropy loss for centerness L cntr , and IoU loss for location L loc . Since we address the 3D detection task instead of the 2D detection task, we replace 2D IoU loss with rotated 3D IoU loss <ref type="bibr" target="#b41">[42]</ref>. In addition, we update ground truth centerness with the third dimension. The resulting indoor loss can be written as</p><formula xml:id="formula_11">L indoor = 1 n pos (L loc + L cls + L cntr ),</formula><p>where n pos is the number of positive 3D locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Extra 2D Head</head><p>In some indoor benchmarks, the 3D object detection task is formulated as a sub-task of scene understanding. Accordingly, evaluation protocols imply solving various scene understanding tasks rather than only estimating 3D bounding boxes. Following <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>, we predict camera rotations and room layouts. Similar to <ref type="bibr" target="#b26">[27]</ref>, we add a simple head for joint R t and 3D layout estimation. This extra head consists of two parallel branches: two fully connected layers output room layout and the other two fully connected layers estimate camera rotation.</p><p>Input. The input is a single tensor of shape 8c 0 , obtained through global average pooling of the backbone output.</p><p>Output. The head outputs camera pose as a tuple of pitch ? and roll ? and a 3D layout box as a 7-tuple (x, y, z, w, l, h, ?). As <ref type="bibr" target="#b26">[27]</ref>, we set yaw angle and shift to zeros.</p><p>Loss. We modify losses used in <ref type="bibr" target="#b26">[27]</ref> to make them consistent with the losses used to train a detection head. Accordingly, we define layout loss L layout as rotated 3D IoU loss between predicted and ground truth layout boxes; this is the same loss as we use in 3.3.2. For camera rotation estimation, we use L pose = | sin(? gt ? ?)| + | sin(? gt ? ?)| similar to 3.3.1. Overall, the extra loss can be formulated as</p><formula xml:id="formula_12">L extra = ? layout L layout + ? pose L pose ,</formula><p>where ? layout = 0.1 and ? pose = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate the proposed method on four datasets: indoor ScanNet <ref type="bibr" target="#b8">[9]</ref> and SUN RGB-D <ref type="bibr" target="#b36">[37]</ref>, and outdoor KITTI <ref type="bibr" target="#b10">[11]</ref> and nuScenes <ref type="bibr" target="#b2">[3]</ref>. SUN RGB-D and KITTI are benchmarked in monocular mode, while for ScanNet and nuScenes, we address the detection problem in multi-view formulation.</p><p>KITTI. The KITTI object detection dataset <ref type="bibr" target="#b10">[11]</ref> is the most decisive outdoor benchmark for monocular 3D object detection. It consists of 3711 training, 3768 validation and 7518 test images. The common practice <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref> is to report results on validation subset and submit test predictions to an open leaderboard. All 3D object annotations have a difficulty level: easy, moderate, and hard. A 3D object detection method is assessed according to the results on moderate objects from the test set. Following <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23]</ref>, we evaluate our method only on objects of the car category.</p><p>nuScenes. The nuScenes dataset <ref type="bibr" target="#b2">[3]</ref> provides data for developing algorithms addressing self-driving-related tasks. It contains LiDAR point clouds, RGB images captured by six cameras, accompanied by IMU and GPS measurements. The dataset covers 1000 video sequences, each recorded for 20 seconds, totalling 1.4 million images and 390 000 point clouds. Training split covers 28 130 scenes, and validation split contains 6019 scenes. The annotation contains 1.4 million objects divided into 23 categories. Following <ref type="bibr" target="#b34">[35]</ref>, the accuracy of 3D detection is measured only on car category. In this benchmark, not only the average precision (AP) metric but average translation error (ATE), average scale error (ASE), and average orientation error (AOE) are calculated as well.</p><p>SUN RGB-D. SUN RGB-D <ref type="bibr" target="#b36">[37]</ref> is one of the first and most well-known indoor 3D datasets. It contains 10 335 images captured in various indoor places alongside corresponding depth maps obtained with four different sensors and camera poses. The training split is composed of 5285 frames, while the rest 5050 frames comprise the validation subset. The annotation includes 58 657 objects. For each frame, a room layout is provided.</p><p>ScanNet. The ScanNet dataset <ref type="bibr" target="#b8">[9]</ref> contains 1513 scans covering over 700 unique indoor scenes, out of which 1201 scans belong to a training split, and 312 scans are used for validation. Overall, this dataset contains over 2.5 million images with corresponding depth maps and camera poses, alongside reconstructed point clouds with 3D semantic annotation. We estimate 3D bounding boxes from semantic point clouds following the standard protocol <ref type="bibr" target="#b28">[29]</ref>. The resulting object bounding boxes are axis-aligned, so we do not predict the rotation angle ? for ScanNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>3D Volume. We use ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as a feature extractor. Accordingly, the number of convolutions in the first convolutional block c 0 equals 256. We set both the 3D volume feature size c 1 and the ouput feature size c 2 to 256 as proposed in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Indoor and outdoor scenes are of different absolute scales. Therefore, we choose the spatial sizes of the feature volume for each dataset considering the data domain. We use the values provided in previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36]</ref>, as shown in Tab. 1. Thus, using anchor settings of the 3D head in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>, we set voxel size s as 0.32 meters for outdoor datasets. Minimal and maximal values for all three axes for outdoor datasets also follow the point cloud ranges for car class in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36]</ref>. For selecting indoor dataset constraints we follow <ref type="bibr" target="#b25">[26]</ref>, where the room size is 6.4 ? 6.4 ? 2.56 meters. The only change is that we are increasing voxels size s from 0.04 to 0.16 to increase memory efficiency.</p><p>Training. During training, we optimize L indoor for indoor datasets and L outdoor for outdoor datasets, unless told otherwise. We use Adam optimizer with an initial learning rate set to 0.0001 and weight decay of 0.0001. The implementation is based on the MMDetection framework <ref type="bibr" target="#b4">[5]</ref> and uses its default training settings. The network is trained for 12 epochs, and the learning rate is reduced by ten times after the 8th and 11th epoch. For ScanNet, SUN RGB-D, and KITTI, the network sees each scene three times every training epoch. We use 8 Nvidia Tesla P40 GPUs for training, distributing one scene (multi-view scenario) or four images (monocular scenario) per GPU. We randomly apply horizontal flip and resize inputs in monocular experiments by no more than 25% of their original resolution. Moreover, in indoor scenes, we can augment 3D voxel representations similar to point cloud-based methods, so we randomly shift a voxel grid center by at most 1m along each axis.</p><p>Inference. During inference, outputs are filtered with a Rotated NMS algorithm, which is applied to objects projections onto the ground plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results</head><p>First, we report the results of detecting cars on outdoor KITTI and nuScenes benchmarks. Then, we discuss the results of multi-class 3D object detection on SUN RGB-D and ScanNet indoor datasets.</p><p>KITTI. We present the results of monocular car detection on KITTI in Tab. 2. ImVoxelNet achieves the best moderate AP on the test split, which is the main metric in the KITTI benchmark. Moreover, our method surpasses previous state-of-the-art by 6% AP 3D and 4% AP BEV for easy objects. Overall, ImVoxelNet is superior in terms of almost all metrics on both test and val splits. nuScenes. For nuScenes, unlike other methods that only run inference on images from 6 onboard cameras, ImVoxel-Net uses multi-view inputs for training. As shown in Tab. 3, the proposed method outperforms MonoDIS <ref type="bibr" target="#b34">[35]</ref> by more than 1% of mean AP, which is the main metric. According to AP@0.5, ImVoxelNet outputs almost twice as many highly accurate estimates comparing to MonoDIS. For car detection, two boxes might have IoU = 0 when a center distance exceeds 1 meter. By that, AP@1.0m, AP@2.0m, and AP@4.0m might be calculated for non-intersecting bounding boxes, which seems counter-intuitive (e.g., for the KITTI dataset, only boxes with IoU &gt;0.7 are considered to be true positive). Hence, we argue that AP@0.5 is the most decisive metric.</p><p>Moreover, we report values of ATE, ASE, and AOE metrics. As represented in the Tab. 3, ImVoxelNet has at least 0.09 meters smaller ATE than other monocular methods.</p><p>SUN RGB-D. We compare ImVoxelNet with existing  methods on the most recent monocular benchmark introduced in <ref type="bibr" target="#b26">[27]</ref>, which includes objects of NYU-37 categories <ref type="bibr" target="#b33">[34]</ref>. Since the chosen benchmark implies estimating camera pose and layout, we optimize L indoor + L extra for training. For a fair comparison with Total3DUnderstanding <ref type="bibr" target="#b26">[27]</ref>, we report their results without joint training since it requires the additional mesh-annotated dataset. Tab. 4 demonstrates that ImVoxelNet surpasses all previous methods by a margin exceeding 18% in terms of mAP. Furthermore, ImVoxel-Net outperforms Total3DUnderstanding in both layout and camera pose estimation. We also report metrics on other benchmarks: the PerspectiveNet <ref type="bibr" target="#b15">[16]</ref> benchmark with 30 object categories, and the VoteNet <ref type="bibr" target="#b28">[29]</ref> benchmark with 10 categories, which is used by point cloud-based methods (see A).</p><p>ScanNet. We compare ImVoxelNet to existing methods on the common benchmark with 18 classes. During training, we use T = 50 images per scene, as was proposed in <ref type="bibr" target="#b25">[26]</ref>. We conduct an ablation study to choose an optimal number of test images per scene (Tab. 6). We run our method five times on different samples for each number of test images and report an average result with a 0.95 confidence interval. Experiments show that the more images per test scene, the better. The most time-consuming part of the pipeline is processing a voxel volume with 3D convolutions while extracting 2D features gives a minor overhead. forms point cloud-based 3D-SIS <ref type="bibr" target="#b12">[13]</ref> which builds a voxel volume representation using RGB images as an additional modality. Performance. We report the inference time on the KITTI dataset in Tab. 7. All the methods were examined in the same experimental setup on a single GPU. ImVox-elNet uses computationally expensive 3D convolutions, so it is expected to be slower than the methods that rely on 2D convolutions only. In our experiments, ImVoxelNet appeared to be inferior in speed to most of the listed methods, yet the runtime differs within an order of magnitude. The listed methods use different backbones, and this affects the total speed. In ImVoxelNet, extracting features with a backbone is a simple, lightweight procedure compared to processing voxel volume with 3D convolutions. Accordingly, the choice of a backbone is negligible: experiments show that replacing ResNet-50 with a more lightweight version has a minor influence on performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone AP Runtime[s] OFTNet <ref type="bibr" target="#b32">[33]</ref> ResNet-18 3.27 0.50 GS3D <ref type="bibr" target="#b19">[20]</ref> VGG-16 10.97 2.00 MonoGRNet <ref type="bibr" target="#b30">[31]</ref> VGG-16 10.19 0.06 MonoDIS <ref type="bibr" target="#b34">[35]</ref> ResNet- <ref type="bibr" target="#b33">34</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we formulate the task of multi-view RGBbased 3D object detection as an end-to-end optimization problem. To address this problem, we have proposed ImVoxelNet, a novel fully convolutional method of 3D object detection given posed monocular or multi-view RGB inputs. During both training and inference, ImVoxelNet accepts multi-view inputs with an arbitrary number of views. Besides, our method can accept monocular inputs (treated as a special case of multi-view inputs). The proposed method has achieved state-of-the-art results in outdoor car detection on both the monocular KITTI benchmark and the multi-view nuScenes benchmark. Moreover, it has surpassed existing methods of 3D object detection on the indoor SUN RGB-D dataset. For the ScanNet dataset, ImVox-elNet has set a new benchmark for indoor multi-view 3D object detection. Overall, ImVoxelNet successfully works on both indoor and outdoor data, which makes it generalpurpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More results on SUN RGB-D</head><p>For a comprehensive comparison, we also mention Per-spectiveNet <ref type="bibr" target="#b15">[16]</ref>, which is evaluated following a different protocol. In that protocol, the annotations are mapped into 30 object categories. Accordingly, we train ImVoxelNet using the same object categories. The results are reported in Tab. 10. Among these 30 categories, 10 object categories are consistent with 10 categories used in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. So, we can merge these benchmarks and report metrics for <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref> that are obtained on the same subset of 10 object categories 9. Following <ref type="bibr" target="#b15">[16]</ref>, we assume camera poses are known, so we optimize only L indoor and do not use any additional camera pose loss.</p><p>Another SUN RGB-D benchmark has been proposed in <ref type="bibr" target="#b28">[29]</ref> for point cloud-based methods evaluation. This benchmark implies detecting objects of 10 categories with mAP@0.25 chosen as the main metric. In Tab. 8, we report the results of our method against point cloud-based methods. This comparison is unfair, favoring point cloudbased methods since they have access to more complete data. Nevertheless, we report the metrics to establish a baseline for monocular 3D object detection on SUN RGB-D.</p><p>Comparison with Total3DUnderstanding <ref type="bibr" target="#b26">[27]</ref> on all NYU-37 object categories is present in Tab. 11. In this experiment, we optimize L indoor + L extra since camera pose is assumed unknown.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visualization</head><p>All visualized images belong to validation subsets of the corresponding datasets. Different colors of the depicted bounding boxes mark different object categories; the color encoding is consistent within each dataset. 1.53 ---T3DU <ref type="bibr" target="#b26">[27]</ref> 2.27 ---ImVoxelNet 0.53 --- <ref type="table">Table 11</ref>. AP@0.15 scores for 37 object categories <ref type="bibr" target="#b26">[27]</ref> from the SUN RGB-D dataset.   b) Scene n008-2018-09-18-15-12-01-0400 15372981046. <ref type="figure">Figure 9</ref>. Cars detected in the images of two scenes from the validation subset of the nuScenes dataset. The predictions were obtained in multi-view settings. The first two rows correspond to the first scene, and the last two rows correspond to another one. For each scene, the upper row consists of images taken with a front-left, front, and front-right camera (from left to right). The second row contains images taken with a back-left, back, and back-right camera, respectively. <ref type="figure" target="#fig_0">Figure 10</ref>. Examples of the detection failures for images from the validation subset of the SUN RGB-D dataset. These examples depict typical error cases: small objects of sink, garbage bin and recycle bin categories are detected quite precisely, but rotation angles for large object such as cabinet are estimated poorly.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The general scheme of the proposed ImVoxelNet. Dashed lines around network blocks denote that network weights are shared across multiple inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visualization of object detection results for monocular images from validation subset of the KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of object detection results for multi-view inputs from validation subset (scene n008-2018-09-18-15-12-01-0400 15372981046) of the nuScenes dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of object detection results for monocular images from validation subset of the SUN RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visualization of object detection results for multi-view inputs from validation subset (scene 0086 00) of the ScanNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Objects detected on the monocular images from the validation subset of the SUN RGB-D dataset. 13 a) Scene 0169 00. b) Scene 0575 00.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Objects detected on the multi-view inputs from the validation subset of the ScanNet dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Cars detected on the monocular images from the validation subset of the KITTI dataset. . a) Scene n008-2018-08-01-15-16-36-0400 15331512526.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dataset x min xmax y min ymax z min zmax s KITTI -39.68 39.68 0 69.12 -2.92 0.92 0.32 nuScenes -49.92 49.92 -49.92 49.92 -2.92 0.92 SUN RGB-D -3.2 3.2 0 6.4 -2.28 0.28 0.16 ScanNet -3.2 3.2 -3.2 3.2 -1.28 1.28 Table 1. Implementation details. Axis limits and voxel size s are measured in meters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>.34 11.16 / 5.14 9.74 / 4.10 30.21 / 17.03 20.47 / 11.03 17.58 / 9.05 AM3D[24] 32.23 / 16.50 21.09 / 10.74 17.26 / 9.52 43.75 / 25.03 28.39 / 17.32 23.87 / 14.91 D4LCN[10] 26.97 / 16.65 21.71 / 11.72 18.22 / 9.51 34.82 / 22.51 25.83 / 16.02 23.53 / 12.55 OFTNet[33] 4.47 / 1.32 3.27 / 1.61 3.29 / 1.00 11.06 / 7.16 8.79 / 5.69 8.91 / 4.14.41 16.86 / 10.34 16.63 / 8.77 25.56 / 19.17 22.12 / 14.20 20.91 / 11.99 ImVoxelNet 24.54 / 17.15 17.80 / 10.97 15.67 / 9.15 31.67 / 25.19 23.68 / 16.37 19.73 / 13.58 Table 2. Scores for car category on the KITTI dataset. The depth column indicates whether this modality is used for training. Scores for car category on the nuScenes dataset. The RGB and PC columns indicate data modalities used for both training and inference.</figDesc><table><row><cell>Method</cell><cell>Depth</cell><cell>Easy</cell><cell cols="2">AP3D@0.7 (val/test) Moderate</cell><cell>Hard</cell><cell></cell><cell>Easy</cell><cell>APBEV@0.7 (val/test) Moderate</cell><cell>Hard</cell></row><row><cell>MonoFENet[1]</cell><cell></cell><cell cols="7">17.54 / 861</cell></row><row><cell>GS3D[20]</cell><cell></cell><cell cols="5">13.46 / 4.47 10.97 / 2.90 10.38 / 2.47</cell><cell cols="2">-/ 8.41</cell><cell>-/ 6.08</cell><cell>-/ 4.94</cell></row><row><cell cols="7">MonoGRNet[31] 13.88 / 9.61 10.19 / 5.74 7.62 / 4.25</cell><cell cols="2">-/ 18.19</cell><cell>-/ 11.17</cell><cell>-/ 8.73</cell></row><row><cell>MonoDIS[35]</cell><cell></cell><cell cols="7">18.05 / 10.37 14.98 / 7.94 13.42 / 6.40 24.26 / 17.23 18.43 / 13.19 16.95 / 11.12</cell></row><row><cell>SMOKE[23]</cell><cell></cell><cell cols="7">14.76 / 14.03 12.85 / 9.76 11.50 / 7.84 19.99 / 20.83 15.61 / 14.49 15.28 / 12.75</cell></row><row><cell>M3D-RPN[2]</cell><cell></cell><cell cols="7">20.27 / 14.76 17.06 / 9.71 15.21 / 7.42 25.94 / 21.02 21.18 / 13.67 17.90 / 10.23</cell></row><row><cell cols="3">RTM3D[22] 20.77 / Method RGB PC</cell><cell cols="6">AP?[%] 0.5m 1.0m 2.0m 4.0m mean ATE [m] ASE[1-IoU] AOE[rad] TP?</cell></row><row><cell cols="2">PointPillar[19]</cell><cell></cell><cell cols="4">55.5 71.8 76.1 78.6 70.5</cell><cell>0.27</cell><cell>0.17</cell><cell>0.19</cell></row><row><cell cols="2">OFTNet [33, 35]</cell><cell></cell><cell>-</cell><cell cols="2">-27.0 -</cell><cell>-</cell><cell>0.65</cell><cell>0.16</cell><cell>0.18</cell></row><row><cell cols="2">MonoDIS [35]</cell><cell></cell><cell cols="4">10.7 37.5 69.0 85.7 50.7</cell><cell>0.61</cell><cell>0.15</cell><cell>0.08</cell></row><row><cell cols="2">ImVoxelNet</cell><cell></cell><cell cols="4">19.3 44.8 66.3 77.0 51.8</cell><cell>0.52</cell><cell>0.15</cell><cell>0.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>] 58.29 13.56 28.37 12.12 4.79 13.71 8.80 2.18 0.48 2.41 14.47 54.9 7.60 3.12 CooP[14] 63.58 17.12 41.22 26.21 9.55 4.28 6.34 5.34 2.63 1.75 17.80 56.9 3.28 2.19 T3DU[27] 59.03 15.98 43.95 35.28 23.65 19.20 6.87 14.40 11.39 3.46 23.32 57.6 3.68 2.59 ImVoxelNet 79.17 63.07 60.59 51.14 31.20 35.45 38.38 45.12 19.24 13.27 43.66 59.3 2.63 1.96Table 4. AP@0.15 scores for 10 out of 37 object categories<ref type="bibr" target="#b26">[27]</ref> from the SUN RGB-D dataset, alongside room layout and camera pose estimation metrics. AP@0.25 scores for 18 object categories from the ScanNet dataset. All methods but ImVoxelNet accept point cloud (PC) as an input.</figDesc><table><row><cell>Con-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7 .</head><label>7</label><figDesc>AP3D@0.7 for car category, moderate difficulty and runtime measured in seconds per image, estimated for the validation subset of the KITTI dataset.</figDesc><table><row><cell>14.98</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 10 .</head><label>10</label><figDesc>Method RGB PC bath bed bkshf chair desk dresser nstand sofa table toilet mAP F-PointNet[28] 43.3 81.1 33.3 64.2 24.7 32.0 58.1 61.1 51.1 90.9 54.0 VoteNet[29] 74.4 83.0 28.8 75.3 22.0 29.8 62.2 64.0 47.3 90.1 57.7 H3DNet[41] 73.8 85.6 31.0 76.7 29.6 33.4 65.5 66.5 50.8 88.2 60.1 ImVoteNet[30] 75.9 87.6 41.3 76.7 28.7 41.4 69.9 70.7 51.1 90.5 63.4 ImVoxelNet 71.7 69.6 5.7 53.7 21.9 21.2 34.6 51.5 39.1 76.8 40.7 Table 8. AP@0.25 scores for 10 object categories [29] from the SUN RGB-D dataset. All methods but ImVoxelNet use point cloud (PC) as an input. 13.56 28.37 12.12 4.79 16.50 0.63 2.18 1.29 2.41 14.01 CooP[14] 63.58 17.12 41.22 26.21 9.55 58.55 10.19 5.34 3.01 1.75 23.65 PerspectiveNet[16] 79.69 40.42 62.35 44.12 20.19 81.22 22.42 41.35 8.29 13.14 39.09 ImVoxelNet 77.87 65.94 63.89 51.17 31.91 84.53 33.35 39.91 21.65 17.19 48.74Table 9. AP@0.15 scores for 10 out of 30 object categories<ref type="bibr" target="#b15">[16]</ref> from the SUN RGB-D dataset. AP@0.15 scores for 30 object categories<ref type="bibr" target="#b15">[16]</ref> from the SUN RGB-D dataset.</figDesc><table><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell cols="9">bed chair sofa table desk toilet bin sink shelf lamp mAP</cell></row><row><cell></cell><cell>3DGP[8]</cell><cell></cell><cell></cell><cell cols="5">5.62 2.31 3.24 1.23 -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">HoPR[15] 58.29 Method toilet recycle night</cell><cell>end</cell><cell></cell><cell cols="4">drawer computer key</cell><cell></cell><cell>table</cell><cell>chair monitor stool</cell></row><row><cell></cell><cell></cell><cell></cell><cell>bin</cell><cell>stand</cell><cell>table</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">board</cell></row><row><cell cols="3">PerspectiveNet[16] 81.22</cell><cell>37.68</cell><cell>35.16</cell><cell>19.77</cell><cell></cell><cell>1.28</cell><cell></cell><cell>1.24</cell><cell>2.86</cell><cell></cell><cell>44.12</cell><cell>40.42</cell><cell>1.14</cell><cell>22.65</cell></row><row><cell>ImVoxelNet</cell><cell cols="2">84.53</cell><cell>52.20</cell><cell>46.29</cell><cell>25.31</cell><cell></cell><cell>6.05</cell><cell></cell><cell>2.71</cell><cell>0.01</cell><cell></cell><cell>51.17</cell><cell>65.94</cell><cell>19.82</cell><cell>10.37</cell></row><row><cell>Method</cell><cell>lamp</cell><cell></cell><cell cols="4">dresser picture garbage</cell><cell>shelf</cell><cell></cell><cell>sofa</cell><cell cols="2">cabinet</cell><cell>sink</cell><cell>desk</cell><cell>book</cell><cell>coffee</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bin</cell><cell></cell><cell></cell><cell></cell><cell>chair</cell><cell></cell><cell></cell><cell>shelf</cell><cell>table</cell></row><row><cell cols="3">PerspectiveNet[16] 13.14</cell><cell>27.38</cell><cell>0.00</cell><cell>22.42</cell><cell></cell><cell>0.97</cell><cell></cell><cell>51.86</cell><cell>1.70</cell><cell></cell><cell>41.35</cell><cell>20.19</cell><cell>8.29</cell><cell>28.80</cell></row><row><cell>ImVoxelNet</cell><cell cols="2">17.19</cell><cell>22.32</cell><cell>0.82</cell><cell>33.35</cell><cell></cell><cell>4.00</cell><cell></cell><cell>54.61</cell><cell>7.90</cell><cell></cell><cell>39.91</cell><cell>31.91</cell><cell>21.65</cell><cell>36.48</cell></row><row><cell>Method</cell><cell>box</cell><cell></cell><cell>sofa</cell><cell>white</cell><cell>bed</cell><cell></cell><cell cols="2">pillow</cell><cell cols="3">paper painting</cell><cell>cpu</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>board</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PerspectiveNet[16] 1.64</cell><cell></cell><cell>62.35</cell><cell>0.02</cell><cell>79.69</cell><cell></cell><cell>11.36</cell><cell></cell><cell>0.00</cell><cell>0.17</cell><cell></cell><cell>21.60</cell></row><row><cell>ImVoxelNet</cell><cell>3.29</cell><cell></cell><cell>63.89</cell><cell>0.95</cell><cell>77.87</cell><cell></cell><cell>14.65</cell><cell></cell><cell>0.00</cell><cell>0.53</cell><cell></cell><cell>5.30</cell></row><row><cell>Method</cell><cell>cabinet</cell><cell></cell><cell>bed</cell><cell>chair</cell><cell>sofa</cell><cell cols="2">table</cell><cell></cell><cell cols="4">door window book</cell><cell>picture counter blinds</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">shelf</cell></row><row><cell>CooP[14]</cell><cell>10.47</cell><cell></cell><cell>57.71</cell><cell>15.21</cell><cell>36.67</cell><cell cols="2">31.16</cell><cell></cell><cell>0.14</cell><cell>0.00</cell><cell cols="2">3.81</cell><cell>0.00</cell><cell>27.67</cell><cell>2.27</cell></row><row><cell>T3DU[27]</cell><cell>11.39</cell><cell></cell><cell>59.03</cell><cell>15.98</cell><cell>43.95</cell><cell cols="2">35.28</cell><cell></cell><cell>0.36</cell><cell>0.16</cell><cell cols="2">5.26</cell><cell>0.24</cell><cell>33.51</cell><cell>0.00</cell></row><row><cell cols="2">ImVoxelNet 19.24</cell><cell></cell><cell>79.17</cell><cell>63.07</cell><cell>60.59</cell><cell cols="2">51.14</cell><cell></cell><cell>0.74</cell><cell>0.18</cell><cell cols="2">16.37</cell><cell>0.14</cell><cell>14.89</cell><cell>0.26</cell></row><row><cell>Method</cell><cell>desk</cell><cell cols="8">shelves curtain dresser pillow mirror</cell><cell>floor</cell><cell cols="2">clothes books</cell><cell>fridge</cell><cell>tv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>mat</cell><cell></cell></row><row><cell>CooP[14]</cell><cell>19.90</cell><cell></cell><cell>2.96</cell><cell>1.35</cell><cell>15.98</cell><cell></cell><cell>2.53</cell><cell></cell><cell>0.47</cell><cell>-</cell><cell cols="2">0.00</cell><cell>3.19</cell><cell>21.50</cell><cell>5.20</cell></row><row><cell>T3DU[27]</cell><cell>23.65</cell><cell></cell><cell>4.96</cell><cell>2.68</cell><cell>19.20</cell><cell></cell><cell>2.99</cell><cell></cell><cell>0.19</cell><cell>-</cell><cell cols="2">0.00</cell><cell>1.30</cell><cell>20.68</cell><cell>4.44</cell></row><row><cell cols="2">ImVoxelNet 31.20</cell><cell></cell><cell>5.47</cell><cell>3.34</cell><cell>35.45</cell><cell cols="2">11.01</cell><cell></cell><cell>0.22</cell><cell>-</cell><cell cols="2">1.40</cell><cell>0.13</cell><cell>23.28</cell><cell>12.41</cell></row><row><cell>Method</cell><cell>paper</cell><cell></cell><cell cols="2">towel shower</cell><cell>box</cell><cell cols="2">white</cell><cell cols="2">person</cell><cell>night</cell><cell cols="2">toilet</cell><cell>sink</cell><cell>lamp bathtub</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">curtain</cell><cell></cell><cell cols="2">board</cell><cell></cell><cell></cell><cell>stand</cell><cell></cell></row><row><cell>CooP[14]</cell><cell>0.20</cell><cell></cell><cell>2.14</cell><cell>20.00</cell><cell>2.59</cell><cell></cell><cell>0.16</cell><cell cols="2">20.96</cell><cell>11.36</cell><cell cols="2">42.53</cell><cell>15.95</cell><cell>3.28</cell><cell>24.71</cell></row><row><cell>T3DU[27]</cell><cell>0.41</cell><cell></cell><cell>2.20</cell><cell>20.00</cell><cell>2.25</cell><cell></cell><cell>0.43</cell><cell cols="2">23.36</cell><cell>6.87</cell><cell cols="2">48.37</cell><cell>14.40</cell><cell>3.46</cell><cell>27.85</cell></row><row><cell cols="2">ImVoxelNet 0.00</cell><cell></cell><cell>1.92</cell><cell>0.00</cell><cell>2.71</cell><cell></cell><cell>1.17</cell><cell cols="2">42.02</cell><cell>38.38</cell><cell cols="2">77.28</cell><cell>45.12</cell><cell>13.27</cell><cell>43.59</cell></row><row><cell>Method</cell><cell>bag</cell><cell></cell><cell>wall</cell><cell>floor</cell><cell>ceiling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CooP[14]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monofenet: Monocular 3d object detection with feature enhancement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2753" to="2765" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">M3d-rpn: Monocular 3d region proposal network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brazil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9287" to="9296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep manta: A coarse-to-fine many-task network for joint 2d and 3d vehicle analysis from monocular image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chaouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rabarisoa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Teuliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chateau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2040" to="2049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding indoor scenes using 3d geometric phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="33" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5828" to="5839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning depth-guided convolutions for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1000" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d-sis: 3d semantic instance segmentation of rgb-d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4421" to="4430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cooperative holistic scene understanding: Unifying 3d object, layout, and camera pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.13049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holistic 3d scene parsing and reconstruction from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="187" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perspectivenet: 3d object detection from a single rgb image via perspective points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-view pointnet for 3d scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d-rcnn: Instance-level 3d object reconstruction via render-and-compare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3559" to="3568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gs3d: An efficient 3d object detection framework for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1019" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Rtm3d: Real-time monocular 3d detection from object keypoints for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.03343</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smoke: single-stage monocular 3d object detection via keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>T?th</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="996" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6851" to="6860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d bounding box estimation using deep learning and geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kosecka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7074" to="7082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Atlas: End-to-end 3d scene reconstruction from posed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Van As</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10432</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Total3dunderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="55" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep hough voting for 3d object detection in point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9277" to="9286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imvotenet: Boosting 3d object detection in point clouds with image votes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Litany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4404" to="4413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Monogrnet: A geometric reasoning network for monocular 3d object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8851" to="8858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Triangulation learning network: from monocular to stereo 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7615" to="7623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Orthographic feature transform for monocular 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Roddick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08188</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Disentangling monocular 3d object detection: From single to multi-class recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Antequera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mvx-net: Multimodal voxelnet for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7276" to="7282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgbd scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9759" to="9768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">H3dnet: 3d object detection using hybrid geometric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="311" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Iou loss for 2d/3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="85" to="94" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
