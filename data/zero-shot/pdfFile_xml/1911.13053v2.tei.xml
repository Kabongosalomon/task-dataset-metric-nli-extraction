<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Blockwisely Supervised Neural Architecture Search with Knowledge Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuchun</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.cn</email>
							<affiliation key="aff0">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
							<email>xiaojun.chang@monash.edu</email>
							<affiliation key="aff0">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Monash University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Blockwisely Supervised Neural Architecture Search with Knowledge Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural Architecture Search (NAS), aiming at automatically designing network architectures by machines, is hoped and expected to bring about a new revolution in machine learning. Despite these high expectation, the effectiveness and efficiency of existing NAS solutions are unclear, with some recent works going so far as to suggest that many existing NAS solutions are no better than random architecture selection. The inefficiency of NAS solutions may be attributed to inaccurate architecture evaluation. Specifically, to speed up NAS, recent works have proposed undertraining different candidate architectures in a large search space concurrently by using shared network parameters; however, this has resulted in incorrect architecture ratings and furthered the ineffectiveness of NAS. * Changlin Li and Jiefeng Peng contribute equally and share firstauthorship. This work was done when Changlin Li worked as an intern. ? Corresponding Author is Guangrun Wang. V1 V2 V4 IT Teacher Architecture Student Archtecture Candidates Block 4 Block 3 Block 2 Block 1 Figure 1: We consider a network architecture has several blocks, conceptualized as analogous to the ventral visual blocks V1, V2, V4, and IT [25]</p><p>. Then, we search for the candidate architectures (denoted by different shapes and paths) block-wisely guided by the architecture knowledge distilled from a teacher model.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we propose to modularize the large search space of NAS into blocks to ensure that the potential candidate architectures are fully trained; this reduces the representation shift caused by the shared parameters and leads to the correct rating of the candidates. Thanks to the blockwise search, we can also evaluate all of the candidate architectures within a block. Moreover, we find that the knowledge of a network model lies not only in the network parameters but also in the network architecture. Therefore, we propose to distill the neural architecture (DNA) knowledge from a teacher model as the supervision to guide our block-wise architecture search, which significantly improves the effectiveness of NAS. Remarkably, the capacity of our searched architecture has exceeded the teacher model, demonstrating the practicability and scalability of our method. Finally, our method achieves a state-of-theart 78.4% top-1 accuracy on ImageNet in a mobile setting, which is about a 2.1% gain over EfficientNet-B0. All of our searched models along with the evaluation code are available at https://github.com/changlin31/DNA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Due to the importance of automatically designing machine learning algorithms using machines, interest in the prospect of Automated Machine Learning (AutoML) has been a growing recently. Neural architecture search (NAS), as an essential task of AutoML, is hoped and expected to reduce the effort required to be expended by human experts in network architecture design. Research into NAS has been accelerated in the past two years by the industry, and a number of solutions have been proposed. However, the effectiveness and efficiency of existing NAS solutions are unclear. Typically, <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b32">[33]</ref> even suggest that many existing solutions to NAS are no better than or struggle to outperform random architecture selection. Hence, the question of how to efficiently solve a NAS problem remains an active and unsolved research topic.</p><p>The most mathematically accurate solution to NAS is to train each of the candidate architectures within the search space from scratch to convergence and compare their performance; however, this is impractical due to the astonishingly high cost. A suboptimal solution is to train only the architectures in a search sub-space using advanced search strategies like Reinforcement Learning (RL) or Evolutionary Algorithms (EA); although this is still time-consuming, as training even one architecture costs a long time (e.g., more than 10 GPU days for a ResNet on ImageNet). To speed up NAS, recent works have proposed that rather than training each of the candidates fully from scratch to convergence, different candidates should be trained concurrently by using shared network parameters. Subsequently, the ratings of different candidate architectures can be determined by evaluating their performance based on these undertrained shared network parameters. However, several questions remain: does the evaluation based on the undertrained network parameters correctly rank the candidate models? Can the architecture that achieves the highest accuracy defend its top ranking when trained from scratch to convergence? <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b16">[17]</ref> have suggested that when the search space is small and all the candidates fully and fairly trained, the answer to the above questions is guaranteed to be "yes". Unfortunately, it is not recommended to narrow down the search space, as a small search space will lead to a very narrow accuracy range, making the search meaningless.</p><p>To address the above-mentioned issues, we propose a new solution to NAS where the search space is large, while the potential candidate architectures can be fully and fairly trained. We consider a network architecture that has several blocks, conceptualized as analogous to the ventral visual blocks V1, V2, V4, and IT [25] (see <ref type="figure" target="#fig_0">Fig. 1</ref>). We then train each block of the candidate architectures separately. As guaranteed by the mathematical principle, the number of candidate architectures in a block reduces exponentially compared to the the number of candidates in the whole search space. Hence, the architecture candidates can be fully and fairly trained, while the representation shift caused by the shared parameters is reduced, leading to the correct candidate ratings. The correct and visiting-all evaluation improves the effectiveness of NAS. Moreover, thanks to the modest amount of the candidates in a block, we can even search for the depth of a block, which further improves the performance of NAS.</p><p>Moreover, lack of supervision for the hidden block creates a technical barrier in our greedy block-wise search of network architecture. To deal with this problem, we propose a novel knowledge distillation method, called DNA, that distills the neural architecture from an existing architecture. As <ref type="figure" target="#fig_0">Fig. 1</ref> shows, we find that different blocks of an existing architecture have different knowledge in extracting different patterns of an image. For example, the lowest block acts like the V1 of the ventral visual area, which extracts low-level features of an image, while the upper block acts like the IT area, which extracts high-level features. We also find that the knowledge not only lies, as the literature suggests, in the network parameters, but also in the network architecture. Hence, we use the block-wise representation of existing models to supervise our architecture search. Note that the capacity of our searched archi-tectures is not bounded by the capacity of the supervising model. We have searched a number of architectures that have fewer parameters but significantly outperforms the supervising model, demonstrating the practicability and scalability of our DNA method. Furthermore, inspired by the remarkable success of the transformers (e.g., BERT <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b28">[29]</ref>) in natural language domain that discard the inefficient sequential training of RNN, we propose to parallelize the block-wise search in an analogous way. Specifically, for each block, we use the output of the previous block of the supervising model as the input for each of our blocks. Thus, the search can be sped up in a parallel way.</p><p>Overall, our contributions are three-fold:</p><p>? We propose to modularize the large search space of NAS into blocks, ensuring that the potential candidate architectures are fairly trained, and the representation shift caused by the shared parameters is reduced, which leads to correct ratings of the candidates. The correct and visiting-all ratings improve the effectiveness of NAS. Novelly, we also search for the depth of the architecture with the help of our block-wise search.</p><p>? We find that the knowledge of a network model lies not only, as the literature suggests, in the network parameters, but also in the network architecture. Therefore, we use the architecture knowledge distilled from a teacher model to guide our block-wise architecture search. Remarkably, the capacity of our searched architecture has exceeded the teacher model, proving the practicability and scalability of our proposed DNA. ? Strong empirical results are obtained on ImageNet and CIFAR10. Typically, our DNA with 6.4M parameters obtains a 78.4% top-1 accuracy on ImageNet, which is about 2.1% higher than the result obtained by EfficientNet-B0 with a similar parameter number. To the best of our knowledge, this is the state-of-the-art model in a mobile setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Neural Architecture Search (NAS). NAS is hoped to replace the effort of human experts in network architecture design by machines. Early works <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b19">20]</ref> adopt an agent (e.g., an RNN or an EA method) to sample an architecture and get its performance through a complete training procedure. This type of NAS is computationally expensive and difficult to deploy on large-datasets. More recent studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5]</ref> encode the entire search space as a weight sharing supernet. Gradient-based approches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31]</ref> jointly optimize the weight of the supernet and the architecture choosing factors by gradient descent. However, optimizing these choosing factors brings inevitable bias between sub-models. Since the sub-model performing poor in the beginning will get trained less and easily stay behind others, these methods depend heavily on the initial states, making it difficult to reach the best architecture. One-shot approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b3">4]</ref> ensure fairness among all sub-models. After training the supernet via path dropout or path sampling, sub-models are sampled and evaluated with the weights inherited from the supernet. However, as identified in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17]</ref>, there is a gap between the accuracy of the proxy sub-model with shared weights and the retrained stand-alone one. This gap narrows as the amount of weight sharing sub-models decrease <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>Knowledge Distillation. Knowledge distillation is a classical method of model compression, which aims at transferring knowledge from a trained teacher network to a smaller and faster student model. Existing works on knowledge distillation can be roughly classified into two categories. The first category is to use soft-labels generated by the teacher to teach the student, which is first proposed by <ref type="bibr" target="#b1">[2]</ref>. Later, Hinton et al. <ref type="bibr" target="#b13">[14]</ref> redefined knowledge distillation as training a shallower network to approach the teacher's output after the softmax layer. However, when the teacher model gets deeper, learning the soft-labels alone is insufficient. To address this problem, the second category of knowledge distillation proposes to employ the internal representation of the teacher to guide the training of the student <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="bibr" target="#b33">[34]</ref> proposed a distillation method to train a student network to mimic the teacher's behavior in multiple hidden layers jointly. <ref type="bibr" target="#b29">[30]</ref> proposed a progressive block-wise distillation to learn from several of the teacher's intermediate feature maps, which eases the difficulty of joint optimization but increases the gap between the student and the teacher model during the progressive distillation. All existing works assume that the knowledge of a network model lies in the network parameter, while we find that the knowledge also lies in the network architecture. Moreover, in contrast to <ref type="bibr" target="#b29">[30]</ref> , we proposed a parallelized distillation procedure to reduce both the gap and the time consumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>We begin with the inaccurate evaluation problem of existing NAS, based on which we define our block-wise search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Challenge of NAS and our Block-wise Search</head><p>Let ? ? A and ? ? denote the network architecture and the network parameters, respectively, where A is the architecture search space. A NAS problem is to find the optimal pair (? * , ? * ? ) such that the model performance is maximized. Solving a NAS problem often consists of two iterative steps, i.e., search and evaluation. A search step is to select an appropriate architecture for evaluation, while an evaluation step is to rate the architecture selected by the search step. The evaluation step is of most importance in the solution to NAS because an inaccurate evaluation leads to the ineffectiveness of NAS, and a slow evaluation results in the inefficiency of NAS. Inaccurate Evaluation in NAS. The most mathematically accurate evaluation for a candidate architecture is to train it from scratch to convergence and test its performance, which, however, is impractical due to the awesome cost. For example, it may cost more than 10 GPU days to train a ResNet on ImageNet. To speed up the evaluation, recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17]</ref> propose not to train each of the candidates fully from scratch to convergence, but to train different candidates concurrently by using shared network parameters. Specifically, they formulate the search space A into an over-parameterized supernet such that each of the candidate architecture ? is a sub-net of the supernet. Let W denote the network parameters of the supernet. The learning of the supernet is as follows:</p><formula xml:id="formula_0">W * = min W Ltrain(W, A; X, Y),<label>(1)</label></formula><p>where X and Y denote the input data and the ground truth labels, respectively. Here, L train denotes the training loss. Then, the ratings of different candidate architectures are determined by evaluating their performance based on these shared network parameters, W * . However, as analyzed in Section 1, the optimal network parameter W * does not necessarily indicate the optimal network parameters ? * for the sub-nets (i.e., the candidate architectures) because the sub-nets are not fairly and fully trained. The evaluation based on W * does not correctly rank the candidate models because the search space is usually large (e.g., &gt; 1e 15 ). The inaccurate evaluation has led to the ineffectiveness of the existing NAS. Block-wise NAS. <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b16">[17]</ref> have suggested that when the search space is small, and all the candidates are fully and fairly trained, the evaluation could be accurate. To improve the accuracy of the evaluation, we divide the supernet into blocks of smaller sub-space. Specifically, Let N denote the supernet. We divide N into N blocks by the depth of the supernet and have:</p><formula xml:id="formula_1">N = NN . . . Ni+1 ? Ni ? ? ? ? N1,<label>(2)</label></formula><p>where N i+1 ? N i denotes that the (i + 1)-th block is originally connected to the i-th block in the supernet. Then we learn each block of the supernet separately using:</p><formula xml:id="formula_2">W * i = min Wi L train (W i , A i ; X, Y), i = 1, 2 ? ? ? , N,<label>(3)</label></formula><p>where A i denote the search space in the i-th block. Are the candidate architectures in each block fully trained? How large is the search space in a block? Let d denote the depth of the i-th block and C denote the number of the candidate operations in each layer. Then the size of the search space of the i-th block is  <ref type="figure">Figure 2</ref>: Illustration of our DNA. The teacher's previous feature map is used as input for both teacher and student block. Each cell of the supernet is trained independently to mimic the behavior of the corresponding teacher block by minimizing the l2-distance between their output feature maps. The dotted lines indicate randomly sampled paths in a cell. exponential drop in the size of the search space:</p><formula xml:id="formula_3">C di , ?i ? [1, N ]; the size of the search space A is N i=0 C d i .</formula><formula xml:id="formula_4">Drop rate = C di /( N i=0 C di ).<label>(4)</label></formula><p>In our experiment, the search space in a block reduces significantly (e.g., Drop rate ? 1/(1e 15 N )), ensuring each candidate architecture ? i ? A i to be optimized sufficiently. Finally, the architecture is searched across the different blocks in the whole search space A:</p><formula xml:id="formula_5">? * = arg min ??A N i=1 ?iLval(W * i (?i), ?i; X, Y),<label>(5)</label></formula><p>where ? i represents the loss weights. Here, W * i (? i ) denotes the learned shared network parameters of the sub-net ? i and the supernet. Note that different from the learning of the supernet, we use the validation set to evaluate the performance of the candidate architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Block-wise Supervision with Distilled Architecture Knowledge</head><p>Although we motivate well in Section 3.1, a technical barrier in our block-wise NAS is that we lack of internal ground truth in Eqn. <ref type="bibr" target="#b2">(3)</ref>. Fortunately, we find that different blocks of an existing architecture have different knowledge 1 in extracting different patterns of an image. We also find that the knowledge not only lies, as the literature suggests, in the network parameters, but also in the network <ref type="bibr" target="#b0">1</ref> The definition of knowledge is a matter of ongoing debate among philosophers. In this work, we specially define KNOWLEDGE as follows. Knowledge is the skill to recognize some patterns; Parameter Knowledge is the skill of using appropriate network parameter to recognize some patterns. Architecture Knowledge is the skill of using appropriate network structrue to recognize some patterns.</p><p>architecture. Hence, we use the block-wise representation of existing models to supervise our architecture search. Let Y i be the output feature maps of the i-th block of the supervising model (i.e., teacher model) and? i (X ) be the output feature maps of the i-th block of the supernet. We take L2 norm as the cost function. The loss function in Eqn. (3) can be written as:</p><formula xml:id="formula_6">Ltrain(Wi, Ai; X, Yi) = 1 K Yi ??i(X ) 2 2 ,<label>(6)</label></formula><p>where K denotes the numbers of the neurons in Y. Moreover, inspired by the remarkable success of the transformers (e.g., BERT <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b28">[29]</ref>) in natural language domain that discards the inefficient sequential training of RNN, we propose to parallelize the block-wise search in an analogous way. Specifically, for each block, we use the output Y i?1 of the (i ? 1)-th block of the teacher model as the input of the i-th block of the supernet. Thus, the search can be sped up in a parallel way. Eqn. (6) can be written as:</p><formula xml:id="formula_7">Ltrain(Wi, Ai; Yi?1, Yi) = 1 K Yi ??i(X ) 2 2 ,<label>(7)</label></formula><p>Note that the capacity of our searched architectures is not bounded by the capacity of the supervising model, e.g., we have searched a number of architectures that have fewer parameters but significantly beats the supervising model. By scaling our architecture to the same model size as the supervising architecture, a more remarkable gain is further obtained, demonstrating the practicability and scalability of our DNA. <ref type="figure">Fig.2</ref> shows a pipeline of our block-wise supervision with knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Automatic Computation Allocation with Channel and Layer Variability</head><p>Automatically allocating model complexity of each block is especially vital when performing block-wise NAS under a certain constraint. To better imitate the teacher, the model complexity of each block may need to be allocated according to the learning difficulty of the corresponding teacher block adaptively. With the input image size and the stride of each block fixed, generally, the computation allocation is only related to the width and depth of each block, which are burdensome to search in a weight sharing supernet. Both the width and depth are usually pre-defined when designing the supernet for a one-shot NAS method. Most previous works include identity as a candidate operation to increase supernet scalability <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b16">17]</ref>. However, as pointed out in <ref type="bibr" target="#b7">[8]</ref>, adding identity as a candidate operation can lead to convergence difficulty of the supernet, as well as an unfair comparison of sub-models. In addition, adding identity as a candidate operation may lead to a detrimental and unnecessary increase in the possible sequence of operations. For example, a sequence of operation {conv, identity, conv} is equivalent to {conv, conv, identity}. This unnecessary increase of search space results in a drop of the supernet stability and fairness. Besides, <ref type="bibr" target="#b17">[18]</ref> searches for the layer number with fixed operations for the first step, and subsequently searched for three operations with a fixed layer number. However, the choice of operations is not independent from the layer number of each block. To search for more candidate operations by this two-step method could lead to a bigger gap from the real target.</p><p>Thanks to our block-wise search, we can train several cells with different channel numbers or layer numbers independently in each stage to ensure channel and layer variability without the interference of identity operation, As shown in <ref type="figure">Figure 2</ref>, in each training step, the teacher's previous feature map is first fed to several cells (as suggested by the solid line), and one of the candidate operations of each layer in the cell is randomly chosen to form a path (as suggested by the dotted line). The weight of the supernet is optimized by minimizing the MSE loss with the teacher's feature map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Searching for Best Student Under Constraint</head><p>Our typical supernet contains about 10 17 sub-models, which stops us from evaluating all of them. In previous one-shot NAS methods, random sampling, evolutionary algorithms and reinforcement learning have been used to sample sub-models from the trained supernet for further evaluation. In most recent work <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>, a greedy search algorithm is used to progressively shrink the search space by selecting the top-performing partial models layer by layer. Considering our block-wise distillation, we propose a novel method to estimate the performance of all sub-models according to their block-wise performance and subtly traverse all the sub-models to select the top-performing ones under certain constraints.</p><p>Evaluation. In our method, we aim to imitate the behavior of the teacher in every block. Thus, we estimate the  learning ability of a student sub-model by its evaluation loss in each block. Our block-wise search make it possible to evaluate all the partial models (about 10 4 in each cell). To accelerate this process, we forward-propagate a batch of input node by node in a manner similar to deep first search, with intermediate output of each node saved and reused by subsequent nodes to avoid recalculating it from the beginning. The feature sharing evaluation algorithm is outlined in Algorithm 1. By evaluating all cells in a block of the supernet, we can get the evaluation loss of all possible paths in one block. We can easily sort this list with about 10 4 elements in a few seconds with a single CPU. After this, we can select the top-1 partial model from every block to assemble a best student. However, we still need to find efficient models under different constraints to meet the needs of real-life applications.</p><p>Searching. After performing evaluation and sorting, the partial model rankings of each stage are used to find the best model under a certain constraint. To automatically al-  <ref type="table" target="#tab_1">1  7  48  2  24  3  24  2  32  2  7  80  2  40  3  40  4  40  3  10 160 2  80  3  80  4  80  4  10 224 3  112 4  112 4  96  5  13 384 4  192 5  192 5  160  6</ref> 4 640 1 320 ---locate computational costs to each block, we need to make sure that the evaluation criteria are fair for each block. We notice that MSE loss is related to the size of the feature map and the variance of the teacher's feature map. To avoid any possible impact of this, a fair evaluation criterion, called relative l1 loss, is defined as:</p><formula xml:id="formula_8">L R (x, y) = ||x ? y|| 1 ?(y) ,<label>(8)</label></formula><p>where ?(?) means standard deviation among all elements. All the L R in each block of a sub-model is added up to estimate the ability to learn from the teacher. However, it is unnecessarily time-consuming to calculate the complexity and add up the loss for all 10 17 candidate models. With ranked partial models in each block, a time-saving search algorithm (Alg. 2) is proposed to visit all possible models subtly. Note that we get the complexity of each candidate operation by a precalculated lookup table to save the time.</p><p>The testing of next block is skipped if current partial model combining with the smallest partial model in the following blocks already exceed the constraint. Moreover, it returns to the previous block after finding a model satisfying the constraint, to prevent testing of subsequent models with lower rank in current block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setups</head><p>Choice of dataset and teacher model. We evaluated our method on ImageNet, a large-scale classification dataset that has been used to evaluate various NAS methods. We randomly select 50 images from each class of the original training set to form a 50000-image validation set for search procedure and use the remainder as training set. Note that all of our results are tested on the original validation set. We select EfficientNet-B7 <ref type="bibr" target="#b26">[27]</ref> as our teacher model to guide our supernet training due to its state-of-the-art performance and relatively low computational cost comparing to ResNeXt-101 <ref type="bibr" target="#b31">[32]</ref> and other manually designed models. We part the teacher model into 6 blocks by number of filters. The details of these blocks are presented in <ref type="table" target="#tab_2">Table 1</ref>. Search space and supernet design. We perform our search in two operation search spaces, both of which consist of variants of MobileNet V2's <ref type="bibr" target="#b22">[23]</ref> Inverted Residual Block with Squeeze and Excitation <ref type="bibr" target="#b15">[16]</ref>. We keep our first search space similar with most of the recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref> to facilitate fair comparison in Section 4.2. We search among convolution kernel sizes of {3, 5, 7} and expansion rates {3, 6}, six operations in total. For fast evaluation in Section 4.3 and 4.4, a smaller search space with four operations (kernel sizes of {3, 5} and expansion rates {3, 6}) is used. Upon operation search space, we further build a higher level search space to search for channel and layer numbers, as introduced in Section 3.3. We search among three cells in each of the first 5 blocks and one in the last block. The layer and channel numbers of each cell is shown in <ref type="table" target="#tab_2">Table 1</ref>. The whole search space contains 2 ? 10 17 models. Training details We separately train each cell in the supernet for 20 epochs under the guidance of teacher's feature map in corresponding block. We use 0.002 as start learning rate for the first block and 0.005 for all the other blocks. We use Adam as our optimizer and reduce the learning rate by 0.9 every epoch.</p><p>It takes 1 day to train a simple supernet (6 cells) using 8 NVIDIA GTX 2080Ti GPUs and 3 days for our extended supernet <ref type="bibr">(16 cells)</ref>. With the help of Algorithm 1, our evaluation cost is about 0.6 GPU days. To search for the best model under certain constraint, we perform Algorithm 2 on CPUs and the cost is less than one hour.</p><p>As for ImageNet retraining of searched models, we used the similar setting with <ref type="bibr" target="#b26">[27]</ref>: batchsize 4096, RMSprop optimizer with momentum 0.9 and initial learning rate of 0.256 which decays by 0.97 every 2.4 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Performance of searched models</head><p>As shown in <ref type="table" target="#tab_1">Table 2</ref> fair comparison with EfficientNet-B0, DNA-b and DNA-c are obtained with target FLOPs of 399M and parameters of 5.3M respectively. Both of them outperform B0 by a large margin (1.2% and 1.5%). In particular, our DNA-d achieves 78.4% top-1 accuracy with 6.4M parameters and 611M FLOPs. When tested with the same input size (240 ? 240) as EfficientNet-B1, DNA-d achieves 78.8% top-1 accuracy, being evenly accurate but 1.4M smaller than B1. The accuracy of MixNet-M, who uses the more efficient MixConv operation that we don't use, is 0.4% inferior to our smaller DNA-b. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the curve of Model size vs. Accuracy and FLOPs vs. Accuracy for most recent NAS models. Our DNA models significantly mark a new state-of-theart with much smaller model size and lower computation complexity.</p><p>To test the transfer ability of our model, We evaluate our model on two widely used transfer learning datasets, CIFAR-10 and CIFAR-100. Our models maintain superiority after the transfer. The result is shown in <ref type="table" target="#tab_4">Table 3</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effectiveness</head><p>Model ranking. To evaluate the effectiveness of our NAS method, we compared the model ranking abilities between our method and SPOS (Single Path One-shot <ref type="bibr" target="#b12">[13]</ref>) by visualizing the relationship between the evaluation metrics on proxy one-shot models and the actual accuracy of the standalone models. The two supernets are both 18 layers, with 4 candidate operations in each layer. The search space is described in Section 4.1. We trained our supernet with 20 epochs for each block, adding up to 120 epochs in total. The supernet of Single Path One-shot is also trained for 120 epochs as they proposed <ref type="bibr" target="#b12">[13]</ref>.</p><p>We sample 16 models from the search space and train them from scratch. As for model ranking test, we evaluate these sampled models in both supernets to get their predictive performance. The comparison of these two methods on model ranking is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Each of the sampled model has two corresponding points in the figure, representing the correlation between its predicted and true performance by two methods. <ref type="figure" target="#fig_2">Figure 4</ref> indicates that SPOS barely rank the candidate models correctly because the subnets are not fairly and fully trained as analyzed in Section 3.1. While in our block-wise supernet, the predicted performance is highly correlated with the real accuracy of sampled models, which proves the effectiveness of our method.</p><p>Training progress. To analyse our supernet training process, we pick the intermediate models searched in every two training epochs (approximate to 5000 iterations) and retrain  them to convergence. As shown in <ref type="figure" target="#fig_4">Figure 6</ref>, the accuracy of our searched models increase progressively as the training goes on until it converges between 16-th and 20-th epoch. It illustrates that the predictive metric of candidate models becomes more precise as the supernet converge. Note that the accuracy increase rapidly in the early stage with the same tendency of training loss decreasing, which evidences a correlation between accuracy of searched model and loss of supernet.</p><p>Part of the teacher and student feature map of block 2 and 4 at epoch 16 is shown in <ref type="figure" target="#fig_3">Figure 5</ref>. As we can see, our student supernet can imitate the the teacher extraordinarily well. The textures are extremely close at every channel, even on highly abstracted 14 ? 14 feature maps. Which proves the effectiveness of our distillation training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Distillation strategy. We tested two progressive blockwise distillation strategy and compare their effectiveness with ours by experiments. All the three strategy is performed block by block by minimizing the MSE loss between feature maps of student supernet and the teacher. In strategy S1, the student is trained from scratch with all previous blocks in every stage. In strategy S2, the trained student  parameters of the previous blocks is kept and freezed, thus those parameters are only used to generate the input feature map of current block. As discussed in Section 3.2, our strategy directly takes the teacher's previous feature map as input of the current block. The experimental results shown in <ref type="table" target="#tab_5">Table 4</ref> prove the superiority of our strategy.</p><p>Impact of multi-cell design. To test the impact of multicell search, we preform DNA with single cell in each block for comparison. As shown in <ref type="table" target="#tab_5">Table 4</ref>, multi-cell search improves the top-1 accuracy of searched models by 0.2% under the same constraint (5.3M) and 0.3% for the best model in the search space without any constrain. Note that the single cell case of our method searched a model with lower parameters under the same constrain, this can be ascribed to the relatively lower variability of channel and layer numbers.</p><p>Analysis of teacher-dependency. To test the dependency of DNA on the performance of teacher model, EfficientNet-B0 is used as the teacher model to search for a student in the similar size. The results is shown in <ref type="table" target="#tab_6">Table 5</ref>. Surprisingly, performance of the model searched with EfficientNet-B0 is almost the same with the one searched with EfficientNet-B7, which means that the performance of our DNA method does not necessarily rely on high-performing teacher. Furthermore, the DNA-B0 outperforms its teacher by 1.5% with the same model size, which proves that the performance of our architecture distillation is not restricted by the performance of the teacher. Thus, we can improve the structure of any model by self-distillation architecture search. Thirdly, DNA-B7 achieves same top-1 accuracy with its 12.5? heavier teacher; by scaling our DNA-B7 to the similar model size as the supervising architecture, a more remarkable gain is further obtained. The scaled student outperforms its heavy teacher by 2.1%, demonstrating the practicability and scalability of our DNA method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, DNA, a novel architecture search method with block-wise supervision is proposed. We modularized the large search space into blocks to increase the effectiveness of one-shot NAS. We further designed a novel distillation approach to supervise the architecture search in a block-wise fashion. We then presented our multi-cell supernet design along with efficient evaluation and searching algorithms. We demonstrate that our searched architecture can surpass the teacher model and can achieve state-of-theart accuracy on both ImageNet and two commonly used transfer learning datasets when trained from scratch without the helps of the teacher. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>Feature sharing evaluation Input: Teacher's previous feature map Gprev, Teacher's current feature map Gcurr, Root of the cell Cell, loss function loss Output: List of evaluation loss L define DFS-Forward(N , X): Y = N (X); if N has no child then append(L, loss(Y, Gcurr)); else for C in N.child do DFS-Forward(C, Y ); end end DFS-Forward(Cell, Gprev); output L;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>, our DNA models achieve the stateof-the-art results compared with the most recent NAS models. Searched under a FLOPs constraint of 350M, DNA-a surpasses SCARLET-A with 1.8M fewer parameters. For a Trade-off of parameters-accuracy and FLOPsaccuracy on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Comparison of ranking effectiveness for DNA and Single Path One-Shot<ref type="bibr" target="#b12">[13]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Feature map comparison between teacher (top) and student (bottom) of two blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>ImageNet accuracy of searched models and training loss of the supernet in training progress.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Architectures of DNA-a,b,c,d. 'MB x y ? y' stands for an Inverted bottleneck convolution module with expand rate x and kernel size y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 2 :</head><label>2</label><figDesc>Traversal search</figDesc><table><row><cell>Input: Block index B, the teacher's current feature map G, constrain</cell></row><row><cell>C, model pool list P ool</cell></row><row><cell>Output: best model M</cell></row><row><cell>define SearchBlock(B, sizeprev, lossprev):</cell></row><row><cell>for i &lt; length(P ool[B]) do</cell></row><row><cell>size ? sizeprev + size[i];</cell></row><row><cell>if size &gt; C then</cell></row><row><cell>continue;</cell></row><row><cell>end</cell></row><row><cell>loss ? lossprev + loss[i];</cell></row><row><cell>if B is last block then</cell></row><row><cell>if loss ? loss best then</cell></row><row><cell>loss best ? loss;</cell></row><row><cell>M ? index of each block</cell></row><row><cell>end</cell></row><row><cell>break;</cell></row><row><cell>else</cell></row><row><cell>SearchBlock(B + 1, size, loss);</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>SearchBlock(0);</cell></row><row><cell>output M ;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Our supernet design. "l#" and "ch#" means layer and channel number of each cell. model teacher student supernet block l# ch# l# ch# l# ch# l# ch#</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison of state-of-the-art NAS models on Im-ageNet. The input size is 224 ? 224.</figDesc><table><row><cell>model</cell><cell cols="3">Params FLOPs Acc@1 Acc@5</cell></row><row><cell>SPOS [13]</cell><cell>-</cell><cell cols="2">319M 74.3% -</cell></row><row><cell>ProxylessNAS [6]</cell><cell>7.1M</cell><cell cols="2">465M 75.1% 92.5%</cell></row><row><cell>FBNet-C [31]</cell><cell>-</cell><cell cols="2">375M 74.9% -</cell></row><row><cell>MobileNetV3 [15]</cell><cell>5.3M</cell><cell cols="2">219M 75.2% -</cell></row><row><cell>MnasNet-A3 [26]</cell><cell>5.2M</cell><cell cols="2">403M 76.7% 93.3%</cell></row><row><cell>FairNAS-A [10]</cell><cell>4.6M</cell><cell cols="2">388M 75.3% 92.4%</cell></row><row><cell>MoGA-A [9]</cell><cell>5.1M</cell><cell cols="2">304M 75.9% 92.8%</cell></row><row><cell>SCARLET-A [8]</cell><cell>6.7M</cell><cell cols="2">365M 76.9% 93.4%</cell></row><row><cell>PC-NAS-S [17]</cell><cell>5.1M</cell><cell>-</cell><cell>76.8% -</cell></row><row><cell>MixNet-M [28]</cell><cell>5.0M</cell><cell cols="2">360M 77.0% 93.3%</cell></row><row><cell cols="2">EfficientNet-B0 [27] 5.3M</cell><cell cols="2">399M 76.3% 93.2%</cell></row><row><cell>DNA-a (ours)</cell><cell>4.2M</cell><cell cols="2">348M 77.1% 93.3%</cell></row><row><cell>DNA-b (ours)</cell><cell>4.9M</cell><cell cols="2">406M 77.5% 93.3%</cell></row><row><cell>DNA-c (ours)</cell><cell>5.3M</cell><cell cols="2">466M 77.8% 93.7%</cell></row><row><cell>DNA-d (ours)</cell><cell>6.4M</cell><cell cols="2">611M 78.4% 94.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison of transfer learning performance of NAS models on CIFAR-10 and CIFAR-100. ? : Our transfer learning results with officially released model. Accuracy within the parentheses are reported by the original paper.</figDesc><table><row><cell></cell><cell>Model</cell><cell cols="3">CIFAR-10 Acc CIFAR-100 Acc</cell></row><row><cell></cell><cell>MixNet-M[28]</cell><cell>97.9%</cell><cell>87.4%</cell></row><row><cell></cell><cell cols="4">EfficientNet-B0 98.0%(98.1%)  ? 87.1%(88.1%)  ?</cell></row><row><cell></cell><cell>DNA-c (ours)</cell><cell>98.3%</cell><cell>88.3%</cell></row><row><cell></cell><cell>1.3</cell><cell></cell><cell></cell><cell>71.1</cell></row><row><cell>Total Evaluation Loss (minus)</cell><cell>1.7 1.6 1.5 1.4</cell><cell cols="2">DNA Single-path One-shot</cell><cell>71.0 70.7 70.8 70.9 One-shot Model Accuracy 70.5 70.6</cell></row><row><cell></cell><cell>73.5</cell><cell>74.0 Stand-Alone Model Accuracy 74.5 75.0</cell><cell>75.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Impact of each component of DNA. Our strategy is better than S1 and S2. Adding cells to increase channel and layer variability can boost performance of searched model both with and without constraint.</figDesc><table><row><cell cols="2">Strategy Cell Constrain Params Acc@1 Acc@5</cell></row><row><cell>S1</cell><cell>5.18M 77.0% 93.34%</cell></row><row><cell>S2</cell><cell>5.58M 77.15% 93.51%</cell></row><row><cell>Ours</cell><cell>5.69M 77.49% 93.68%</cell></row><row><cell>Ours</cell><cell>6.26M 77.84% 93.74%</cell></row><row><cell>Ours</cell><cell>5.09M 77.21% 93.50%</cell></row><row><cell>Ours</cell><cell>5.28M 77.38% 93.60%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of DNA with different teacher. Note that all the searched models are retrained from scratch without any supervision of the teacher. ? :EfficientNet-B7 is tested with 224 ? 224 input size, to be consistent with distillation procedure.</figDesc><table><row><cell>Model</cell><cell cols="2">Params Acc@1 Acc@5</cell></row><row><cell cols="3">EfficientNet-B0 (Teacher) 5.28M 76.3% 93.2%</cell></row><row><cell>DNA-B0</cell><cell cols="2">5.27M 77.8% 93.7%</cell></row><row><cell cols="2">EfficientNet-B7 (Teacher) 66M</cell><cell>77.8%  ? 93.8%  ?</cell></row><row><cell>DNA-B7</cell><cell cols="2">5.28M 77.8% 93.7%</cell></row><row><cell>DNA-B7-scale</cell><cell cols="2">64.9M 79.9% 94.9%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank DarkMatter AI Research team for providing computational resources. C. Li and X. Chang gratefully acknowledge the support of Australian Research Council (ARC) Discovery Early Career Researcher Award (DE-CRA) under grant no. DE190100626, Air Force Research Laboratory and DARPA under agreement number FA8750-19-2-0501.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive stochastic natural gradient method for one-shot neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youhei</forename><surname>Akimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinichi</forename><surname>Shirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nozomu</forename><surname>Yoshinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shota</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kouhei</forename><surname>Nishida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (ICML)</title>
		<meeting>the 36th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do deep nets really need to be deep?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2654" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otkrist</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Raskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning (ICML)</title>
		<meeting>the International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Smash: one-shot model architecture search through hypernetworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodore</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ternational Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Renas: Reinforced evolutionary neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisen</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4787" to="4796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06022</idno>
		<title level="m">Scarletnas: Bridging the gap between scalability and fairness in neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01314</idno>
		<title level="m">Searching beyond mobilenetv3</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Fairnas: Rethinking evaluation fairness of weight sharing neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangxiang</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruijun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jixiang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.01845</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Single path oneshot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Squeeze-andexcitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Improving one-shot nas by suppressing the posterior fading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.02543</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Computation reallocation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno>2020. 5</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Darts: Differentiable architecture search. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeparchitect: Automatically designing and training deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Negrinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fitnets</surname></persName>
		</author>
		<title level="m">Hints for thin deep nets. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Segregation of pathways leading from area v2 to areas v4 and v5 of macaque monkey visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Shipp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Semir</forename><surname>Zeki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">315</biblScope>
			<biblScope unit="issue">6017</biblScope>
			<biblScope unit="page" from="322" to="324" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2820" to="2828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mixconv: Mixed depthwise convolutional kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th British Machine Vision Conference (BMVC)</title>
		<meeting>the 30th British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Progressive blockwise knowledge distillation for neural network acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<meeting>the International Joint Conference on Artificial Intelligence (IJCAI)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2769" to="2775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoliang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">M</forename><surname>Esperan?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<idno>2020. 1</idno>
	</analytic>
	<monogr>
		<title level="j">Nas evaluation is frustratingly hard. International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Knowledge projection for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihai</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09505</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Practical block-wise neural network architecture generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2423" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<title level="m">Neural architecture search with reinforcement learning. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
