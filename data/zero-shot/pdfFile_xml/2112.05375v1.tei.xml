<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking the Two-Stage Framework for Grounded Situation Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sea-NExT Joint Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
							<email>jiwei@nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Sea-NExT Joint Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><forename type="middle">Xiaoyu</forename><surname>Yue</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Centre for Perceptual and Interactive Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
							<email>chuats@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Sea-NExT Joint Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking the Two-Stage Framework for Grounded Situation Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Grounded Situation Recognition (GSR), i.e., recognizing the salient activity (or verb) category in an image (e.g., buying) and detecting all corresponding semantic roles (e.g., agent and goods), is an essential step towards "human-like" event understanding. Since each verb is associated with a specific set of semantic roles, all existing GSR methods resort to a two-stage framework: predicting the verb in the first stage and detecting the semantic roles in the second stage. However, there are obvious drawbacks in both stages: 1) The widely-used cross-entropy (XE) loss for object recognition is insufficient in verb classification due to the large intraclass variation and high inter-class similarity among daily activities. 2) All semantic roles are detected in an autoregressive manner, which fails to model the complex semantic relations between different roles. To this end, we propose a novel SituFormer for GSR which consists of a Coarse-to-Fine Verb Model (CFVM) and a Transformer-based Noun Model (TNM). CFVM is a two-step verb prediction model: a coarse-grained model trained with XE loss first proposes a set of verb candidates, and then a fine-grained model trained with triplet loss re-ranks these candidates with enhanced verb features (not only separable but also discriminative). TNM is a transformer-based semantic role detection model, which detects all roles parallelly. Owing to the global relation modeling ability and flexibility of the transformer decoder, TNM can fully explore the statistical dependency of the roles. Extensive validations on the challenging SWiG benchmark show that SituFormer achieves a new state-of-the-art performance with significant gains under various metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Understanding activities in images is one of the core tasks for computer vision. With the maturity of action recognition <ref type="bibr" target="#b3">(Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b27">Wang et al. 2016)</ref> and object detection <ref type="bibr" target="#b21">(Ren et al. 2015)</ref>, today's computers can recognize action or object categories well. However, "human-like" activity understanding goes beyond action-centric or object-centric recognition. A more crucial step is to identify how objects participate in activities,  <ref type="figure">Figure 1</ref>: (a) An example of GSR. Given this image, a GSR model needs to not only predict the verb category buying, but also detect (i.e., classify and ground) all corresponding semantic roles for buying event, such as agent, goods, and payment. (b) An overview of the existing two-stage GSR framework, which consists of a verb model and an RNN-based noun model to detect all roles autoregressively. such as "SOMEONE DO SOMETHING WITH SOME-TOOL AT SOMEPLACE". Hence, the task Situation Recognition (SR) <ref type="bibr" target="#b32">(Yatskar, Zettlemoyer, and Farhadi 2016)</ref> is proposed for comprehensive event extraction. As the example in <ref type="figure">Figure 1 (a)</ref>, SR not only recognize the salient activity(or verb) in the image (e.g., buying), but also recognize all semantic roles (e.g., agent is woman, place is store). To further ground the semantic roles in the image, a more challenging task Grounded Situation Recognition (GSR) <ref type="bibr" target="#b20">(Pratt et al. 2020</ref>) was proposed (cf. bounding boxes in <ref type="figure">Figure 1 (a)</ref>). By describing activities with verb and grounded semantic roles, GSR provides a visually-grounded structure representation (named verb frame) for the activity, which benefits many downstream scene understanding tasks, such as image-text  <ref type="figure">Figure 2</ref>: Left: A failure example of the verb model trained with XE loss and its predicted verb distributions. Its groundtruth verb is buying. Right: Some randomly selected images from the training set of the same category of hard negative verbs (i.e., browsing, shopping, and selling).</p><p>retrieval <ref type="bibr" target="#b13">(Gordo et al. 2016;</ref><ref type="bibr" target="#b19">Noh et al. 2017)</ref>, image captioning <ref type="bibr" target="#b18">(Mallya and Lazebnik 2017;</ref><ref type="bibr" target="#b4">Chen et al. 2021a</ref><ref type="bibr" target="#b8">Chen et al. , 2017</ref>, visual grounding <ref type="bibr" target="#b5">(Chen et al. 2021b)</ref>, and VQA <ref type="bibr" target="#b1">(Cadene et al. 2019;</ref><ref type="bibr" target="#b6">Chen et al. 2020</ref><ref type="bibr" target="#b9">Chen et al. , 2021c</ref><ref type="bibr" target="#b29">Xiao et al. 2022)</ref>.</p><p>Since each verb is inherently associated with a specific set of semantic roles (e.g., semantic role set agent, goods, payment, seller, place for verb buying), almost all existing SR methods resort to a two-stage framework: 1) predicting the verb (or action categories) for the whole image in the first stage; and 2) predicting nouns (or object categories) for all semantic roles in the second stage. Inspired by the success of SR methods, state-of-the-art GSR methods also follow the same two-stage framework by replacing the second-stage role classification model with a semantic role detection model. To the best of our knowledge, there are two existing SOTA GSR models: ISL and JSL <ref type="bibr" target="#b20">(Pratt et al. 2020)</ref>. Specifically, as summarized in <ref type="figure">Figure 1</ref> (b), they are all twostage models. For verb prediction, they train a verb model with N-way cross-entropy (XE) loss. For semantic role detection, they utilize an RNN-based noun model to predict and ground the noun for each semantic role autoregressively, i.e., they feed the predicted noun embedding of the last semantic role back into the RNN to guide the next prediction.</p><p>Although existing two-stage GSR methods have achieved satisfactory performance, we argue that there are still some unreasonable designs in both two stages: Verb Model (the first-stage): Since each verb can have different combinations of nouns w.r.t the semantic role set, the activity patterns are much more complex than objects (i.e., larger intra-class variation and higher inter-class similarity). Thus, even using a deep ConvNet (e.g., <ref type="bibr">ResNet-50 (He et al. 2016)</ref>) trained with XE loss can still fail to discriminate ambiguous verbs which place emphasis on different semantic roles. For example, in <ref type="figure">Figure 2</ref>, due to the frequent occurrences of "people browsing books at bookstore" and similar scene appearance (cf. images of browsing), the test image is tended to be wrongly predicted as browsing. Instead, if the verb model can focus more on some discriminative roles (e.g., the payment is happening with cash in hands), it would be easier to distinguish the buying from these plau-sible verb choices. Noun Model (the second-stage): 1) RNN-based models simply formulate each situation as a sequence of semantic roles, i.e., this link structure fails to model the complex relations between different semantic roles. 2) This autoregressive sequential prediction manner is prone to result in error accumulation. 3) They only utilize noun category embeddings to guide the training, which is easier to suffer from severe semantic sparsity issue <ref type="bibr" target="#b31">(Yatskar et al. 2017)</ref>, especially when the number of noun categories is extremely large (e.g., ? 10, 000 categories in SWiG benchmark).</p><p>In this paper, to address the above-mentioned issues, we propose a novel two-stage model (i.e., a verb model and a noun model): Situation Transformer (dubbed SituFormer).</p><p>For the verb model, since the verb feature learned with XE loss is not discriminative enough, we enhance it using triplet loss with a carefully designed hard triplet mining scheme. Similar practice are common in face recognition <ref type="bibr" target="#b23">(Schroff, Kalenichenko, and Philbin 2015;</ref><ref type="bibr" target="#b28">Wen et al. 2016)</ref>. Specifically, it is a coarse-to-fine two-step model. In the coarsegrained step, we first predict the top-N verbs with a coarsegrained verb model trained by XE loss. Then, in the finegrained step, we mine hard triplets from images of the top-N verbs considering the semantic role feature similarity. After further training a lightweight fine-grained model with triplet loss to obtain effectively enhanced verb features of all training samples (the gallery), the fine-grained model can re-rank top-N verbs by considering the feature similarity with the support image samples from the gallery. For the noun model, it is a transformer-based encoderdecoder model. The input queries for the decoder are a set of learnable embeddings for a verb and its corresponding semantic roles. The outputs of the decoder for each query are the predicted object category and grounding location. The built-in self-attention mechanism in the decoder implicitly formulates each verb frame as a fully-connected graph structure (vs. the sequence structure in existing GSR models). Meanwhile, our parallel decoding paradigm can avoid error accumulation. Moreover, sharing semantic role query embeddings across different verb frames introduces useful inductive bias, which alleviates the semantic sparsity issue.</p><p>We evaluate our proposed SituFormer on the challenging GSR benchmark: SWiG <ref type="bibr" target="#b20">(Pratt et al. 2020)</ref>. Extensive experiments have demonstrated the effectiveness of each component. Without bells and whistles, SituFormer outperforms all state-of-the-art GSR models on all evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Situation Recognition (SR). SR is first proposed by <ref type="bibr" target="#b14">(Gupta and Malik 2015;</ref><ref type="bibr" target="#b32">Yatskar, Zettlemoyer, and Farhadi 2016)</ref>, which generalizes action classification <ref type="bibr" target="#b3">(Carreira and Zisserman 2017;</ref><ref type="bibr" target="#b13">Girish, Singh, and Ralescu 2020)</ref>, HOI <ref type="bibr" target="#b33">Zou et al. 2021;</ref><ref type="bibr">Wei et al. 2020</ref>) and SGG <ref type="bibr" target="#b7">(Chen et al. 2019;</ref><ref type="bibr" target="#b11">Cong et al. 2021)</ref>, and aims to provide a structured representation for an activity (event) with a verb frame. Typically, the verb frame consists of a verb with a specific semantic role set drawn from FrameNet <ref type="bibr" target="#b0">(Baker, Fillmore, and Lowe 1998)</ref>. The early CRF-based SR methods <ref type="bibr" target="#b32">(Yatskar, Zettlemoyer, and Farhadi 2016;</ref>  2017) jointly predict verb and nouns by structured learning. However, sharing the visual representation for the two tasks has been proved inferior to training separate models in a two-stage way <ref type="bibr" target="#b18">(Mallya and Lazebnik 2017)</ref>. Hence, recent RNN-based methods <ref type="bibr" target="#b18">(Mallya and Lazebnik 2017)</ref>, GNNbased methods <ref type="bibr" target="#b16">(Li et al. 2017;</ref><ref type="bibr" target="#b25">Suhail and Sigal 2019)</ref> and attention-based methods (Cooray, Cheung, and Lu 2020) always predict the verb in the first stage and recognize the semantic roles in the second stage.</p><p>Ground Situation Recognition (GSR). GSR <ref type="bibr" target="#b30">(Yang et al. 2016;</ref><ref type="bibr" target="#b24">Silberer and Pinkal 2018;</ref><ref type="bibr" target="#b20">Pratt et al. 2020</ref>) extends the SR task and aims to further ground the semantic roles which is critical to visual reasoning. The two existing GSR methods (JSL and ISL) <ref type="bibr" target="#b20">(Pratt et al. 2020</ref>) follow the RNNbased two-stage SR pipeline to first predict the verb and then sequentially detect the semantic roles. Our method differs in two aspects: 1) the verb prediction is in a coarse-to-fine manner. 2) the semantic roles are detected in parallel rather than in autoregressive sequence. Another concurrent work <ref type="bibr" target="#b10">(Cho et al. 2021</ref>) also resorts to Transformer structure for GSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach Overview</head><p>Given an image I, GSR aims to detect a structured visuallygrounded verb frame G = {v, R, N , B}, where v ? V is the category of the salient activity (or verb) in image I, and R = {r 1 , ..., r m } is the set of manually predefined semantic roles 1 for verb v. N = {n 1 , ..., n m } and B = {b 1 , ..., b m } are the set of object (or noun) categories and bounding boxes for all semantic roles, i.e., n i ? O is the object category of semantic role r i , and b i ? R 4 is the bounding box location of semantic role r i . V and O denote the predefined ontology of all possible verb and noun categories, respectively. Currently, almost all existing GSR (or SR) models decompose this task into two steps: verb classification and noun detection (or classification). Thus, for GSR:</p><formula xml:id="formula_0">p(G|I) = p(v, R|I) Verb classification p(N , B|v, R, I) Noun detection .</formula><p>(1)</p><p>In this paper, we propose a novel Situation Transformer (dubbed as SituFormer). It follows the same spirit and consists of two components: a Transformer-based Noun Model (TNM) and a Coarse-to-Fine Verb Model (CFVM). As illustrated in <ref type="figure" target="#fig_0">Figure 3</ref>, given an image I, we first use a coarsegrained verb model (Verb-c) to propose a set of verb candidates (and their corresponding semantic roles), denoted as V c and {R} c . Then, for all verb candidates, the TNM will output their respective noun categories {N } c and bounding boxes {B} c . Lastly, a lightweight fine-grained verb model (Verb-f) selects the final verb frame prediction. Thus, we reformulate GSR as:</p><formula xml:id="formula_1">p(G|I) = p({G} c |I)p(G|{G} c , I) = p(V c , {R} c , {N } c , {B} c |I)p(G|{G} c , I) = p(V c , {R} c |I) Verb-c p({N } c , {B} c |V c , {R} c , I) TNM p(G|{G} c , I) Verb-f ,</formula><p>where {G} c denotes the set of all verb frame candidates.</p><p>In this section, we first introduce each component of Situ-Former, including TNM and CFVM (Verb-c and Verb-f). Then, we demonstrate the details of all training objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer-based Noun Model (TNM)</head><p>The noun model TNM is designed to detect (i.e., classify and ground) all semantic roles of a verb frame. Inspired from recently proposed end-to-end transformer-based object detector DETR <ref type="bibr" target="#b2">(Carion et al. 2020)</ref>, TNM is also a transformerbased model. As shown in <ref type="figure" target="#fig_1">Figure 4</ref>, TNM consists of four sub-networks: a CNN backbone, a transformer encoder, a transformer decoder and a noun detection head.</p><p>Given image I, the CNN backbone first extracts a feature map X N ? R C?H?W . Since the input for the transformer encoder is a sequence of tokens, the feature map X N is flattened to a sequence of "visual" tokens: [x N 1 , ..., x N H * W ], and each token x N i ? R C is a C-dim visual feature. Then, the visual token sequence is fed into the transformer encoder:</p><formula xml:id="formula_2">x N 1 , ...,x N H * W = F TNM enc ( x N 1 , ..., x N H * W ),<label>(2)</label></formula><p>where F TNM enc is a vanilla transformer encoder, which consist of a position embedding layer, and a set of stacked multihead self-attention layers. We refer the readers to the original Transformer <ref type="bibr" target="#b26">(Vaswani et al. 2017</ref>) paper for more details.</p><p>Given the encoded visual featureX N = [x N 1 , ...,x N H * W ], verb v and its semantic role set R = {r 1 , ..., r m }, we first encode the verb v and all semantic roles {r i } into query embeddings: q v and {q ri }. Then, these query embeddings are regarded as the input queries for the transformer decoder (cf. <ref type="figure" target="#fig_1">Figure 4)</ref>, and the encoded visual featureX N is the key and value for the cross-attention layer in the decoder, i.e.,</p><formula xml:id="formula_3">[x v , x n1 , ..., x nm ] = F TNM dec (X N , [q v , q r1 , ..., q rm ]),</formula><p>(3) where x v and x ni are the output of query q v and q ri , respectively. Lastly, a lightweight noun detection head (MLP) predicts the object category and regresses the normalized bounding box coordinates for each semantic role query, i.e.,</p><formula xml:id="formula_4">{n i , b i } = MLP(x ni ),<label>(4)</label></formula><p>and the output of TNM is the set of object categories and bounding boxes of all roles, i.e., N and B (cf. Eq. <ref type="formula" target="#formula_2">(2)</ref>).   Differences with DETR. Although TNM has a similar architecture with DETR, there are several notable differences: 1) Meaning of decoder input queries. For DETR, these queries can be regarded as priors for potential objects with different sizes and locations. Thus, the query number should be large (e.g., 100 queries for COCO). Instead, each query in TNM is the embedding of a semantic role, which is responsible for detecting this specific role, and the maximum number of semantic roles is small (e.g., 6 queries for SWiG). 2) Matching algorithm for optimization. For DETR, a matching algorithm is needed for optimal bipartite matching between ground-truth and set predictions. While in TNM, there is a perfect one-to-one match for each role query. Thanks to this design, TNM can not only take advantage of the prior knowledge of the verb but also reduce computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-to-Fine Verb Model (CFVM)</head><p>CFVM is a two-step verb classification model, which consists of two modules: 1) a coarse-grained verb model (Verbc) to propose a set of verb candidates; 2) a fine-grained verb model (Verb-f) to make the final verb prediction. The overview architecture of CFVM is illustrated in <ref type="figure" target="#fig_2">Figure 5</ref>.</p><p>Coarse-grained Verb Classification. The coarse-grained verb classification model (Verb-c) aims to propose a set of verbs as initial candidates. Since the verb (or activity) classification task inherently needs to model the semantic relationships between multiple objects in the image, we combine a CNN backbone with a transformer encoder as our Verb-c (cf. <ref type="figure" target="#fig_2">Figure 5 (left)</ref>), i.e., the self-attention mechanism in the transformer encoder helps to capture global context in the image. Instead, almost all existing GSR (or SR) methods directly use a plain CNN backbone (e.g., ResNet or VGG) as their verb classification model. Similarly to the TNM, for given image I, the CNN backbone first extracts feature map X V ? R C?H?W , and X V is flatten to a sequence of tokens:</p><formula xml:id="formula_5">[x V 1 , ..., x V H?W ].</formula><p>Following the convention of BERT-family works <ref type="bibr" target="#b12">(Devlin et al. 2019)</ref>, we also add a learnable embedding x cls of special token [CLS] to the token sequence. Then, this augmented token sequence is fed into the transformer encoder F Verb-c enc :</p><formula xml:id="formula_6">x cls ,x V 1 ,x V 2 , ... = F Verb-c enc ( x cls , x V 1 , x V 2 , ... ). (5)</formula><p>The encoded embedding of [CLS] token (i.e.,x cls ) is used to represent the gist of the whole image, and it is fed into a fully-connected layer to make the coarse verb prediction. We select top-N verbs as candidates and denote them as V c . Fine-grained Verb Classification. Up to now, for image I, we have obtained the top-N candidate verbs V c and semantic role set {R} c from Verb-c. Then, for each candidate v i ? V c and R i ? {R} c , we can also get the corresponding semantic role detection results from noun model TNM:</p><formula xml:id="formula_7">N i , B i = TNM(v i , R i , I).<label>(6)</label></formula><p>Thus, we obtain the semantic role detection results for all N verb candidates: {N } c and {B} c .</p><p>To determine the final verb prediction, we first retrieve M support images from the training set for each verb candidate v i ? V c . The support image set for v i is denoted as I i = {I </p><p>where D i is the set of all training set images with groundtruth verb category v i . The semantic role feature similarity score S(?) is the average cosine similarity of all roles:</p><formula xml:id="formula_9">S(I, I k ) = 1 m m i=1 sim(x I ni , x I k ni ),<label>(8)</label></formula><p>where x I ni and x I k ni is the i-th semantic role features from the output of TNM (cf. Eq. (3)) of image I and I k , respectively. Similarity function sim is cosine similarity.</p><p>After retrieving support image set I i for each verb candidate v i , our fine-grained verb model (Verb-f) uses a lightweight MLP ?(?) to map the original coarse verb featur? x cls (cf. Eq. (5)) into a more distinctive embedding space, i.e., ? is designed to project coarse verb features of image I and all retrieved support images to focus on fine distinctive details. (We train ?(?) with hard triplets, and the training details are in the following sections).</p><p>In the inference stage, given the coarse-grained classification scores {p(v 1 ), p(v 2 ), ..., p(v N )} of all top-N candidates, the Verb-f model re-ranks all verb candidates based on the similarity scores between image I and support images. If p(v 1 ) &gt;= ( is a threshold score), the final verb prediction is v 1 , i.e., the original Verb-c model performs well. If p(v 1 ) &lt; , the re-ranked scores p r (v i ) is calculated as:</p><formula xml:id="formula_10">p r (v i ) = ? I k ?Ii sim(?(x I cls ), ?(x I k cls )) * S(I, I k ) + ?p(v i ),<label>(9)</label></formula><p>where ? and ? are weights for the trade-off between original verb prediction probability p(v i ) from Verb-c and the confidence from support image set in Verb-f.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Objectives</head><p>In the training stage, we train all components in SituFormer separately, including TNM, Verb-c, and Verb-f: Training Objective of TNM. We denote the ground-truth noun categories and bounding boxes as N gt , B gt and the predicted noun categories and bounding boxes asN ,B. The detection loss L TNM of TNM is calculated as:</p><formula xml:id="formula_11">L TNM = m i=1 XE(n gt i ,n i ) + L box (b gt i ,b i ) ,<label>(10)</label></formula><p>where XE is the cross-entropy loss and L box consists of a generalize IoU loss <ref type="bibr" target="#b22">(Rezatofighi et al. 2019</ref>) and a L1 regression loss.</p><p>Training Objective of Verb-c. We denote the ground-truth verb category as v gt and the predicted verb category a? v.The classification loss of Verb-c L verb-c is calculated as:</p><formula xml:id="formula_12">L verb-c = XE(v gt ,v).<label>(11)</label></formula><p>Training Objective of Verb-f. For each training sample I a (anchor image), we regard all support images with the same ground-truth verb category as hard positive sample set I + , and all support images for other verb categories as hard negative sample set I ? , i.e., I ? = {I i }\I + . The margin-based triplet loss of Verb-f L verb-f is calculated as:</p><formula xml:id="formula_13">L verb-f = max(0, ? + sim(?(x I a cls ), ?(x X n i cls )) ? sim(?(x I a cls ), ?(x X p i cls ))),<label>(12)</label></formula><p>where X p i ? I + is the hard positive image and X n i ? I ? is the hard negative image. ? is a margin value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments Experimental Settings</head><p>Datasets. We evaluated our method for GSR on the challenging SWiG benchmark <ref type="bibr" target="#b20">(Pratt et al. 2020)</ref>. It is an extension dataset of the SR dataset imSitu <ref type="bibr" target="#b32">(Yatskar, Zettlemoyer, and Farhadi 2016)</ref>. Specifically, each image in imSitu is annotated with three verb frames by three annotators. SWiG adds bbox annotations for all visible semantic roles (63.9% roles have bbox annotations). SWiG consists of 126, 102 images with 9, 928 object categories, 190 semantic role types and 504 verb categories. The official splits are 75K/25K/25K images for training, validation, and test set, respectively. Evaluation Metrics. We followed prior work <ref type="bibr" target="#b20">(Pratt et al. 2020)</ref> to evaluate our method on five metrics: 1) verb: The accuracy of verb prediction. 2) value: The accuracy of noun prediction w.r.t each semantic role. 3) value-all (val-all):</p><p>The accuracy of noun prediction w.r.t. the whole semantic role set. 4) grounded-value (grnd): The accuracy of noun prediction with correct grounding w.r.t each semantic role. By "correct grounding", we mean the IoU between predicted bounding box and ground-truth is large than threshold 0.5. 5) grounded-value-all (grnd-all): The accuracy of noun prediction with correct grounding w.r.t to the whole semantic role set. Meanwhile, there are three different evaluation settings: 1) Ground-Truth-Verb: The ground-truth verbs is assumed to be known. 2) Top-1-Verb: verb reports the top-1 accuracy, and all other four value metrics are considered wrong if verb is wrong. 3) Top-5-Verb: verb reports the top-5 accuracy, and all other four value metrics are conditioned on the correct verb having been predicted. Implementation Details. The CNN backbone of both TNM and CFVM were ResNet-50 pretrained on ImageNet. The decoder of TNM used sine position encodings. For TNM, we followed DETR and set the layer number of the encoder and decoder as 6 by default except as otherwise noted. Following prior works, TNM only predicted the top 2, 000 most frequent object categories, which covers about 95% noun annotations. TNM was trained with AdamW optimizer and the initial learning rate of transformer and CNN backbone was set to 10 ?4 and 10 ?5 respectively. We trained it for 20 epoch with a learning rate drop by a factor of 10 after 10 epoch on 4 V100 GPUs. The total batch size was set to 128. Verb-c model had the same training strategy with TNM. For Verb-f model, the hard negative sample set was constructed from the top-5 verb candidates, and the size of support image set of each verb was set to 10. At each training step, we randomly chose a negative sample and a positive sample to compose the training triplet. Verb-f was trained for 20 epoch with an initial learning rate 5 ? 10 ?4 drop by a factor of 10 after 10 epoch. The margin ? was set to 0.2. In the inference stage, the threshold score = 0.4 and weights ? = ? = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons with State-of-the-Arts</head><p>Settings. We compared our SituFormer with state-of-the-art GSR and SR models on SWiG dataset. Based on their model architectures, existing SR models are be coarsely grouped into: 1) CRF-based models: CRF <ref type="bibr" target="#b32">(Yatskar, Zettlemoyer, and Farhadi 2016)</ref> and CRF+DataAug <ref type="bibr" target="#b31">(Yatskar et al. 2017)</ref>.  <ref type="table" target="#tab_3">Table 1 and Table 2</ref>, respectively. Results under Ground-Truth-Verb Setting. Under this setting, we can evaluate the model performance on semantic role detection (i.e., TNM). Based on results on <ref type="table" target="#tab_3">Table 1 and Table 2</ref>, we can have the following observations: 1) For role classification (i.e., value and val-all metrics), Situ-Former outperforms all existing GSR (and SR) models on both metrics. Compared to the best performer Kernel-Graph, we achieve 2.94% (76.08% vs. 73.14%) and 0.47% absolute Models  <ref type="bibr">44.20 35.24 21.86 29.22 13.41 71.21 55.75 33.27 46.00 20.10 75.85 42.13 61.89 24.89 +4.26 +3.80 +2.99 +4.36 +3.75 +3.61 +3.87 +3.88 +5.40 +5.38 +2.64 +4.31 +5.32 +6.44</ref>  performance gains under value and val-all metrics (on the dev set), respectively. 2) As for the grounding metrics (i.e., grnd and grnd-all metrics), SituFormer also outperforms all existing GSR models. Compared to JSL, performance gains are much more significant, e.g., 5.36% (24.65% vs. 19.29%) and 6.44% (24.89% vs. 18.45%) absolute performance gains under grnd-all metric on dev and test set, respectively. Results under Top-N-Verb settings. From the verb metric, we can observe that SituFormer outperforms all existing GSR (and SR) models on both top-1 and top-5 verb accuracy, which demonstrate the superiority of CFVM. With the SOTA results of both TNM and CFVM, SituFormer also achieves the best results on val-all, grnd and grnd-all under this setting. Although SR model Kernel-Graph outperforms SituFormer slightly on the value metric (i.e., 0.47% under Top-5-Verb setting), they actually significantly sacrifice their performance on val-all metric due to the joint training of their verb model and noun model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head><p>We conducted extensive ablation studies to demonstrate the effectiveness of each component of our Situformer. Effectiveness and Hyper-parameters Choices of CFVM. Effectiveness of Coarse-to-Fine Classification. To validate the effectiveness of Verb-f, we conducted ablations by using only Verb-c as the verb model and TNM as the noun model (i.e., denoted as "SituFormer w/o Verb-f"). The results under Top-1-Verb setting are reported in <ref type="table" target="#tab_6">Table 3</ref>. From the results, we can observe that the Verb-f model (i.e., the coarseto-fine strategy) can directly improve the final top-1 verb accuracy by 1.12%. Accordingly, all <ref type="bibr">value-related (i.e., value, val-all, grnd, grnd-all)</ref>   Layer Numbers of the Encoder in Verb-c. We investigated verb accuracy (both top-1 and top-5) of Verb-c with different layer numbers of transformer encoder (up to 6), and the results are reported in <ref type="table" target="#tab_9">Table 4</ref> (a). The baseline model (denoted as 0 layer) is the ResNet-50 model, which is the same verb model used in JSL. From <ref type="table" target="#tab_9">Table 4</ref> (a), we can observe that applying the transformer encoder can gradually improve the verb accuracy (e.g., 43.08% vs. 39.94%). And when the stacked layer number more than 4 layers, their performances reach the plateaus. To trade-off between accuracy and computation, we used four encoder layers in our Verb-c model. The Size of Support Images Set in Verb-f. We explored various support image set sizes to show the robustness of the retrieve-and-rerank scheme of the verb-f, and we reported the top-1 verb accuracy in <ref type="table" target="#tab_9">Table 4</ref> (b). From the results, we can observe that larger support image set size can perform better but the accuracy plateaus when n &gt; 5. This is because the possible number of hard support image is limited. Query Designs in TNM. Since one of key differences between TNM and DETR-family models is the design of decoder queries, we also investigated several different designs: Importance of Verb Query (V-queries). Since the verb itself provide useful inductive bias for semantic role prediction, it is intuitive that introducing auxiliary verb query is helpful. To validate the effectiveness of the verb query, we conducted ablations under the Ground-Truth-Verb setting,  <ref type="figure">Figure 6</ref>: Left: For each image, the top-5 verb candidates and re-ranked verbs are shown below "Verb-c" and "Verb-f", respectively. The semantic role detection are shown in the third row. Incorrectly object category prediction is scratched out with ground-truth shown in red brackets. The correct groundings are shown in solid boxes while the incorrect ones shown in dotted boxes. ? means no ground-truth grounding for that role. Right: The retrieved support images of top-4 verb candidates for I4.   and the results are reported in <ref type="table" target="#tab_10">Table 5</ref>. From the results, we can observe that the verb query brings 1.68% and 2.30% absolute performance gains on the value metric. It is also worth noting that TNM without verb query already achieves new state-of-the-art performance on all four metrics. Effectiveness of Sharing Role Queries (R-queries). To mitigate semantic sparsity issue (i.e., numerous triplets verbrolenoun are rare in the SWiG dataset), TNM shares the role embeddings as queries among all different verbs. To validate the effectiveness of sharing role queries, we conducted ablations by using TNM without sharing role queries (i.e., each verb has independent semantic role queries). Results under Ground-Truth-Verb setting are reported in <ref type="table" target="#tab_10">Table 5</ref>. From the results, we can observe that the improvement of sharing role queries is obvious (e.g., ? 2% and 3% performance gains for the value and val-all metrics. Effectiveness of parallel decoding. As shown in <ref type="table" target="#tab_3">Table 1</ref> &amp; <ref type="table" target="#tab_4">Table 2</ref> Ground-Truth-Verb setting where the effect of verb model is exempted, the quantitative improvements to ISL and JSL attest to the effectiveness of parallel decoding. Qualitative Results. In <ref type="figure">Figure 6</ref>, we display coarse-to-fine verb predictions and semantic role detection results of some images (I1 ? I4) from the test set (left) and retrieved support image sets (right). For all four examples, their initial top-1 verb predictions from Verb-c are wrong but the groundtruth verbs are probabilities ascend to the 1st place after re-ranking by Verb-f. We can see that discriminative details are needed to distinguish ground-truth verb from these candidates (e.g., tiny interaction between the cat and the stick in I4). We also display some errors of TNM. In I2, the AGENT of Swinging is incorrectly predicted as man. This error may be caused by the rare occurrence of "a cat is swinging". While in I3, the incorrect team for STUDENT is actually more reasonable than the ground-truth people. On the right part of <ref type="figure">Figure 6</ref>, we show retrieved support images of Pawing and the top-3 wrong predicted verbs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we argue that the existing two-stage GSR models have drawbacks in both verb prediction stage (insufficient to handle high diversity of daily activities) and semantic role detection stage (using autoregressive model). To alleviate these drawbacks, we propose SituFormer which consists of a two-step verb model and a transformer-based noun model. Specifically, the verb model predicts verb in a coarseto-fine process, and the noun model makes use of the flexibility of transformer to integrate the recognition and grounding of roles. We achieved significant gains over all metrics on challenging benchmark SWiG, and conducted ablative analysis for each component of SituFormer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FineFigure 3 :</head><label>3</label><figDesc>The overview pipeline of our SituFormer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The architecture of TNM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>The architecture of CFVM, including Verb-c (left) and Verb-f (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>on the semantic role feature similarity scores S(I, I k ), i.e., I i = arg top-M I k ?Di S(I, I k ),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>2) RNN-based models: VGG+RNN (Mallya and Lazebnik 2017). 3) GNN-based models: FC-Graph (Li et al. 2017), Kernel-Graph (Suhail and Sigal 2019). 4) Attention-based: CAQ (Cooray, Cheung, and Lu 2020). For GSR, all existing model: ISL and JSL (Pratt et al. 2020) are RNN-based. The results on the development (dev) set and test set are illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>arXiv:2112.05375v1 [cs.CV] 10 Dec 2021</figDesc><table><row><cell></cell><cell></cell><cell>browsing</cell><cell>shopping</cell><cell>selling</cell></row><row><cell cols="2">browsing 0.37</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">shopping 0.26</cell><cell></cell><cell></cell><cell></cell></row><row><cell>selling</cell><cell>0.07</cell><cell></cell><cell></cell><cell></cell></row><row><cell>buying</cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell></row><row><cell>shelving</cell><cell>0.01</cell><cell>?</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Verb verb value val-all grnd grnd-all verb value val-all grnd grnd-all value val-all grnd grnd-all 44.32 35.35 22.10 29.17 13.33 71.01 55.85 33.38 45.78 19.77 76.08 42.15 61.82 24.65 +4.72 +4.17 +3.25 +4.14 +3.17 +3.30 +3.79 +3.65 +4.53 +4.70 +2.55 +3.83 +4.32 +5.36 Performance (%) of state-of-the-art GSR (and SR) methods on SWiG dataset development (dev) set.</figDesc><table><row><cell cols="9">Top-1-Verb Ground-Truth-Situation Recognition Models Top-5-Verb</cell><cell></cell></row><row><cell>CRF</cell><cell>32.25 24.56 14.28</cell><cell>-</cell><cell>-</cell><cell>58.64 42.68 22.75</cell><cell>-</cell><cell>-</cell><cell>65.90 29.50</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">CRF+DataAug 34.20 25.39 15.61</cell><cell>-</cell><cell>-</cell><cell>62.21 46.72 25.66</cell><cell>-</cell><cell>-</cell><cell>70.80 34.82</cell><cell>-</cell><cell>-</cell></row><row><cell>VGG+RNN</cell><cell>36.11 27.74 16.60</cell><cell>-</cell><cell>-</cell><cell>63.11 47.09 26.48</cell><cell>-</cell><cell>-</cell><cell>70.48 35.56</cell><cell>-</cell><cell>-</cell></row><row><cell>FC-Graph</cell><cell>36.93 27.52 19.15</cell><cell>-</cell><cell>-</cell><cell>61.80 45.23 29.98</cell><cell>-</cell><cell>-</cell><cell>68.89 41.07</cell><cell>-</cell><cell>-</cell></row><row><cell>CAQ</cell><cell>37.96 30.15 18.58</cell><cell>-</cell><cell>-</cell><cell>64.99 50.30 29.17</cell><cell>-</cell><cell>-</cell><cell>73.62 38.71</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Kernel-Graph 43.21 35.18 19.46</cell><cell>-</cell><cell>-</cell><cell>68.55 56.32 30.56</cell><cell>-</cell><cell>-</cell><cell>73.14 41.68</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">Grounded Situation Recognition Models</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ISL</cell><cell cols="9">38.83 30.47 18.23 22.47 7.64 65.74 50.29 28.59 36.90 11.66 72.77 37.49 52.92 15.00</cell></row><row><cell>JSL</cell><cell cols="9">39.60 31.18 18.85 25.03 10.16 67.71 52.06 29.73 41.25 15.07 73.53 38.32 57.50 19.29</cell></row><row><cell>SituFormer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gains (?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell cols="9">Top-1-Verb verb value val-all grnd grnd-all verb value val-all grnd grnd-all value val-all grnd grnd-all Top-5-Verb Ground-Truth-Verb</cell></row><row><cell>ISL</cell><cell cols="9">39.36 30.09 18.62 22.73 7.72 65.51 50.16 28.47 36.60 11.56 72.42 37.10 52.19 14.58</cell></row><row><cell>JSL</cell><cell cols="9">39.94 31.44 18.87 24.86 9.66 67.60 51.88 29.39 40.60 14.72 73.21 37.82 56.57 18.45</cell></row><row><cell>SituFormer</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Gains (?)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance (%) of state-of-the-art GSR methods on SWiG dataset test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>metrics are further boosted.</figDesc><table><row><cell>Models</cell><cell>verb value val-all grnd grnd-all</cell></row><row><cell cols="2">SituFormer w/o Verb-f 43.08 34.20 21.24 28.45 12.90</cell></row><row><cell>SituFormer</cell><cell>44.20 35.24 21.86 29.22 13.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Performance (%) under Top-1-Verb setting.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results (%) on different hyper-parameters choices of CFVM.</figDesc><table><row><cell>V-query Shared R-query value val-all grnd grnd-all</cell></row><row><cell>75.85 42.13 61.89 24.89</cell></row><row><cell>74.17 39.37 60.16 22.86</cell></row><row><cell>73.26 38.13 57.02 20.42</cell></row><row><cell>70.96 34.87 55.37 19.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Results (%) of different query designs in TNM under Ground-Truth-Verb setting.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">These predefined semantic roles can be easily retrieved from the verb lexicon such as PropBank or FrameNet.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This research is funded by Sea-NExT Joint Lab, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Berkeley FrameNet Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Murel: Multimodal relational reasoning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human-like Controllable Image Captioning with Verb-specific Semantic Roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Ref-NMS: Breaking Proposal Bottlenecks in Two-Stage Referring Expression Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Counterfactual Samples Synthesizing for Robust Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Counterfactual critic multi-agent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.01013</idno>
		<title level="m">Counterfactual samples synthesizing and training for robust visual question answering</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Grounded Situation Recognition with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention-Based Context Aware Reasoning for Situation Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-M</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.12309</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Spatial-Temporal Transformer for Dynamic Scene Graph Generation</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Girish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ralescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Situation recognition with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recurrent models for situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Grounded situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Weihs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Faster rcnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Grounding semantic roles in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Silberer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mixture-kernel graph attention network for situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suhail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<editor>ECCV. Wei, M</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Hose-net: Higher order structure embedded network for scene graph generation</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video as Conditional Graph Hierarchy for Multi-Granular Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Grounded semantic role labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Commonly uncommon: Semantic sparsity in situation recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Situation recognition: Visual semantic role labeling for image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
