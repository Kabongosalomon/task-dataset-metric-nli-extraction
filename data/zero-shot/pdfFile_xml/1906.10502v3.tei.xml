<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SampleFix: Learning to Generate Functionally Diverse Fixes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Hajipour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apratim</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian-Alexandru</forename><surname>Staicu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">CISPA Helmholtz Center for Information Security</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SampleFix: Learning to Generate Functionally Diverse Fixes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Program repair ? Generative models ? Conditional varia- tional autoencoder</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic program repair holds the potential of dramatically improving the productivity of programmers during the software development process and correctness of software in general. Recent advances in machine learning, deep learning, and NLP have rekindled the hope to eventually fully automate the process of repairing programs. However, previous approaches that aim to predict a single fix are prone to fail due to uncertainty about the true intend of the programmer. Therefore, we propose a generative model that learns a distribution over potential fixes. Our model is formulated as a deep conditional variational autoencoder that can efficiently sample fixes for a given erroneous program. In order to ensure diverse solutions, we propose a novel regularizer that encourages diversity over a semantic embedding space. Our evaluations on common programming errors show for the first time the generation of diverse fixes and strong improvements over the state-of-the-art approaches by fixing up to 45% of the erroneous programs. We additionally show that for the 65% of the repaired programs, our approach was able to generate multiple programs with diverse functionalities.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Software development is a time-consuming and expensive process. Unfortunately, programs written by humans typically come with bugs, so significant effort needs to be invested to obtain code that is only likely to be correct. Debugging is also typically performed by humans and can contain mistakes. This is neither desirable nor acceptable in many critical applications. Therefore, automatically locating and correcting program errors <ref type="bibr" target="#b10">[11]</ref> offers the potential to increase productivity as well as improve the correctness of software.</p><p>Advances in deep learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, computer vision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, and NLP <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b2">3]</ref> have dramatically boosted the machine's ability to automatically learn representations of natural data such as images and natural language contents for various tasks. Deep learning models also have been successful in learning the distribution over continuous <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b15">16]</ref> and discrete data <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b13">14]</ref>, to generate new and diverse data points <ref type="bibr" target="#b9">[10]</ref>. These advances in machine learning and the advent of large corpora of source code <ref type="bibr" target="#b0">[1]</ref> provide new opportunities toward harnessing deep learning methods to understand, generate, or debug programs. <ref type="figure">Fig. 2</ref>: SampleFix captures the inherent ambiguity of the possible fixes by sampling multiple potential fixes for the given erroneous real-world program. Potential fixes with the same functionality are highlighted with the same color and the newly added tokens are underlined. <ref type="figure">Fig. 1</ref>: Our SampleFix approach with diversity regularizer promotes sampling of diverse fixes, that account for the inherent uncertainty in the automated debugging task.</p><p>Prior works in automatic program repair predominantly rely on expert-designed rules and error models that describe the space of the potential fixes <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8]</ref>. Such hand-designed rules and error models are not easily adaptable to the new domains and require a timeconsuming process.</p><p>In contrast, learning-based approaches provide an opportunity to adapt such models to the new domain of errors. Therefore, there has been an increasing interest to carry over the success stories of deep learning in NLP and related techniques to employ learningbased approaches to tackle the "common programming errors" problem <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>. Such investigations have included compiletime errors such as missing scope delimiters, adding extraneous symbols, using incompatible operators. Novice programmers and even experienced developers often struggled with these types of errors <ref type="bibr" target="#b24">[25]</ref>, which is usually due to lack of attention to the details of programs and/or programmer's inexperience.</p><p>Recently, Gupta et al. <ref type="bibr" target="#b12">[13]</ref> proposed a deep sequence to sequence model called DeepFix where, given an erroneous program, the model predicts the locations of the errors and a potential fix for each predicted location. The problem is formulated as a deterministic task, where the model is trained to predict a single fix for each error. However, different programs -and therefore also their fixescan express the same functionality. Besides, there is also uncertainty about the intention of the programmer. <ref type="figure">Figure 1</ref> illustrates the issue. Given an erroneous program (buggy program), there is a large number of programs within a cer-tain edit distance. A subset of these, will result in successful compilation. The remaining programs will still implement different functionalities and -without additional information or assumptions -it is impossible to tell which program/functionality was intended. In addition, previous work <ref type="bibr" target="#b27">[28]</ref> also identified overfitting as one of the major challenges for learning-based automatic program repair. We believe that one of the culprits for this is the poor objectives used in the training process, e.g., training a model to generate a particular target fix.</p><p>Let us consider the example in <ref type="figure">Figure 2</ref> from the dataset of DeepFix <ref type="bibr" target="#b12">[13]</ref>. This example program is incorrect due to the imbalanced number of curly brackets. In a traditional scenario, a compiler would warn the developer about this error. For example, when trying to compile this code with GCC, the compiler terminates with the error "expected declaration or statement at end of input", indicating line 10 as the error location. Experienced developers would be able to understand this cryptic message and proceed to fixing the program. Based on their intention, they can decide to add a curly bracket either at line 6 (patch P 1 ) or at line 9 (patch P 2 ). Both these solutions would fix the compilation error in the erroneous program, but the resulting solutions have different semantics.</p><p>Hence, we propose a deep generative framework to automatically correct programming errors by learning the distribution of potential fixes. We investigate different solutions to model the distribution of the fixes and sample multiple fixes, including different variants of Conditional Variation Autoencoders (CVAE) and beam search decoding. It turns out (as we will also show in our experiments) CVAE and beam search decoding are complementary, while CVAE is computationally more efficient in comparison to beam search decoding. Furthermore, we encourage diversity in the candidate fixes through a novel regularizer which penalizes similar fixes for an identical erroneous program and significantly increases the effectiveness of our approach. The candidate fixes in <ref type="figure">Figure 2</ref> are generate by our approach, illustrating its potential for generating both diverse and correct fixes. For a given erroneous program, our approach is capable of generating diverse fixes to resolve the syntax errors.</p><p>To summarize, the contributions of this paper are as follows, 1. We propose an efficient generative method to automatically correct common programming errors by learning the distribution over potential fixes. 2. We propose a novel regularizer to encourage the model to generate diverse fixes. 3. Our generative model together with the diversity regularizer shows an increase in the diversity and accuracy of fixes, and a strong improvement over the state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work builds on the general idea of sequence-to-sequence models as well as ideas from neural machine translation. We phrase our approach as a variational auto-encoder and compare it to prior learning-based program repair approaches. We review the related work in order below:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>Sutskever et al. <ref type="bibr" target="#b29">[30]</ref> introduces neural machine translation and casts it as a sequence-to-sequence learning problem. The popular encoder-decoder architecture is introduced to map the source sentences into target sentences. One of the major drawbacks of this model is that the sequence encoder needs to compress all of the extracted information into a fixed-length vector. Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> addresses this issue by using attention mechanism in the encoder-decoder architecture, where it focuses on the most relevant part of encoded information by learning to search over the encoded vector. In our work, we employ a sequenceto-sequence model with attention to parameterize our generative model. This model gets an incorrect program as input and maps it to many potential fixes by drawing samples on the estimated distribution of the fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational Autoencoders</head><p>The variational autoencoders <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref> is a generative model designed to learn deep directed latent variable based graphical models of large datasets. The model is trained on the data distribution by maximizing the variational lower bound of the log-likelihood as the objective function. Bowman et al. <ref type="bibr" target="#b4">[5]</ref> extend this framework by introducing an RNN-based variational autoencoder to enable the learning of latent variable based generative models on text data. The proposed model is successful in generating diverse and coherent sentences. To model conditional distributions for the structured output representation Sohn et al. <ref type="bibr" target="#b28">[29]</ref> extended variational autoencoders by introducing an objective that maximizes the conditional data log-likelihood. In our approach, we employ an RNN-based conditional variational autoencoder to model the distribution of the potential fixes given erroneous programs. Variational autoencoder approaches enable the efficient sampling of accurate and diverse fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning-based Program Repair</head><p>Recently there has been a growing interest in using learning-based approaches to automatically repair the programs <ref type="bibr" target="#b21">[22]</ref>. Long and Rinard <ref type="bibr" target="#b19">[20]</ref> proposed a probabilistic model by designing code features to rank potential fixes for a given program. Pu et al. <ref type="bibr" target="#b22">[23]</ref> employ an encoder-decoder neural architecture to automatically correct programs. In these works and many learning-based programming repair approaches, enumerative search over programs is required to resolve all errors. However, our proposed framework is capable of predicting the location and potential fixes by passing the whole program to the model. Besides this, unlike our approach, which only generates fixes for the given erroneous program, Pu et al. <ref type="bibr" target="#b22">[23]</ref> need to predict whole program statements to resolve the errors.</p><p>There are two important program repair tasks explored in the literature: fixing syntactic errors and fixing semantic ones. While in the current work we propose a technique for fixing syntactic errors, we believe that our observation about the diversity of the fix has implications for the approaches aimed at repairing semantic bugs as well. Most of the recent work in this domain aim to predict a unique fix, often extracted from a real-world repository. For example, Getafix <ref type="bibr" target="#b1">[2]</ref>, a recent approach for automatically repairing six types of semantic bugs, is evaluated on a set of 1,268 unique fixes written by developers. Similarly, DLfix <ref type="bibr" target="#b18">[19]</ref> considers a bug to be fixed only if it exactly matches a patch provided by the developer. While this is an improved methodology in the spirit of our proposal it is highly dependent on the performance of the test suite oracle which may not always capture the developer's intent.</p><p>DeepFix <ref type="bibr" target="#b12">[13]</ref>, RLAssist <ref type="bibr" target="#b11">[12]</ref>, and DrRepair <ref type="bibr" target="#b31">[32]</ref> uses neural representations to repair syntax errors in programs. In detail, DeepFix <ref type="bibr" target="#b12">[13]</ref> uses a sequenceto-sequence model to directly predict a fix for incorrect programs. In contrast, our generative framework is able to generate multiple fixes by learning the distribution of potential correctness. Therefore, our model does not penalize, but rather encourages diverse fixes. RLAssist <ref type="bibr" target="#b11">[12]</ref> repairs the programs by employing a reinforcement learning approach. They train an agent that navigate over the program to locate and resolve the syntax errors. In this work, they only address the typographic errors, rely on a hand-designed action space, and meet problems due to the increasing size of the action space. In contrast, our method shows improved performance on typographic errors and also generalizes to issues with missing variable declaration errors by generating diverse fixes.</p><p>In a recent work, Yasunaga and Liang <ref type="bibr" target="#b31">[32]</ref> proposed DrRepair to resolve the syntax error by introducing a program feedback graph. They connect the relevant symbols in the source code and the compile error messages and employ the graph neural network on top to model the process of the program repair. In this work, they rely on the compiler error messages which can be helpful, but it also limits the generality of the method. However, our proposed approach does not rely on additional information such as compiler error messages, and it resolves the errors by directly modeling the underlying distribution of the potential correct fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SampleFix: Generative Model for Diversified Code Fixes</head><p>Repairing the common program errors is a challenging task due to ambiguity in potential corrections and lack of representative data. Given a single erroneous program and a certain number of allowed changes, there are multiple ways to fix the program resulting in different styles and functionality. Without further information, the true, intended style and/or functionality remains unknown. In order to account for this inherent ambiguity, we propose a deep generative model to learn a distribution over potential fixes given the erroneous programin contrast to predicting a single fix. We frame this challenging learning problem as a conditional variational autoencoders (CVAE). However, standard sampling procedures and limitations of datasets and their construction make learning and generation of diverse samples a challenge. We address this issue by a beam search decoding scheme in combination with a novel regularizer that encourages diversity of the samples in the embedding space of the CVAE.  <ref type="figure" target="#fig_0">Figure 3</ref> provides an overview of our proposed approach at inference time. For a given erroneous program, the generative model draws T intermediate, candidate fixes? from the learned conditional distribution. We use a compiler to select a subset of promising intermediate candidate fixes based on the number of remaining errors. This procedure is applied iteratively until arrive at a set of candidate fixes within the maximum number of prescribed changes. We then select a final set of candidate fixes that compile, have unique syntax according to our measure described below (Subsection 3.5).</p><p>In the following, we formulate our proposed generative model with the diversity regularizer and provide details of our training and inference process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Conditional Variational Autoencoders for Generating Fixes</head><p>Conditional Variational Autoencoders (CVAE) <ref type="bibr" target="#b28">[29]</ref>, model conditional distributions p ? (y|x) using latent variables z. The conditioning introduced through z enables the modelling of complex multi-modal distributions. As powerful transformations can be learned using neural networks, z itself can have a simple distribution which allows for efficient sampling. This model allows for sampling from p ? (y|x) given an input sequence x, by first sampling latent variables? from the prior distribution p(z). During training, amortized variational inference is used and the latent variables z are learned using a recognition network q ? (z|x, y), parametrized by ?. In detail, the variational lower bound of the model (Equation 1) is maximized,</p><formula xml:id="formula_0">log(p(y|x)) ? E q ? (z|x,y) log(p ? (y|z, x)) ? D KL (q ? (z|x, y), p(z|x)).<label>(1)</label></formula><p>Penalizing the divergence of q ? (z|x, y) to the prior in Equation 1 allows for sampling from the prior p(z) during inference. In practice, the variational lower bound is estimated using Monte-Carlo integration,</p><formula xml:id="formula_1">L CVAE = 1 T T i=1 log(p ? (y|? i , x)) ? D KL (q ? (z|x, y), p(z|x)) .<label>(2)</label></formula><p>where,? i ? q ? (z|x, y), and T is the number of samples. We cast our model for resolving program errors in the Conditional Variational Autoencoder framework. Here, the input x is the erroneous program and y is the fix.</p><p>However, the plain CVAE as described in <ref type="bibr" target="#b28">[29]</ref> suffers from diversity issues. Usually, the drawn samples do not reflect the true variance of the posterior p(y|x). This would amount to the correct fix potentially missing from our candidate fixes. To mitigate this problem, next we introduce an objective that aims to enhance the diversity of our candidate fixes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Enabling Diverse Samples using a Best of Many Objective</head><p>Here, we introduce the diversity enhancing objective that we use. Casting our model in the Conditional Variational Autoencoder framework would enable us to sample a set of candidate fixes for a given erroneous program. However, the standard variational lower bound objective does not encourage diversity in the candidate fixes. This is because the average likelihood of the candidate fixes is considered. In detail, as the average likelihood is considered, all candidate fixes must explain the "true" fix in training set well. This discourages diversity and constrains the recognition network, which is already constrained to maintain a Gaussian latent variable distribution. In practice, the learned distribution fails to fully capture the variance of the true distribution. To encourage diversity, we employ "Many Samples" (MS) objective proposed by Bhattacharyya et al. <ref type="bibr" target="#b3">[4]</ref>,</p><formula xml:id="formula_2">L MS = log 1 T T i=1 p ? (y|? i , x) ? D KL (q ? (z|x, y), p(z|x)) .<label>(3)</label></formula><p>In comparison to Equation 2, this objective (Equation 3) encourages diversity in the model by allowing for multiple chances to draw highly likely candidate fixes. This enables the model to generate diverse candidate fixes, while maintaining high likelihood. In practice, due to numerical stability issues, we use "Best of Many Samples" (BMS) objective, which is an approximation of 3. This objective retains the diversity enhancing nature of Equation 3 while being easy to train,</p><formula xml:id="formula_3">L BMS = max i log(p ? (y|? i , x)) ? D KL (q ? (z|x, y), p(z|x)) .<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DS-SampleFix: Encouraging Diversity with a Diversity-sensitive Regularizer</head><p>To increase the diversity using Equation 4 we need to use a substantial number of samples during training. This is computationally prohibitive especially for large models, as memory requirements and computation time increases linearly in the number of such samples. On the other hand, for a small number of samples, the objective behaves similarly to the standard CVAE objective as the recognition network has fewer and fewer chances to draw highly likely samples/candidate fixes, thus limiting diversity. Therefore, in order to encourage the model to generate diverse fixes even with a limited number of samples, we propose a novel regularizer that aims to increase the distance between the two closest candidate fixes (Equation <ref type="formula" target="#formula_4">5</ref>). This penalizes generating similar candidate fixes for a given erroneous program and thus encourages diversity in the set of candidate fixes. In comparison to Equation 4, we observe considerable gains even with the use of only T = 2 candidate fixes. In detail, we maximize the following objectiv?</p><formula xml:id="formula_4">L DS-BMS = max i log(p ? (y|? i , x)) + min i,j d(? i ,? j ) ?D KL (q ? (z|x, y), p(z|x)) .<label>(5)</label></formula><p>Distance Metric. Here, we discuss the distance metric d in Equation <ref type="bibr" target="#b4">5</ref>. Note, that the samples ? i ,? j can be of different lengths. Therefore, we first pad the shorter sample to equalize lengths. Empirically, we find that the Euclidean distance performs best. This is mainly because, in practice, Euclidean distance is easier to optimize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Beam Search Decoding for Generating Fixes</head><p>Beam search decoding is a classical model to generate multiple outputs from a sequence-to-sequence model <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b6">7]</ref>. Given the distributions p ? (y|x) of a sequenceto-sequence model we can generate multiple outputs by unrolling the model in time and keeping the top-K tokens at each time step, where K is the beam width. In our generative model, we employ beam search algorithm to sample multiple fixes. In detail, we decode with beam width of size K for each sample z and in total for T samples from p(z). We set T = 100 during inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Selecting Diverse Candidate Fixes</head><p>We extend the iterative repair procedure introduced by Gupta et al. <ref type="bibr" target="#b12">[13]</ref> in the context of our proposed generative model, where the iterative procedure now leverages multiple candidate fixes. Given an erroneous program, the generative model outputs T candidate fixes. Each fix contains a potential erroneous line with the corresponding fix. So in each iteration we only edit one line of the given program. To select the best fixes, we take the candidate fixes and the input erroneous program, reconcile them to create T updated programs. We evaluate these fixes using a compiler, and select up to the best N fixes, where N ? T . We only select the unique fixes which do not introduce any additional error messages. In the next iterations, we feed up to N programs back to the model. These programs are updated based on the selected fixes of the previous iteration. We keep up to N programs with the lower number of error messages over the iterations. At the end of the repairing procedure, we obtain multiple potential candidate fixes. In the experiments where we are interested in a single repaired program, we pick the best fix with the highest probability score according to our deep generative model. To ensure a fair comparison, our generative model is based on the sequence-tosequence architecture, similar to Gupta et al. <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_1">Figure 4</ref> shows the architecture of our approach in detail. Note that the recognition network is available to encode the fixes to latent variables z only during training. All of the networks in our framework consists of 4-layers of LSTM cells with 300 units. The network is optimized using Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with the default setting. We use T = 2 samples to train our models, and T = 100 samples during inference. To process the program through the networks, we tokenize the programs similar to the setting used by Gupta et al. <ref type="bibr" target="#b12">[13]</ref>. During inference, the conditioning erroneous program x is input to the encoder, which encodes the program to the vector v. To generate multiple fixes using our decoder, the code vector v along with a sample of z from the prior p(z) is input to the decoder. For simplicity, we use a standard Gaussian N (0, I) prior, although more complex priors can be easily leveraged. The decoder is unrolled in time and output logits (p ? (y|? i , x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Model Architecture and Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our approach on the task of repairing common programming errors. We evaluate the diversity and accuracy of our sampled error corrections as well as compare our proposed method with the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the dataset published by Gupta et al. <ref type="bibr" target="#b12">[13]</ref> as it's sizable and includes real-world data. It contains C programs written by students in an introductory programming course. The dataset consists of 93 different tasks that were written by students in an introductory programming course. The programs were collected using a web-based system <ref type="bibr" target="#b5">[6]</ref>. These programs have token lengths in the range <ref type="bibr">[75,</ref><ref type="bibr">450]</ref>, and contain typographic and missing variable declaration errors. To tokenize the programs and generate training and test data different type of tokens, such as types, keywords, special characters, functions, literals and variables are used. The dataset contains two sets of data which are called synthetic and real-world data. The synthetic data contains the erroneous programs which are synthesized by mutating correct programs written by students. The real-world data contains 6975 erroneous programs with 16766 error messages.  We evaluate our approach on synthetic and real-world data. To evaluate our approach on the synthetic test set we randomly select 20k pairs. This data contains pairs of erroneous programs with the intended fixes. To evaluate our approach on real-world data we use a realworld set of erroneous programs. Unlike synthetic test set, we don't have access to the intended fix(es) in the real-world data. However, we can check the correctness of the program using the evaluator (compiler). Following the prior work, we train two networks, one for typographic errors and another to fix missing variables declaration errors. Note that there might be an overlap between the error resolved by the network for typographic errors and the network for missing variables declaration errors, so we also provide the overall results of the resolved error messages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>Synthetic Data. Real-World Data. In <ref type="table" target="#tab_0">Table 2</ref> we compare our approaches, with state-of-the-art approaches (DeepFix <ref type="bibr" target="#b12">[13]</ref>, RLAssist <ref type="bibr" target="#b11">[12]</ref>, and DrRepair <ref type="bibr" target="#b31">[32]</ref>) on the real-world data. In our experiments ( <ref type="table" target="#tab_0">Table 2)</ref> we show the performance of beam search decoding, CVAEs (SampleFix), and our proposed diversity-sensitive regularizer (DS-SampleFix). Furthermore, we show that DS-SampleFix can still take advantage of beam search algorithm (DS-SampleFix + BS). To do that, for each sample z we decode with beam width of size 5, and to sample 100 fixes we draw 20 samples from p(z). We also provide the sampling speed in terms of sampling 100 fixes for a given program using an average over 100 runs. The running time results show that CVAE-based models are at least 4x faster than beam search in sampling the fixes. In this experiment, we feed the programs up to 5 iterations. <ref type="table" target="#tab_0">Table 2</ref> shows that our approaches outperform DeepFix <ref type="bibr" target="#b12">[13]</ref>, RLAssist <ref type="bibr" target="#b11">[12]</ref>, and DrRepair <ref type="bibr" target="#b31">[32]</ref> in resolving the error messages. This shows that generating multiple diverse fixes can lead to substantial improvement in performance. Beam search, SampleFix, DS-SampleFix, and DS-SampleFix + BS resolve 63.9%, 56.3%, 61.0%, and 65.2% of the error messages respectively. Overall, our DS-SampleFix + BS is able to resolve all compile-time errors of the 45.2% of the programs -around 12% points improvement over DeepFix and 11% points improvement over DrRepair. Furthermore, the performance advantage of DS-SampleFix over SampleFix shows the effectiveness of our novel regularizer.</p><p>Note that DrRepair <ref type="bibr" target="#b31">[32]</ref> has achieved further improvements by relying on the compiler. While utilizing the compiler output seems to be beneficial, it also limits the generality of the approach. For a fair comparison, we report the performance of DrRepair without the compiler output, but consider informing our model by the compiler output an interesting avenue of future work. Qualitative Example. We illustrate diverse fixes generated by our DS-SampleFix in <ref type="figure" target="#fig_2">Figure 5</ref> using a code example with typographic errors, with the corresponding two output samples of DS-SampleFix. In the examples given in <ref type="figure" target="#fig_2">Figure 5</ref>, there is a missing closing curly bracket after line 13. We can see that DS-SampleFix generates multiple correct fixes to resolve the error in the given program. This indicates that our approach is capable of handling the inherent ambiguity and uncertainty in predicting fixes for the erroneous programs. The two fixes in <ref type="figure" target="#fig_2">Figure 5</ref> are unique and compileable fixes that implement different functionalities for the given erroneous program. Note that generating multiple diverse fixes gives the programmers the opportunity of choosing the desired fix(es) among the compileable ones, based on their intention.</p><p>Generating Functionally Diverse Programs. Given an erroneous program, our approach can generate multiple potential fixes that result in a successful compilation. Since we do not have access to the user's intention, it is desirable to suggest multiple potential fixes with diverse functionalities. Here, we evaluate our approach in generating multiple programs with different functionalities.</p><p>In order to assess different functionalities, we use the following approach based on tests. The dataset of Gupta et al. <ref type="bibr" target="#b12">[13]</ref> consists of 93 different tasks. The description of each task, including the input/output format, is provided in the dataset. Based on the input/output format, we can provide input examples for each task. To measure the diversity in functionality of the programs in each task, we generate 10 input examples. For instance, given a group of programs for a specific task, we can run each program using the input examples and get the outputs. We consider two programs to have different functionalities if they return different outputs given the same input example(s).</p><p>In order to generate multiple programs we use our iterative selecting strategy (Subsection 3.5). In each iteration, we keep up to N programs with the less number of error messages over the iterations. At the end of the repairing procedure, we obtain multiple repaired programs. As discussed <ref type="figure">(Figure 1)</ref>, a subset of these programs will successfully compile. In this experiment, we use the real-world test set, and we set N = 50 as this number is large enough to allow us to study the diversity of the fixes, without incurring an unnecessarily large load on our infrastructure. Our goals in the remaining of this section are: 1. For each erroneous program, to measure the number of generated unique fixes that successfully compile. 2. For each erroneous program, to measure the number of generated programs with different functionalities. <ref type="figure" target="#fig_4">Figure 6a</ref> and <ref type="figure" target="#fig_4">Figure 6b</ref> show the syntactic diversity of the generated programs, and the diversity in functionality of these programs, respectively. In <ref type="figure" target="#fig_4">Figure 6a</ref> we show the percentage of the successfully compiled programs with unique fixes for a given erroneous program. The x-axis refers to the number of generated and successfully compiled unique programs, and y-axis to the percentage of repaired programs for which these many unique fixes were generated. For example, for almost 20% of the repaired programs, DS-SampleFix + BS generates two  unique fixes. Overall, we can see that DS-SampleFix and DS-SampleFix + BS generate more diverse programs in comparison to the other approaches.  <ref type="figure" target="#fig_4">Figure 6b</ref> shows the percentage of the successfully compiled programs with different functionalities, for a given erroneous program. Here, the x-axis refers to the number of the generated functionally different programs, and y-axis refers to the percentage of erroneous programs with at least one fix, for which we could generate that many diverse fixes. One can observe that in many cases, e.g., up to 60% of the times for SampleFix, the methods generate programs corresponding to a single functionality. However, in many other cases they generate functionally diverse fixes. For example, in almost 10% of the cases, DS-SampleFix generate 10 or more fixes with different functionalities. In <ref type="figure" target="#fig_4">Figure 6b</ref> we can see that all of the approaches have higher percentage for generating program with the same functionality in comparison to the results in <ref type="figure" target="#fig_4">Figure 6a</ref>. This indicates that for some of the given erroneous programs, we generate multiple unique programs with approximately the same functionality. These results show that DS-SampleFix and DS-SampleFix + BS generate programs with more diverse functionalities in comparison to the other approaches.</p><p>In <ref type="table" target="#tab_3">Table 3</ref> we compare the performance of our approaches in generating diverse programs and functionalities. We provide results for all of our four approaches, i.e., Beam search (BS), SampleFix , DS-SampleFix , and DS-SampleFix + BS. We consider that an approach can generate diverse programs if it can produce two or more successfully compiled, unique programs for a given erroneous program. Similarly, we say that the approach produces functionally diverse programs if it can generate two or more programs with observable differences in functionality for a given erroneous program. Here we consider the percentage out of the total number of erroneous programs for which the model generates at least one successfully compiled program. The results of this table show that our DS-SampleFix + BS approach generates programs with more diverse functionalities in comparison to the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a novel approach to correct common programming errors. We recognize and model the inherent ambiguity and uncertainty when predicting multiple fixes. In contrast to previous approaches, our approach is able to learn the distribution over candidate fixes rather than the most likely fix. We achieve increased diversity of the sampled fixes by a novel diversity-sensitive regularizer. We show that our approach is capable of generating multiple diverse fixes with different functionalities. Furthermore, our evaluations on synthetic and real-world data show improvements over state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Overview of SampleFix at inference time, highlighting the generation of diverse fixes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Overview of network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>An example illustrating that our DS-SampleFix can generate diverse fixes. Left: Example of a program with a typographic error. The error, i.e., missing bracket, is highlighted at line 13. Right: Our DS-SampleFix proposes multiple fixes for the given error (line number with the corresponding fix), highlighting the ability of DS-SampleFix to generate diverse and accurate fixes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Diversity of the generated programs. (b) Diversity of the functionality of the generated programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 :</head><label>6</label><figDesc>The results show the performance of Beam search (BS), SampleFix , DS-SampleFix , and DS-SampleFix + BS. (a) Percentage of the number of the generated successfully compiled, unique programs for the given erroneous programs. (b) Percentage of the successfully compiled programs with different functionalities for the given erroneous programs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Results of performance comparison of DeepFix, RLAssist, DrRepair, Beam search (BS), SampleFix , DS-SampleFix, and DS-SampleFix + BS. Typo, Miss Dec, and All refer to typographic, missing variable declarations, and all of the error messages respectively. Speed denotes computational time for sampling 100 fixes. denotes successfully compiled programs, while refers to resolved error messages.</figDesc><table><row><cell>Models</cell><cell>Typo</cell><cell></cell><cell cols="2">Miss Dec</cell><cell>All</cell><cell></cell><cell>Speed (s)</cell></row><row><cell>DeepFix [13]</cell><cell cols="6">23.3% 30.8% 10.1% 12.9% 33.4% 40.8%</cell><cell>-</cell></row><row><cell>RLAssist [12]</cell><cell cols="2">26.6 % 39.7 %</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DrRepair [32]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>34.0%</cell><cell>-</cell><cell>-</cell></row><row><cell>Beam search (BS)</cell><cell cols="6">25.9% 42.2% 20.3% 47.0% 44.7% 63.9%</cell><cell>4.82</cell></row><row><cell>SampleFix</cell><cell cols="6">24.8% 38.8% 16.1% 22.8% 40.9% 56.3%</cell><cell>0.88</cell></row><row><cell>DS-SampleFix</cell><cell cols="6">27.7% 40.9% 16.7% 24.7% 44.4% 61.0%</cell><cell>0.88</cell></row><row><cell cols="7">DS-SampleFix + BS 27.8% 45.6% 19.2% 47.9% 45.2% 65.2%</cell><cell>1.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">Results of performance com-</cell></row><row><cell cols="2">parison of DeepFix, Beam search</cell></row><row><cell cols="2">(BS), SampleFix ,and DS-SampleFix</cell></row><row><cell cols="2">on synthetic data. Typo, Miss Dec,</cell></row><row><cell cols="2">and All refer to typographic, missing</cell></row><row><cell cols="2">variable declarations, and all of the</cell></row><row><cell cols="2">errors respectively.</cell></row><row><cell>Models</cell><cell>Typo Miss Dec All</cell></row><row><cell>DeepFix</cell><cell>84.7% 78.8% 82.0%</cell></row><row><cell cols="2">Beam search (BS) 91.8% 89.5% 90.7%</cell></row><row><cell>SampleFix</cell><cell>86.8% 86.5% 86.6%</cell></row><row><cell>DS-SampleFix</cell><cell>95.6% 88.1% 92.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>shows the comparison of our proposed approaches, Beam search (BS), SampleFix and DS-SampleFix, with DeepFix [13] on the synthetic data in the first iteration. In this table (Table 1), we can see that our approaches outperform DeepFix in generating intended fixes for the typographic and missing variable declaration errors. Beam search (BS), SampleFix and DS-SampleFix generate 90.7%, 86.6%, and 92.2% of the intended fixes respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of performance comparison of Beam Search (BS), SampleFix , DS-SampleFix , and DS-SampleFix +BS on generating diverse programs. Diverse Prog refers to the percentage of cases where the models generate at least two or more successfully compiled unique programs. Diverse Func denotes the percentage of cases where the models generate at least two or more programs with different functionalities.</figDesc><table><row><cell>Models</cell><cell cols="2">Diverse Prog Diverse Func</cell></row><row><cell>Beam search</cell><cell>55.6%</cell><cell>45.1%</cell></row><row><cell>SampleFix</cell><cell>44.6%</cell><cell>34.9%</cell></row><row><cell>DS-SampleFix</cell><cell>68.8%</cell><cell>53.4%</cell></row><row><cell>DS-SampleFix + BS</cell><cell>69.5%</cell><cell>60.4%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Getafix: learning to fix bugs automatically</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. ACM Program. Lang</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Accurate and diverse sampling of sequences based on a &quot;best of many&quot; sample objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGNLL Conference on Computational Natural Language Learning (CoNLL)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karkare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<title level="m">Prutor: A system for tutoring CS1 and collecting student programs for analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast, diverse and accurate image captioning guided by part-of-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aneja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Qlose: Program repair with quantitative objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>D&amp;apos;antoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CAV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The three pillars of machine programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gottschlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tatbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rinard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amarasinghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mattson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MAPL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automated program repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Goues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychoudhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Commun. ACM</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep reinforcement learning for programming language correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shevade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deepfix: Fixing common c language errors by deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Shevade</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised learning of hierarchical representations with convolutional deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dlfix: Context-based code transformation learning for automated program repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Software Engineering (ICSE)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Automatic patch generation by learning correct code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rinard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Automatic software repair: a bibliography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monperrus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CSUR)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">sk p: a neural program corrector for moocs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Barzilay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM SIGPLAN</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Programmers&apos; build errors: a case study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sadowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aftandilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowdidge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICSE</publisher>
		</imprint>
	</monogr>
	<note>at google</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Automated feedback generation for introductory programming assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solar-Lezama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PLDI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Is the cure worse than the disease? overfitting in automated program repair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Goues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Software Engineering</title>
		<imprint>
			<publisher>ESEC/FSE</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Diverse and accurate image description using a variational auto-encoder with an additive gaussian encoding space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graph-based, self-supervised program repair from diagnostic feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
