<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Zhang</surname></persName>
							<email>binbinzhang@mobvoi.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoyuan</forename><surname>Yao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Wang2</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yu2</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">WeNet Open Source Community</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyong</forename><surname>Guo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Audio, Speech and Language Processing Group (ASLP@NPU)</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mobvoi Inc</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: streaming speech recognition</term>
					<term>two-pass</term>
					<term>dy- namic chunk</term>
					<term>U2</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are particularly modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming manner. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes negligible sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.33% CER with 640ms latency in a streaming ASR setup.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>End-to-end (E2E) models have gained more and more attention to speech recognition over the last few years. E2E models combine the acoustic, pronunciation and language models into a single neural network, showing competitive results compared to conventional ASR systems. There are mainly three popular E2E approaches, namely CTC <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, recurrent neural network transducer (RNN-T) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> and attention based encoder-decoder (AED) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. They all have advantages and limitations in terms of recognition accuracy and application scenario, and many efforts have been paid for comparison of these models <ref type="bibr" target="#b7">[8]</ref> or join some of them into one model <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>While these models have great performance in a none streaming application, it usually requires a lot of work or a lot of accuracy degradation to make the model work in a streaming way, and a lot of works have been done for that. While RNN-T has the streaming ability in nature, with more suprior performance, AED models have to be modified to realize streaming function. For RNN-T, a two pass <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> method was proposed to close the accuracy gap to the non-streaming Listen, Attend, Spell(LAS) model. For AED, Hard Monotonic Attention <ref type="bibr" target="#b11">[12]</ref> is first proposed for monotonically align the input and output of the AED model, and then it could work in a stream-ing way. With the same idea, Monotonic Chunkwise Attention(MoChA) <ref type="bibr" target="#b12">[13]</ref> and Monotonic Multihead Attention <ref type="bibr" target="#b13">[14]</ref> were proposed to further improve performance and stability of monotonic attention.</p><p>Recently there have been also increasing interests in unifying non-streaming and streaming speech recognition models into one model. Some transducer based models such as Y-model <ref type="bibr" target="#b14">[15]</ref> and UNIVERSAL ASR <ref type="bibr" target="#b15">[16]</ref> have been designed for this goal with good performance. The unified model not only reduces the accuracy gap between the streaming model and the non-streaming counterpart, but also alleviates the burden of model development, training and deployment.</p><p>In this work, we propose a new framework namely U2 to unify non-streaming and streaming speech recognition. Our framework is based on the hybrid CTC/attention architecture with conformer blocks. The training process is simple and it avoids the RNN-T model's complicated tricks and instability issues. To support streaming, we modify the conformer block while bringing negligible performance degradation. In further, by using a dynamic chunk training strategy, our framework allows users to control the latency at inference time. Our results show that U2 achieves state-of-the-art streaming accuracy on the public Aishell-1 dataset <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Hybrid CTC/attention end-to-end ASR in <ref type="bibr" target="#b8">[9]</ref> adopted both CTC and attention decoder loss in training to achieve fast convergence and to improve robustness of the AED model. However, during decoding, it combined attention score and CTC score and performs joint decoding. Both of the scores can only be computed until the whole speech utterance is available, which makes it apparently a non-streaming model. Two pass based model <ref type="bibr" target="#b9">[10]</ref> was proposed on RNN-T and achieves comparable accuracy to a LAS model. However, RNN-T training is very memory consuming <ref type="bibr" target="#b17">[18]</ref> so that we can not use a large batch for training on typical GPUs, which results in a very slow training speed as well as poor performance. Besides, RNN-T training is also unstable. CTC pre-training <ref type="bibr" target="#b19">[19]</ref> and Cross Entropy(CE) pre-training were proposed in <ref type="bibr" target="#b20">[20]</ref> to assist RNN-T training, while pre-training was also tricky and complicated. These increase the difficulty of using RNN-T in speech recognition application, especially when lacking computing and research resources. And it was pointed out that training directly from scratch is unstable using combined RNN-T loss and LAS loss, so a three-step training strategy was proposed in <ref type="bibr" target="#b9">[10]</ref> to solve the problem, which further complicates the training pipeline.</p><p>For unified non-streaming and streaming model, Y-model uses variable context at training and several optional contexts at inference. However, the optional contexts are predefined at the training stage, and the contexts have to be carefully designed in terms of the number of encoder layers, the kernel size of the convolution operation. Moreover dual-mode only has one streaming configuration in both training and inference. If we want another streaming model with different latency at inference, the model needs to be totally retrained. Besides, both Y-model and dual-mode are RNN-T based models. They have the same drawbacks as RNN-T.</p><p>Our proposed U2, a CTC-AED joint model, is trained by combined CTC and AED loss and dynamic chunk attention. It not only unifies the non-streaming and streaming model, giving promising result, but also significantly simplifies the training pipeline, as well as dynamically controls the trade-off between latency and accuracy in streaming applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">U3</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model architecture</head><p>The proposed three-pass architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It contains three parts, a Shared Encoder, a CTC Decoder and a Attention Decoder. The Shared Encoder consists of multiple Transformer <ref type="bibr" target="#b22">[21]</ref> or Conformer <ref type="bibr" target="#b23">[22]</ref> encoder layers. The CTC Decoder consists of a linear layer and a log softmax layer, The CTC loss function is applied over the softmax output in training. The Attention Decoder consists of multiple Transformer decoder layers. We can make the Shared Encoder only see limited right contexts, then CTC decoder could run in a streaming mode in the first pass. In the second pass, the output of the Shared Encoder and CTC Decoder can be used in different ways. The training and decoding processes are detailed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>rescoring CTC Decoder</head><p>Attention Decoder attention Shared Encoder </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Combined Loss</head><p>The training loss is combined with CTC loss and AED loss as listed in 1, where x is the acoustic feature, y is the corresponding annotation, LCTC (x, y), LAED (x, y) are the CTC and AED loss respectively, ? is a hyperparameter which balance the importance of CTC and AED loss. Unlike RNN-T based two pass in <ref type="bibr" target="#b9">[10]</ref> where a three step process was used to stable the training, we can directly train our model by the combined loss from scratch, which significantly simplify our training pipeline. And As shown in <ref type="bibr" target="#b8">[9]</ref>, the combined loss also help the model converge faster and have better performance.</p><formula xml:id="formula_0">Lcombined (x, y) = ?LCTC (x, y)+(1??)(LAED-L (x, y)+LAED-R (x, y))<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Dynamic Chunk Training</head><p>A Dynamic chunk training technique is proposed in this section to unify the none streaming and streaming model and enable latency control. As described before, our U2 could only be streaming when the Shared Encoder is streaming. Full self attention is used in standard Transformer encoder layers, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (a), every input at time t depends on the whole inputs, green means there is a dependency, while white means there is no dependency. The simplest way to stream it is to make the input t only see itself and the input before t, namely left attention, seeing no right context, as shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b), but there is very big degradation compared to full context model. Another common technique is to limited input t only see a limited right context t + 1, t + 2, ..., t + W , where W is the right context for each encoder layer, and the total context is accumulated through all the encoder layers, for example, if we have N encoder layers, each has W right context, the total context is N * W . Right context usually improves performance compared to pure left attention, however, we should carefully design the number of layers and every right context for each layer to control the final right context for the whole model, and things get more difficult when we use conformer encoder layer, in which convolution through time with right context is used. We adopt a chunk attention in  <ref type="figure" target="#fig_1">Figure 2</ref> (c), we split the input to several chunks by a fixed chunk size C, the dark green is for the current chunk, for each chunk we have inputs [t+1, t+2, ..., t+C], every chunk depends on itself and the all the previous chunks. Then the whole latency of the encoder depends on the chunk size, which is easy to control and implement. We can train the model using a fixed chunk size, we call it static chunk training, and decoding with the same chunk.</p><p>Motivated by the idea of unified E2E model, we further propose a dynamic chunk training. We can use dynamic chunk size for different batches in training, the dynamic chunk size range is a uniform distribution from 1 to max utterance length, namely the attention varies from left context attention to full context attention, and the model captures different information on various chunk size, and learns how to do accurate prediction when different limited right context provided. We call the chunks which sizes are from 1 to 25 as streaming chunk for streaming model and size which is max utterance length as none streaming chunk for none streaming model. However, the results of this method is not good enough, so next we change the distribution of chunk size during training process as follows. chunksize = lmax x &gt; 0.5 l ? U (1, min(25, lmax ? 1)) x ? 0.5</p><p>(2) As shown in Equation 2, x is sampled from 0 to 1.0 in each batch during the training process, lmax is the max utterance length of current batch, and U is a uniform distribution. So the distribution of chunk size changed, half is full chunk for none streaming, and the other half from 1 to 25 is used for streaming.</p><p>Our later experiments will show, this is a simple but efficient way, the model trained by dynamic chunk size has a comparable performance compared to static chunk training.</p><p>Besides this batch level method, we also tried epoch level -using full chunk for the first half epochs and streaming chunk for the second half or in turn. But these strategies do not work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Causal Convolution</head><p>The convolution units in conformer consider both left and right context. The total right context depends on convolution layer's context and the stack number of conformer layers. So this structure not only brings in additional latency, but also ruin the benefits of chunk-based attention, that the latency is independent on the network structure and could be just controlled by chunk at inference time. To overcome this issue, we use casual convolution <ref type="bibr" target="#b14">[15]</ref> instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoding</head><p>The Shared Encoder consumes the audio feature chunk by chunk. The larger chunk size usually means higher latency and better accuracy and the maximum latency is proportional to the frame number of one chunk. The proper decoding chunk size depends on specific task requirements.</p><p>The CTC Decoder outputs first pass hypotheses in a streaming way. At the end of the input, the Attention Decoder uses full context attention to get better results. Two different modes are explored here:</p><p>? Attention Decoder mode. The CTC results are ignored in this mode. Attention Decoder generate outputs in an auto-regressive way with the attention of the output of Shared Encoder.</p><p>? Rescoring mode. The n-best hypotheses from CTC are scored by the Attention Decoder with the output of the Shared Encoder in a teacher-forcing mode. The best rescored hypothesis is used as the final result. This mode avoids the auto-regressive process and achieves better real-time factor(RTF). Besides, the CTC scores could be weighted combined to get a better result in a simple way.</p><formula xml:id="formula_1">SCORESfinal = ? * SCORESCTC + SCORESattention (3)</formula><p>In order to get a better result, ctc weighted score was added during rescoring mode decoding as shown in <ref type="bibr">Equation 3</ref>, and our later experiments will show that it is always beneficial to decoding results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In order to evaluate our proposed U2, we carried out our experiments on the open-source Chinese Mandarin speech corpus AISHELL-1 <ref type="bibr" target="#b16">[17]</ref>, which contains a 150-hour training set, a 10hour develoment set and a 5-hour test set. The test set contains 7176 utterances in total. We use wenet 1 end-to-end speech recognition toolkit for all experiments.</p><p>We use the state-of-the-art ASR network-Conformer <ref type="bibr" target="#b23">[22]</ref> as our shared encoder, and the decoder part is the same as the traditional transformer decoder. Conformer adds convolution module on the basis of transformer so that it can model both local and global context and results in better results on different ASR tasks. As for the dynamic chunk training of the conformer model, causal convolution is used instead in the experiments making our encoder is independent to the right context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">AISHELL-1 Task</head><p>For AISHELL-1, we use 80 dimensional log-mel filter bank (FBank) splice 3 dimensional pitch computed on 25ms window with 10ms shift as feature. And we do speed perturb with 0.9, 1.0 1.1 on the whole data to generate 3-fold speed changes. SpecAugment <ref type="bibr" target="#b24">[23]</ref> is applied with 2 frequency masks with maximum frequency mask (F = 10), and 2 time masks with maximum time mask(T = 50). Two convolution sub-sampling layers with kernel size 3*3 and stride 2 is used in the front of the encoder, namely 4 times sub-sampling in total. For encoder, we use 12 conformer layers with 4 multi head attention. For the Attention Decoder, we use 6 transformer layers with 4 multi head attention. Each conformer layer uses 256 attention dimension and 2048 feed forward dimension. Accumulating grad was also used to stabilize training, and we update parameters every four steps. Attention dropout, feed forward dropout and label smoothing regularization are applied in each encoder and decoder layer in order to prevent over-fitting. We use Adam optimizer and transformer learning rate schedule with 25000 warmup steps to train models. Moreover, we get our final model by averaging the top 10 best models which have a lower loss on the dev set at the training. First, we explore different decoding methods on a none streaming model, in which full context and a conformer with standard convolution kernel size 15 are used in training, to ensure both CTC and AED decoder give a reasonable result. For AED decoder, we use beam 10 for decoding. We use prefix beam search for CTC, which is used for generating top-n different hypothesises for later rescoring. As shown in the <ref type="table" target="#tab_0">Table 1</ref>, the attention rescoring result outperforms both CTC prefix beam search and attention decoder results, which is out of our expectation.</p><p>After analyzing the decoding results of CTC prefix beam search and attention rescoring, we found that a lot of wrong results generated by CTC prefix beams search could be corrected by attention rescoring, However some good cases in were false corrected after attention rescoring, which means CTC plays an Since standard attention decoder is running in an autoregressive fashion, which is time consuming, while attention rescoring just uses attention decoder for rescoring, it can be processed in parallel, and it should be faster in theory. So here we also investigate the RTF of both attention decoder and attention rescoring method, and single thread is used during decoding in Pytorch. As expected in <ref type="table" target="#tab_0">Table 1</ref>, we got 2.40 times speed up by attention rescoring compared to attention decoder in decoding.</p><p>To conclude here, we can see the attention rescoring is both faster and more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Dynamic chunk evaluation</head><p>As mentioned before, causal convolution is used in dynamic chunk training to unify none streaming and streaming model, and a kernel size of 8 is used, which is half of the previous experiment since the model is limited to see left context only here.</p><p>In order to compare with static chunk training, we trained the five different models with different static chunk size full/16/8/4/1, and then decode with the same chunk size as our baseline. And we trained only one unified model with the aforementioned dynamic chunk strategy as in Equation 2. The result is shown in <ref type="table" target="#tab_1">Table 2</ref>, and we mainly pay our attention to the attention rescoring result here since it's the final performance of our system. As we can see from the table, dynamic chunk trained model has a little degradation on full chunk and chunk 1, which are the two boundary points of the dynamic chunk with infinite latency and no latency respectively. We guess it's more difficult to learn boundary information in the unified model. However, we see a slight gain over static chunk trained model when chunk size is 16/8/4, which means dynamic chunk strategy benefits the unified model by varying chunk training in this case. Overall, the dynamic chunk trained model is comparable static chunk trained models, so we can easily unify the none streaming model and streaming model into one single model  <ref type="table" target="#tab_2">Table 3</ref> lists several published streaming solutions on AISHELL-1 test set, including Sync-Transformer <ref type="bibr" target="#b25">[24]</ref>, SCAMA <ref type="bibr" target="#b26">[25]</ref>, and MMA <ref type="bibr" target="#b13">[14]</ref>. ? is the additional latency introduced by attention rescoring at the end of decoding in our U2, but it's fast enough as we have talked before it can be paralleled into one batch computing, and it's 50-100ms as analysed in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref>. We can see our U2 has far surpassed other solutions with a small additional latency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">Comparison to other solutions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">15,000-hour Tasks</head><p>We extend our experiments on a mixed 15,000-hour dataset which collected several domains, all in Mandarin, including variety show, talk show, tv soap, podcast, and radio. The same acoustic features as mentioned in Section 4.1 was used. First, we trained a conventional full attention conformer model which uses the same layers mentioned in 4.1 but uses 384 attention units. For the second experiment, we trained a u2 model which parameters are the same as the first experiment using the method mentioned in Section 3. Three test set was used to evaluate models including AISHELL-1, tv domain, and conversation domain. The results are reported in the . U2 gets comparable results to EXP1 baseline and even better on AISHELL-1 test set when using full attention during inference. Though both of convolution and self-attention in conformer encoder was limited to the current and left context when chunk size is 16 during inference, CER does not appear obvious decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a framework to train a single model which can do recognition in both streaming and full context way. This framework can be trained directly and stably without complicated training process. A fast weighted re-score method is used to get full-context performance with little additional latency. We also propose a dynamic chunk based strategy to improve the model performance and enable trading off the latency and accuracy conveniently at inference time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Two pass CTC and AED joint architecture3.2. Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Full attention, Left attention, Chunk Attention this work, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Decoding method comparison</figDesc><table><row><cell>decoding method</cell><cell cols="2">CTC weight RTF</cell><cell>CER</cell></row><row><cell>attention decoder</cell><cell>/</cell><cell cols="2">0.197 4.92</cell></row><row><cell cols="2">ctc prefix beam search /</cell><cell>/</cell><cell>4.93</cell></row><row><cell>attention rescoring</cell><cell>0.0</cell><cell>/</cell><cell>4.72</cell></row><row><cell>attention rescoring</cell><cell>0.5</cell><cell cols="2">0.082 4.64</cell></row><row><cell>4.1.1. Decoding Method</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Dynamic vs static chunk training</figDesc><table><row><cell>training method</cell><cell>decoding mode</cell><cell>full</cell><cell>decoding chunk size 16 8 4</cell><cell>1</cell></row><row><cell></cell><cell>attention decoder</cell><cell cols="3">5.35 5.95 5.99 6.15 6.36</cell></row><row><cell>static chunk training, static chunk inference</cell><cell cols="4">ctc prefix beam search 5.18 6.30 6.50 6.69 6.73</cell></row><row><cell></cell><cell>attention rescoring</cell><cell cols="3">4.86 5.55 5.78 6.06 6.02</cell></row><row><cell></cell><cell>attention decoder</cell><cell cols="3">5.27 5.51 5.67 5.72 5.88</cell></row><row><cell>dynamic chunk training, static chunk inference</cell><cell cols="4">ctc prefix beam search 5.49 6.08 6.41 6.64 7.58 attention rescoring 4.90 5.33 5.52 5.71 6.23</cell></row><row><cell cols="2">important role in some cases. So we added CTC weight dur-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ing attention rescoring as Equation 3. And we tested different</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CTC weights from 0.1 to 0.9, all of them helps attention rescor-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">ing in our experiments, and 0.5 is the most stable one. Here</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">we just show the result when CTC weight is 0.5, as we see,</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">when combining with CTC weight, the CER can be further re-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">duced to 4.72. To our knowledge, it's the best published result</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">on AISHELL-1. And 0.5 is the default CTC weight of attention</cell><cell></cell><cell></cell><cell></cell></row><row><cell>rescoring mode in our later experiments.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison to other streaming solutions</figDesc><table><row><cell>model</cell><cell cols="3">params(M) latency(ms) CER</cell></row><row><cell cols="2">Sync-Transformer[24] /</cell><cell>400</cell><cell>8.91</cell></row><row><cell>SCAMA[25]</cell><cell>43</cell><cell>600</cell><cell>7.39</cell></row><row><cell>MMA[14]</cell><cell>/</cell><cell>640</cell><cell>6.60</cell></row><row><cell>U2</cell><cell>47</cell><cell>320+?</cell><cell>5.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison U2 and static full attention on a 15000hour Mandarin speech recognition task</figDesc><table><row><cell>test set</cell><cell>EXP1</cell><cell cols="2">EXP2 full</cell><cell>16</cell></row><row><cell>aishell</cell><cell>3.96</cell><cell>3.70</cell><cell cols="2">4.41</cell></row><row><cell>tv</cell><cell cols="4">10.92 11.96 13.51</cell></row><row><cell cols="5">conversation 12.95 14.01 15.35</cell></row><row><cell cols="5">by our U2 framework via the two pass decoding and dynamic</cell></row><row><cell>chunk training.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/wenet-e2e/wenet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.3711</idno>
		<title level="m">Sequence transduction with recurrent neural networks</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end continuous speech recognition using attention-based recurrent nn: First results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1602</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Listen, attend and spell</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01211</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention-based models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="577" to="585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A comparison of sequence-to-sequence models for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="939" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Joint ctc-attention based end-to-end speech recognition using multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4835" to="4839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Two-pass endto-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rybach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Visontai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strohman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10992</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A streaming on-device end-to-end model surpassing server-side conventional model quality and latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bruguier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6059" to="6063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Online and linear-time attention by enforcing monotonic alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00784</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Monotonic chunkwise attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05382</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Enhancing monotonic multihead attention for streaming asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kawahara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09394</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Transformer transducer: One model unifying streaming and non-streaming speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03192</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Universal asr: Unify and improve streaming asr with full-context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06030</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
	<note>20th Conference of the Oriental Chapter</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving rnn transducer modeling for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="114" to="121" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prabhavalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exploring pre-training with alignments for rnn transducer based end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint/>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="7079" to="7083" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Synchronous transformers for end-to-end speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7884" to="7888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Streaming chunk-aware multihead attention for online end-toend speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.01712</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
