<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discrete and Continuous Action Representation for Practical RL in Video Games</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Delalleau</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ubisoft La Forge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Peter</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ubisoft La Forge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eloi</forename><surname>Alonso</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ubisoft La Forge</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Logut</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ubisoft La Forge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Discrete and Continuous Action Representation for Practical RL in Video Games</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While most current research in Reinforcement Learning (RL) focuses on improving the performance of the algorithms in controlled environments, the use of RL under constraints like those met in the video game industry is rarely studied. Operating under such constraints, we propose Hybrid SAC, an extension of the Soft Actor-Critic algorithm able to handle discrete, continuous and parameterized actions in a principled way. We show that Hybrid SAC can successfully solve a highspeed driving task in one of our games, and is competitive with the state-of-the-art on parameterized actions benchmark tasks. We also explore the impact of using normalizing flows to enrich the expressiveness of the policy at minimal computational cost, and identify a potential undesired effect of SAC when used with normalizing flows, that may be addressed by optimizing a different objective.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Reinforcement Learning (RL) applications in video games have recently seen massive advances coming from the research community, with agents trained to play Atari games from pixels <ref type="bibr" target="#b16">(Mnih et al. 2015)</ref> or to be competitive with the best players in the world in complicated imperfect information games like DOTA 2 (OpenAI 2018) or StarCraft II <ref type="bibr" target="#b21">(Vinyals et al. 2019a;</ref><ref type="bibr" target="#b22">2019b)</ref>. These systems have comparatively seen little use within the video game industry, and we believe lack of accessibility to be a major reason behind this. Indeed, really impressive results like those cited above are produced by large research groups with computational resources well beyond what is typically available within video game studios.</p><p>Our contributions are geared towards industry practitioners, by sharing experiments and practical advice for using RL with a different set of constraints than those met in the research community. For example, in the industry, experience collection is usually a lot slower, and there are time budget constraints over the runtime performance of RL agents. We thus favor off-policy algorithms to improve data efficiency by re-using past experience, and constrain our architectures  Workshop on Reinforcement Learning in Games * Equal contribution, alphabetical order 1 Now at Facebook AI Research to relatively small feedforward networks. The approach we propose in this paper is based on Soft Actor-Critic <ref type="bibr" target="#b8">(Haarnoja et al. 2018b)</ref>, which was originally designed for continuous actions problems. We explore ways to extend it to a hybrid setting with both continuous and discrete actions, a situation commonly encountered in video games. We also attempt to use normalizing flows (Rezende and Mohamed 2015) to improve the quality of the resulting policy with roughly the same number of parameters, and analyze why this approach may not be working as well as we initially expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background and related work</head><p>We consider the classical Markov Decision Process (MDP) setting where at each discrete time step t the agent observes a state s t and must take an action a t ? ?(a t |s t ), where ? is the agent's policy. On the next time step, the environment transitions to the new state s t+1 ? P (s t+1 |s t , a t ) and gives the agent a reward r t ? P (r t |s t , a t , s t+1 ). The agent's objective is to find an optimal policy ? * that maximizes the expected discounted return E ? t ? t r t , where ? ? [0, 1] is the discount factor.</p><p>In the following, we assume that a state is represented by a real-valued vector, in a format suitable to be provided as input to a neural network (e.g. with one-hot encoding of discrete state variables, and normalization of continuous features). Actions may be either discrete, continuous, or a mix of both: a key contribution of this paper is to present a simple generic approach to action representation, suitable for most situations one may encounter when training game-playing agents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft Actor-Critic</head><p>Soft Actor-Critic (SAC) <ref type="bibr" target="#b8">(Haarnoja et al. 2018b;</ref><ref type="bibr" target="#b9">2018c</ref>) is a state-of-the-art model-free algorithm that was originally proposed for continuous control tasks. It is based on the idea of adding an entropy bonus to the objective optimized by the agent, i.e. maximizing E ? t ? t r t + ?H(?(?|s t )) . A higher ? encourages the agent to take actions that are more random, which in particular can help with exploration. This ? parameter can be learned during training by setting a target entropy for the policy <ref type="bibr" target="#b9">(Haarnoja et al. 2018c</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalizing flows</head><p>Normalizing flows (Rezende and Mohamed 2015) are invertible transformations applied on top of an initial distribution to transform it into another distribution, usually with the goal of making it more expressive. The original SAC <ref type="bibr" target="#b8">(Haarnoja et al. 2018b</ref>) parameterizes the actor using a spherical Gaussian and uses the reparameterization trick to backpropagate through the parameters of the distribution. It is possible to apply normalizing flows on top of this Gaussian policy to make it more expressive <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref>, while still being able to sample from the policy as well as compute the log-density at any point. This makes it possible to use normalizing flows to reparameterize the actor in SAC to get more complex policies while keeping the training algorithm unchanged. <ref type="bibr" target="#b19">(Tang and Agrawal 2018)</ref> show that using an Inverse Autoregressive Flow (IAF) for on-policy trust region policy optimization can significantly improve exploration in high-dimensional tasks. <ref type="bibr" target="#b23">(Ward, Smofsky, and Bose 2019)</ref> use Real-valued Non Volume Preserving (Real NVP) flows to improve exploration in sparse reward settings, while <ref type="bibr" target="#b7">(Haarnoja et al. 2018a</ref>) use Real NVP flows to train maximum entropy policies in a hierarchical setting where each layer is trained on its own reward function. Our experiments with normalizing flows are similar to and inspired by <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref>, who suggest that normalizing flows can be used to improve the expressiveness of policies in SAC to get a policy with the same level of quality using less parameters.</p><p>As suggested in <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref>, we use radial flows in our experiments. We sample ? ? N (0, 1) (since we use the reparameterization trick), and denote by h ? the function returning a sample from a Gaussian distribution with mean and standard deviation given by the policy parameterized by ?. With {f ?i } N i=1 the set of normalizing flows, we can sample from the policy as follows:</p><formula xml:id="formula_0">w 0 = h ? (?, s t ) w i = f ?i ? f ?i?1 ? ... ? f ?1 (w 0 ) a t = tanh(w N )</formula><p>We denote by q 0 the density of the state-dependent Gaussian distribution w 0 is sampled from. The density of the policy is then tractable according to:</p><formula xml:id="formula_1">log ?(a t , s t ) = log q 0 (w 0 )? N i=1 log det ?f ?i (w i?1 ) ?w i?1<label>(1)</label></formula><p>The equations corresponding to the radial flows are taken from (Rezende and Mohamed 2015) and can be found in the Appendix <ref type="table" target="#tab_2">(Table 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixing discrete and continuous actions</head><p>Most reinforcement learning research papers focus on environments where the agent's actions are either discrete or continuous. However, when training an agent to play a video game, it is common to encounter situations where actions have both discrete and continuous components. Typical examples include:</p><p>? Playing with the same inputs as a player, whose controller may be equipped with both an analog stick (providing a range of continuous values) and buttons that can be pressed (yielding potentially many discrete actions through the various button combinations).</p><p>? Letting the agent choose among a set of high-level discrete actions (ex: move, jump, fire), each of them being associated with continuous parameters (ex: target coordinates for the move action, direction for the jump action, aiming angle for the fire action).</p><p>? Wanting the agent to control systems that have both discrete and continuous components, like driving a car by combining steering and acceleration (both continuous) with usage of the hand brake (a discrete binary action).</p><p>Such situations require algorithms that are able to handle a combination of discrete and continuous actions. In what follows, we propose a parameterization of the policy that can be easily implemented in SAC, yielding a powerful generic off-policy RL algorithm for training game-playing agents.</p><p>In order to deal with a mix of discrete and continuous action components, a first approach would be to use a fully continuous actor and somehow find a way to convert part of its continuous output into discrete actions <ref type="bibr" target="#b20">(van Hasselt and Wiering 2009;</ref><ref type="bibr" target="#b10">Hausknecht and Stone 2016;</ref><ref type="bibr" target="#b5">Cianflone et al. 2019</ref>). Alternatively, one may use instead a fully discrete actor by discretizing the continuous actions, taking special care to prevent their number from exploding <ref type="bibr" target="#b15">(Metz et al. 2017;</ref><ref type="bibr" target="#b1">Andriotis and Papakonstantinou 2018;</ref><ref type="bibr" target="#b19">Tang and Agrawal 2019)</ref>.</p><p>What we would like instead is a method that would naturally incorporate both discrete and continuous actions within the same algorithm (SAC) in a principled way. In order to accommodate for the wide range of potential ways for an agent to interact with a video game environment, we generalize several existing ideas regarding action representation. We first describe below our proposed generic setting, then relate it to specific examples from the literature.</p><p>We denote an agent action a by a combination of discrete components a d = (a d 1 , . . . , a d D ) and continuous components a c = (a c 1 , . . . , a c C ). Each a d i is an integer between 1 and K i , and represents the i-th discrete action that can be taken by the agent. Each a c j is an m j -dimensional continuous vector in X j ? R mj , and represents its j-th continuous action. Discrete components are assumed to be independent given the observed state s, while continuous components are independent given both s and the discrete actions, yielding the following decomposition: ?(a|s) = ?(a d |s)?(a c |s, a d ) = ?(a d 1 |s) . . . ?(a d D |s)?(a c 1 |s, a d ) . . . ?(a c C |s, a d ) = ? i ?(a d i |s)? j ?(a c j |s, a d ) Here we slightly abuse notations by using the same letter ? to denote both discrete probability mass functions and probability density functions applied to different components of the action. A rigorous treatment would rely on measure theory but is beyond the scope of this paper. We observe that many classical action representations fit the above decomposition:</p><p>1. A single discrete action taken in the set 1, . . . , K, as in Atari games <ref type="bibr" target="#b2">(Bellemare et al. 2013</ref>). Here D = 1, K 1 = K and C = 0. This yields ?(k|s) = ?(a d 1 = k|s)</p><p>2. C independently sampled 1D continuous actions, as is typically done in continuous control tasks when computing a c j = tanh(? j (s) + ?? j (s)) with ? sampled from a standard normal distribution <ref type="bibr" target="#b8">(Haarnoja et al. 2018b)</ref>. Here D = 0, C is the total dimension of the continuous action space, and m j = 1 for all j. This yields</p><formula xml:id="formula_2">?(x|s) = ? C j=1 ?(a c j = x j |s)</formula><p>3. A single m-dimensional continuous action vector, getting rid of the independence constraint from the previous case #2. This can be achieved for instance with normalizing flows, where the continuous distribution being learned does not need to be be axis aligned anymore <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref>. Here D = 0, C = 1 and m 1 = m. This yields</p><formula xml:id="formula_3">?(x|s) = ?(a c 1 = x|s)</formula><p>4. An m-dimensional continuous action whose value should depend on a discrete action taken in 1, . . . , K, as proposed for parameterized action spaces by <ref type="bibr" target="#b24">(Wei, Wicke, and Luke 2018)</ref>. Here (in the general case with no independence assumption on the individual continuous components), this means that D = 1, K 1 = K, C = 1 and m 1 = m. This yields ?(k, x|s) = ?(a d 1 = k|s)?(a c 1 = x|s, a d 1 = k)</p><p>5. An alternative action representation for parameterized action spaces, where the agent takes a discrete action in 1, . . . , K, and each discrete action k is parameterized by a different continuous m k -dimensional vector. This is similar to what has been used e.g. by <ref type="bibr" target="#b3">(Bester, James, and Konidaris 2019)</ref>. Here D = 1, K 1 = K, C = K and m k is the dimension of the parameter being used when the discrete action is k. This yields ?(k, x k |s) = ?(a d 1 = k|s)?(a c k = x k |s) The difference compared to the previous formulation #4 is that instead of using a single continuous parameter whose value depends on the discrete action being taken, we create multiple independent continuous parameters (one for each discrete action). Since each continuous parameter a c k is only used when the agent takes its associated discrete action k, its value does not need to depend on the discrete action chosen by the agent, which is why ?(a c k = x k |s) does not need to be conditioned on a d 1 . 6. A set of D discrete components, with each component a d i (1 ? i ? D) being a discrete action taken in (1, . . . , K i ). Such a representation has been used in particular by <ref type="bibr" target="#b19">(Tang and Agrawal 2019)</ref> to tackle continuous control tasks by discretizing each continuous dimension i into K i discrete bins. In this example D is the number of original continuous dimensions, K i is the number of bins in the discretization of the i-th dimension, while there are no continuous actions anymore (C = 0). This yields ?(k 1 , . . . , k D |s) = ? D i=1 ?(a d i = k i |s) As motivated by <ref type="bibr" target="#b19">(Tang and Agrawal 2019)</ref>, such a representation avoids the exponential explosion of discrete actions that would occur if one chose to use instead a single discrete component as in #1. Note that a similar idea is used in the action branching architecture of <ref type="bibr" target="#b20">(Tavakoli, Pardo, and Kormushev 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SAC with mixed discrete-continuous actions</head><p>Choosing an appropriate policy parameterization</p><p>The examples from the previous section are a subset of all possible ways one can represent the action distribution over a mix of discrete and continuous components, using our generic proposed decomposition. From a practitioner point of view, there is no single best representation that will fit all use cases. For instance, if the agent needs to press buttons on a controller, and there are four buttons which can be set on/off, one can either consider a single discrete component with 2 4 = 16 actions, or four independent binary discrete components. The latter approach has the benefit of reducing the number of parameters that need to be learned, thanks to the factored representation, and thus generally scales better as the number of discrete components increases. On the other hand, the independence assumption can make it harder for the agent to learn coordinated button presses, so the factored approach may perform badly when interactions between the discrete components really matter. In general, we give the following advice to obtain an appropriate representation for a given task, based on our own experience:</p><p>? Identify which action components (both discrete and continuous) should be made dependent of each other. When in doubt, it is advised to start with a simpler parameterization based on independent components, and only investigate later the potential benefits of more complex parameterizations. Note that in an MDP there always exists an optimal deterministic policy, for which all action components are independent given the state. As a result, it could be tempting to assume that everything can always be made independent (in order to simplify the model), but in practice this may slow down learning, in particular because it can prevent coordinated exploration across components (think of the above example with button presses and (x f , y f ) associated to each discrete action, as in point #5 above. If this is too costly (due to a large number of discrete actions), you can instead (as in #4) build the policy network in such a way that the continuous component head takes as input the discrete one: for details refer to <ref type="bibr" target="#b24">(Wei, Wicke, and Luke 2018)</ref>. ? If possible, try to avoid dependencies among continuous dimensions, so as to keep a simple parameterization where each action dimension can be sampled independently. For instance, if your continuous action is a pair (a x , a y ) giving the acceleration of your agent along the x and y axes, the agent may struggle to explore properly in situations where it needs to navigate narrow corridors that may not be axis aligned, since accelerations on both axes must be correlated to avoid bumping into the walls. In this specific case, one could for instance make the acceleration actions relative to the direction the agent is currently facing (by rotating the axes accordingly), making it easier for the agent to explore a wide range of forward accelerations without deviating from its trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Practical implementation</head><p>Network architecture <ref type="figure">Fig. 1a</ref> shows the typical architecture for the actor and critic networks used in standard continuous SAC implementations. Using a different policy parameterization (like one of those described previously) calls for a different network architecture. One common case is shown in <ref type="figure">Fig. 1b</ref>, in the situation where the agent must take a combination of one discrete action a d with a set of independently sampled continuous parameters a c . Note that here, we chose to take an approach similar to <ref type="bibr" target="#b25">(Xiong et al. 2018)</ref> where the critic's output layer contains the predicted Q-values of all discrete actions, instead of feeding the discrete action as input as done in the so-called "multi-pass" architecture of <ref type="bibr" target="#b3">(Bester, James, and Konidaris 2019)</ref>. This is because the former is the most commonly used architecture for discrete actions when using the popular Deep Q-Network algorithm and its variants <ref type="bibr" target="#b16">(Mnih et al. 2015;</ref><ref type="bibr" target="#b11">Hessel et al. 2017</ref>), but we acknowledge that the multi-pass architecture of (Bester, James, and Konidaris 2019) is also a valid alternative. We actually implemented it in SAC, but our preliminary results did not show meaningful improvements, so we did not investigate it further at this time.</p><p>More complex policy parameterizations would lead to more elaborate architectures for the actor network than shown in <ref type="figure">Fig. 1b</ref>, e.g.:</p><p>? In the case of multiple independent discrete components, the actor would output several corresponding discrete distributions ? d 1 , . . . , ? d D . ? If the continuous dimensions must be correlated, a different parameterization of a c may be used, for instance using normalizing flows <ref type="bibr" target="#b14">(Mazoure et al. 2019</ref>). ? If the continuous action a c must depend on the discrete action chosen by the agent, then a d can be used as input when computing ? c and ? c (Wei, Wicke, and Luke 2018).</p><p>Learning algorithm The SAC algorithm <ref type="bibr" target="#b8">(Haarnoja et al. 2018b</ref>) is based on the idea of giving an entropy bonus proportional to the entropy of ?(a|s). When the action has a discrete component, the joint entropy definition yields H ?(a d , a c |s) = H ?(a d |s) + a d ?(a d |s)H ?(a c |s, a d )</p><p>Although we could simply give a bonus proportional to this entropy, we argue that in some situations it may be beneficial to give different weights to its discrete and continuous parts. This is because otherwise, depending in particular on the number of discrete and continuous actions, there would be a risk for one of these two entropies to "overshadow" the other, which could harm exploration. As a result, we use as entropy bonus the weighted sum</p><formula xml:id="formula_4">? d H ?(a d |s) + ? c a d ?(a d |s)H ?(a c |s, a d )<label>(2)</label></formula><p>where hyperparameters ? d and ? c encourage exploration for discrete and continuous actions respectively. Note that these two hyperparameters can be tuned automatically during learning, using the same optimization technique as described in <ref type="bibr" target="#b9">(Haarnoja et al. 2018c</ref>), by setting target values for the discrete and continuous parts in eq. 2.</p><p>In terms of practical implementation, a list of the changes between our proposed Hybrid SAC and the original version can be found in the Appendix. Note that when there are only discrete actions, our approach is equivalent to the one proposed concurrently by <ref type="bibr" target="#b4">(Christodoulou 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments with parameterized actions</head><p>We evaluate our Hybrid SAC implementation on the same three parameterized actions environments used by <ref type="bibr" target="#b3">(Bester, James, and Konidaris 2019)</ref>:</p><p>? Platform is a simple platformer-like game where the agent has three discrete actions (run, hop and leap), each associated with a 1D continuous parameter controlling the horizontal displacement.</p><p>? Goal is a soccer-based game where the agent needs to score a goal past a keeper that tries to intercept the ball. There are again three discrete actions, with respectively 2D, 1D and 1D continuous parameters.</p><p>? Half Field Offense is another soccer-based game, also with three discrete actions, but this time with respectively 2D, 1D and 2D continuous parameters.</p><p>In order to allow for a fair comparison with the stateof-the-art Multi-Pass Q-Network (MP-DQN) algorithm of (Bester, James, and Konidaris 2019), we re-used their evaluation code and tried to match their hyperparameters whenever possible. We list the main remaining differences between our work and theirs in the appendix.</p><p>Results are summarized in <ref type="table">Table 1</ref>. Both algorithms perform equally well on Platform, while MP-DQN exhibits slightly better performance on Goal and significantly better performance on HFO. Note however that the MP-DQN results on HFO are based on an implementation that mixes  <ref type="figure">Figure 1: (a)</ref> On the left, the standard SAC architecture for continuous actions. The actor outputs the mean and standard deviation vectors ? c and ? c that are used to sample an action a c by injecting standard normal noise ? and applying a tanh non-linearity (to keep the action within a bounded range). The critic takes both the state s and the actor's action a c to estimate their corresponding Q-value. (b) On the right, an example of our proposed Hybrid SAC architecture, with two independent components (one discrete and one continuous). The actor computes a shared hidden state representation h that is used to produce both a discrete distribution ? d (typically from a softmax layer) as well as the mean ? c and standard deviation ? c of the continuous component. The discrete action a d is sampled from ? d while the continuous action a c is computed as in the standard SAC. The critic network still takes both the state s and the continuous action a c as input, but now predicts the Q-values of all discrete actions in its output layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Platform</head><p>Goal Monte-Carlo returns with one-step returns to speed up convergence, an improvement that we did not implement in our Hybrid SAC. The second row reports the performance of MP-DQN without mixing Monte-Carlo returns on HFO, showing that it degrades considerably (at least with the same hyper-parameters as MP-DQN). While investigating potential reasons for the slightly worse average performance of Hybrid SAC on Goal, we realized that the entropy bonus from eq. 2 may have an undesirable effect. Discrete actions with a small ?(a d |s) lead to a reduced entropy bonus for their associated ?(a c |s, a d ). This may cause the distribution of some continuous parameters to sometimes "collapse". Our preliminary experiments with a variant aimed at avoiding this collapse matched the results of MP-DQN, but a more in-depth analysis of this variant is still needed before we can confidently report on its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results in a commercial video game</head><p>We trained a vehicle in a Ubisoft game, using the proposed Hybrid SAC with two continuous actions (acceleration and steering) and one binary discrete action (hand brake). The objective of the car is to follow a given path as fast as possible. A video of the resulting behavior is available at https://youtu.be/bmrNMDEkPyQ. Note that the agent operates in a test environment that it did not see during training, and that the discrete hand brake action plays a key role in staying on the road at such a high speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments with Normalizing Flows</head><p>Our main objective in an industry setting is to optimize the final performance of the policy under a budget constraint on its inference runtime. A potential avenue that could help is to augment the Gaussian policy obtained from a standard SAC algorithm with radial flows as advised in <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref>, who report significantly improved performance with a reduced number of parameters. They suggest that such improvements could be related to the ability of the policy to be more expressive, for example by allowing it to be multimodal. In theory, training multimodal policies could yield agents that behave more naturally, for example in a driving situation where they could avoid an obstacle by turning either left or right. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bullet Roboschool benchmarks</head><p>Our SAC baseline consists of a two-layer feedforward network outputting the mean and the standard deviation parameterizing a spherical Gaussian. The SAC-NF agent has the same architecture, but adds several radial flows on the output of the Gaussian. The resulting action is then squashed using a tanh as in <ref type="bibr" target="#b8">(Haarnoja et al. 2018b)</ref>. All the networks are trained using the Adam optimizer <ref type="bibr" target="#b12">(Kingma and Ba 2014)</ref>, details of the models' architectures can be found in appendix in <ref type="table">Table 2</ref>.</p><p>We evaluate the different architectures on the PyBullet Roboschool benchmark <ref type="bibr" target="#b6">(Coumans and Bai 2016)</ref>. We take one step of training every ten environment steps, and evaluate the policy every 50,000 steps. All the results are averaged over 5 random seeds. Since our intention is to see if the boost in policy expressiveness provided by normalizing flows can really help during training, we use bigger networks for the two critics so that the training is not limited by their relatively low capacity. Results of this comparison can be found in <ref type="figure" target="#fig_1">Fig. 2</ref>. <ref type="figure" target="#fig_1">Fig. 2</ref> shows that while a smaller policy with two hidden layers of 64 neurons with normalizing flows can get results that are competitive with bigger networks during the first million iterations as also reported by <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref>, this advantage does not always hold as training goes further. Our results suggest that using normalizing flows on top of SAC does not yield a significant advantage compared to simply using the Gaussian policy of the baseline. In the following section, we will conduct an experiment on a toy environment to try to understand why.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalizing Flows and SAC</head><p>In SAC, the actor tries to optimize the Kullback-Leibler (KL) divergence between the policy and a softmax on the soft Q-values with temperature ?. <ref type="bibr" target="#b8">(Haarnoja et al. 2018b)</ref> demonstrate that updating the policy in such a way improves it until convergence, and <ref type="bibr" target="#b0">(Abdolmaleki et al. 2018)</ref> show that this update constrains the change of the policy. We thus try to minimize:</p><formula xml:id="formula_5">J ? (?) = E st?? ? D KL ? ? (?|s t ) exp Q ? (st,?) ? Z ? (s t )<label>(3)</label></formula><p>where Z ? is the partition function. However, the KL is not symmetric, and there is no theoretical ground in why ? ? should be the first argument. The main advantage of minimizing eq. 3 with ? ? in first position in the KL is tractability, as using the reparameterization trick allows us to minimize it without knowing the partition function. To do this, we rewrite the objective as an expectation on standard normal noise ? and then sample this expectation:</p><formula xml:id="formula_6">J ? (?) = E st?? ? ?t?N log ? ? f ? (? t ; s t )|s t ?Q ? s t , f ? (? t ; s t )</formula><p>(4) where f ? reparameterizes the policy in terms of the noise ?. The particular choice of using the KL divergence from ? ? to the target softmax is motivated mainly by the convenience of its implementation. However, in classification tasks we generally try to minimize the negative loglikelihood, which is equivalent to minimizing the KL divergence from the empirical distribution to the parameterized one. In policy distillation, <ref type="bibr" target="#b7">(Czarnecki et al. 2019;</ref><ref type="bibr" target="#b17">Parisotto, Ba, and Salakhutdinov 2015;</ref><ref type="bibr" target="#b18">Schmitt et al. 2018)</ref> good results are reported when trying to mini-</p><formula xml:id="formula_7">mize E? ? ? t=1 ? ? H ? ?(s t )||? ? (s t )</formula><p>where H ? is the cross-entropy, and the trajectories are sampled according to the student policy ? ? instead of the teacher policy ?. We also did some experiments with distillation (not included here) which confirm that this way of doing policy distillation yields good results. All these observations suggest that if we interpret eq. 3 as trying to distill the "teacher" softmax over Q-values into the "student" parameterized policy ? ? , a KL in the other direction would yield better results. This motivates the following comparison to measure the difference between these alternative objectives.</p><p>In this comparison, we fix a random state s 0 and try to get our policy to approximate a toy distribution ?, in this case a Gaussian mixture, for this fixed state. Several objectives are evaluated, and we monitor the impact of using each objective on the shape of the final distribution after 10,000 steps of training. All hyperparameters are identical to our Roboschool experiment. We compare the Gaussian policy as used in SAC to the SAC-NF policy which adds three radial flows on top of the Gaussian. We compare the two direc- <ref type="figure">Figure 3</ref>: Comparison between the final shapes of the policy distribution with several objectives after trying to match a Gaussian mixture for 10,000 steps. The blue and orange densities correspond to the target Gaussian mixture ? and the learned distribution ? ? respectively. Top row uses normalizing flows, while the bottom row is using a Gaussian policy. Various divergence metrics are evaluated from left to right.</p><p>tions of the KL divergence, as well as the Jensen-Shannon divergence <ref type="bibr" target="#b13">(Lin 1991)</ref>. We also tried to linearly switch from D KL (? ? ||?) to D KL (?||? ? ) during training. Results of this comparison can be found in <ref type="figure">Fig. 3</ref>. We use the kernel density estimate provided in the seaborn library <ref type="bibr">(Waskom et al. 2018)</ref> to estimate the density of the distributions.</p><p>From this toy experiment, one reason why normalizing flows did not seem to improve performance on Roboschool could be that any advantage gained in expressiveness of the policy by enriching it with normalizing flows is lost by the optimization procedure used in SAC. Indeed, when using the same objective as SAC (leftmost column in <ref type="figure">Fig. 3)</ref>, there seems to be very little difference between using a Gaussian policy and one with normalizing flows, since both collapse on a single mode of the target distribution. Note that in this comparison we did not take the impact of the temperature into account (another comparison on the impact of the temperature can be found in <ref type="figure">Fig. 4</ref> and 5 in the Appendix). However, when we invert the KL (as we do in supervised learning and distillation) or use the Jensen-Shannon divergence, it appears that the normalizing flows help the policy better match the complete target distribution.</p><p>These results suggest that using normalizing flows could yield some benefits when used with other objectives than the one used in SAC. We ran some experiments reverting the KL using importance sampling, but training was too unstable. We identify the exploration of other metrics between distributions, such as the Jensen-Shannon divergence or the Wasserstein distance, as potential research avenues that could yield significant improvements when used in conjunction with normalizing flows in SAC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>We introduced Hybrid SAC, an extension to the SAC algorithm that can handle discrete, continuous and mixed discrete-continuous actions. It exhibits competitive performance with the state-of-the-art on parameterized actions benchmarks. We showed that Hybrid SAC can be successfully applied to train a car on a high-speed driving task in a commercial video game, demonstrating the practical usefulness of such an algorithm for the video game industry. Our study of the use of normalizing flows with the SAC algorithm also suggests that future approaches could further improve SAC by using other objectives than the KL, so as to better leverage normalizing flows. Additional experiments with normalizing flows <ref type="figure">Fig. 4</ref> and 5 extend the results presented in <ref type="figure">Fig. 3</ref> where we train a policy ? ? to match a target distribution ?. <ref type="figure">Figure 4</ref>: Comparison between the final shapes of the policy distribution ? ? (in orange) trained with the same objective as SAC after trying to match a Gaussian mixture ? (in blue) with different temperatures ? for 10,000 steps. Note the collapse of the policies on one mode of ? unless ? gets very high, for both normalizing flows (top row) and Gaussian policy (bottom row). <ref type="figure">Figure 5</ref>: Same as <ref type="figure">Fig. 4</ref> but swapping the arguments of the KL divergence objective. Note that the policies no longer collapse onto a single mode of the target ?, and the normalizing flow policy is better able to approximate the shape of ?.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Hybrid SAC (one discrete component and one continuous)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the performance of SAC with and without radial normalizing flows on three Roboschool PyBullet environments. Curves are averaged on 5 random seeds, and smoothed using Savitzky-Golay filtering with window size 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>).? When a continuous component depends on a discrete component, consider duplicating it (one for each discrete action) as long as the model size remains reasonable: this will allow you to consider them as independent, making it easier for the model to specialize the value of the component</figDesc><table /><note>to each discrete action. For instance, consider an (x, y) continuous component which gives the 2D co- ordinates of a mouse click, where the agent has to se- lect among several discrete actions before clicking (ex: attack, heal, follow): this continuous component may be replaced with three independent ones (x a , y a ), (x h , y h )</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Parameterization of the normalizing flows.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We would like to thank the authors of <ref type="bibr" target="#b14">(Mazoure et al. 2019)</ref> for insightful conversations and providing us with their implementation, as well as Paul Barde for his valuable feedback while writing this paper.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Implementation differences between Hybrid SAC and SAC</head><p>? The entropy bonus used in the target value for the critic network Q is computed as in eq. 2, where the discrete part can be computed exactly (due to the finite number of discrete actions) while the continuous one needs to be approximated by sampling, as is usually done for continuous SAC.</p><p>? When optimizing the critic network Q with a transition sampled from the replay buffer, only the output associated with the discrete action taken in this transition is optimized, similar to the Deep Q-Network algorithm <ref type="bibr" target="#b16">(Mnih et al. 2015)</ref>.</p><p>? The discrete part ?(a d |s) of the policy is optimized by minimizing the KL divergence between this distribution and the one induced by the softmax on the Q-values with temperature ? d . Since these Q-values depend on the continuous components a c , we sample a c ? ?(a c |s, a d ) in order to compute q d = Q(s, a d , a c ) for each a d , and take a gradient step to minimize the KL divergence between ?(a d |s) and P (a d ) ? exp(q d /? d ). As is usually done in continuous SAC, we multiply this gradient by ? d so as to prevent it from blowing up for small values of ? d .</p><p>? Finally, the same a c sampled above are re-used to compute the update step for the continuous part of the policy. This update is essentially the same as in continuous SAC, as in eq. 7 of <ref type="bibr" target="#b9">(Haarnoja et al. 2018c)</ref>, except that it is performed as a weighted average over all discrete actions a d , where the weight is given by ?(a d |s) (i.e. mimicking the weighting scheme of eq. 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Differences between MP-DQN and Hybrid SAC</head><p>The main differences between our implementation of Hybrid SAC and the Multi-Pass DQN algorithm from (Bester, James, and Konidaris 2019) are the following:</p><p>? We do not use the Multi-Pass architecture in our critic Q, because it significantly slows down learning and did not seem to really help in our preliminary experiments with SAC. Additional experiments are needed to fully investigate the potential benefits of this architecture for Hybrid SAC.</p><p>? For the sake of simplicity, we use a squashing tanh to bound the actions instead of the inverting gradient technique (Hausknecht and Stone 2016), and do not use gradient clipping.</p><p>? Since our approach is based on SAC, while MP-DQN is based on a combination of DQN <ref type="bibr" target="#b16">(Mnih et al. 2015)</ref> and <ref type="bibr">DDPG (Lillicrap et al. 2016</ref>), we do not use ?-greedy exploration nor add noise to continuous actions, but instead rely on the actor's stochasticity for exploration.</p><p>? We tweaked our actor and critic learning rates (as well as SAC-specific hyperparameters like the target discrete and continuous entropies) by a cursory search over reasonable-looking values.</p><p>? In the Platform environment, we do not use the custom initialization of the continuous parameters from (Bester, James, and Konidaris 2019) because we found it easy enough to get good results without it. ? In the Half Field Offense environment, we do not mix Monte-Carlo returns with one-step returns. Incorporating Monte-Carlo returns is not entirely straightforward in SAC due to the need to account for the entropy bonus, so we leave it to future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Roboschool hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Normalizing flows parameterization</head><p>We note d the dimension of the action space. We parameterize ? = (z 0 , x, y) ? R 3 as follows: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdolmaleki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Degrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02256</idno>
		<title level="m">Relative entropy regularized policy iteration</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Managing engineering systems with large state and action spaces through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Andriotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Papakonstantinou</surname></persName>
		</author>
		<idno>abs/1811.02052</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multipass q-networks for deep reinforcement learning with parameterised action spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Bester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Konidaris</surname></persName>
		</author>
		<idno>abs/1905.04388</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Soft actor-critic for discrete action settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Christodoulou</surname></persName>
		</author>
		<idno>abs/1910.07207</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Discrete off-policy policy gradient using continuous relaxations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cianflone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>unpublished</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Pybullet, a python module for physics simulation for games, robotics and machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Coumans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>GitHub repository</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02186</idno>
		<idno>arXiv:1804.02808</idno>
		<title level="m">Latent space policies for hierarchical reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Distilling policy distillation</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01290</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hartikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.05905</idno>
		<title level="m">Soft actor-critic algorithms and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in parameterized action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rainbow: Combining improvements in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<editor>Bengio, Y., and LeCun, Y., eds., ICLR</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adam: A method for stochastic optimization</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Divergence measures based on the shannon entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mazoure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06893</idno>
		<title level="m">Leveraging exploration in offpolicy algorithms via normalizing flows</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Discrete sequential prediction of continuous actions for deep RL</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<idno>abs/1705.05035</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Humanlevel control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<editor>529. OpenAI</editor>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Openai five</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Actormimic: Deep multitask and transfer reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06342</idno>
		<idno>arXiv:1505.05770</idno>
	</analytic>
	<monogr>
		<title level="m">Variational inference with normalizing flows</title>
		<editor>Rezende, D. J., and Mohamed, S. 2015</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zidek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03835</idno>
		<title level="m">Kickstarting deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Boosting trust region policy optimization by normalizing flows policy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrawal</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10326</idno>
		<idno>abs/1901.10500</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Discretizing continuous action space for on</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Action branching architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tavakoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kormushev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Wiering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 International Joint Conference on Neural Networks, IJCNN&apos;09</title>
		<meeting>the 2009 International Joint Conference on Neural Networks, IJCNN&apos;09<address><addrLine>Piscataway, NJ, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1144" to="1151" />
		</imprint>
	</monogr>
	<note>AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kroiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dalibard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sulsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mckinney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Apps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<ptr target="https://deepmind.com/blog/alphastar-mastering-real-time-strategy-game-starcraft-ii/" />
		<title level="m">AlphaStar: Mastering the realtime strategy game StarCraft II</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Grandmaster level in StarCraft II using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Georgiev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Nature 1-5</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improving exploration in soft-actor-critic with normalizing flows policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smofsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Botvinnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Okane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostblom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lukauskas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Qalieh</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo1313201</idno>
		<idno type="arXiv">arXiv:1906.02771</idno>
		<ptr target="https://doi.org/10.5281/zenodo1313201" />
	</analytic>
	<monogr>
		<title level="m">mwaskom/seaborn: v0. 9</title>
		<imprint>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical approaches for reinforcement learning in parameterized action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Spring Symposium Series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Parametrized deep q-networks learning: Reinforcement learning with discrete-continuous hybrid action space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1810.06394</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
