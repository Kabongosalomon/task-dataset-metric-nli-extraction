<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Language Modeling for Automated Theorem Proving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislas</forename><forename type="middle">Polu</forename><surname>Openai</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><forename type="middle">Sutskever</forename><surname>Openai</surname></persName>
							<email>ilyasu@openai.com</email>
						</author>
						<title level="a" type="main">Generative Language Modeling for Automated Theorem Proving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -the generation of original mathematical terms -might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep learning based system has contributed proofs that were adopted by a formal mathematics community.</p><p>Preprint. Under review.</p><p>Deep learning applied to premise selection and proof guidance Research on automated theorem proving dates back to the 50s [24], but mainstream proof assistants still suffer from combinatorial explosion of their search space as they are scaled to large corpuses, motivating the use of deep learning. Early applications of deep learning to formal mathematics focused primarily on premise selection and proof guidance. DeepMath [26] explored the use of CNNs and RNNs to predict whether a premise is useful to demonstrate a given conjecture, their results were later improved with FormulaNet [27] by the use of graph neural networks, reminiscent of NeuroSAT <ref type="bibr" target="#b27">[28]</ref>. Proof guidance consists in selecting the next clause to process inside an automated theorem prover. Loos et al. <ref type="bibr" target="#b28">[29]</ref> investigated the use of models similar to DeepMath's for proof guidance and demonstrated a significant uplift on the Mizar library.</p><p>Deep learning applied to automated theorem-proving HOList [30] proposes a formal environment based on HOL Light. They achieve their best performance [31] with a GNN model designed for premise selection and the use of exploration. More recently, the same team studied the use of the BERT objective with Transformers on formal statements [32], demonstrating the potential of leveraging Transformers for formal reasoning. Their study focuses on preliminary tasks that are related but not directly consisting of proving formal theorems (such as typing and conjecturing). GamePad <ref type="bibr" target="#b32">[33]</ref> and CoqGymn/ASTactic [34] introduce environments based on the Coq theorem prover. ASTactic generates tactics as programs by sequentially expanding a partial abstract syntax tree. <ref type="figure">Holophrasm [35]</ref> and MetaGen-IL [25] propose RNN-based models to generate proofs for Metamath (the formal system we focus on). They rely on three different models, one to value goals, one to select premises and one to generate substitutions. MetaGen-IL also demonstrates an uplift in performance by generating synthetic data by forward proving.</p><p>Use of Transformers for symbolic tasks Several lines of work have been exploring language modeling using Transformers <ref type="bibr" target="#b17">[18]</ref>. Language modeling improvements have been demonstrated from better pre-training tasks, using various objectives such as auto-regressive generation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, token masking <ref type="bibr" target="#b21">[22]</ref> or sequence masking <ref type="bibr" target="#b35">[36]</ref>, but resulting language models have so far felt short when applied to reasoning oriented tasks such as algebraic word problems <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref>. Recently, Lample and Charton <ref type="bibr" target="#b38">[39]</ref> successfully applied Transformers to anti-derivative calculus and solving differential equations, hinting that Transformers are capable of generating the exogenous terms involved in the substitutions required for successful symbolic integration. The Universal Transformer [40], a Transformer with tied weights, was also shown to be successful at more algorithmic tasks. Also,  Saxton et al. [41]  evaluated the Transformer architecture on a variety of mathematical problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Artificial neural networks have enjoyed a spectacularly successful decade, having made considerable advances in computer vision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, translation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, speech recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, image generation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, game playing <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, and robotics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Especially notable is the recent rapid progress in language understanding and generation capabilities <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>With the possible exception of AlphaGo <ref type="bibr" target="#b12">[13]</ref> and AlphaZero <ref type="bibr" target="#b22">[23]</ref>, reasoning tasks are conspicuously absent from the list above. In this work we take a step towards addressing this absence by applying a transformer language model to automated theorem proving. Automated theorem proving <ref type="bibr" target="#b23">[24]</ref> is an appealing domain for exploring reasoning in general and the reasoning capabilities of language models in particular for several reasons:</p><p>? Reasoning-complete: Proving theorems very likely require general and flexible reasoning; thus an advance in theorem proving is also an advance in reasoning more broadly. ? Search: Automated theorem proving systems can quickly check the correctness of proofs, making it a productive environment for the use and development of search methods. ? Automated data generation: The ability to verify proofs makes it possible to automatically generate new problems that could then be used as training data. This is especially important, since collecting high quality data for reasoning tasks can be difficult.</p><p>Learning to prove theorems is somewhat analogous to learning to play Go: both offer an automated way of determining success (the game of Go is a miniature formal system), and both offer an automated way for generating new data via self play-type approaches. This similarity, together with the clear success of AlphaZero, suggests that automated theorem proving might prove to be a fruitful domain for the study of reasoning in neural networks where significant progress may be possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contribution</head><p>Our contributions are the following:</p><p>? We verify that generative pre-training substantially improves performance and that pretraining on mathematical data (such as arXiv) leads to better performance compared to pre-training on generic text from the web.</p><p>? We find that model size is positively correlated with performance, even though the size of the Metamath dataset is relatively small.</p><p>? We demonstrate that iteratively training a value function on statements generated by our language model leads to improved prover performance, which immediately suggests a strategy for continuous self improvement: keep training on proofs generated by the prover.</p><p>? We also achieve a new state of the art for the Metamath environment with our best model capable of closing 56.22% of proofs from a held-out test set (vs 21.16% for the current state of the art, MetaGen-IL <ref type="bibr" target="#b24">[25]</ref>), demonstrating that the Transformer architecture may be suitable to formal reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Formal Environment</head><p>We chose Metamath <ref type="bibr" target="#b41">[42]</ref> as our formal environment. Metamath is powered by a simple meta logic system based on a single substitution rule <ref type="bibr" target="#b42">[43]</ref>.</p><p>The main Metamath library is called set.mm, which is a collection of ? 38k proofs based on ZFC set theory (while other formalisms can also be used on top of Metamath's meta logic, they are not used in set.mm).</p><p>Metamath has several advantages that make it convenient to use with neural networks:</p><p>? Verification is fast and can be implemented in several hundreds lines of code.</p><p>? Proof steps are context-free: a goal or subgoal that we wish our system to prove, together with a list of the statements of the theorems proven so far, completely define the state of the Metamath system at any stage of a proof. Other formal systems are generally wrapped in high-level programming languages that make them easier to use for humans (by including convenient features like module imports or custom user-defined tactics) but are harder to integrate with a neural network. While proofs in such systems are generally shorter and more human-readable, they are impacted by long-distance interactions which makes the complete description of the intermediary states of proofs longer, and therefore less suitable for neural language models.</p><p>? Access to clean and compact subgoal representations makes searching the proof tree relatively straightforward. It is not the case for systems where the proving objective resembles program synthesis more than an explicit proof tree search.</p><p>? set.mm is one of the largest libraries available and its foundations are accepted as compatible with modern mathematics.</p><p>But it also has a number of weaknesses:</p><p>? Metamath does not have high-level tactics, which means that all of its proof steps are very low-level. As an example, the de-bruijn factor <ref type="bibr" target="#b43">[44]</ref>-the quotient of the size of a formalization of a mathematical text and the size of its informal original version-of a Metamath proof is ? 10 ? 20 while it is around ? 1 ? 3 in Coq, HOL Light or Lean. Lower level proof steps mean longer proofs with greater chance of compounding errors during search.</p><p>? The current state of the tooling around Metamath makes it a very "DIY" system, one that is not yet ready for broad adoption by the mathematics community.</p><p>While our approach would be applicable to other formal systems (such as Lean, Coq, or HOL Light), Metamath's features allow faster prototyping and reduced iteration time in the near term, which is why we chose it for this project.</p><p>The set.mm library contains the background theorems required to demonstrate most Olympiad or undergraduate Mathematics type of problems. For example, assisted by the GPT-f proof assistant described in this work in section 6.2, we formalized IMO 1972 problem B2 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Proving in Metamath</head><p>Proving in Metamath consists of applying a previously demonstrated theorem or axiom by providing a substitution of the variables appearing in the hypotheses and conclusion of the theorem being applied, such that the substituted conclusion unifies to (which means that it "matches") the current goal which we wish to prove. The substituted hypotheses, if any, become the new subgoals left to prove.</p><p>This mechanism, a proof step, can be used in a forward manner (where we start with the axioms and reach the desired statement, one proof step at a time) and a backward manner (where we start with the statement we wish to prove and, after applying enough proof steps, end up at axioms or previously demonstrated theorems with whose hypothesis we already determined to be true). As it is more naturally amenable to proof search, we will be operating backward.</p><p>As an example, assume we want to prove (3 + 2) = 5 using the definition of 4 and 5 as respective successors of 3 and 4. As a first step, we should use an equality transitivity theorem such as:</p><formula xml:id="formula_0">[[ |-A = B # first hypothesis |-C = B # second hypothesis ]] |-A = C # conclusion</formula><p>To apply the transitivity theorem, we need to provide a substitutions that substitutes A with (3 + 2) and B with 5 such that the conclusion of the theorem unifies to the current goal. We are left with providing a substitution for B which can hardly be discovered mechanically (hence the appeal to use generative language modeling). We can substitute B with (4 + 1) as is the case in the actual proof 2 in Metamath's set.mm library.</p><p>Putting it all together, the goal here is:</p><formula xml:id="formula_1">|-( 3 + 2 ) = 5</formula><p>The proof step we apply:</p><formula xml:id="formula_2">[[ |-A = B # first hypothesis |-C = B # second hypothesis ]] |-A = C # conclusion {{ A : ( 3 + 2 ) }} # substitution of A {{ B : ( 4 + 1 ) }} # substitution of B {{ C : 5 }} # substitution of C</formula><p>And finally the new subgoals are:</p><p>|-( 3 + 2 ) = ( 4 + 1 ) |-( 4 + 1 ) = 5</p><p>Applying the following proof step with no hypothesis (the definition of 5 3 ) to the second subgoal allows us to prove it.</p><p>[[ ]] |-( 4 + 1 ) = 5</p><p>Note that this proof step has no hypothesis and no substitution involved. It therefore closes that branch of the proof tree. From there the proof can be continued with the first subgoal, proving backward, until no subgoal is left. Also note that a proof for a given theorem of the library can only use theorems proven before the appearance of the theorem to prove; we enforce that constraint when benchmarking our models despite them being trained on the library as a whole.</p><p>In most formal systems, a proof step, consists of a goal and a mechanism that, given a goal produces new subgoals, generally referred to as a tactic. In Metamath, there is only one type of tactic based on substitution as illustrated above. Additionally since the substituted theorem must unify to the current goal, the current goal can be deduced from the tactic itself (theorem and substitution pair), which is not generally the case in other systems. As such, we'll use tactic and proof step interchangeably in the rest of the paper.</p><p>This informal presentation of Metamath is sufficient to understand the objectives we use to train our models. A more formal definition of Metamath's meta-logic can be found in the Metamath Book <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dataset</head><p>Metamath's set.mm uses a binary compressed format to represent proofs of statements. We process the library and extract a dataset of proof steps, stored as JSON blobs using the representation presented <ref type="bibr" target="#b1">2</ref> Metamath Proof Explorer -3p2e5 http://us.metamath.org/mpeuni/3p2e5.html 3 Metamath Proof Explorer -df-5 http://us.metamath.org/mpeuni/df-5.html above. For each proof step we store a GOAL, a PROOFSTEP and a reference to the parent goal if any, encoding the tree structure of the proofs: The dataset contains ? 3m of such proof steps for ? 38k theorems (different proof labels). We split that dataset between a train set and two valid and test sets each containing ? 1k proofs sampled randomly (? 90k proof steps each).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Glossary term</head><p>A string that comply to the Metamath grammar. statement or proposition A potentially empty set of hypotheses (terms) and a conclusion (term) entailed by the hypotheses. theorem A proven statement. axiom An assumed statement. goal A statement in the context of a proof search. substitutions</p><p>A list of pairs of variables and terms (to substitute the variables within a theorem or an axiom). tactic</p><p>A theorem and substitutions that unify to a goal. subgoals Goals generated by a tactic (the substituted hypotheses of the tactic's theorem).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>proof step</head><p>A goal and a tactic, potentially generating new subgoals. proof A tree of goals and tactics whose root is the demonstrated theorem; leaves of the tree are tactics with no subgoals or goals that are hypotheses of the root theorem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture</head><p>We use decoder-only Transformers similar to GPT-2 <ref type="bibr" target="#b19">[20]</ref> and GPT-3 <ref type="bibr" target="#b20">[21]</ref>. The largest model we study has 36 layers and 774m trainable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Objective</head><p>The proofstep objective we use for training is a conditional language modeling objective that is asked to generate the PROOFSTEP given a GOAL, which is directly applicable to proof searches. To do so, we format our data in the following way:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GOAL &lt;GOAL&gt; PROOFSTEP &lt;PROOFSTEP&gt;&lt;EOT&gt;</head><p>There is one such objective for each JSON line in our dataset. We train with only one sentence per context (no-chunking), masking the rest of the context by assigning a loss weight w loss = 0. As we train we track the valid loss and sequence accuracy while masking the query part of the objective:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GOAL &lt;GOAL&gt; PROOFSTEP</head><p>We regularize the training by early-stopping at the point of minimum valid loss and applying a weight decay wd = 0.1.</p><p>Here is a randomly sampled context as presented to our models for training: </p><formula xml:id="formula_3">GOAL [[ ]] |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Goal Expansion</head><p>We find proofs by running proof searches. A proof search maintains a proof tree and a queue of open goals sorted by their cumulative logprob, initialized with the root goal that we wish to demonstrate (see <ref type="figure" target="#fig_2">figure 1</ref>). The cumulative logprob of a goal is defined by the sum of the logprobs of the tactics that were used to reach that goal from the root goal. Intuitively we expand goals for which the generative model is the most confident globally. This has a tendency to explore breadth first as deeper goals have more parent tactics and therefore typically a higher cumulative logprob.</p><p>Each time we expand an open goal we sample e = 32 tactics (the proofstep objective described above) from the model at temperature t = 1.0, deduplicate them, and apply the valid tactics (of which there are at most e) to the goal being expanded. Each successful tactic application generates new subgoals that are added to the proof tree and the proof search queue. The expanded goal is then removed from the queue. Note that the subgoals associated with a successfully applied tactic all share the same cumulative logprob and will eventually be expanded together (as subgoals generated from their own expansion will mechanically have a higher cumulative logprob, and will therefore be inserted behind in the queue). We denote the process of selecting the minimal cumulative logprob goal and expanding it as a proof search expansion.</p><p>Each proof search involves d = 128 goal expansions, so proofs generated have at most d proof steps. When evaluating our models, we attempt a proof search for each statement in the valid set a = 4 times, starting from an empty proof tree each time. In the above, a, e, and d are hyperparameters of the search process that we can vary to achieve better performance (at the cost of more compute), but keep constant as we compare models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Formal Verifier</head><p>Performing such proof searches requires to tightly couple a Metamath verifier with our models. We implemented a Metamath kernel in Python to avoid the performance cost and brittleness of interacting with an external kernel over its REPL through standard I/O. It also provides us with a flexible environment to experiment with new ideas in ways that were not anticipated by existing verifiers. The kernel consists of a modified LR(0) parser to check that terms generated by our models comply with the Metamath grammar, along with Goal and Tactic objects that implement the Metamath substitution and represent proof trees. Our implementation is capable of exporting our in-memory representations to both our JSON marshalled format and the official set.mm proof format. The latter allows us to verify the proofs we generate with an external Metamath kernel implementation such as mmverify.py or metamath-exe.</p><p>Collectively, this proof search procedure and the formal verifier tied with it are what we refer to as the GPT-f automated prover.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation</head><p>We report the performance Perf valid a,e,d (?) of a model ?, as the percentage of proofs found by this procedure within the valid or test set. We evaluate our models on the valid set of ? 1k theorems and once at the end of this paper on the held-out test set. The hyperparameters we chose to evaluate our models attempt to minimize the variance in the evaluation process while using the least amount of compute. Decreasing the number of expansions per goal e increases the variance as we less systematically explore the action space when expanding each goal. The e value can be taken quite high as each auto-regressive proof step generation uses the same query for a given goal and can therefore be batched together. Increasing e too much may also hurt performance given the breadth first nature of the cumulative logprob ordering. Increasing the number of attempts per proposition a decreases variance and consistently improves performance up to reasonably high values (We use a = 32 attempts for our final benchmarking). We found that a = 4 limited the variance in our evaluations while remaining tractable given the amount of compute we had available. Finally the proof search depth d has little impact on variance but naturally improves performance (we take d = 128 to evaluate our models and d = 256 for our final benchmarking).</p><p>The number of expansions we use per proof search may appear as relatively low, but it's important to realize that it already requires a substantial amount of compute as each expansion consists in the auto-regressive generation of e = 32 tactics (generally hundreds of tokens and therefore forward passes each). Empirically, the hyperparameters we chose, require on average around ? 1k GPU.hours (with V100s) to evaluate our 700m parameters model (which leverages GPT-3's sparse attention as well as key-value caching).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pre-training</head><p>We study the effect of pre-training on the performance of our models. We pre-train our models on both GPT-3's post-processed version of CommonCrawl as well as a more reasoning-focused mix of Github, arXiv and Math StackExchange.</p><p>Github is downloaded using BigQuery 4 and filtered to only include deduplicated files from selected programming languages (excluding markdown, stylesheets, HTML). arXiv is downloaded using Bulk Data Access 5 and filtered to only include articles labeled as Mathematics and whose LaTeX source is available. Math StackExchange is downloaded from their snapshot on the Internet Archive 6 and post-processed to remove HTML tags and correlate questions and answers. We demote the mix reported in the table below as WebMath: To achieve this goal we designed synthetic datasets allowing us to generate proofs for each of these domains at will while controlling precisely by how many proofs we augment our training set.</p><p>We describe below the synthetic datasets we designed and report in section 5 the sample complexity associated with these synthetic tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">n-digit Arithmetic</head><p>We synthetically generate proofs for arithmetic formulas such as 11 * 22 = 242 by following the basic algorithm for addition and multiplication, repeatedly applying theorems such as decadd 9 or decaddc <ref type="bibr" target="#b9">10</ref> . Divisions and subtractions are translated to their equivalent additions and multiplications theorems in one proof step. We also support generating modulos and exponentiations.</p><p>We accept one hyperparameter for these synthetic proof generators, ndigits which controls the number of digits involved in these arithmetic tasks. When generating a new proof we sample uniformly in [?10 ndigits , 10 ndigits ] each of the numbers involved. To illustrate the level at which Metamath operates, <ref type="table" target="#tab_1">Table 2</ref> shows the average number of proof steps generated as a function of ndigits for each generator. These statements are generally proved with one tactic application in other higher-level systems, which is a good example of one of Metamath's drawbacks we identified earlier. Our goal is to leverage these synthetic generators to ensure our models are confident when faced with such subgoals in order to mitigate the large number of proof steps they require.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Ring Algebra</head><p>Our ring equalities generator is largely inspired by the INT inequality generator <ref type="bibr" target="#b44">[45]</ref> . They propose an inequality generator that starts from simple formulas (such as A = A) and iteratively transforms them into more complex equalities or inequalities using a predefined list of axioms (such as commutativity of addition or distributivity of addition-multiplication). At each transformation, the axiom to be applied is chosen uniformly.</p><p>Our generator operates similarly within the Metamath formalism based on theorems equivalent to the axioms they propose. We accept two hyperparameters, the number of variables nbvar involved in the seed formulas (of the form A = A) as well as the number of theorems applied to transform the expression, denoted as depth. In addition, we use hand-crafted weights as we sample theorems in order to obtain formulas that we judged qualitatively better.</p><p>Here is a list of the theorems we use and their associated sampling weights.  By default in all of our experiments we add synthetically generated proofs to the dataset extracted from set.mm as shown in <ref type="table" target="#tab_3">Table 4</ref>. We'll denote this dataset as our augmented dataset. The synthetically generated proofs account for approximately 1% of our training data which empirically appeared as big enough to achieve decent performance on the tasks we cared about and small enough not to hurt performance on the valid set, especially for small models. We attempted scaling the portion of synthetic proofs to 5% of the dataset and found out that it hurt performance for the model sizes we studied. It is nonetheless possible that including more synthetic data may turn out to be beneficial for larger models than the ones studied in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Learned Value Function</head><p>To achieve better performance, we also iteratively train a value function to guide the proof search, in place of the cumulative logprob priority described above.</p><p>We implement the value function by means of an outcome objective as follows. Any time we attempt to prove a statement, we will generate a significant number of intermediate goals. Some of these 50 1499 goals will lead to the proof, other goals will be proved without being part of the final proof, while others will not be resolved at all. To obtain a value function, we simply train our model to predict whether a goal produced during proof search ended up being resolved by generating a new dataset of the following form:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GOAL &lt;GOAL&gt; OUTCOME &lt;P|N&gt;&lt;EOT&gt;</head><p>Where a goal ends with a "P" if was resolved, and "N" otherwise.</p><p>The binary nature of the OUTCOME allows the definition of a provability function f P as the conditional probability of token P given a GOAL without having to introduce a separate value head. Given a goal g, for a model parametrized by ?:</p><formula xml:id="formula_4">f ? P (g) = p ? ("P"|g) ? trained 1 ? p ? ("N"|g)</formula><p>We then define our value function V on goals with:</p><formula xml:id="formula_5">V ? (g) = g ?siblings(g) f ? P (g )</formula><p>Not having to introduce a separate value head greatly simplifies the overall architecture. Training only involves augmenting the dataset with outcome objectives (as additional masked sentences) and sampling the "provability" function simply consists in reading the probability distribution for the token following the OUTCOME keyword (which can be done in one forward pass).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Iterative Data Generation and Training</head><p>Having access to a formal verifier enables us to generate the training data for f P in a fully synthetic manner by first training a model on the proofstep objective, then sampling proofs (using cumulative logprob priority) for statements from the training set, and finally, annotating goals visited by the proof searches as positives if they were proved and as negatives otherwise.</p><p>These annotations are used to train f P and the entire process can be run iteratively, similarly to Expert Iteration <ref type="bibr" target="#b45">[46]</ref>, sampling proofs using the newly trained V (instead of cumulative logprob) to guide proof search for subsequent iterations.</p><p>At each iteration we entirely re-train the model on both objectives at the same time on the dataset constructed as follows:</p><p>? We extract the full proofs that were found for statements from the training set at each previous iteration and merge them with the original training set. We deduplicate proof steps at the proof level. This dataset becomes our new train set for the proofstep objective. ? We extract the annotated goals visited by the proof searches for statements from the train set as well as the goals from the original train set (annotated positively) and deduplicate the goals giving priority to positive outcomes annotations. This dataset becomes our new train set for the outcome objective.</p><p>This iterative training allows controlling for overfitting on both objectives by processing in the same way the data generated by proof searches on statements from the valid set and using the resulting datasets to track their associated valid loss.</p><p>Training a value function gives an opportunity to the model to learn from its errors on data it generates. It also shifts proof searches from breadth first exploration to one that is more focused, adaptively based on the level of confidence modeled by V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We fine-tune all of our models with 1024 examples per global batch and a context size of 2048 tokens, for at most 32B tokens (our augmented dataset contains ? 1B tokens), early stopping at min valid loss when applicable. We anneal the learning-rate to zero (over 32B tokens). We found that restarting the training with an annealing to zero that matches the early-stopping for a given model only provides a marginal improvement, and avoided doing so.</p><p>The models are trained with the BPE encoding reported in <ref type="bibr" target="#b20">[21]</ref>, the same tokenization being used for text, code or formalized statements. We leave as future work a thorough ablation of the encoding as preliminary experimental results demonstrate possible gains with specialized tokenization techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Baselines</head><p>We report three baselines: (i) the state of the art for Metamath's set.mm as reported in MetaGen-IL <ref type="bibr" target="#b24">[25]</ref> (their methodology for benchmarking their solution is close to ours so the numbers are directly comparable); (ii) a 160m parameters trained from scratch on our raw dataset using the proofstep objective; and (iii) a 160m parameters trained from scratch on our augmented dataset (same objective). We explain the improvement over MetaGen-IL (despite not relying on forward proving data generation techniques) by our use of a simpler architecture (one unique Transformer vs 3 separate GRU networks); a more straightforward objective (direct auto-regressive generation of the full tactic as text vs separate premise selection and generation of the substitutions); more learnable parameters (160m vs 300k (3 2-layers bi-directional GRUs with 128 hiddens)); and more compute at training as well as test time.</p><p>Note that the dataset augmentation may have a marginal negative effect on performance on the valid set with our 160m model (but we're within typical variance). We report in section 5.5 a more reliably positive effect with a pre-trained 700m model. These results demonstrate that model size positively impacts performance in our formal setup, despite the training dataset being limited in size (we train for ? 18 epochs). Note that the bigger the model the more compute we use at training time as well as benchmarking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Model Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Pre-training</head><p>Models are pre-trained on CommonCrawl using GPT-3's <ref type="bibr" target="#b20">[21]</ref> methodology for 260B tokens. When studying the effect of pre-training on WebMath we start from a CommonCrawl pre-trained model and continue pre-training on WebMath for 16B additional tokens. We also report results after pre-training on GitHub only instead of WebMath for the same number of tokens. We hypothesize that the positive pre-training effect is primarily driven by the emergence and transfer of features that are relevant to formal reasoning. It is possible to argue that most of these features are probably shallow and mostly relevant at the syntactical level but the lower performance achieved with Github only in comparison to WebMath suggests that some features may be more elaborate. We leave as future work a broader investigation of this question, which could be achieved by studying the performance of linear probes on the features of the different pre-trained models with respect to a formal objective, such as the truthiness of a set of statements provided in the Metamath (or any other formal) language. It is unclear why we do not observe a smooth improvement in performance between the 700m and the 1.5b models in table 8. The lack of guarantee that the valid set has a smooth difficulty pattern may play a role here. Another effect may originate from the limited size of the training set, leading the training dynamics to saturate as we grow the number of parameters. We leave as future work a closer study of this effect which could be accomplished by training on various fractions of the training dataset and checking for similar saturation plateaux.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Learned Value Function</head><p>We report the performance of our models as we iteratively train on data generated by sampling proofs against the verifier. While overfitting on the train set does not generally appear to negatively impact performance on the valid set (and can even often help noticeably if not too catastrophic), we discovered that it dramatically hurts our iterative training process. We hypothesize that overfitting collapses the data generation in a mode where exploration is weakened, the model being overly optimistic about its predictions on the train set. We therefore carefully avoid overfitting by tracking the loss on the associated valid set, early stopping as we reach a minimum.</p><p>There is probably additional performance to be extracted by running more iterations given how continuous this iterative process appears to be. We leave as future work the design of an iterative data generation process that is less compute intensive. Indeed, we believe that a lot of computation is spent on subgoals that are not necessarily providing a lot of signal for the value function, and each iteration is quite compute intensive as it requires sampling proofs for the entire training set (which takes ? 20k GPU.hours on V100s in our current setup).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Sample Complexity</head><p>Ablation of our synthetic dataset augmentation demonstrates that synthetically generated proofs generalize to some extent and provide a noticeable uplift in performance on the valid set for larger models. Our main motivation for including synthetic proofs in our training, beyond the relative uplift achieved, is the study of the effect of model size and training a value function on the sample complexity of our models, as we control exactly how many examples from the synthetic domain we use for training. <ref type="table" target="#tab_0">Table 11</ref> reports the performance on 100 synthetically generated statements (different from the train set) as well as the number of synthetic proofs present in the training set for each model (in parenthesis). This demonstrates the close (yet not perfectly correlated) relationship between sample complexity and performance in our formal reasoning setup, suggesting that sample complexity is an important driver of improved performance with formal mathematics.</p><p>More importantly it demonstrates that our models are capable of acquiring new non-trivial capabilities with a number of training examples that is compatible with manual formalization. We plan in the future to study similar learning dynamics for more challenging tasks for which we don't have a synthetic generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Results</head><p>We attempted to push the performance of our models by increasing both the number of expansions per proof search from d = 128 to d = 256, and the number of attempts per proofs from a = 4 to a = 32. We report the achieved performance as a function of the number of attempts per statements on the valid set in <ref type="table" target="#tab_0">Table 12</ref>.</p><p>Finally, we performed a final evaluation with d = 256 and a = 32 of our 700m model policy+value (iteration 2) on the held-out test set: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Output</head><p>We describe in this section two projects we executed, aimed at sharing with the Metamath community results and tools based on our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Proof Shortening</head><p>We contributed 23 shortened proofs 1112 of theorems to the Metamath library. These proofs were generated by the GPT-f automated prover. To discover shorter proofs, we sampled proofs for statements from the set.mm library, comparing the length of the solutions found by our models to their ground truth versions, also verifying that the shorter proofs didn't rely on additional axioms.</p><p>The reception 13 from the Metamath community was positive, proof length being a metric the community care about:</p><p>"I had a look at the proofs-very impressive results! Especially because we had a global minimization recently, and your method found much shorter proofs nevertheless."</p><p>"Any ML-based system is impressive if it can find many shorter proofs than the ones we already have. Nice work."</p><p>"The shorter proof is easier to translate. It's more symmetric in that it treats A and B identically. It's philosophically more concise in that it doesn't rely on the existence of a universal class of all sets."</p><p>To our knowledge, these shortened proofs are the first effective contribution of a deep learning system to a formal mathematics library 14</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">GPT-f Proof Assistant</head><p>We created an online proof assistant <ref type="bibr" target="#b14">15</ref> to allow interactive proof constructions with the assistance of our models.</p><p>We used it to formalize more than 200 theorems and exercises. We found our models to be particularly useful to automatically generate a variety of technical low level proofsteps required in most Metamath proofs, search the library by adapting existing theorems to the format needed by the user (e.g., <ref type="bibr" target="#b10">11</ref> https://github.com/metamath/set.mm/pull/1547 12 https://github.com/metamath/set.mm/pull/1561 13 https://groups.google.com/g/metamath/c/-FNsw2wyllI <ref type="bibr" target="#b13">14</ref> To determine whether other deep learning-based provers have made contributions to their respective libraries, we looked for such contributions in the following systems: Holist family in HOL Light, CoqGym+ASTatic in Coq, TacticToe in HOL4. In addition, we interviewed 6 experts in formal mathematics and/or deep learning applied to formal mathematics. <ref type="bibr" target="#b14">15</ref> https://groups.google.com/g/metamath/c/D09W2QVR-_I/m/g_rsqGj0AAAJ <ref type="figure">Figure 2</ref>: Screenshot of the GPT-f Proof Assistant deduction form <ref type="bibr" target="#b15">16</ref> ) and suggest theorems to use. Even when mistaken, our models generally go for the right theorems, whose erroneous substitutions are often easy to fix by humans.</p><p>We shared the proof assistant with the Metamath community with the objective for it to be mutually beneficial, helping the community to be more productive and reciprocally helping us improve our models' accuracy by automatically gathering human feedback. We also plan to extend GPT-f to other formal systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we present the GPT-f automated prover and proof assistant and show that the Transformer is suitable to formal reasoning, achieving a new state of the art result on the Metamath library.</p><p>In particular we demonstrate the importance of pre-training as well as iterative training of a value function. Our results suggest that tightly coupling a deep learning system with a formal system opens up interesting opportunities for further research, with the goal of better leveraging the generative power of the former and the verification capabilities of the latter. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Key Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Example Proofs Generated</head><p>In this appendix, we display a selection of proofs generated by GPT-f (from our valid set). The right column contains the current goal. The left column displays the name of the theorem applied by to the goal. Proofs are read bottom-up and the statement being demonstrated is the last goal of the table. The subgoals generated by a proof step can be retrieved by looking at the theorem names that are indented by one additional space. The statement of the theorems can be retrieved with the Metamath Proof Explorer. Substitutions are omitted for clarity, but can be inferred by looking at the statement of the theorem being applied and comparing it with the current goal and associated subgoals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Proof of nn0onn0ex</head><p>This proof demonstrates that n ? N ? n+1 2 ? N =? ?m ? N : n = 2m + 1. It is interesting for its first proof step. syl2anc 17 states that assuming P =? Q, P =? R, Q ? R =? S then P =? S. P is mechanically unified with n ? N ? n+1 2 ? N, and S with ?m ? N : n = 2m + 1 but the model freely generates substitutions for Q and R. Looking at the subgoals, Q is substituted with n?1 2 ? N and R with n = 2 n?1 2 + 1 which materialises a witness for the existence of m. The model is left to demonstrate n ? N ? n+1 2 ? N =? n?1 2 ? N, then n ? N ? n+1 2 ? N =? n = 2 n?1 2 + 1 and finally n?1 2 ? N ? n = 2 n?1 2 + 1 =? ?m ? N : n = 2m + 1 using the existential specialization provided by rspcev 18 . Such generation of exogenous terms, here to demonstrate an existence proof, is exactly what motivated our work. It's therefore encouraging to witness it effectively happening in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of uznn0sub</head><p>This proof demonstrates that n ? m ? Z =? (n ? m) ? N. It exhibits another form of term generation. Here, sylibr 19 states that assuming P =? Q, R ? Q then P =? R. Again, P is mechanically unified to n ? m ? Z, and R with (n ? m) ? N. The model is left to generate freely a substitution for Q: (n ? m) ? Z ? 0 ? (n ? m). The equivalence R ? Q to demonstrate becomes <ref type="bibr" target="#b16">17</ref> Metamath Proof Explorer -syl2anc http://us.metamath.org/mpeuni/syl2anc.html <ref type="bibr" target="#b17">18</ref> Metamath Proof Explorer -rspcev http://us.metamath.org/mpeuni/rspcev.html <ref type="bibr" target="#b18">19</ref> Metamath Proof Explorer -sylibr http://us.metamath.org/mpeuni/sylibr.html (n ? m) ? N ? (n ? m) ? Z ? 0 ? (n ? m) which is exactly the statement of a theorem available in the Metamath library, elnn0z 20 . The statement of elnn0z is memoized by the model, and the generation of the substitution term for Q is driven by this memoization. This proof displays the model capabilities to demonstrate non-trivial propositional logic statements, a task of interest because of its relationship to SAT solving. + pm2.21 |-( -. ph -&gt; ( ph -&gt; ps ) ) + orcd |-( -. ph -&gt; ( ( ph -&gt; ps ) \/ ( ph -&gt; ch ) ) ) + ax-1.</p><p>|-( ps -&gt; ( ph -&gt; ps ) ) + ax-1 |-( ch -&gt; ( ph -&gt; ch ) ) + orim12i |-( ( ps \/ ch ) -&gt; ( ( ph -&gt; ps ) \/ ( ph -&gt; ch ) ) ) + ja |-( ( ph -&gt; ( ps \/ ch ) ) -&gt; ( ( ph -&gt; ps ) \/ ( ph -&gt; ch ) ) ) + orc |-( ps -&gt; ( ps \/ ch ) ) + imim2i |-( ( ph -&gt; ps ) -&gt; ( ph -&gt; ( ps \/ ch ) ) ) + olc |-( ch -&gt; ( ps \/ ch ) ) + imim2i |-( ( ph -&gt; ch ) -&gt; ( ph -&gt; ( ps \/ ch ) ) ) + jaoi |-( ( ( ph -&gt; ps ) \/ ( ph -&gt; ch ) ) -&gt; ( ph -&gt; ( ps \/ ch ) ) ) + impbii |-( ( ph -&gt; ( ps \/ ch ) ) &lt;-&gt; ( ( ph -&gt; ps ) \/ ( ph -&gt; ch ) ) ) + bicomi |-( ( ( ph -&gt; ps ) \/ ( ph -&gt; ch ) ) &lt;-&gt; ( ph -&gt; ( ps \/ ch ) ) ) <ref type="bibr" target="#b19">20</ref> Metamath Proof Explorer -elnn0z http://us.metamath.org/mpeuni/elnn0z.html</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>{</head><label></label><figDesc>"proof_label": "unidmrn", "goal": "[[ ]] |-U. U. '' A = ( dom A u. ran A )", "proof_step": "[[ |-A = B |-C = B ]] |-A = C \\ {{ A : U. U. '' A }} \\ {{ B : ( ran '' A u. dom '' A ) }} \\ {{ C : ( dom A u. ran A ) }}", "proof_step_hash": "37yZVNorgF8=", "parent_hash": ["n4Kl7judEN4="] }</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>-( ( ( J e. Nrm /\ f e. ( J Homeo K ) ) /\ ( x e. K /\ y e. ( ( Clsd ' K ) i^i~P x ) ) ) -&gt; ( '' f " x ) e. J ) PROOFSTEP [[ |-( ph -&gt; ps ) |-( ph -&gt; ch ) |-( ( ps /\ ch ) -&gt; th ) ]] |-( ph -&gt; th ) {{ ch : x e. K }} {{ ph : ( ( J e. Nrm /\ f e. ( J Homeo K ) ) /\ ( x e. K /\ y e. ( ( Clsd ' K ) i^i~P x ) ) ) }} {{ ps : f e. ( J Cn K ) }} {{ th : ( '' f " x ) e. J }} &lt;|endoftext|&gt; 4.3 Proof Search</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Proof search consists in maintaining a proof tree where multiple tactics are explored for each goal, starting from the root goal. Goals are expanded by cumulative (tactic) logprob priority.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Examples of equalities produced by the generator: ABBA(AB) 2 + (C + A) = A + (ABBA) 2 + C (AA) 2 = A 2 AA ((BA + CA) 2 ) 2 = (BA + CA) 2 (BAAB + ACCA + BAAC + ABCA) ((A + B) 2 ) 2 (A + A) = ((A + B) 2 (AB + AB + AA + BB) + (A + B) 2 (AB + AB + AA + BB))A 4.6.3 Default augmented Dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>( N e. CC /\ 1 e. CC ) -&gt; ( N -1 ) e. CC ) + sylancl |-( N e. NN0 -&gt; ( N -1 ) e. CC ) + 2cnd |-( N e. NN0 -&gt; 2 e. CC ) + 2ne0 |-2 =/= 0 + a1i |-( N e. NN0 -&gt; 2 =/= 0 ) + divcan2d |-( N e. NN0 -&gt; ( 2 x. ( ( N -1 ) / 2 ) ) = ( N -1 ) ) + adantr |-( ( N e. NN0 /\ ( ( N + 1 ) / 2 ) e. NN0 ) -&gt; ( 2 x. ( ( N -1 ) / 2 ) ) = ( N -1 ) ) + oveq1d |-( ( N e. NN0 /\ ( ( N + 1 ) / 2 ) e. NN0 ) ( N e. CC /\ 1 e. CC ) -&gt; ( ( N -1 ) + 1 ) = N ) + sylancl |-( N e. NN0 -&gt; ( ( N -1 ) + 1 ) = N ) + adantr |-( ( N e. NN0 /\ ( ( N + 1 ) / 2 ) e. NN0 ) -&gt; ( ( N -1 ) + 1 ) = N ) + eqtr2d |-( ( N e. NN0 /\ ( ( N + 1 ) / 2 ) e. NN0 ) -&gt; N = ( ( 2 x. ( ( N -1 ) / 2 ) ) + 1 ) ) + oveq2 |-( m = ( ( N -1 ) / 2 ) -&gt; ( 2 x. m ) = ( 2 x. ( ( N -1 ) / 2 ) ) ) + oveq1d |-( m = ( ( N -1 ) / 2 ) -&gt; ( ( 2 x. m ) + 1 ) = ( ( 2 x. ( ( N -1 ) / 2 ) ) + 1 ) ) + eqeq2d |-( m = ( ( N -1 ) / 2 ) -&gt; ( N = ( ( 2 x. m ) + 1 ) &lt;-&gt; N = ( ( 2 x. ( ( N -1 ) / 2 ) ) + 1 ) ) ) + rspcev |-( ( ( ( N -1 ) / 2 ) e. NN0 /\ N = ( ( 2 x. ( ( N -1 ) / 2 ) ) + 1 ) ) -&gt; E. m e. NN0 N = ( ( 2 x. m ) + 1 ) ) + syl2anc |-( ( N e. NN0 /\ ( ( N + 1 ) / 2 ) e. NN0 ) -&gt; E. m e. NN0 N = ( ( 2 x. m ) + 1 ) )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>N e. ( ZZ&gt;= ' M ) -&gt; N e. ZZ ) + eluzel2 |-( N e. ( ZZ&gt;= ' M ) -&gt; M e. ZZ ) + zsubcld |-( N e. ( ZZ&gt;= ' M ) -&gt; ( N -M ) e. ZZ ) + eluzle |-( N e. ( ZZ&gt;= ' M ) -&gt; M &lt;_ N ) + eluzelre |-( N e. ( ZZ&gt;= ' M ) -&gt; N e. RR ) + eluzel2 |-( N e. ( ZZ&gt;= ' M ) -&gt; M e. ZZ ) + zred |-( N e. ( ZZ&gt;= ' M ) -&gt; M e. RR ) + subge0d |-( N e. ( ZZ&gt;= ' M ) -&gt; ( 0 &lt;_ ( N -M ) &lt;-&gt; M &lt;_ N ) ) + mpbird |-( N e. ( ZZ&gt;= ' M ) -&gt; 0 &lt;_ ( N -M ) ) + jca |-( N e. ( ZZ&gt;= ' M ) -&gt; ( ( N -M ) e. ZZ /\ 0 &lt;_ ( N -M ) ) ) + elnn0z |-( ( N -M ) e. NN0 &lt;-&gt; ( ( N -M ) e. ZZ /\ 0 &lt;_ ( N -M ) ) ) + sylibr |-( N e. ( ZZ&gt;= ' M ) -&gt; ( N -M ) e. NN0 ) B.3 Proof of pm4.78</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mix and source of data involved in the WebMath dataset. Despite being among the largest formal mathematics libraries, the Metamath library remains scarce in the context of deep learning, especially in light of the advantages demonstrated on various NLP tasks by pre-training on large corpora. Also set.mm mostly focuses on well-known high-level theorems and does not include a large number of technical lemmas resembling the type of mathematics exercises used as curriculum for humans. Finally, Metamath lacking high level tactics such as HOL Light's ARITH_RULE 7 , or Lean's ring 8 , it is critical to ensure that our models are capable of proving at least basic technical theorems generally handled by high-level tactics in other systems (in domains such as arithmetic or ring equalities and inequalities)</figDesc><table><row><cell>Dataset</cell><cell>Size</cell><cell>Mix</cell></row><row><cell>Github</cell><cell cols="2">23 GB 33%</cell></row><row><cell>arXiv Math</cell><cell cols="2">10 GB 33%</cell></row><row><cell cols="3">Math StackExchange 2 GB 33%</cell></row><row><cell>4.6 Synthetic Datasets</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Average number of proofsteps produced by our synthetic generators for ndigits = 3, 9, 18.</figDesc><table><row><cell></cell><cell>3</cell><cell>9</cell><cell>18</cell></row><row><cell cols="4">Addition (in Z) 19 48 94</cell></row><row><cell>Division</cell><cell cols="3">13 93 292</cell></row><row><cell>Modulo</cell><cell cols="3">25 82 206</cell></row><row><cell>Exponentiation</cell><cell cols="3">7 27 68</cell></row></table><note>4 https://console.cloud.google.com/marketplace/details/github/github-repos5 https://arxiv.com/help/bulk_data6 https://archive.org/details/stackexchange7 https://www.cl.cam.ac.uk/~jrh13/hol-light/HTML/ARITH_RULE.html8 https://leanprover-community.github.io/mathlib_docs/algebra/ring/basic.html#ring9 Metamath Proof Explorer -decadd http://us.metamath.org/mpeuni/decadd.html 10 Metamath Proof Explorer -decaddc http://us.metamath.org/mpeuni/decaddc.html</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Metamath theorems use by our Ring Algebra synthetic generators. Theorems are available in the Matmath Proof Explorer.</figDesc><table><row><cell>Theorem</cell><cell cols="2">Weight Description</cell></row><row><cell>eqcomd</cell><cell>1</cell><cell>Commutative law for class equality.</cell></row><row><cell>int-addcomd</cell><cell>1</cell><cell>Addition commutativity.</cell></row><row><cell>int-addassocd</cell><cell>1</cell><cell>Addition associativity.</cell></row><row><cell>int-mulcomd</cell><cell>1</cell><cell>Multiplication commutativity.</cell></row><row><cell>int-mulassocd</cell><cell>1</cell><cell>Multiplication associativity.</cell></row><row><cell>int-leftdistd</cell><cell>3</cell><cell>Left distribution of multiplication over addition.</cell></row><row><cell>int-rightdistd</cell><cell>3</cell><cell>Right distribution of multiplication over addition.</cell></row><row><cell>int-sqdefd</cell><cell>5</cell><cell>Definition of the square.</cell></row><row><cell>muladdd2</cell><cell>5</cell><cell>Product of two sums</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Number of proofs and proofsteps adjunct to constitute our augmented dataset.</figDesc><table><row><cell>Generator</cell><cell cols="2">Number of Proofs Number of Proofsteps</cell></row><row><cell>9-digit Addition (in Z)</cell><cell>100</cell><cell>4541</cell></row><row><cell>9-digit Division</cell><cell>100</cell><cell>10047</cell></row><row><cell>9-digit Modulo</cell><cell>50</cell><cell>4438</cell></row><row><cell>9-digit Exponentiation</cell><cell>50</cell><cell>910</cell></row><row><cell>Ring Equalities (depth = 6, nbvar = 2)</cell><cell>50</cell><cell>1373</cell></row><row><cell>Ring Equalities (depth = 6, nbvar = 3)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Baseline performance from MetaGen-IL as well as a 160m parameters model trained on the raw and augmented datasets.</figDesc><table><row><cell>Model</cell><cell cols="2">Performance # Tokens</cell></row><row><cell>MetaGen-IL[25]</cell><cell>21.16%</cell><cell>N/A</cell></row><row><cell>160m raw dataset (ours)</cell><cell>29.22%</cell><cell>18B</cell></row><row><cell>160m augmented dataset (ours)</cell><cell>28.96%</cell><cell>18B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Performance for various model sizes trained on the augmented datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">Performance Perplexity # Tokens</cell></row><row><cell>160m augmented</cell><cell>28.96%</cell><cell>1.041</cell><cell>18B</cell></row><row><cell>400m augmented</cell><cell>30.23%</cell><cell>1.042</cell><cell>18B</cell></row><row><cell>700m augmented</cell><cell>31.58%</cell><cell>1.040</cell><cell>18B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Performance for various model sizes and pre-training datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">Performance Perplexity # Tokens</cell></row><row><cell>160m from scratch</cell><cell>28.96%</cell><cell>1.041</cell><cell>18B</cell></row><row><cell>160m CommonCrawl</cell><cell>32.34%</cell><cell>1.030</cell><cell>16B</cell></row><row><cell>160m Github</cell><cell>33.61%</cell><cell>1.030</cell><cell>16B</cell></row><row><cell>160m WebMath</cell><cell>34.79%</cell><cell>1.029</cell><cell>16B</cell></row><row><cell>700m from scratch</cell><cell>31.58%</cell><cell>1.040</cell><cell>18B</cell></row><row><cell>700m CommonCrawl</cell><cell>39.61%</cell><cell>1.026</cell><cell>15B</cell></row><row><cell>700m Github</cell><cell>41.55%</cell><cell>1.025</cell><cell>15B</cell></row><row><cell>700m WebMath</cell><cell>42.56%</cell><cell>1.024</cell><cell>15B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>Performance for model sizes ranging from 160m to 1.5b parameters, pre-trained on WebMath.</figDesc><table><row><cell>Model</cell><cell cols="3">Performance Perplexity # Tokens</cell></row><row><cell>160m (WebMath)</cell><cell>34.79%</cell><cell>1.029</cell><cell>16B</cell></row><row><cell>400m (WebMath)</cell><cell>39.94%</cell><cell>1.026</cell><cell>15B</cell></row><row><cell>700m (WebMath)</cell><cell>42.56%</cell><cell>1.024</cell><cell>15B</cell></row><row><cell>1p5b (WebMath)</cell><cell>42.39%</cell><cell>1.024</cell><cell>13B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>Performance of the 160m and 700m parameters models as we iterate through the learned value function data generation and re-training process. policy only consists in adding new positive proofs found to the policy training (without training a value function) while policy+value consists in the full iterative data-generation and training described in section 4.7.</figDesc><table><row><cell>Model</cell><cell cols="3">Iteration 0 Iteration 1 Iteration 2</cell></row><row><cell>160m (WebMath) policy only</cell><cell>34.79%</cell><cell>38.17%</cell><cell>38.34%</cell></row><row><cell>160m (WebMath) policy+value</cell><cell></cell><cell>39.27%</cell><cell>40.70%</cell></row><row><cell>700m (WebMath) policy only</cell><cell>42.56%</cell><cell>42.23%</cell><cell>43.15%</cell></row><row><cell>700m (WebMath) policy+value</cell><cell></cell><cell>44.59%</cell><cell>47.21%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Ablation of the augmented dataset for 160m and 700m parameters models.</figDesc><table><row><cell>Model</cell><cell cols="3">Performance Perplexity # Tokens</cell></row><row><cell>160m (WebMath) raw dataset</cell><cell>34.12%</cell><cell>1.029</cell><cell>16B</cell></row><row><cell>160m (WebMath) augmented dataset</cell><cell>34.79%</cell><cell>1.029</cell><cell>16B</cell></row><row><cell>700m (WebMath) raw dataset</cell><cell>40.28%</cell><cell>1.024</cell><cell>15B</cell></row><row><cell>700m (WebMath) augmented dataset</cell><cell>42.56%</cell><cell>1.024</cell><cell>15B</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Performance of our models on 100 test statements from our synthetic generators (run with the same parameters used to augment the training set (seetable 4).</figDesc><table><row><cell>Model</cell><cell cols="3">9-digit addition 9-digit division Ring equalities</cell></row><row><cell>160m raw</cell><cell>13% (0)</cell><cell>4% (0)</cell><cell>6% (0)</cell></row><row><cell>160m augmented</cell><cell>78% (100)</cell><cell>27% (100)</cell><cell>77% (100)</cell></row><row><cell>160m policy+value (iteration 1)</cell><cell>87% (100)</cell><cell>24% (100)</cell><cell>71% (100)</cell></row><row><cell>160m policy+value (iteration 2)</cell><cell>90% (100)</cell><cell>28% (100)</cell><cell>79% (100)</cell></row><row><cell>700m raw</cell><cell>12% (0)</cell><cell>5% (0)</cell><cell>7% (0)</cell></row><row><cell>700m augmented</cell><cell>76% (100)</cell><cell>32% (100)</cell><cell>82% (100)</cell></row><row><cell>700m policy+value (iteration 1)</cell><cell>90% (100)</cell><cell>40% (100)</cell><cell>78% (100)</cell></row><row><cell>700m policy+value (iteration 2)</cell><cell>92% (100)</cell><cell>47% (100)</cell><cell>88% (100)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 :</head><label>12</label><figDesc>Performance of our 700m model policy+value (iteration 2) as we double the number of attempts a per proposition (with d = 256).</figDesc><table><row><cell cols="2">Attempts Performance</cell><cell>Delta</cell></row><row><cell>a = 2</cell><cell>42.90%</cell><cell></cell></row><row><cell>a = 4</cell><cell>47.29%</cell><cell>+4.39%</cell></row><row><cell>a = 8</cell><cell>51.26%</cell><cell>+3.97%</cell></row><row><cell>a = 16</cell><cell>54.05%</cell><cell>+2.99%</cell></row><row><cell>a = 32</cell><cell>56.50%</cell><cell>+2.45%</cell></row><row><cell cols="3">Perf test a=32,e=32,d=256 (? 700m ) = 56.22%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 13 :</head><label>13</label><figDesc>Key results described in this paper (on the valid set) with a summary of the source of performance gains.</figDesc><table><row><cell>Model</cell><cell>Performance</cell><cell>Gain</cell><cell>Main ablation</cell></row><row><cell>MetaGen-IL [25]</cell><cell>21.16%</cell><cell></cell><cell>Baseline and state of the art.</cell></row><row><cell>160m (ours)</cell><cell>28.96%</cell><cell cols="2">+7.8% Use of Transformers.</cell></row><row><cell>700m (ours)</cell><cell>31.58%</cell><cell cols="2">+2.5% Increase in parameters count.</cell></row><row><cell>700m WebMath (ours)</cell><cell>42.56%</cell><cell cols="2">+10.9% Pre-training.</cell></row><row><cell>700m policy+value (ours)</cell><cell>47.21%</cell><cell cols="2">+4.6% Iterated learned value function.</cell></row><row><cell>700m policy+value a = 32 (ours)</cell><cell>56.50%</cell><cell cols="2">+9.2% Increased test-time compute.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Metamath Proof Explorer -imo72b2 http://us.metamath.org/mpeuni/imo72b2.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">Deduction Form and Natural Deduction http://us.metamath.org/mpeuni/mmnatded.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Szymon Sidor, Jakub Pachocki, Harri Edwards, Yura Burda and Vedant Misra inspired many of the ideas presented in this work, offering their guidance throughout the process of building GPT-f. Auguste Poiroux implemented the synthetic dataset generators presented in this paper, and formalized a large number of theorems using the proof assistant, providing invaluable feedback in the process. Szymon Sidor, Pranav Shyam, John Schulman, Jared Kaplan, Ryan Lowe and Jack Clark slogged through drafts of this paper, identifying errors and sources of confusion as well as providing helpful suggestions. Finally, the authors would like to thank the whole Metamath community for their support, feedback, and encouragement, in particular, David A. Wheeler for his motivating enthusiasm and Mario Carneiro for his precious help on a wide variety of technical questions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Towards end-to-end speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1764" to="1772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Sundaram Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingliang</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dota 2 with large scale deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Przemys?aw</forename><surname>D?biak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christy</forename><surname>Dennison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Farhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quirin</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shariq</forename><surname>Hashme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06680</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Grandmaster level in starcraft ii using multi-agent reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dudzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petko</forename><surname>Ewalds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georgiev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">575</biblScope>
			<biblScope unit="issue">7782</biblScope>
			<biblScope unit="page" from="350" to="354" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Solving rubik&apos;s cube with a robot hand</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilge</forename><surname>Akkaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciek</forename><surname>Chociej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Petron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Paino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Powell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Ribas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.07113</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end training of deep visuomotor policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1334" to="1373" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thore</forename><surname>Graepel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.01815</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">History of interactive theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freek</forename><surname>Wiedijk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Logic</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="135" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning to prove theorems by learning to generate theorems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07019</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepmath-deep sequence models for premise selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>E?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2235" to="2243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Premise selection for theorem proving by deep graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2786" to="2796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>B?nz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David L</forename><surname>Dill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03685</idno>
		<title level="m">Learning a sat solver from single-bit supervision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep network guided proof search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cezary</forename><surname>Kaliszyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06972</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Holist: An environment for machine learning of higher order logic theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stewart</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="454" to="463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning to reason in large theories without imitation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><forename type="middle">N</forename><surname>Loos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.10501</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Mathematical reasoning via self-supervised skip-tree training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Markus N Rabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kshitij</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Gamepad: A learning environment for theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00608</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning to prove theorems via interacting with proof assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09381</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Holophrasm: a neural automated theorem prover for higher-order logic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Whalen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.02644</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Program induction by rationale generation: Learning to solve and explain algebraic word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04146</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aida</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saadia</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mathqa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13319</idno>
		<title level="m">Towards interpretable math word problem solving with operation-based formalisms</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning for symbolic mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Charton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01412</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Universal transformers. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01557</idno>
		<title level="m">Analysing mathematical reasoning abilities of neural models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Metamath: A Computer Language for Pure Mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Megill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wheeler</surname></persName>
		</author>
		<ptr target="http://us.metamath.org/downloads/metamath.pdf" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">How Metamath Proofs Work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Megill</surname></persName>
		</author>
		<ptr target="http://us.metamath.org/mpeuni/mmset.html#proofs" />
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">The &quot;de Bruijn factor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freek</forename><surname>Wiedijk</surname></persName>
		</author>
		<ptr target="http://www.cs.ru.nl/freek/factor/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Int: An inequality benchmark for evaluating generalization in theorem proving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.02924</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Thinking fast and slow with deep learning and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5360" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
