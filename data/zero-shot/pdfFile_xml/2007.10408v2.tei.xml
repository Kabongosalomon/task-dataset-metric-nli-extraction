<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingshen</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwen</forename><surname>Ma</surname></persName>
						</author>
						<title level="a" type="main">PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent research has shown that incorporating equivariance into neural network architectures is very helpful, and there have been some works investigating the equivariance of networks under group actions. However, as digital images and feature maps are on the discrete meshgrid, corresponding equivariance-preserving transformation groups are very limited.</p><p>In this work, we deal with this issue from the connection between convolutions and partial differential operators (PDOs). In theory, assuming inputs to be smooth, we transform PDOs and propose a system which is equivariant to a much more general continuous group, the n-dimension Euclidean group. In implementation, we discretize the system using the numerical schemes of PDOs, deriving approximately equivariant convolutions (PDO-eConvs). Theoretically, the approximation error of PDO-eConvs is of the quadratic order. It is the first time that the error analysis is provided when the equivariance is approximate. Extensive experiments on rotated MNIST and natural image classification show that PDO-eConvs perform competitively yet use parameters much more efficiently. Particularly, compared with Wide ResNets, our methods result in better results using only 12.6% parameters.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the past few years, convolutional neural network (CNN) models have become the dominant machine learning methods in the field of computer vision for various tasks, such as image recognition, objective detection and semantic segmentation. Compared with fully-connected neural networks, 1 School of Mathematical Sciences and LMAM, Peking University, Beijing 100871 2 Key Lab. of Machine Perception (MoE), School of EECS, Peking University, Beijing 100871. Correspondence to: Zhouchen Lin &lt;zlin@pku.edu.cn&gt;, Jinwen Ma &lt;jwma@math.pku.edu.cn&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s). a significant advantage of CNNs is that they are shift equivariant: shifting an image and then feeding it through a number of layers is the same as feeding the original image and then shifting the resulted feature maps. In other words, the translation symmetry is preserved by each layer. Also, the equivariance property brings in weight sharing, with which we can use parameters more efficiently.</p><p>Motivated by this, <ref type="bibr" target="#b2">Cohen and Welling (2016)</ref> proposed group equivariant CNNs (G-CNNs), showing how convolutional networks can be generalized to exploit larger groups of symmetries, including rotations and reflections. G-CNNs are equivariant to the group p4m or p4 1 , and work on square lattices. In addition, <ref type="bibr" target="#b12">Hoogeboom et al. (2018)</ref> proposed HexaConv and showed how one can implement planar convolutions and group convolutions over hexagonal lattices, instead of square ones. As a result, the equivariance is expanded to p6m. However, it seems impossible to design CNNs that are equivariant to the rotation angles other than ?/2 (p4m) and ?/3 (p6m) as there does not seem to exist other rotational symmetric discrete lattices on the 2D plane, if one considers equivariance in the ways as <ref type="bibr" target="#b2">(Cohen &amp; Welling, 2016)</ref> and <ref type="bibr" target="#b12">(Hoogeboom et al., 2018)</ref>.</p><p>In order to exploit more symmetries, <ref type="bibr" target="#b35">Weiler et al. (2018)</ref> employed harmonics as steerable filters to achieve exact equivariance to larger transformation groups in the continuous domain. However, they are difficult to preserve strong equivariance when operating on discrete pixel grids, for two main reasons: (i) When a harmonic is sampled on grids with a low rate, it could appear as a lower harmonic, which introduces aliasing artifacts. (ii) With Gaussian radial profiles as radial functions, harmonics ranged out of the sampled kernel support, leading to a high equivariance error on implementation.</p><p>From another point of view, a conventional convolutional filter can also be viewed as a linear combination of PDOs, which was proposed by <ref type="bibr" target="#b30">(Ruthotto &amp; Haber, 2018)</ref>. With this new understanding, we assume inputs are smooth functions, and then show how to transform the PDOs and get a system which is exactly equivariant to a much more general continu-ous transformation group, the n-dimension Euclidean group. To implement our theory on discrete digital images, we discretize the system using the numerical schemes of PDOs and get approximately equivariant convolutions. Particularly, the discretized convolutions can achieve a quadratic order equivariance approximation, and it is the first time that the error analysis is provided when the equivariance is approximate. As the derived equivariant convolutions are based on PDOs, we refer to them as PDO-eConvs.</p><p>We evaluate the performance of PDO-eConvs on rotated MNIST and natural image classification tasks. Extensive experiments verify that PDO-eConv produces very competitive results and is significantly efficient on parameter learning..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our contributions are as follows:</head><p>? With the assumption that inputs are smooth, we use PDOs to design a system that is equivariant to a much more general continuous group, the n-dimensional Euclidean group.</p><p>? The equivariance is exact in the continuous domain. It becomes approximate only after the discretization. Moreover, it is the first time that the error analysis is provided when the equivariance is approximate. To be specific, the approximation error of PDO-eConvs is of the quadratic order, indicating a precise approximation.</p><p>? Extensive experiments on PDO-eConvs show that our methods perform competitively and have significant parameter efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior and Related Work</head><p>2.1. Equivariant CNNs <ref type="bibr" target="#b23">Lenc &amp; Vedaldi (2015)</ref> showed that the AlexNet CNN <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref> trained on ImageNet spontaneously learned representations that are equivariant to flips, scalings and rotations, which supported the idea that equivariance is a good inductive bias for CNNs. <ref type="bibr" target="#b2">Cohen &amp; Welling (2016;</ref> succeeded in incorporating equivariance into neural networks. However, these methods can only deal with a 4-fold rotational symmetry for images with square pixels. <ref type="bibr" target="#b12">Hoogeboom et al. (2018)</ref> alleviated this limit by implementing planar convolutions and group convolutions over hexagonal lattices. Consequently, they can deal with a 6fold rotational symmetry.</p><p>Since there does not seem to have more rotational symmetries on lattices in the 2D plane, some works designed approximately equivariant networks w.r.t. larger groups. <ref type="bibr" target="#b39">Zhou et al. (2017)</ref> and <ref type="bibr" target="#b27">Marcos et al. (2017)</ref> utilized bilinear interpolation to help produce feature maps at different orientations. They are inherently approximately equivariant. By comparison, ours is exactly equivariant in the continuous domain. <ref type="bibr">Worral et al. (2017)</ref> used harmonics to extract features and achieve equivariance to 360-rotation, but the equivariance is destroyed after Gaussion-resampling. <ref type="bibr" target="#b35">Weiler et al. (2018)</ref> and <ref type="bibr" target="#b34">Weiler &amp; Cesa (2019)</ref> employed harmonics as steerable filters to achieve exact equivariance w.r.t. larger groups in the continuous domain, but the equivariance is difficult to preserve in the discrete domain due to aliasing artifacts and limited kernel support. So they used much larger filters to achieve approximate equivariance, resulting in CNNs with a large computational burden. By contrast, PDO-eConvs can use a relatively small kernel size to achieve theoretically guaranteed exact equivariance in the discrete domain, which makes big difference.</p><p>There are also some empirical approaches for enforcing equivariance. A commonly utilized technique is data augmentation, see e.g. <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref>. The basic idea is to enrich the training set by transformed samples. <ref type="bibr" target="#b20">Laptev et al. (2016)</ref> used parallel siamese architectures for the considered transformation set and applying the transformationinvariant pooling (TI-Pooling) operator on their outputs. <ref type="bibr" target="#b14">Jaderberg et al. (2015)</ref> applied a differentiable module to actively transform feature maps, and then <ref type="bibr" target="#b5">Esteves et al. (2018)</ref> used this method to help enforce equivariance under rotation and scale transformations. In <ref type="bibr" target="#b31">(Sabour et al., 2017;</ref><ref type="bibr" target="#b11">Hinton et al., 2018)</ref>, capsules are used to represent the location information and enforce equivariance. However, these methods learn the transformations directly from datas, which are inferior to those methods incorporating equivariance into architectures for lack of interpretability and reliability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Relationship between Convolutions and PDOs</head><p>There have been extensive works <ref type="bibr" target="#b15">(Jain &amp; Jain, 1978;</ref><ref type="bibr" target="#b36">Witkin, 1987;</ref><ref type="bibr" target="#b17">Koenderink, 1984;</ref><ref type="bibr" target="#b29">Perona &amp; Malik, 1990;</ref><ref type="bibr" target="#b28">Osher &amp; Rudin, 1990)</ref> utilizing PDOs to process images. The relationship between convolutions and PDOs was presented in <ref type="bibr" target="#b4">(Dong et al., 2017;</ref><ref type="bibr" target="#b30">Ruthotto &amp; Haber, 2018)</ref>, where the authors translated convolutional filters to linear combinations of PDOs, and this approximation has good analytical properties. Some works <ref type="bibr" target="#b25">(Long et al., 2018;</ref> used this new understanding to help design CNNs. Also, this relationship is an important theoretical foundation of our work.</p><p>Actually, there exist some works using PDOs to investigate equivariance. <ref type="bibr" target="#b24">Liu et al. (2013)</ref> designed a partial differential equation (PDE) using a linear combination of equivariant PDOs and proposed learning based PDEs, which are naturally shift and rotation equivariant. <ref type="bibr" target="#b6">Fang et al. (2017)</ref> further adopted this technique on face recognition task. However, the capacity of learning based PDEs cannot be compared with that of nowadays widely used CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Mathematical Framework</head><p>In this section we design a group equivariant system using PDOs. To make concepts and notations more explicit, we give a preliminary introduction of groups and equivariance formally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prior Knowledge</head><p>The Isometry Group In mathematics, the isometry group is a group consisted of isometry transformations, which preserve the distance of any two points. Particularly, the Euclidean group is the largest isometry group defined on R n , which we denote as E(n). Given y ? R n , the isometry transformation is:</p><formula xml:id="formula_0">y :? Ay + x,<label>(1)</label></formula><p>where A is an orthogonal matrix, i.e., A A = I, and x ? R n . When A = I, the transformations in (1) compose the translation group R n ; + . Without ambiguity, we use R n to denote the translation group in the following text. When x = 0, E(n) degenerates to the orthogonal group, O(n), which contains all the orthogonal transformations, including reflections and rotations. We use A to parameterize O(n). R n and O(n) are both subgroups of E(n), and E(n) = R n O(n) ( is a semidirect-product). We use (x, A) to represent the element in E(n), where x and A represent a translation and an orthogonal transformation, respectively. Restricting the domain of A and x, we can also use this representation to parametrize any subgroup of E(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Actions on Functions</head><p>Inputs and intermediate feature maps can be naturally modeled as functions defined in the continuous domain. To be specific, we model the input r as a smooth function defined on R n and the intermediate feature map e as a smooth function defined on E(n), where the smoothness of e means that if we use the representation (x, A) mentioned above, the feature map e(x, A) is smooth w.r.t. x when A is fixed. So e can also be viewed as a function defined on R n with infinite channels indexed by A.</p><p>We use C ? (R n ) and C ? (E(n)) 2 to denote the function spaces of r and e, respectively .</p><p>In this way, transformations like rotations and reflections on inputs and feature maps can be mathematically formulated.</p><p>Here, we introduce two transformations used in our theory.</p><p>? Suppose that r ? C ? (R n ) and A ? O(n), then the transfomation A acts on r in the following way 3 :</p><formula xml:id="formula_1">?x ? R n , ? R A [r](x) = r( A ?1 x).<label>(2)</label></formula><p>2 For the simplicity of our theory, we require that r ? C ? (R n ). However, in implementation, we only require that r ? C 4 (R n ). The requirement on e is the same. <ref type="bibr">3</ref> We use [?] to denote that an operator acts on a function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>! "</head><p>!? " ? ? <ref type="figure">Figure 1</ref>. The transformation g can be preserved by the mapping ?.</p><p>? Suppose that e ? C ? (E(n)) and A ? O(n), then A acts on e in the following way:</p><formula xml:id="formula_2">?a ? E(n), ? E A [e](a) = e( A ?1 a),<label>(3)</label></formula><p>where A ?1 a is group product on E(n). Using the representation of E(n), it is of the following more detailed form:</p><formula xml:id="formula_3">? E A [e](x, A) = e( A ?1 x, A ?1 A),<label>(4)</label></formula><p>where (x, A) is the representation of a.</p><p>Equivariance Equivariance measures how the outputs of a mapping transform in a predictable way with the transformation of the inputs. Here, we formulate it in detail. Let ? be a mapping from the input feature space to the output feature space and G is a group. A group equivariant ? satisfies that</p><formula xml:id="formula_4">?g ? G, ?[? g [f ]] = ? g [?[f ]],</formula><p>where f can be any input feature map in the input feature space, and ? g and ? g denote how the transformation g acts on input features and output features, respectively.</p><p>That is, transforming an input f by a transformation g (forming ? g [f ]) and then passing it through the mapping ? should give the same result as first mapping f through ? and then transforming the representation. The schema of equivariance is shown in <ref type="figure">Figure 1</ref>. It is easy to see that if each layer of a network is equivariant, the equivariance can be preserved by the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Group Equivariant Differential Operators</head><p>We refer to H(u 1 , u 2 , ? ? ? , u n ; ?) as a polynomial of n variables parameterized by ?. ? ?xi denotes the derivative with respect to the ith coordinate of x. Obviously, as a polyno-</p><formula xml:id="formula_5">mial of PDOs ? ?xi n i=1 , H( ? ?x1 , ? ?x2 , . . . , ? ?xn ; ?)</formula><p>is a linear combination of PDOs parameterized by ?. For example,</p><formula xml:id="formula_6">! " ! # ! $ ! " (&amp;) ! # (&amp;) ! $ (&amp;) ? ? (&amp;) ) *" Figure 2. Transformation over coordinate frame. if H(u 1 , u 2 ; ?) = ? 1 u 1 + ? 2 u 1 u 2 , then H( ? ?x1 , ? ?x2 ; ?) = ? 1 ? ?x1 + ? 2 ? 2 ?x1?x2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">UNDER ORTHOGONAL TRANSFORMATION</head><p>We transform these PDOs with orthogonal matrices, and define the following differential operator:</p><formula xml:id="formula_7">? (A) = H ? ?x (A) 1 , ? ?x (A) 2 . . . , ? ?x (A) n ; ? ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">? ? ? ? ? ? ? ? ?x (A) 1 ? ?x (A) 2 . . . ? ?x (A) n ? ? ? ? ? ? ? = A ?1 ? ? ? ? ? ? ?x1 ? ?x2 . . . ? ?xn ? ? ? ? ? ,<label>(6)</label></formula><p>and A is an orthogonal matrix. As a compact format, we can also rewrite <ref type="formula" target="#formula_8">(6)</ref> as</p><formula xml:id="formula_9">? (A) = A ?1 ?,<label>(7)</label></formula><p>where ? = [ ? ?x1 , ? ?x2 , ? ? ? , ? ?xn ] T , which is a gradient operator. Particularly, the canonical operator ? (I) = H( ? ?x1 , ? ?x2 , ? ? ? , ? ?xn ; ?). From another point of view, the transformation on PDOs can also be viewed as that we transform the coordinate frame according to A, and then conduct differential operators on the new coordinate frame (see <ref type="bibr">Figure 2)</ref>. Particularly, PDOs can be viewed as steerable filters in the sense of <ref type="bibr" target="#b10">(Helor &amp; Teo, 1996)</ref>, because the transformed versions of PDOs can be expressed as linear combinations of PDOs.</p><p>Next, we employ ? (A) 's to define two differential operators ? and ?. To be specific, we use ? to deal with inputs, which maps an input r ? C ? (R n ) to a feature map defined on</p><formula xml:id="formula_10">E(n): ?(x, A) ? E(n), ?(x, A) ? E(n), ?[r](x, A) = ? (A) [r](x). (8)</formula><p>Then we use ? to deal with the resulting feature maps, which maps one feature map e ? C ? (E(n)) to another feature map defined on E(n):</p><formula xml:id="formula_11">?(x, A) ? E(n), ?[e](x, A) = O(n) ? (A) B [e](x, AB)d?(B),<label>(9)</label></formula><p>where B is an orthogonal matrix and ? is a measure on O(n). As for ? (A) B , we use the subscript B to distinguish the differential operators parameterized by different ? B 's. The e on the right hand side should be viewed as a function defined on R n indexed by AB when the operator ?</p><formula xml:id="formula_12">(A) B acts on it.</formula><p>We now show that the above two operators are equivariant under orthogonal transformations and describe how the outputs transform w.r.t. the transformations of inputs.</p><formula xml:id="formula_13">Theorem 1 If r ? C ? (R n ), e ? C ? (E(n)) and A ? O(n)</formula><p>, the following rules are satisfied:</p><formula xml:id="formula_14">? ? R A [r] =? E A [?[r]] ,<label>(10)</label></formula><formula xml:id="formula_15">? ? E A [e] =? E A [?[e]] ,<label>(11)</label></formula><p>where ? R A , ? E A , ? and ? are defined in <ref type="formula" target="#formula_1">(2)</ref>, <ref type="formula" target="#formula_3">(4)</ref>, <ref type="formula">(8)</ref> and <ref type="formula" target="#formula_11">(9)</ref>, respectively.</p><p>Proof 1 To prove (10), we need to prove that ?x ? R n , A ? O(n),</p><formula xml:id="formula_16">? (A) ? R A [r] (x) = ? E A ? (A) [r](x) = ? (? ?1 A) [r]( A ?1 x).<label>(12)</label></formula><p>We first show that</p><formula xml:id="formula_17">? (A) ? R A [r] (x) = (A ?1 ?) ? R A [r] (x) = (A ?1 ?) r( A ?1 x) = A ?1 A?[r]( A ?1 x) = ( A ?1 A) ?1 ?[r]( A ?1 x) = ? ( A ?1 A) [r]( A ?1 x).</formula><p>The derivation from the third line to the fourth line is due to the orthogonality of A. Thus for any element xi in x, we have</p><formula xml:id="formula_18">? ?x (A) i ? R A [r] (x) = ? ?x (? ?1 A) i [r](? ?1 x).</formula><p>Furthermore,</p><formula xml:id="formula_19">? (A) ? ?x (A) i ? R A [r] (x) =A ?1 ? ? ?x ( A ?1 A) i [r]( A ?1 x) =( A ?1 A) ?1 ? ? ?x ( A ?1 A) i [r] ( A ?1 x) =? ( A ?1 A) ? ?x ( A ?1 A) i [r] ( A ?1 x).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PDO-eConvs: Partial Differential Operator Based Equivariant Convolutions</head><p>Then we have that for any elements xi and xj in x,</p><formula xml:id="formula_20">? ?x (A) i ? ?x (A) j ? R A [r] (x) = ? ?x (? ?1 A) i ? ?x (? ?1 A) j [r](? ?1 x).</formula><p>In this way, it is easy to prove that <ref type="formula" target="#formula_0">(12)</ref> is satisfied for all the differential operator terms in ? <ref type="bibr">(?)</ref> . Finally, as ? (?) is a linear combination of above terms, (12) is satisfied. Easily, (10) is satisfied.</p><p>As for <ref type="formula" target="#formula_0">(11)</ref>, similarly, ?x ? R n , A ? O(n),</p><formula xml:id="formula_21">? ? E A [e] (x, A) = ? e( A ?1 x, A ?1 A) = O(n) ? (A) B e( A ?1 x, A ?1 AB) d?(B) = O(n) ? (A) B ? R A [e](x, A ?1 AB) d?(B) = O(n) ? ( A ?1 A) B [e]( A ?1 x, A ?1 AB)d?(B) = ? E A O(n) ? (A) B [e](x, AB)d?(B) = ? E A [?[e]](x, A).</formula><p>The derivation from the third line to the fourth line is due to (12). So <ref type="formula" target="#formula_0">(11)</ref> is satisfied.</p><p>Furthermore, as differential operators are naturally translation-equivariant, it is easy to verify that ? and ? are also equivariant over E(n). Consequently, according to the working spaces, we set a ? as the first layer, followed by multiple ?'s, inserted by pointwise nonlinearities, e.g., ReLUs, that do not disturb the equivariance. Finally, we can get a system where equivariance can be preserved across multiple layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">UNDER SUBGROUP OF ORTHOGONAL TRANSFORMATION</head><p>The above theorem can be easily extended to subgroups of E(n). Here we consider a subgroup?(n) with the form R n S, where S is a subgroup of O(n). Similarly, we denote the smooth feature map defined on?(n) as? and the function space as C ? <ref type="figure">(?(n)</ref>).</p><p>The definition of the differential operator ? S is the similar with <ref type="formula">(8)</ref>:</p><formula xml:id="formula_22">?(x, A) ??(n), ? S [r](x, A) = ? (A) [r](x),<label>(13)</label></formula><p>where the only difference is that A ? S. If S is a discrete group, the differential operator ? S is:</p><formula xml:id="formula_23">?(x, A) ??(n), ? S [?](x, A) = B?S ? (A) B [?](x, AB),<label>(14)</label></formula><p>where A ? S. Following <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_3">(4)</ref>, we can define ? R A and ?? A , where? ? S. We can get the similar result:</p><formula xml:id="formula_24">? S ? R A [r] =?? A ? S [r] ,<label>(15)</label></formula><formula xml:id="formula_25">? S ?? A [?] =?? A ? S [?] .<label>(16)</label></formula><p>Easily, they are also equivariant w.r.t.?(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">PDO-eConvs</head><p>In this section, we apply our theory to 2D digital images, and derive approximately equivariant convolutions in the discrete domain. As they are designed using PDOs, we refer to them as PDO-eConvs. To begin with, we show how to apply PDOs on discrete images and feature maps with convolutional filters, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Differential Operators Acting on Discrete Features</head><p>We can view discrete digital images as samples from smooth functions defined on the 2D plane. Formally, we assume that an image data I ? R n?n represents a two-dimensional grid function obtained by discretizing a smooth function r : [0, 1] ? [0, 1] :? R at the cell-centers of a regular grid with n ? n cells and a mesh size h = 1/n, i.e., for i, j = 1, 2, . . . , n,</p><formula xml:id="formula_26">I i,j = r(x i , y j ),</formula><p>where x i = (i ? 1 2 )h and y j = (j ? 1 2 )h. Accordingly, intermediate feature maps in CNNs are multichannel matrices. Similarly, it can be seen as the discretizations of continuous functions defined on?, wher? E = R 2 S and S is a subgroup of O(2). Formally, a feature map F represents a three-dimensional grid function sampled from a smooth function e : [0, 1] 2 ? S :? R. For i, j = 1, 2, . . . n,</p><formula xml:id="formula_27">F k i,j = e(x i , y j , k),<label>(17)</label></formula><p>where x i = (i ? 1 2 )h, y j = (j ? 1 2 )h and k ? S which represents its channel index. Here, for ease of presentation, we only consider that inputs and intermediate feature maps are all single-valued functions, and the theory can be easily extended to multi-valued functions.</p><p>With the understanding that features are sampled from continuous functions, we can implement differential operations on features. Particularly, we use convolutions to approximate differential operations, which have been widely used in image processing. For example, the operator ? ?x acting on images and feature maps can be approximated by the following 3 ? 3 convolutional filter with quadratic precision:</p><formula xml:id="formula_28">? ?x [r](x i , y j ) = ? ? 1 2h ? ? 0 0 0 ?1 0 1 0 0 0 ? ? * I ? ? i,j + O(h 2 ), ? ?x [e](x i , y j , k) = ? ? 1 2h ? ? 0 0 0 ?1 0 1 0 0 0 ? ? * F k ? ? i,j + O(h 2 ),</formula><p>where * denotes the convolution operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">From Group Equivariant Differential Operators to PDO-eConvs</head><p>Firstly, we choose the polynomial H from the connection between differential operators and convolutions. <ref type="bibr" target="#b30">Ruthotto &amp; Haber (2018)</ref> showed that we can relate a 3 ? 3 convolutional filter to a differential operator, D, which is a linear combination of 9 linearly independent PDOs 4 .</p><formula xml:id="formula_29">D =? 1 ? 0 + ? 2 ? x + ? 3 ? y + ? 4 ? xx + ? 5 ? xy (18) + ? 6 ? yy + ? 7 ? xxy + ? 8 ? xyy + ? 9 ? xxyy .</formula><p>In addition, we observe that all differential operators in <ref type="formula" target="#formula_0">(19)</ref> can be approximated using 3 ? 3 convolutional filters (see Supplementary Material 1.1) with quadratic precision. It is to say that we can always approximate the differential operators defined in (19) using a 3 ? 3 filter with quadratic precision. For this reason, we choose</p><formula xml:id="formula_30">H(u, v; ?) =? 1 + ? 2 u + ? 3 v + ? 4 u 2 + ? 5 uv (19) + ? 6 v 2 + ? 7 u 2 v + ? 8 uv 2 + ? 9 u 2 v 2 .</formula><p>In this way, D equals ? (I) , which is also the canonical differential operator of ? (A) 's, indexed by the identity matrix. Using the transformation in <ref type="formula" target="#formula_8">(6)</ref>, we can calculate all the expressions of ? (A) 's easily. Particularly, these transformed differential operators share the same parameters ?, indicating greater parameter efficiency.</p><p>In computation, we observe that some new partial derivatives, e.g., ? xxx , ? xxxx , may occur in some ? (A) 's, where A ? S. Fortunately, the orders of these new partial derivatives are all below five, and we can use the filters with the size of 5 ? 5 (see <ref type="figure">Supplementary Material 1</ref>.2) to approximate them with quadratic precision. Now we investigate the group we use. According to <ref type="formula" target="#formula_11">(9)</ref> and <ref type="formula" target="#formula_0">(14)</ref>, if S is a continuous group, we need to conduct integration. However, for the computation issue, it seems impossible to consider all the orthogonal transformations in O(2). So we consider S to be a discrete subgroup of O(2). Still, our theory is satisfied for feature maps defined on? (see Section 3.2.2). Particularly, noting that O(2) is generated by reflections and rotations, we set the subgroup S to be generated by reflections and rotations by 2?/n. As a result,? = pnm. If without reflections,? = pn. Discrete groups pnm and pn have been introduced in Section 1.</p><p>Finally, we discretize the equivariant differential operator ? with corresponding convolutional filters. As a result, we can get a new operator,?, which is actually a set of convolution operators indexed by A:</p><formula xml:id="formula_31">?A ? S,? (A) = i?? C (A) i? i ,<label>(20)</label></formula><p>where ? indexes all the filters we use, C (A) i are derived by substituting (6) into (5) and? i is the convolutional filter related to the PDO ? i (e.g.,? 0 and? xy are related to ? 0 and ? xy , respectively), then</p><formula xml:id="formula_32">( ? * I) A =? (A) * I.<label>(21)</label></formula><p>Similarly, we can get a new convolution operator? by discretizing <ref type="formula" target="#formula_0">(14)</ref>. Without ambiguity, we also use * to denote the corresponding convolution operation. To be specific,</p><formula xml:id="formula_33">?A ? S, ? * F A = k?S? (A) k * F Ak ,<label>(22)</label></formula><p>where Ak is a group product on the group S, which respresents the channel index of F , and F Ak ? R n?n .</p><p>We refer to? and? as PDO-eConvs, because they are equivariant convolutions based on PDOs. Following <ref type="bibr" target="#b2">(Cohen &amp; Welling, 2016)</ref>, we replace all the conventional convolutions in an existing CNN with our PDO-eConvs, and get the corresponding group equivariant CNN w.r.t.?.</p><p>Let us have a more detailed look at <ref type="bibr">(20)</ref>. Some convolutional filters like u xxxx are of size 5 ? 5, thus for some A ??, ? (A) is also of size 5 ? 5, while the canonical convolutional filter? (I) is of size 3 ? 3. We can explain the phenomenon in this way. By definition, the differential operator ? (A) is transformed from ? (I) . Intuitively, we can also view the convolutional filter? (A) as a transformed version of ? <ref type="bibr">(I)</ref> . We assume the transformation to be the rotation. As shown in <ref type="figure" target="#fig_0">Figure 3</ref>,? (A) is a rotated version of? <ref type="bibr">(I)</ref> , which overflows the original 3 ? 3 area. So it makes sense to use a larger filter to represent some transformed filters. That 5 ? 5 is sufficient is because the rotated 3 ? 3 mask can always be covered by a 5 ? 5 square, noting that 5 ? 3 ? 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Approximation Error of Equivariance</head><p>When we discretize the differential operators ? and ?, errors occur, leading to equivariance disturbance. Nonetheless, we can still achieve approximate equivariance. Here, we analyze the approximation error of our PDO-eConvs.</p><p>Theorem 2 ?? ? S,</p><formula xml:id="formula_34">? * ? R A [I] = ?? A ? * I + O(h 2 ),<label>(23)</label></formula><formula xml:id="formula_35">? * ?? A [F ] = ?? A ? * F + O(h 2 ),<label>(24)</label></formula><p>where transformations such as rotations or mirror reflections acting on images are defined as (? R A [I]) i,j = (? R A [r])(x i , y j ) and transformations acting on feature maps are (??</p><formula xml:id="formula_36">A [F ]) k i,j = (?? A [e])(x i , y j , k).</formula><p>Proof 2 ?A ? S, the operator ? (A) is a linear combination of differential operators and? (A) is a combination of corresponding convolution operators. Hence if r is a smooth function,</p><formula xml:id="formula_37">? (A) [r](x i , y j ) = ? (A) * I i,j + O(h 2 ), ? (A) ? R A [r] (x i , y j ) = ? (A) * ? R A [I] i,j + O(h 2 ),</formula><p>i.e.,</p><formula xml:id="formula_38">?[r](x i , y j , A) = ? * I A i,j + O(h 2 ), ? ? R A [r] (x i , y j , A) = ? * ? R A [I] A i,j + O(h 2 ). (25)</formula><p>Easily, we have</p><formula xml:id="formula_39">?? A [?[r]] (x i , y j , A) = ?? A ? * I A i,j + O(h 2 ). (26)</formula><p>From <ref type="formula" target="#formula_0">(10)</ref> we know that the left hand sides of (25) and (26) equal, hence the right hand sides of the two equation are the same, which results in (23). We can prove (24) analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Weight Initialization Scheme</head><p>An important practical issue in the training phase is an appropriate initialization of weights. When the variances of weights are chosen too high or too low, the signals propagating through the network are amplified or suppressed exponentially with depth. <ref type="bibr" target="#b7">Glorot &amp; Bengio (2010)</ref> and <ref type="bibr" target="#b8">He et al. (2015)</ref> investigated this problem and proposed widely used initialization schemes. However, our filters are not parameterized in a pixel basis but as linear combinations of several PDOs, thus the above-mentioned initialization schemes cannot directly be adopted for our PDO-eConvs.</p><p>To be specific, we consider the canonical filter? (I) in each PDO-eConv, and initialize it with He's initialization scheme <ref type="bibr" target="#b8">(He et al., 2015)</ref>. Then we initialize the parameters ? of the PDO-eConv by solving the linear equatio?</p><formula xml:id="formula_40">? (I) =? 1?0 + ? 2?x + ? 3?y + ? 4?xx + ? 5?xy (27) + ? 6?yy + ? 7?xxy + ? 8?xyy + ? 9?xxyy .</formula><p>with the initialized? <ref type="bibr">(I)</ref> . In this way, the canonical filter is initialized with He's initialization scheme. Since other filters are obtained by transforming the canonical filters, they also have appropriate variances. We initialize each? k in (22) in the same way. We use this method to initialize all the PDO-eConvs in experiments and all the experiments are implemented using Tensorflow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Rotated MNIST</head><p>The most commonly used dataset for validating rotationequivariant algorithms is MNIST-rot-12k <ref type="bibr" target="#b21">(Larochelle et al., 2007)</ref>. It contains the handwritten digits of the classical MNIST, rotated by a random angle from 0 to 2? (full angle). This dataset contains 12,000 training images and 50,000 test images, respectively. We randomly select 2,000 training images as a validation set. We choose the model with the lowest validation error during training. For preprocessing, we normalize the images using the channel means and standard deviations.</p><p>Without Data Augmentation Firstly, we evaluate the performance of PDO-eConvs on MNIST-rot-12k without data augmentation via the CNN architecture used in <ref type="bibr" target="#b2">(Cohen &amp; Welling, 2016)</ref>. It contains 6 layers of 3 ? 3 convolutions, 20 channels in each layer, ReLU functions, batch normalization <ref type="bibr" target="#b13">(Ioffe &amp; Szegedy, 2015)</ref>, and max pooling after layer 2.</p><p>We consider the group p8 and replace each convolution by a p8-convolution, divided the number of filters by ? 8, in order to keep the numbers of parameters nearly the same. Thus we use 7 filters on each layer. Particularly, batch normalization should be implemented with a single scale and a single bias <ref type="table" target="#tab_0">Table 1</ref>. Error rates on MNIST-rot-12k without data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network</head><p>Test Error (%) params ScatNet-2 <ref type="bibr" target="#b0">(Bruna &amp; Mallat, 2013)</ref> 7.48 -PCANet-2 <ref type="bibr" target="#b1">(Chan et al., 2015)</ref> 7.37 -TIRBM <ref type="bibr" target="#b32">(Sohn &amp; Lee, 2012)</ref> 4.2 -ORN-8 (ORNAlign)  2.25 0.53M TI-Pooling <ref type="bibr" target="#b20">(Laptev et al., 2016)</ref> 2.2 13.3M CNN 5.03 22k G-CNN <ref type="bibr" target="#b2">(Cohen &amp; Welling, 2016)</ref> 2.28 25k PDO-eConv (ours) 1.87 26k per PDO-eConv map to preserve equivariance.</p><p>The model is trained using the Adam algorithm <ref type="bibr" target="#b16">(Kingma &amp; Ba, 2015)</ref> with a weight decay of 0.01. We use the weight initialization method introduced in Section 4.4 for PDO-eConvs and Xavier initialization <ref type="bibr" target="#b7">(Glorot &amp; Bengio, 2010)</ref> for the fully connected layer. We train using batch size 128 for 200 epochs. The initial learning rate is set to 0.001 and is divided by 10 at 50% and 75% of the total number of training epochs. We set the dropout rate as 0.2. Competitive Result with Data Augmentation We compare the performance of our PDO-eConv with some more competitive models, using data augmentation and a larger model with 7 layers. These layers have <ref type="bibr">16,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">32,</ref><ref type="bibr">32,</ref><ref type="bibr">64</ref> and 64 output channels, respectively. We use spatial pooling and orientation pooling after the final PDO-eConv </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (%) H-Net <ref type="bibr" target="#b37">(Worrall et al., 2017)</ref> 1.69 OR-TIPooling  1.54 RotEqNet <ref type="bibr" target="#b27">(Marcos et al., 2017)</ref> 1.09 PTN-CNN <ref type="bibr" target="#b5">(Esteves et al., 2018)</ref> 0.89 E2CNN <ref type="bibr" target="#b34">(Weiler &amp; Cesa, 2019)</ref> 0.716 SFCNN <ref type="bibr" target="#b35">(Weiler et al., 2018)</ref> 0.714 <ref type="bibr">PDO-eConv (ours)</ref> 0.709 layer, in order to get rotation-invariant features. Following <ref type="bibr" target="#b35">(Weiler et al., 2018)</ref>, we augment the dataset with continuous rotations during training time. This model is trained using stochastic gradient descent (SGD) and a Nesterov momentum <ref type="bibr" target="#b33">(Sutskever et al., 2013)</ref> of 0.9 without dampening. We train this model for 300 epochs, starting with a learning rate of 10 ?2 and reducing it gradually to 10 ?5 .</p><p>As shown in <ref type="table" target="#tab_1">Table 2</ref>, E2CNN and SFCNN achieve 0.716% and 0.714% test error on rotated MNIST, respectively. Compared with SFCNN, our method achieves a comparable result, 0.709% test error, using only 10% parameters. To be specific, our method uses 0.65M parameters, while SFCNN needs 6.5M parameters. Also, SFCNN used a much larger architecture and larger kernel sizes (7 ? 7 and 9 ? 9), which relate to a much larger computational cost. E2CNN replicates the architecture used in SFCNN, so it also relates to a huge computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Natural Image Classfication</head><p>Although most objects in natural scene images are up-right, rotations could exist in small scales. Besides, equivariance to a transformation group brings in more parameter sharing, which may improve the parameter efficiency. Here we evaluate the performance of our PDO-eConvs on two common natural image datasets, CIFAR-10 (C10) and CIFAR-100 (C100) <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009</ref>), respectively.</p><p>The two CIFAR datasets consist of colored natural images with 32 ? 32 pixels. C10 consists of images drawn from 10 classes and C100 from 100. The training and the test sets contain 50,000 and 10,000 images, respectively. We randomly select 5,000 training images as a validation set. We choose the model with the lowest validation error during training. We adopt a standard data augmentation scheme (mirroring/shifting) <ref type="bibr" target="#b22">(Lee et al., 2015)</ref> that is widely used for these two datasets. For preprocessing, we normalize the images using the channel means and standard deviations.</p><p>To evaluate our method, we take ResNet <ref type="bibr" target="#b9">(He et al., 2016)</ref> as the basic model, which consists of an initial convolution layer, followed by three stages of 2n convolution layers using k i filters at stage i, followed by a final classification layer (6n + 2 layers in total). We replace all convolution layers of ResNets by our PDO-eConvs and implement batch normalization with a single scale and a single bias per PDO-eConv map. Also, we scale the number of filters to keep the numbers of parameters approximately the same. All the models are trained using SGD and a Nesterov momentum <ref type="bibr" target="#b33">(Sutskever et al., 2013)</ref> of 0.9 without dampening. We train using batch size 128 for 300 epochs, weight decay of 0.001. The initial learning rate is set to 0.1 and is divided by 10 at 50% and 75% of the total number of training epochs. Similarly, we use the weight initialization method introduced in Section 4.4 for our PDO-eConvs and Xavier initialization for the fully connected layer. We report the results of our methods in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Following HexaConv, we use our PDO-eConvs to establish models that are equivariant to group p6 (p6m), where n = 4 and k i = 6, 13, 26 (k i = 6, 9, 18). Using comparable numbers of parameters, our methods perform significantly better than HexaConv (5.38% vs. 8.64% on C10). In addition, HexaConvs require extra memory to store hexagonal images while our PDO-eConvs do not need so.</p><p>We evaluate PDO-eConvs using ResNet-44, where n = 7 and k i = 11, 23, 45. Compared with G-CNNs, our PDO-eConvs achieve significantly better performance using comparable numbers of parameters (3.68% vs. 4.94% on C10, and 20.01% vs. 23.19% on C100). When evaluated on ResNet-26, where n = 4, k i = 20, 40, 80, PDO-eConv results in 3.50% test error, much better than 4.17% resulted from G-CNN, yet using much fewer parameters (4.6M vs. 7.2M). This is mainly because that PDO-eConvs can deal with an 8-fold rotational symmetry, which exploit more rotational symmetries compared with G-CNN.</p><p>Finally, we compare our models with deeper ResNets (ResNet-1001) and wider ResNets (Wide ResNet). As shown in <ref type="table" target="#tab_2">Table 3</ref>, PDO-eConvs perform betterr (3.50% vs. 4.00% in C10 and 18.40% vs. 19.25% in C100) using only 12.6% parameters (4.6M vs. 36.5M). Particularly, PDO-eConvs can also be viewed as introducing a weight sharing scheme across channels, and the results indicate that our method can not only save parameters, but also improve the performance remarkably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We utilize PDOs to design a system which is exactly equivariant to a much more general continuous group, the ndimension Euclidean group. We use numerical schemes to implement these PDOs and derive approximately equivari-ant convolutions, PDO-eConvs. Particularly, we provide an error analysis and show that the approximation error is of the quadratic order. Extensive experiments verify the effectiveness of our method.</p><p>In this work, we only conduct experiments on 2D images. Actually, our theory can deal with the data with any dimension. We will explore more possibilities in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>A. Numerical Schemes of Partial Differential Operators </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>The canonical convolutional filter? (I) and its rotated version?(A)  .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>, with comparable numbers of param-</cell></row><row><cell>eters, our proposed PDO-eConv achieves 1.87% test er-</cell></row><row><cell>ror, outperforming conventional CNN (5.03%) and G-CNN</cell></row><row><cell>(2.28%), which is equivariant on group p4. This is mainly</cell></row><row><cell>because that our model is rotation-equivariant w.r.t. smaller</cell></row><row><cell>rotation angles, which brings in better generalization. ORN-</cell></row><row><cell>8 also deals with an 8-fold rotational symmetry and adopts</cell></row><row><cell>an extra strategy, ORNAlign, to refine feature maps. Com-</cell></row><row><cell>pared with ORN-8 (ORNAlign), our method still results</cell></row><row><cell>in lower test error, using far fewer numbers of parameters</cell></row><row><cell>(26k vs. 0.53M). TI-Pooling is a representative model of</cell></row><row><cell>transformation-invariant CNNs, which use parallel siamese</cell></row><row><cell>architectures. Compared with it, PDO-eConv performs bet-</cell></row><row><cell>ter (1.87% vs. 2.2%) using far fewer parameters (26k vs.</cell></row><row><cell>13.3M) and has much lower computational complexity.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Competitive results on MNIST-rot-12k.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on the natural image classification benchmark. In the second column, G is the group where equivariance can be preserved.</figDesc><table><row><cell>Method</cell><cell>G</cell><cell cols="4">Depth C10 C100 params</cell></row><row><cell>ResNet (He et al., 2016)</cell><cell>Z 2</cell><cell>26</cell><cell cols="3">11.5 31.66 0.37M</cell></row><row><cell>HexaConv (Hoogeboom et al., 2018)</cell><cell>p6</cell><cell>26</cell><cell>9.98</cell><cell>-</cell><cell>0.34M</cell></row><row><cell></cell><cell>p6m</cell><cell>26</cell><cell>8.64</cell><cell>-</cell><cell>0.34M</cell></row><row><cell>PDO-eConv (ours)</cell><cell>p6</cell><cell>26</cell><cell cols="3">5.65 27.13 0.36M</cell></row><row><cell></cell><cell>p6m</cell><cell>26</cell><cell cols="3">5.38 27.00 0.37M</cell></row><row><cell>ResNet</cell><cell>Z 2</cell><cell>44</cell><cell cols="3">5.61 24.08 2.64M</cell></row><row><cell>G-CNN (Cohen &amp; Welling, 2016)</cell><cell>p4m</cell><cell>44</cell><cell cols="3">4.94 23.19 2.62M</cell></row><row><cell>PDO-eConv (ours)</cell><cell>p8</cell><cell>44</cell><cell cols="3">3.68 20.01 2.62M</cell></row><row><cell>ResNet</cell><cell>Z 2</cell><cell cols="4">1001 4.92 22.71 10.3M</cell></row><row><cell>Wide ResNet (Zagoruyko &amp; Komodakis, 2016)</cell><cell>Z 2</cell><cell>26</cell><cell cols="3">4.00 19.25 36.5M</cell></row><row><cell>G-CNN (Cohen &amp; Welling, 2016)</cell><cell>p4m</cell><cell>26</cell><cell>4.17</cell><cell>-</cell><cell>7.2M</cell></row><row><cell>PDO-eConv (ours)</cell><cell>p8</cell><cell>26</cell><cell cols="2">3.50 18.40</cell><cell>4.6M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Generally, the group pnm, which we will use in Section 4, denotes the group generated by translations, reflections and rotations by 2?/n. The group pn denotes the group only generated by translations and rotations by 2?/n.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For ease of presentation, we denote the identity operator as ?0, and view it as a special PDO.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">PCANet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Steerable</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cnns</surname></persName>
		</author>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image restoration: Wavelet frame shrinkage, nonlinear evolution pdes, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="606" to="660" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Polar transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allenblanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature learning via partial differential equation with applications to face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="14" to="25" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Canonical decomposition of steerable functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Helor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Teo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="95" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix capsules with EM routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hexaconv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Partial differential equations and finite difference methods in image processing-Part II: Image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="817" to="834" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The structure of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="363" to="370" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">TI-POOLING: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toward designing intelligent PDEs for computer vision: An optimal control approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="56" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning PDEs from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pde-Net</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Machine Learning Society (IMLS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5067" to="5078" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">399</biblScope>
			<biblScope unit="page">108925</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Feature-oriented image enhancement using shock filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rudin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="940" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Scale-space and edge detection using anisotropic diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="629" to="639" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1339" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">General E(2)-equivariant steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="14334" to="14345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scale-space filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="329" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
