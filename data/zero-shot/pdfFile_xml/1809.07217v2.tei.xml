<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D Human Pose Estimation with Siamese Equivariant Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M?rton</forename><surname>V?ges</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">E?tv?s Lor?nd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Varga</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">E?tv?s Lor?nd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>L?rincz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">E?tv?s Lor?nd University</orgName>
								<address>
									<settlement>Budapest</settlement>
									<country key="HU">Hungary</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D Human Pose Estimation with Siamese Equivariant Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>3D Pose Estimation</term>
					<term>Siamese Network</term>
					<term>Equivariant embedding</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In monocular 3D human pose estimation a common setup is to first detect 2D positions and then lift the detection into 3D coordinates. Many algorithms suffer from overfitting to camera positions in the training set. We propose a siamese architecture that learns a rotation equivariant hidden representation to reduce the need for data augmentation. Our method is evaluated on multiple databases with different base networks and shows a consistent improvement of error metrics. It achieves state-of-the-art cross-camera error rate among algorithms that use estimated 2D joint coordinates only.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating human 3D poses from still images has received an increase of interest lately. The problem has many important potential applications, such as activity recognition, interaction analysis between people (e.g. object passing) and surveillance. Having the 3D coordinates of the human skeleton also helps in augmented reality applications or remote sensing.</p><p>However, the task is harder than traditional 2D pose estimation due to some fundamental differences. First, the problem formulation is inherently ambiguous: during the perspective projection information is lost and can not be retrieved. It is impossible to tell the difference between a close, short person and a tall, far away one. Second, it is difficult to create 3D pose datasets, especially in the wild. While special equipment exists to capture the position of markers attached to the body, it restricts the recordings to lab environments. The problem is aggravated by the fact that deep learning networks are data hungry and need large amounts of training examples to be robust against variations in lighting, actor appearance and background change.</p><p>One approach to solve the latter issue is to take advantage of the abundance of 2D pose annotated data by using an off-the-shelf 2D pose estimator.</p><p>State-of-the-art 2D pose estimators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> have reached superior results that enable us to employ them as standalone components. Martinez et al. <ref type="bibr" target="#b3">[4]</ref> used a pretrained Stacked Hourglass network <ref type="bibr" target="#b1">[2]</ref> to generate 2D positions and then a simple fully connected network with residual blocks to achieve state-of-the-art results. Since the network does not receive the image at all, only the 2D keypoints, this approach is robust against changes in illumination and background.</p><p>However, as identified by Fang et al. <ref type="bibr" target="#b4">[5]</ref>, the above algorithm overfits to existing camera angles and does not generalize well to unseen positions. In the standard evaluation protocol of the popular Human3.6M dataset <ref type="bibr" target="#b5">[6]</ref>, all cameras are included both in the training and test set. When excluding one of the four cameras from the training set and restricting the test set to that camera only, the error increases significantly. Augmenting the dataset by rotating existing poses helps but only to an extent. The error is still higher compared to the original protocol even after augmentation.</p><p>To alleviate this problem, we propose a siamese network <ref type="bibr" target="#b6">[7]</ref> based architecture that learns an equivariant embedding stable to rotations. The equivariant hidden representation has the property that applying a rotation on the input rotates the embedding the same way. This reduces the need for artificial data augmentation as some of it is already baked into the network. The siamese architecture makes it easy to teach the equivariance to the network and circumvents the need for an autoencoder. Using an autoencoder for this task has the downside that it has to learn to recreate a random noise to generate a rotated output (see Section 3.2 for detailed explanation).</p><p>Our contribution can be summarized as follows: We introduce a siamese architecture that learns a geometrically interpretable embedding. The embedding is rotationally equivariant that makes the network robust to new camera views. The architecture is tested on multiple datasets and with different base networks. We achieve state-of-the-art results on unseen camera poses on the Human3.6m dataset <ref type="bibr" target="#b5">[6]</ref> among methods that do not use image input directly. We also make our code publicly available 1 .</p><p>The structure of the paper is the following: in Section 2 we review the literature, in Section 3 we introduce equivariance and in Section 4 the network architecture is detailed. The performed experiments and their results can be found in Section 5. Finally, we summarize our findings in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Pose Estimation</head><p>Previous approaches focused on predicting the 3D pose directly from an image, in an end-to-end fashion. For example, in <ref type="bibr" target="#b7">[8]</ref>, the authors predict a 3D heatmap, gradually refining it along the depth dimension, increasing the resolution step-by-step. Zhou et al <ref type="bibr" target="#b8">[9]</ref> places a 3D regression network on top of a 2D pose estimator and extends the network with a semi-supervised loss allowing the usage of images with only 2D annotations for training. Another approach uses bone representation instead of joint coordinates <ref type="bibr" target="#b9">[10]</ref>.</p><p>Compared to the above methods, Martinez et al. <ref type="bibr" target="#b3">[4]</ref> use a 2D pose estimator and 2D pose to 3D pose regressor as separate components. The 3D regressor is a 6-layer fully connected neural network using standard techniques only, such as batch normalization or residual connections. With this simple architecture, they achieved state-of-the-art results at the time. This simplicity inspired new research expanding on the 2D to 3D pose estimator capabilities. Hossain et al. <ref type="bibr" target="#b10">[11]</ref> used temporal information by adding recurrence to the network. Fang et al. <ref type="bibr" target="#b4">[5]</ref> added bi-directional RNNs to learn additional constraints, such as symmetry or bone structure.</p><p>Another direction of research aims to combine heatmap based approaches used extensively in 2D pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> with regression based approaches used in 3D pose estimation. In <ref type="bibr" target="#b11">[12]</ref>, the authors connect a 2D pose estimator generating joint location heatmaps and the 3D regression network with the softargmax function. The soft-argmax is a differentiable approximation of argmax whose derivative is not everywhere zero, thus the network becomes end-to-end trainable. Luvizon et. al. <ref type="bibr" target="#b12">[13]</ref> similarly use the soft-argmax function in a multitask estimation network.</p><p>Recently, many works included the estimation of pairwise depth rankings of joints, where the relative distance of two joints from the camera is predicted. The motivation behind the method is that it is easy for humans to annotate 2D images with depth rankings thus existing 2D pose datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> can be extended and used as auxiliary training data. In <ref type="bibr" target="#b15">[16]</ref>, depth ranking was added to the MPII-HP and LSP datasets. The method uses these two datasets for additional weak supervision. Shi et al. <ref type="bibr" target="#b16">[17]</ref> do not require the full ranking of all joints, only the bones. Wang et al. <ref type="bibr" target="#b17">[18]</ref> predicts a pairwise depth ranking matrix from the image and then fuses the predicted matrix with the 2D joint location heatmaps. Their method does not use any of the extended 2D datasets. Finally, in <ref type="bibr" target="#b18">[19]</ref> the authors analyze the performance of the human annotators on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Siamese networks</head><p>Unlike traditional deep networks, siamese networks have two identical branches sharing the same weights. Instead of a single input image, pairs of images are fed to the network and the loss is computed on the difference of the output of the two branches. Since the branches share the same weights, they are updated the same way during backpropagation and remain identical through training. Thus, in inference time, it is enough to use only one of the branches.</p><p>Siamese networks were originally proposed to solve handwriting verification <ref type="bibr" target="#b6">[7]</ref>. Since then, it was widely used in face verification <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Siamese regression methods were also used in 3D object pose estimation. Doumanoglou et al. <ref type="bibr" target="#b21">[22]</ref> used a loss that ensures that the distribution of hidden representations in the feature space is similar to that of the target datapoints in the pose space. Unlike us, they do not use an equivariant embedding on the hidden representations. In <ref type="bibr" target="#b22">[23]</ref>, the authors predict head poses using a siamese architecture. Compared to our work, they only have a siamese loss on the last output layer and not on an intermediate layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Equivariant networks</head><p>Equivariant networks have the promise to achieve similar performance to standard deep networks with smaller capacity and less data augmentation. In <ref type="bibr" target="#b23">[24]</ref> a new state-of-the-art is achieved on rotated-MNIST <ref type="bibr" target="#b24">[25]</ref> while reaching its maximum performance using less data than a standard CNN. In <ref type="bibr" target="#b25">[26]</ref>, the authors extend the standard CNNs to spheres, providing equivariance over three dimensional rotations. That formulation produces an output on the space of transformations, while the method of Esteves et al. <ref type="bibr" target="#b26">[27]</ref> has the sphere as an output. The latter also achieved results comparable to or better than the state-of-the-art while using a much smaller network on the ModelNet40 <ref type="bibr" target="#b27">[28]</ref> and SHREC'17 <ref type="bibr" target="#b28">[29]</ref> datasets. In pose estimation, Rhodin et al. <ref type="bibr" target="#b29">[30]</ref> used an equivariant network to create an autoencoder to generate images of human poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>For completeness, we introduce equivariance <ref type="bibr" target="#b30">[31]</ref>, and its weaker version, rotational equivariance.</p><p>Definition 1 (Equivariant function). Let f : X ? Y be a function and T ? : X ? X and U ? : Y ? Y two sets of transformations parametrized by ?. We say f is equivariant to T and U if</p><formula xml:id="formula_0">f (T ? (x)) = U ? (f (x))</formula><p>for all x ? X and ?.</p><p>What this means is that T ? and U ? are a pair of transformations whose order with f can be exchanged upon replacing one with the other. That is, transforming the input with T and then applying f is the same as first applying f and then transforming the output with U . If a neural network is equivariant, the network will automatically learn to be robust against transformations in T . This way less augmentation is needed as the augmentation transformations are already handled by the network. Typical examples are fully convolutional networks. They are translation-equivariant and during training usually no translation augmentations are applied, just rotations and reflections. Now, we move on to rotational equivariance, defined in <ref type="bibr" target="#b29">[30]</ref>. The definition below is specific to how equivariance is used in our algorithm. First note that following <ref type="bibr" target="#b3">[4]</ref> we split the task into two steps: first, predicting the 2D pose P 2D ? R 2?n from the input image, then predicting the 3D pose P 3D ? R 3?n from P 2D only where n is the number of joints. In the second step no image information was used, just the coordinates of the 2D skeleton.</p><p>Definition 2 (Rotational equivariance). Let the hidden representation h be a set of M 3-dimensional vectors (i.e. h ? R 3?M ) and f : R 2?n ? R 3?M be an encoder that takes the input 2D position into the hidden representation h,</p><formula xml:id="formula_1">that is f (P 2D ) = h. f is rotationally equivariant, if: f (? (RP 3D )) = Rf (?P 3D ),<label>(1)</label></formula><p>where ? is the 3D to 2D projection and R ? R 3?3 is a rotation matrix.</p><p>In other words, rotating the input pose and applying the encoder f has the same effect as encoding the pose and then rotating the hidden representation. Equivalently, the order of the rotation and the encoder can be swapped.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>As mentioned in the previous section, the 3D pose estimation is performed in two steps: first the 2D pose is determined with an off-the-shelf component and then the 3D position is predicted from the 2D skeleton. We focus on the second step here.</p><p>The goal is to create a network that is robust against unseen camera angles without excessive augmentation. Note that seeing a pose P from a new (rotated) camera angle is equivalent to seeing that same pose from a fixed angle but the pose itself rotated the other direction. So we can rephrase our goal as being robust against unseen rotations of a pose. To achieve this, we would like our network to learn a hidden representation h that is rotationally equivariant to the input.</p><p>To learn equivariance, it is possible to use an autoencoder with dynamically rotating the hidden representation during training <ref type="bibr" target="#b29">[30]</ref>. However, our inputs are noisy 2D pose estimations from another detector, thus an autoencoder would have to learn to simulate the prediction error of the 2D pose estimator. Instead we are opting to use a siamese architecture, which has the advantage that it does not have to learn a complete encoding of the input, contrary to an autoencoder. This makes further extension of the model to image inputs much easier as only information needed for the pose estimation must be encoded in the hidden representation.</p><p>A high level overview of our network is presented on <ref type="figure" target="#fig_2">Figure 1</ref>. It has two identical branches split into an encoder f and decoder g. Equation <ref type="formula" target="#formula_1">(1)</ref> is enforced by a siamese loss described in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Equivariant siamese loss</head><p>Assume we have calibrated cameras and know their rotation matrices relative to a suitable 3D coordinate system. Let C 1 and C 2 be two cameras, and the rotation matrix taking the view of C 1 into C 2 to be R. Let P  the same pose in the first and second camera coordinate system respectively to the two branches of the network. The encoder network f converts them into hidden representations h <ref type="bibr" target="#b0">(1)</ref> , h (2) ? R 3?M . Afterwards, the decoder network g converts h (i) into the final 3D predictionsP</p><formula xml:id="formula_2">DNN P ( 1 ) 2 D h ( 1 ) DNN P ( 1 ) 3 D DNN P ( 2 ) 2 D h ( 2 ) DNN P ( 2 ) 3 D ? S f f g g P ( 1 ) 3 D P ( 2 ) 3 D ? ( 1 ) 2 ? ( 2 ) 2</formula><formula xml:id="formula_3">(i)</formula><p>3D . Both outputs have an L 2 loss applied on them. We also apply an additional siamese loss function S based on the hidden representations h (i) . and denote its hidden representations under the two cameras with h 1 and h 2 . Using (1) and the fact that h i = f ?P (i) 3D :</p><formula xml:id="formula_4">Rh 1 = Rf ? P (1) 3D = = f ? RP (1) 3D = f ? P (2) 3D = h 2 .<label>(2)</label></formula><p>The equation above can be enforced by a siamese network naturally. If the input poses are P (1)</p><formula xml:id="formula_5">2D = ? P (1) 3D and P (2) 2D = ? P (2) 3D</formula><p>then adding a loss on Rh 1 ? h 2 forces the network to optimize for Equation <ref type="bibr" target="#b1">(2)</ref>.</p><p>However, this would only work for input pairs where the two inputs represent the same pose from different angles. To allow inputs representing different poses, first assume that there is some canonical coordinate system and R 1 and R 2 are the rotation matrices going from this absolute system to one relative to C 1 and C 2 , respectively. Let P</p><p>3DA be the pose in this absolute coordinate system supplied to the first camera and P <ref type="bibr" target="#b1">(2)</ref> 3DA supplied to the second camera. Then the 2D inputs for the network are denoted by P</p><formula xml:id="formula_7">(1) 2D = ? R 1 P (1) 3DA and P (2) 2D = ? R 2 P<label>(2)</label></formula><p>3DA . Thus:</p><formula xml:id="formula_8">R 2 R ?1 1 h 1 = R 2 R ?1 1 f ? R 1 P (1) 3DA = = f ? R 2 R ?1 1 R 1 P (1) 3DA = f ? R 2 P (1) 3DA</formula><p>.</p><p>Since</p><formula xml:id="formula_9">h 2 = f ? R 2 P (2) 3DA</formula><p>by definition, it is reasonable to have</p><formula xml:id="formula_10">R 2 R ?1 1 h 1 ? h 2 ? ? 1 R 2 P (1) 3DA ? R 2 P (2) 3DA = ? 1 P (1) 3DA ? P (2) 3DA ,</formula><p>where ? 1 is a scaling parameter. In the second equality we used the fact that R 2 is a rotation matrix thus orthonormal. Now we can formulate the loss as:</p><formula xml:id="formula_11">S = R 2 R ?1 1 h 1 ? h 2 ? ? 1 P (1) 3DA ? P (2) 3DA 2 .<label>(3)</label></formula><p>This is similar to the loss used in <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Residual modules used in the network. One residual module consists of two fully connected layers of 1024 nodes followed by batch normalization <ref type="bibr" target="#b31">[32]</ref> and dropout <ref type="bibr" target="#b32">[33]</ref>. The activation layer is Leaky-ReLU to solve problems with dying ReLUs.</p><p>The structure of the network is illustrated on <ref type="figure" target="#fig_2">Figure 1</ref>. It has two identical branches, each branch is built up from an encoder f and a decoder g, for which g(f (P 2D )) = g(h) = P 3D .</p><p>The main component of both f and g is a single residual module, depicted on <ref type="figure">Figure 2</ref>. The architecture was inspired by <ref type="bibr" target="#b3">[4]</ref>. Each fully connected layer has 1024 nodes. The encoder f has a dense layer before the residual block to scale up the input to a dimension of 1024. In the decoder, after the residual block a dense layer with 48 nodes produces the final output. We have found that dying ReLUs were a problem so used Leaky-ReLUs as activation functions instead of regular ReLUs.</p><p>To resize the output of the encoder from 1024 to 3M a dense layer is used with no activation function. The resulting vector of length 3M is reshaped to 3 ? M and normalized along the first axis, similarly to <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b33">[34]</ref>. After the embedding, the output tensor is resized back to 1024 with another fully connected layer. It was found empirically that placing a batch normalization and dropout layer after this layer decreased the performance considerably so they were omitted.</p><p>Additionally to the siamese loss introduced in the previous section, we also add an L 2 loss on both outputs of the siamese network. Thus the total loss is the following: = are the squared L 2 losses on the two branches and ? 2 is a hyperparameter.</p><formula xml:id="formula_12">(1) 2 + (2) 2 + ? 2 S , where<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We have evaluated our method on multiple databases both qualitatively and quantitatively. Also extensive ablation studies were performed to validate each component of the network. In this section we introduce these experiments and describe the implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Database and Evaluation Protocols</head><p>Currently one of the largest databases having 3D human poses is the Hu-man3.6M dataset which contains 11 actors performing 15 different actions recorded from four camera angles. The standards error metric is the mean per joint position error (MPJPE) which is the average L2 error over all joints. There are three protocols, the last of them recently introduced by Fang et al. <ref type="bibr" target="#b4">[5]</ref> to measure cross-camera efficiency.</p><p>Protocol #1 splits the dataset to training and test set by subjects. Subjects 1, 5, 6, 7, 8 are in the training set; subjects 9 and 11 are in the test set. The two splits share the same cameras and actions.</p><p>Protocol #2 has the same split as Protocol #1. The difference is in the error metric. The MPJPE is calculated after an affine Procrustean alignment to the ground truth using rotations and translations. This protocol aims to evaluate the correctness of the pose relative to itself, without taking into account scaling or rotations.</p><p>Protocol #3 aims to measure how well the method generalizes to unknown camera angles. This is similar to Protocol #1, using the same split of subjects. However, only 3 of the cameras are in the training set and the fourth one is in the test set. Like with Protocol #1, all actions occur in both subsets. We also call Protocol #3 cross-camera setup. <ref type="table">Table 1</ref> contains a summary of the size of the training and test set split by actions. Note that Protocol #2 uses the same split of training set as Protocol #1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Preprocessing and augmentation</head><p>To predict the 2D pose, we use a Stacked Hourglass network <ref type="bibr" target="#b1">[2]</ref> pretrained on the MPII-HP dataset <ref type="bibr" target="#b13">[14]</ref> and fine-tuned on the Human3.6M <ref type="bibr" target="#b5">[6]</ref> database.</p><p>Following the standard setup, P 3D is represented in a self-centered coordinate system. The hip is moved to the origin and the coordinate axes are parallel to the camera plane. Similarly to Martinez et al. <ref type="bibr" target="#b3">[4]</ref>, we normalize both the 2D inputs and 3D targets by subtracting the mean and dividing with the standard deviation.</p><p>To help training, we also generate augmented camera angles using the method described in <ref type="bibr" target="#b4">[5]</ref>. Note that we restrict ourselves to rotations around the central vertical axis only thus new cameras are generated on the circle the original cameras reside on. This is because in the Human3.6m dataset all cameras are on the same plane. Unlike <ref type="bibr" target="#b4">[5]</ref>, we synthesize a camera every 15 degrees and not 30. We have removed the two closest synthetic cameras to the test camera, as in <ref type="bibr" target="#b4">[5]</ref>. To have comparable results to previously published algorithms, we did not use the augmentation on Protocols #1 and #2. For Protocol #3, the input data was subsampled at 10fps. This was done for two reasons: first, due to augmentation the training data is quite large and using a subset of the data speeds up training; second, it helps comparing to previous work as the same sampling was applied there.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Training details</head><p>We used a dropout rate of 0.2. We have found empirically that it yielded better results then the standard value of 0.5. This confirms our hypothesis that the siamese loss acts as a regularizer.</p><p>For training we used the Adam optimizer with a learning rate of 0.001 and an exponential decay with a rate of 0.96. The batch size was set to 256. The training ran for 100 epochs. The siamese scaling factor was empirically set to ? 1 = 0.01. We have found that while changes to ? 1 larger than a magnitude affect the performance considerably, smaller changes have negligible effect. The size of the embedding was M = 128. Using larger M s did not provide better results.</p><p>The selection of the pairs fed to the network was the following: in a single batch, half of the input pairs were the same poses (the same frame of the same video sequence) from different random camera angles and half of them were randomly selected poses from random camera angles. We did not investigate the effects of other sampling techniques.</p><p>To show the stability of the model we present training curves on <ref type="figure" target="#fig_3">Figure 3</ref>.</p><p>Our model converges well and has a similar test error variance to the baseline algorithm. Neither the baseline nor our model overfits, the test error decreases steadily and then reaches a plateau. Due to the exponential decay of the learning rate the test accuracy stabilizes over time.   The results are presented in <ref type="table" target="#tab_4">Tables 2-4</ref>. In Protocol #3, among methods using only 2D pose information and no image input, our method achieves stateof-the-art result, improving 7mm (9.6%) over the previous best. It performs comparably to methods that use image information as well, only being 3mm (4.7%) worse than the best method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Quantitative results</head><p>In Protocol #1, our method performs better than the baseline (61.1mm vs 62.9). However, it can not beat algorithms that use image information or different network structure. This is in line with our expectations, as our extension is primarily a regularization for cross camera setup and adds little value if all the camera angles are present in both the test and training set.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Qualitative results</head><p>We also show qualitative results on the MPII-HP in-the-wild dataset <ref type="figure" target="#fig_4">(Figure 4</ref>). For this evaluation, we trained the model on Human3.6m using protocol #3. The MPII-HP database does not have 3D annotations so quantitative results are not available, however the presented images show that our model generalizes to new environments well. One limitation of our method is that it does not handle joints not present in the image (e.g. <ref type="figure" target="#fig_4">Figure 4</ref>, bottom row third image) since it was trained on images with full body poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Visualizing the hidden representation</head><p>To show that the encoded hidden representation indeed behaves rotationally equivariant, we rotated the embedding, applied the decoder and compared the results to the expected rotated output. Formally, for an input 2D pose P 2D , we calculated both g(Rf (P 2D )) and RP 3D , where R is a 3D rotation matrix and P 3D is the ground truth 3D pose for P 2D . Results are presented in <ref type="figure" target="#fig_5">Figure 5</ref>.</p><p>As seen in the figure, rotating the hidden embedding produces accurate predictions close to the ground truth even under large angles (bottom left pose on <ref type="figure" target="#fig_5">Figure 5</ref>). A failure case is shown on the bottom right of the figure. We found  In each triplet of images, the first image is the input 3D skeleton P 3D , the second is the result after rotating the hidden layer (g(Rh)), the third is the ground truth 3D skeleton, but rotated (RP 3D ). Bottom row, left triplet shows that even under large (180 degree) rotations the model produces high quality results. Bottom row, right triplet shows a failure case. that sitting poses have higher errors probably because the database contains mostly standing poses.  We performed an ablation study to confirm the necessity of the components of our algorithm. If we remove all the components, our method is the same as the one in <ref type="bibr" target="#b3">[4]</ref>. It is called Baseline in <ref type="table" target="#tab_9">Table 5</ref>. Note that our implementation (marked with ? in the table) produces results slightly worse (1.6mm) than the one reported in <ref type="bibr" target="#b4">[5]</ref>. The table shows the performance of our method when turning off a single components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant</head><p>Removing the siamese loss decreases the performance by 5.3mm, compared to all components turned on (71.1mm vs 65.8mm). Turning off augmentation decreases the performance the most among the components, however it is still better by 5.5mm (6.4%) than the baseline algorithm. Finally, without Leaky ReLU the performance drops 1.3mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variant</head><p>Error PoseGrammar* 72.8 PoseGrammar ? 67.8 Siamese PoseGrammar 65.0 <ref type="table">Table 6</ref>: Using PoseGrammar as a base network. Mean joint error for different PoseGrammar implementations. *Results published in <ref type="bibr" target="#b4">[5]</ref>. ? Our implementation with more augmented viewpoints.</p><p>Furthermore, our equivariant embedding can be applied to other network structures, not only to Baseline. To show that the method is general, we also extended Fang et al.'s PoseGrammar <ref type="bibr" target="#b4">[5]</ref> network to have a siamese structure.</p><p>The PoseGrammar network has a bottom part consisting of 4 residual blocks, and a top part built up from multiple bidirectional RNNs. The geometric embeddings together with the siamese loss were placed after the first and third residual blocks since the network has an intermediate supervision after the second and fourth blocks. Following the original training protocol, we first trained the bottom residual network only for 200 epochs and then finetuned the whole network with the RNN blocks on top for another 200 epochs. Results are presented in <ref type="table">Table 6</ref>. First note that our implementation uses more augmented viewpoints than the original, already improving the error from 72.8mm to 67.8mm. Adding the siamese architecture with equivariant embedding further decreases the loss to 65mm. <ref type="figure">Figure 6</ref>: The effect of synthetic camera placement on the model performance (Protocol #3). The x axis shows how far the closest camera is from the test camera in degrees. The closer a training camera is to the test camera the better the results are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">The effect of data augmentation</head><p>We also investigated how the siamese loss compares to augmentation. Note that the augmentation process has two steps: creating synthetic cameras by rotating existing ones around the subject and simulating the noise of the 2D pose estimator (for more details see <ref type="bibr" target="#b4">[5]</ref>). The siamese loss is only capable of replacing the camera rotation and not generating additional noise. Thus a lower bound on the performance of our architecture without augmentation is the performance of the baseline algorithm with only camera rotation augmentation (76.6mm). We achieve results that are halfway to the lower bound (81.1mm).</p><p>Additionally, we analyzed how the number of synthetic cameras affect the prediction performance. We found that it is not the number of cameras that the prediction performance depends on but the distance of the closest training cameras from the test camera. <ref type="figure">Figure 6</ref> shows how the prediction performance changes as we create cameras closer to the test cam. In all cases, our method is better than the baseline. As we get closer to the test cam the gap in MPJPE decreases. Our method can achieve the same level of performance as the baseline with less augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We have introduced a siamese network with an equivariant embedding that provides regularization for cross-camera 3D human pose estimation. It was shown that the method performs state-of-the-art if only 2D pose detection information is used. This distinction is important, as our method is orthogonal to others using image information (e.g. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18]</ref>) and can be integrated with those easily.</p><p>There are promising ways for improvements. One option is to go beyond 2D keypoint coordinates and use other information derived from the image, such as a pairwise ranking matrix <ref type="bibr" target="#b17">[18]</ref>. Other avenues not yet investigated include changing the siamese loss to a triplet loss and/or improvments in the input sampling. Both were found to have large effect on network performance <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Our siamese architecture. We feed the input 2D detections P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Training curves. The training loss and test error of the baseline and our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on MPII-2D. The (cropped) input images are on the left and our network's prediction on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Rotation of the hidden embedding There are 6 ? 3 images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Results on Protocol #1. The table shows mean joint errors in millimeters. Best results among 2D to 3D methods are selected in bold, best overall results are underlined.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Results on Protocol #2. The table shows mean joint errors in millimeters. Best results among 2D to 3D methods selected are in bold, best overall results are underlined.</figDesc><table><row><cell>Protocol #3</cell><cell cols="7">Uses Image Direct. Discuss Eating Greet Phone Photo</cell><cell>Pose</cell><cell>Purch.</cell></row><row><cell>Zhou et al. [9]*</cell><cell>Y</cell><cell>61.4</cell><cell>70.7</cell><cell>62.2</cell><cell>76.9</cell><cell>71.0</cell><cell>81.2</cell><cell>67.3</cell><cell>71.6</cell></row><row><cell>DRPose3D [18]  ?</cell><cell>Y</cell><cell>55.8</cell><cell>56.1</cell><cell>59.0</cell><cell>59.3</cell><cell>66.8</cell><cell>70.9</cell><cell>54.0</cell><cell>55.0</cell></row><row><cell>Martinez et al. [4]*</cell><cell>N</cell><cell>65.7</cell><cell>68.8</cell><cell>92.6</cell><cell>79.9</cell><cell>84.5</cell><cell>100.4</cell><cell>72.3</cell><cell>88.2</cell></row><row><cell>Martinez et al. [4]  ?</cell><cell>N</cell><cell>58.4</cell><cell>58.4</cell><cell>69.9</cell><cell>65.4</cell><cell>70.3</cell><cell>80.5</cell><cell>61.6</cell><cell>69.4</cell></row><row><cell>Fang et al. [5]  ?</cell><cell>N</cell><cell>57.5</cell><cell>57.8</cell><cell>81.6</cell><cell>68.8</cell><cell>75.1</cell><cell>85.8</cell><cell>61.6</cell><cell>70.4</cell></row><row><cell>Fang et al. [5]  ?</cell><cell>N</cell><cell>57.8</cell><cell>57.6</cell><cell>66.3</cell><cell>65.0</cell><cell>68.4</cell><cell>79.5</cell><cell>61.8</cell><cell>67.9</cell></row><row><cell>Ours  ?</cell><cell>N</cell><cell>54.5</cell><cell>57.6</cell><cell>58.7</cell><cell>62.3</cell><cell>66.7</cell><cell>74.6</cell><cell>59.9</cell><cell>65.6</cell></row><row><cell>Protocol #3</cell><cell cols="8">Uses Image Sitting SitingD. Smoke Wait WalkD. Walk WalkT.</cell><cell>Avg.</cell></row><row><cell>Zhou et al. [9]*</cell><cell>Y</cell><cell>96.7</cell><cell>126.1</cell><cell>68.1</cell><cell>76.7</cell><cell>63.3</cell><cell>72.1</cell><cell>68.9</cell><cell>75.6</cell></row><row><cell>DRPose3D [18]  ?</cell><cell>Y</cell><cell>78.8</cell><cell>92.4</cell><cell>58.9</cell><cell>56.2</cell><cell>64.6</cell><cell>56.6</cell><cell>55.5</cell><cell>62.8</cell></row><row><cell>Martinez et al. [4]*</cell><cell>N</cell><cell>109.5</cell><cell>130.8</cell><cell>76.9</cell><cell>81.4</cell><cell>85.5</cell><cell>69.1</cell><cell>68.2</cell><cell>84.9</cell></row><row><cell>Martinez et al. [4]  ?</cell><cell>N</cell><cell>86.8</cell><cell>99.5</cell><cell>64.5</cell><cell>69.5</cell><cell>69.6</cell><cell>60.5</cell><cell>60.2</cell><cell>69.6</cell></row><row><cell>Fang et al. [5]  ?</cell><cell>N</cell><cell>95.8</cell><cell>106.9</cell><cell>68.5</cell><cell>70.4</cell><cell>73.8</cell><cell>58.5</cell><cell>59.6</cell><cell>72.8</cell></row><row><cell>Fang et al. [5]  ?</cell><cell>N</cell><cell>83.3</cell><cell>94.5</cell><cell>63.1</cell><cell>66.8</cell><cell>68.2</cell><cell>59.0</cell><cell>57.1</cell><cell>67.8</cell></row><row><cell>Ours  ?</cell><cell>N</cell><cell>80.5</cell><cell>93.6</cell><cell>60.6</cell><cell>66.9</cell><cell>68.3</cell><cell>59.0</cell><cell>58.6</cell><cell>65.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Results on Protocol #3. The table shows mean joint errors in millimeters. Best results among 2D to 3D methods are selected in bold, best overall results are underlined. * No augmentations.</figDesc><table /><note>? Synthetic cameras every 15 degrees.? Synthetic cameras every 30 degrees.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Results of our implementation.</figDesc><table><row><cell>Error 84.9 86.5 71.1 81.0 67.1 w/ All components 65.8 Baseline* Baseline  ? w/o Siamese loss w/o Augmentation w/o Leaky ReLU (a) Error of our method with compo-nents turned off. *Results from [5].  Variant No Augmentation 86.5 Error Rot. Aug. 76.6 Rot. Aug.+Noise 69.6 Ours w/o Aug 81.0 (b) Error of the Baseline method with different augmentations</cell></row></table><note>?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Ablation studies. a) Mean joint error in millimeters with a single component of our method turned off. In the baseline algorithm all components turned off. It is equivalent to the case of Martinez et al. [4]. b) The effect of different levels of augmentations on the baseline.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/vegesm/siamese-pose-estimation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Author Contributions M?rton V?ges developed the main thesis and performed most of the analysis. Viktor Varga ran the analysis on the MPII-2D database. Andr?s L?rincz was the supervisor of the project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1302" to="1310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2353" to="2362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Signature verification using a &quot;siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>S?ckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Neural Information Processing Systems</title>
		<meeting>the 6th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exploiting temporal information for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08585</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">2d/3d pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5137" to="5146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Schiele, 2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09241</idno>
		<title level="m">Fbi-pose: Towards bridging the gap between 2d images and 3d human poses using forward-orbackward information</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligen</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligen</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="978" to="984" />
		</imprint>
	</monogr>
	<note>Drpose3d: Depth ranking in 3d human pose estimation</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">It&apos;s all relative: Monocular 3d human pose estimation from weakly supervised data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Siamese regression networks with efficient mid-level feature extraction for 3d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02257</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">From depth data to head pose estimation: a siamese approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venturelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Borghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03624</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7168" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">Proceedings of the 6th International Conference on Learning Representation</title>
		<meeting>the 6th International Conference on Learning Representation</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
	<note>Spherical cnns</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning so(3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="52" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
	<note>3d shapenets: A deep representation for volumetric shapes</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Furuya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ohbuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatsuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thermos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Axenopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Daras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Johan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mk</surname></persName>
		</author>
		<title level="m">Shrec&apos;17 track: Large-scale 3d shape retrieval from shapenet core55</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Eurographics Workshop on 3D Object Retrieval</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5737" to="5746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sampling matters in deep embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Manmatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2859" to="2867" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
