<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Light Image Enhancement with Normalizing Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Rapid-Rich Object Search Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Rapid-Rich Object Search Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
							<email>haoliang.li@cityu.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">Rapid-Rich Object Search Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">City University of Hong Kong</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lap-Pui</forename><surname>Chau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Rapid-Rich Object Search Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
							<email>eackot@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Rapid-Rich Object Search Lab</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Light Image Enhancement with Normalizing Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To enhance low-light images to normally-exposed ones is highly ill-posed, namely that the mapping relationship between them is one-to-many. Previous works based on the pixel-wise reconstruction losses and deterministic processes fail to capture the complex conditional distribution of normally exposed images, which results in improper brightness, residual noise, and artifacts. In this paper, we investigate to model this one-to-many relationship via a proposed normalizing flow model. An invertible network that takes the low-light images/features as the condition and learns to map the distribution of normally exposed images into a Gaussian distribution. In this way, the conditional distribution of the normally exposed images can be well modeled, and the enhancement process, i.e.. the other inference direction of the invertible network, is equivalent to being constrained by a loss function that better describes the manifold structure of natural images during the training. The experimental results on the existing benchmark datasets show our method achieves better quantitative and qualitative results, obtaining better-exposed illumination, less noise and artifact, and richer colors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Low-light image enhancement aims to improve the visibility of low-light images and suppress captured noise and artifacts. Deep learning-based methods <ref type="bibr" target="#b40">(Zhang et al. 2019;</ref><ref type="bibr" target="#b37">Zamir et al. 2020;</ref><ref type="bibr" target="#b3">Chen et al. 2018)</ref> achieve promising performance by utilizing the power of large collections of data. However, most of them mainly rely on the pixel-wise loss functions (e.g., l 1 or l 2 ) in the network training that derive a deterministic mapping between the low-light and normally exposed images. This enhancement paradigm encounters two issues. First, this pixel-wise loss cannot provide effective regularization on the local structures in diverse contexts. As one low-light image may correspond to several reference images with different brightness <ref type="bibr" target="#b39">(Zhang et al. 2021)</ref>, this pixel-to-pixel deterministic mapping is easily trapped into the "regression to mean" problem and obtains the results that are the fusion of several desirable ones, which inevitably leads to improperly exposed regions and artifacts. Second, due to the simplified assumption of the pixel-wise losses about the image distribution, these losses might fail in describing the real visual distance <ref type="bibr">Copyright ? 2022</ref>. All rights reserved.  <ref type="figure">Figure 1</ref>: Illustration of the superiority of our normalizing flow model in measuring the visual distance compared to l 1 loss for low-light image enhancement. Although (b) is more visually similar to (c), i.e., reference image, than (a), their l 1 losses are the same. Benefiting from better capturing the complex conditional distribution of normally exposed images, our model can better capture the error distribution and therefore provide the measure results more consistent with human vision. between the reference image and enhanced images in the image manifold as shown in <ref type="figure">Fig. 1</ref>, which further undermines the performance. Though the GAN-based scheme can partly alleviate this issue, these approaches require careful tuning during training <ref type="bibr" target="#b32">(Wolf et al. 2021</ref>) and might overfit certain visual features or the properties of the training data.</p><p>Recently, researchers have shown the effectiveness of normalizing flow in the field of computational photography. <ref type="bibr" target="#b32">(Wolf et al. 2021;</ref><ref type="bibr" target="#b18">Lugmayr et al. 2020;</ref><ref type="bibr" target="#b33">Xiao et al. 2020)</ref> The normalizing flow is capable to learn a more complicated conditional distribution than the classical pixel-wise loss, which can well solve the above-mentioned two issues. Beyond previous CNN-based models that learn a deterministic mapping from the low-light image to an image with specific brightness, the normalizing flow learns to map the multi-modal image manifold into a latent distribution. Then, the loss enforced on the latent space equivalently constructs an effective constraint on the enhanced image manifold. It leads to better characterization of the structural details in various contexts and better measurement of the visual distance in terms of high-quality well-exposed images, which helps effectively adjust the illumination and suppress the image artifacts. However, since the classical normalizing flow is biased towards learning image graphical properties such as local pixel correlations <ref type="bibr" target="#b12">(Kirichenko, Izmailov, and Wilson 2020)</ref>, it may fail to model some global image properties like the color saturation, which can undermine the performance when applying these methods for the low-light image enhancement problem.</p><p>To address the above issues, in this paper, we propose LLFlow, a flow-based low-light image enhancement method to accurately learn the local pixel correlations and the global image properties by modeling the distributions over the normally exposed images. As shown in <ref type="figure">Fig. 2</ref>, to merge the global image information into the latent space, instead of using standard Gaussian distribution as the prior of latent features, we propose to use the illumination-invariant color map as the mean value of the prior distribution. More specifically, the encoder is designed to learn a one-to-one mapping to extract the color map that can be regarded as the intrinsic attributes of the scene that do not change with illumination. Simultaneously, another component of our framework, the invertible network, is designed to learn a one-to-many mapping from a low-light image to distribution of normally exposed images. As such, we expect to achieve better low-light image enhancement performance through our proposed framework.</p><p>In summary, contributions can be concluded as follows.</p><p>? We propose a conditional normalizing flow to model the conditional distribution of normally exposed images. It equivalently enforces an effective constraint on the enhanced image manifold. Via better characterization of the structural details and better measurement of the visual distance, it better adjusts illumination as well as suppresses noise and artifacts. ? We further introduce a novel module to extract the illumination invariant color map inspired by the Retinex theory as the prior for the low-light image enhancement task, which enriches the saturation and reduces the color distortion. ? We conduct extensively experiments on the popular benchmark datasets to show the effectiveness of our proposed framework. The ablation study and related analysis show the rationality of each module in our method.  <ref type="bibr" target="#b24">(Shen et al. 2017;</ref><ref type="bibr" target="#b24">Tao et al. 2017;</ref><ref type="bibr" target="#b20">Lv et al. 2018;</ref><ref type="bibr" target="#b21">Ren et al. 2019</ref>) to obtain better visual quality. The <ref type="bibr" target="#b24">(Shen et al. 2017)</ref> illustrates the close relationship between Retinex and CNN with Gaussian convolution kernels, two separated deep networks are used for decomposition in <ref type="bibr" target="#b30">(Wei et al. 2018)</ref>, and <ref type="bibr" target="#b28">(Wang et al. 2019b</ref>) propose a progressive Retinex framework that the illumination and reflection maps are trained in a mutually reinforced manner. In addition, different losses are used to guide the training, e.g., MSE <ref type="bibr" target="#b17">(Lore et al. 2017;</ref><ref type="bibr" target="#b2">Cai, Gu, and Zhang 2018)</ref>, l 1 loss <ref type="bibr" target="#b2">(Cai, Gu, and Zhang 2018)</ref>, structural similarity (SSIM) <ref type="bibr" target="#b2">(Cai, Gu, and Zhang 2018)</ref>, smoothness loss <ref type="bibr" target="#b26">(Wang et al. 2019a;</ref><ref type="bibr" target="#b40">Zhang et al. 2019</ref>) and color loss <ref type="bibr" target="#b26">(Wang et al. 2019a;</ref><ref type="bibr" target="#b7">Guo et al. 2020;</ref><ref type="bibr" target="#b24">Shen et al. 2017</ref>). Meanwhile, <ref type="bibr" target="#b2">(Cai, Gu, and Zhang 2018)</ref> demonstrates that training the same network with different reconstruction losses will have different performances which demonstrates the significance of conditional distribution design. Introducing carefully designed color loss can be also regarded as refining the conditional distribution, i.e., give color distortion pictures a greater penalty coefficient. Different from previous works that carefully design the reconstruction loss for end-to-end training, in this paper, we propose to utilize a normalizing flow to build the complex posterior distribution which has proven to be more effective and can generate images with higher quality, less noise, and artifact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Normalizing flow</head><p>A normalizing flow is a transformation of a simple probability distribution (e.g., a standard normal) into a more complex distribution by a sequence of invertible and differentiable mappings <ref type="bibr" target="#b13">(Kobyzev, Prince, and Brubaker 2020)</ref>. Meanwhile, the probability density function (PDF) value of a sample can be exactly obtained by transforming it back to the simple distribution. To make the network invertible and computation tractable, the layers of the network need to be carefully designed so that the inversion and the determinant of Jacbian matrix can be easily obtained which limits the capacity of the generative model. To this end, many powerful transformations have been proposed to enhance expressiveness capacity of the model. ... + <ref type="figure">Figure 2</ref>: The architecture of our proposed LLFlow. Our model consists of a conditional encoder to extract the illuminationinvariant color map and an invertible network that learns a distribution of normally exposed images conditioned on a low-light one. For training, we maximize the exact likelihood of a high-light image x h by using change of variable theorem in Eq.</p><p>(3) and a random selector is used to obtain the mean value of latent variable z which obey Gaussian distribution from the color map C(x h ) of reference image or the extracted color map g(x l ) from low-light image through the conditional encoder. For inference, we can randomly select z from N (g(x l ), 1) to generate different normally exposed images from the learned conditional distribution f f low (x|x l ). (The color maps in the blue area are squeezed to the same size with latent feature z.)</p><p>tion, the inductive biases of normalizing flows are explored <ref type="bibr" target="#b9">(Jaini et al. 2020;</ref><ref type="bibr" target="#b12">Kirichenko, Izmailov, and Wilson 2020)</ref>. <ref type="bibr" target="#b12">(Kirichenko, Izmailov, and Wilson 2020)</ref> reveals that the normalizing flow prefers to encode simple graphical structures which may be helpful to suppress the noise in the low-light image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first introduce the limitations of previous pixel-wise reconstruction loss-based low-light enhancement methods. Then, the overall paradigm of our framework in <ref type="figure">Fig. 2</ref> is introduced. Finally, two components of our proposed framework are illustrated separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>The goal of low-light image enhancement is to generate a high-quality image with normal exposure x h using a lowlight image x l . Paired samples (x l , x ref ) are usually collected to train a model ? by minimizing the l 1 reconstruction loss as follows:</p><formula xml:id="formula_0">arg min ? E [l 1 (?(x l ), x ref )] = arg max ? E [log f (?(x l )|x ref )] ,</formula><p>(1) where ?(x l ) is the normal-light image generated by the model and f is the probability density function conditioned on the reference image x ref defined as follows:</p><formula xml:id="formula_1">f (x|x ref ) = 1 2b exp ? |x ? x ref | b ,<label>(2)</label></formula><p>where b is a constant related to the learning rate. However, such a training paradigm has a limitation that the pre-defined distribution (e.g., the distribution in Eq. 2) of images is not strong enough to distinguish between the generated realistic normally exposed image and the images with noises or artifacts such as the example in <ref type="figure">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Framework</head><p>To this end, we propose to model the complicated distribution of normally exposed images using a normalizing flow so that the conditional PDF of a normally exposed image can be expressed as f f low (x|x l ). More specifically, a conditional normalizing flow ? is used to take a low-light image itself and/or its features as input and maps a normally exposed image x to a latent code z which has the same dimension with x, i.e., z = ?(x; x l ). By using the change of variable theorem, we can obtain the relationship between f f low (x|x l ) and f z (z) as follows:</p><formula xml:id="formula_2">f f low (x|x l ) = f z (?(x ref ; x l )) det ?? ?x ref (x ref ; x l ) .</formula><p>(3) To make the model better characterize the properties of high-quality normally exposed images, we use the maximum likelihood estimation to estimate the parameter ?. Specifically, we minimize the negative log-likelihood (NLL) instead of l 1 loss to train the model</p><formula xml:id="formula_3">L(x l , x ref ) = ? log f f low (x ref |x l ) = ? log f z (?(x ref ; x l )) ? N ?1 n=0 log | det ?? n ?z n (z n ; g n (x l ))|,<label>(4)</label></formula><p>where the invertible network ? is divided into a sequence of N invertible layers {? 1 , ? 2 , ..., ? N } and h i+1 = ? i (h i ; g i (x l )) is the output of layer ? i (i ranges from 0 to <ref type="figure">Figure 3</ref>: The components of the input for the encoder g. The low-light image x l , low-light image after histogram equalization h(x l ), color map C(x l ) and noise map N (x l ) are concatenated to form the input with 12 channels.</p><formula xml:id="formula_4">(a) x l (b) h(x l ) (c) C(x l ) (d) N (x l )</formula><formula xml:id="formula_5">N ? 1), h 0 = x ref and z = h N . g n (x l )</formula><p>is the latent feature from the encoder g that has the compatible shape with the layer ? n . f z is the PDF of the latent feature z.</p><p>In summary, our proposed framework includes two components: an encoder g which takes a low-light image x l as input and output illumination invariant color map g(x l ) (which can be regarded as reflectance map inspired by Retinex theory), and an invertible network that maps a normally exposed image to a latent code z. The details of the two components are introduced in the following subsections.</p><p>Encoder for illumination invariant color map: To generate robust and high quality illumination invariant color maps, the input images are first processed to extract useful features and the extracted features are then also concatenated as a part of the input of the encoder built by Residual-in-Residual Dense Blocks (RRDB) . The detailed architecture of the encoder g is in appendix due to limited space. The visualizations of each component are shown in <ref type="figure">Fig. 3</ref> and the details are as follows:</p><p>1) Histogram equalized image h(x l ): Histogram equalization is conducted to increase the global contrast of lowlight images. The histogram equalized image can be regarded as a more illumination invariant one. By including the histogram equalized image as a part of the network's input, the network can better deal with the areas that are too dark or bright. 2) Color map C(x): Inspired by Retinex theory, we propose to calculate the color map of an image x as follows:</p><formula xml:id="formula_6">(a) C(x l ) (b) g(x l ) (c) C(x ref ) (d) x ref</formula><formula xml:id="formula_7">C(x) = x mean c (x) ,<label>(5)</label></formula><p>where mean c calculates the mean value of each pixel among RGB channels. The comparison between the color map from the low-light image, reference image, and the color map fine-tuned by the encoder g is shown in <ref type="figure" target="#fig_1">Fig. 4</ref>. As we can see, the color maps C(x l ) and C(x ref ) are consistent to an extent under different illumination so they can be regarded as representations similar to the reflectance map, degraded with intensive noises in C(x l ). We can also find that the encoder g can generate a high-quality color map that suppresses the strong noises to an extent and preserves the color information.</p><p>3) Noise map N (x l ): To remove the noise in C(x l ), a noise map N (x l ) is estimated and fed into the encoder as an attention map. The noise map N (x l ) is estimated as follows:</p><formula xml:id="formula_8">N (x) = max(abs(? x C(x)), abs(? y C(x))),<label>(6)</label></formula><p>where ? x , and ? y are the gradient maps in the directions of x and y, where max(x, y) is the operation that returns the maximum value between x and y at the pixel channel level.</p><p>Invertible network: Different from the encoder that learns a one-to-one mapping to extract illumination invariant color map which can be seen as the intrinsic invariant properties of the objects, the invertible network aims to learn a oneto-many relationship since the illumination may be diverse for the same scenario. Our invertible network is composed of three levels, and at each level, there are a squeeze layer and 12 flow steps. More details about the architecture can be found in the appendix. According to our assumption that the normalizing flow aims to learn a conditional distribution of the normally exposed images conditioned on the low-light image/the illumination invariant color map, the normalizing flow should work well conditioned on both g(x l ) and C(x ref ) since these two maps are expected to be similar. To this end, we train the whole framework (both the encoder and the invertible network) in the following manner:</p><formula xml:id="formula_9">L(x l , x ref ) = ? log f z (?(x ref ; x l )) ? N ?1 n=0</formula><p>log det ?? n ?z n (z n ; g n (x l )) ,</p><p>where f z is the PDF of the latent feature z defined as follows</p><formula xml:id="formula_11">f z (z) = 1 ? 2? exp ?(x ? r(C(x ref ), g(x l ))) 2 2<label>(8)</label></formula><p>and r(a, b) is a random selection function that is defined as follows:</p><formula xml:id="formula_12">r(a, b) = a ? ? p b ? &gt; p , ? ? U (0, 1),<label>(9)</label></formula><p>in which p is a hyper-parameter and we set p to be 0.2 for all experiments. As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, even without the help of pixel reconstruction loss, the encoder g can learn a similar color map with the reference image.</p><p>To generate a normally exposed image using a low-light image, the low-light image is first passed through the encoder to extract the color map g(x l ) and then the latent features of the encoder are used as the condition for the invertible network. For the sampling strategy of z, one can randomly select a batch of z from the distribution N (g(x L ), 1) to get different outputs and then calculate the mean of generated normally-exposed images to achieve better performance. To speed up the inference, we directly select g(x l ) as the input z and we empirically find that it can achieve a good enough result. So for all the experiments, we just use the mean value g(x l ) as the latent feature z for the conditional normalizing flow if not specified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>The patch size is set to 160 ? 160 and the batch size is set to 16. We use Adam as the optimizer with a learning rate of 5 ? 10 ?4 and without weight decay. For LOL dataset, we train the model for 3 ? 10 4 iterations and the learning rate is decreased with a factor of 0.5 at 1.5 ? 10 4 , 2.25 ? 10 4 , 2.7 ? 10 4 , 2.85 ? 10 4 iterations. For VE-LOL dataset, we train the model for 4 ? 10 4 iterations and the learning rate is decreased with a factor of 0.5 at 2 ? 10 4 , 3 ? 10 4 , 3.6 ? 10 4 , 3.8 ? 10 4 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on LOL</head><p>We first evaluate our method on the LOL datset <ref type="bibr" target="#b30">(Wei et al. 2018)</ref> including 485 images for training and 15 images for testing. Three metrics are adopted for quantitative comparison including PSNR, SSIM <ref type="bibr" target="#b29">(Wang et al. 2004)</ref>, and LPIPS ). The numerical results among different methods are reported in <ref type="table" target="#tab_2">Table 1</ref>. From <ref type="table" target="#tab_2">Table 1</ref>, we can find that our method significantly outperforms all the other competitors. The higher PSNR values show that our method is capable of suppressing the artifacts and better recovering color information. The better SSIM values demonstrate that our method better preserves the structural information with high-frequency details. In terms of LPIPS, a metric designed for the human perception, our method also achieves the best performance, which indicates our method better align with the human perception. The qualitative results are shown in <ref type="figure">Fig. 5</ref> Our method achieves more promising perceptual quality by better suppressing the artifacts and revealing image details. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation on VE-LOL</head><p>To better evaluate the performance and generality of our method, we further perform evaluation on VE-LOL <ref type="bibr" target="#b14">(Liu et al. 2021a)</ref> dataset. It is a large-scale dataset including 2500 paired images with more diversified scenes and contents, thus is valuable for the cross-dataset evaluation. 1) Cross-dataset evaluation: We first evaluate the generality of our method in a cross-dataset manner, i.e., we train our method on the LOL dataset <ref type="bibr" target="#b30">(Wei et al. 2018</ref>) and test the model on the testing set of VE-LOL dataset <ref type="bibr" target="#b14">(Liu et al. 2021a</ref>).</p><p>The quantitative results are reported in <ref type="table" target="#tab_3">Table 2</ref>. From the results, our method significantly outperforms other methods in terms of all metrics. The qualitative comparisons of realcaptured image are given in <ref type="figure">Fig. 7</ref>. The results generated by our methods are with less noise and better color saturation.  <ref type="bibr" target="#b36">(Ying, Li, and Gao 2017)</ref> 15.95 0.6386 0.4573 DeepUPE <ref type="bibr" target="#b26">(Wang et al. 2019a)</ref> 13.19 0.4902 0.4634 JED <ref type="bibr" target="#b22">(Ren et al. 2018)</ref> 16.73 0.6817 0.3899 LIME <ref type="bibr" target="#b8">(Guo, Li, and Ling 2016)</ref> 14.07 0.5274 0.4021 SICE <ref type="bibr" target="#b2">(Cai, Gu, and Zhang 2018)</ref> 18.06 0.7094 0.5078 LLNet <ref type="bibr" target="#b17">(Lore et al. 2017)</ref> 17.57 0.7388 0.4021 SRIE <ref type="bibr" target="#b6">(Fu et al. 2016)</ref> 13.66 0.5509 0.4577 KinD <ref type="bibr" target="#b40">(Zhang et al. 2019)</ref> 18.42 0.7658 0.2879 KinD++ <ref type="bibr" target="#b39">(Zhang et al. 2021)</ref> 17.63 0.7994 0.2257 Zero-DCE <ref type="bibr" target="#b7">(Guo et al. 2020)</ref> 21.12 0.7705 0.2480 EnlightenGAN <ref type="bibr" target="#b10">(Jiang et al. 2021)</ref> 20.43 0.7921 0.2416 LLFlow (Ours) 23.85 0.8986 0.1456</p><p>2) Intra-dataset evaluation: To further evaluate the performance of our proposed model, we compare our method with SOTA methods in an intra-dataset setting, i.e., we retrain all the methods using the training set of VE-LOL dataset and report the performance on its corresponding test set. The quantitative results are reported in <ref type="table" target="#tab_4">Table 3</ref>. We can find that our method has the best performance and outperforms others by a large margin. Meanwhile, with the help of more diverse data, all the metrics of our method are improved comparing with the model trained on LOL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation study</head><p>1) The losses estimated by our method and l 1 : To verify our motivation that conditional normalizing flow can model a more complicated error distribution comparing with pixelwise reconstruction loss, we further compare the losses obtained by our method and l 1 . As shown in <ref type="table" target="#tab_5">Table 4</ref>, the image with intensive noises and that with slightly different brightness have the same likelihood values under the measurement  <ref type="figure">Figure 5</ref>: Visual comparison with state-of-the-art low-light image enhancement methods on LOL dataset. The normally exposed image generated by our method has less noise and artifact, and better colorfulness.</p><p>(a) (b) (c) <ref type="figure">Figure 6</ref>: (a) and (b) are the generated normally exposed images with different z (monotonically, each column) from a well-trained model. There are strong artifacts in (c) obtained from an early checkpoint of the model when it cannot well distinguish the artifacts and the variance of data. Zoom in to see details.</p><p>of l 1 loss, while the latter has much higher likelihood values under the measurement of our model than the former, which is better aligned with human perception. 2) The effect of different z: A major advantage of our method over existing ones is that LLFlow can better encode the brightness variance into the latent space z. To verify the effectiveness of such strategy, we add a constant to the extracted g(x l ) from ?0.4 to 0.4 with a step of 0.2. The results in <ref type="figure">Fig. 6</ref> demonstrate that the brightness of the image is monotonous with the value of z, which indicates that our model can encode the variance of the dataset, i.e., the inevitable uncertainty when collecting the data pairs.</p><p>3) The activation area of LLFlow: To better understand how our model builds a more strong constraint, we visualize the gradient activation map of our method. For a normally exposed image x high which can be a reference image or the output from a low-light enhancement network from its corresponding low-light image x low , the gradient activation map G can be obtained as follows:</p><formula xml:id="formula_13">G = h(||? x L(x l , x high ) | 2 )<label>(10)</label></formula><p>where h is the histogram equalization operation to better visualize the results. From the results in <ref type="figure">Fig. 8</ref>, we can find that the area with artifacts has a higher gradient activation value. It demonstrates that even without the reference image, our model can distinguish the unrealistic areas according to the learned conditional distribution. 4) The effectiveness of model components and training paradigm: To investigate the effectiveness of our training paradigm and different components in our framework, We evaluate the performance of our conditional encoder individually and the performance of our whole framework via training them using l 1 loss. For the evaluation of our whole framework under l 1 loss, we empirically find that training directly with it cannot converge. To this end, we first pretrain the framework for 1,000 iterations by minimizing the negative log likelihood L(x l , x ref ). All the networks are trained with the same batch size, patch size, image prepossessing pipeline in the related experiments. We finetune other hyper-parameters, e.g., learning rate and weight decay, in a wide range to achieve the best performance.</p><p>The results evaluated on LOL dataset <ref type="bibr" target="#b30">(Wei et al. 2018</ref>) are reported in <ref type="table" target="#tab_6">Table 5</ref>. The model trained by minimizing NLL loss has a huge improvement in all metrics comparing with the model trained by l 1 loss. A visual comparison between the results from l 1 loss trained model and NLL trained model are shown in <ref type="figure">Fig. 9</ref>. From the results, the model trained by l 1 loss produces more obvious artifacts. Both quantitative and qualitative results demonstrate the superiority of our flowbased method in modeling the distribution of images with normal brightness over a simplified pixel-wise loss.</p><p>(a) reference image (b) by NLL loss (c) by L1 loss <ref type="figure">Figure 9</ref>: The effect of different training paradigm for the same network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5)</head><p>The effect of different latent feature distributions: To evaluate the effectiveness of our proposed illumination invariant color map and different hyper-parameters p, we evaluate them using the LOL dataset <ref type="bibr" target="#b30">(Wei et al. 2018)</ref>. The results in <ref type="table" target="#tab_7">Table 6</ref> show that our whole model with the newly designed color map achieves better PSNR values. The higher SSIM and LPIPS values show that the color map helps improve the color and brightness consistency. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel framework for lowlight image enhancement through a novel normalizing flow model. Compared with the existing techniques based on the pixel-wise reconstruction losses with deterministic processes, the proposed normalizing flow trained with negative loglikelihood (NLL) loss taking the low-light images/features as the condition naturally better characterizes the structural context and measures the visual distance in image manifold. With these merits, our proposed method naturally better captures the complex conditional distribution of normally exposed images and can achieve better low-light enhancement quality, i.e., well-exposed illumination, suppressed noise and artifacts, as well as rich colors. The experimental results on the existing benchmark datasets show that our proposed framework can achieve better quantitative and qualitative results compared with state-of-the-art techniques.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>The color map directly extracted from the low-light image x l , obtain from the encoder g, directly extracted from the reference image x ref , and the reference image itself.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Visual comparison with state-of-the-art low-light image enhancement methods on the real-captured set of VE-LOL dataset. Gradient activation map from our model. (a): The stitched picture that its left half is from not fully trained model and its right half is from the reference image. (b): The gradient activation map from (a). (c): The stitched picture that its right half is from not fully trained model and its left half is the reference image. (d): The gradient activation map from (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Quantitative comparison on the LOL dataset<ref type="bibr" target="#b30">(Wei et al. 2018</ref>) in terms of PSNR, SSIM and LPIPS. ? (?) denotes that, larger (smaller) values lead to better quality.</figDesc><table><row><cell>Method</cell><cell cols="3">PSNR ? SSIM ? LPIPS ?</cell></row><row><cell>Zero-DCE (Guo et al. 2020)</cell><cell>14.86</cell><cell>0.54</cell><cell>0.33</cell></row><row><cell>LIME (Guo, Li, and Ling 2016)</cell><cell>16.76</cell><cell>0.56</cell><cell>0.35</cell></row><row><cell>EnlightenGAN (Jiang et al. 2021)</cell><cell>17.48</cell><cell>0.65</cell><cell>0.32</cell></row><row><cell>RetinexNet (Wei et al. 2018)</cell><cell>16.77</cell><cell>0.56</cell><cell>0.47</cell></row><row><cell>RUAS (Risheng et al. 2021)</cell><cell>18.23</cell><cell>0.72</cell><cell>0.35</cell></row><row><cell>DRBN (Yang et al. 2020)</cell><cell>20.13</cell><cell>0.83</cell><cell>0.16</cell></row><row><cell>(Lv, Li, and Lu 2021)</cell><cell>20.24</cell><cell>0.79</cell><cell>0.14</cell></row><row><cell>KinD (Zhang et al. 2019)</cell><cell>20.87</cell><cell>0.80</cell><cell>0.17</cell></row><row><cell>KinD++ (Zhang et al. 2021)</cell><cell>21.30</cell><cell>0.82</cell><cell>0.16</cell></row><row><cell>LLFlow (Ours)</cell><cell>25.19</cell><cell>0.93</cell><cell>0.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="4">: Quantitative comparison on the VE-LOL dataset in</cell></row><row><cell cols="4">terms of PSNR, SSIM and LPIPS. The models are trained on</cell></row><row><cell cols="4">the training set of LOL. ? (?) denotes that, larger (smaller)</cell></row><row><cell>values lead to better quality.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">PSNR ? SSIM ? LPIPS ?</cell></row><row><cell>RetinexNet (Wei et al. 2018)</cell><cell>14.68</cell><cell>0.5252</cell><cell>0.6423</cell></row><row><cell>BIMEF</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Quantitative comparison on the VE-LOL dataset in terms of PSNR, SSIM, and LPIPS. The models are re-trained on the training set of VE-LOL dataset. ? (?) denotes that, larger (smaller) values lead to better quality.</figDesc><table><row><cell>Method</cell><cell cols="3">PSNR ? SSIM ? LPIPS ?</cell></row><row><cell>Zero-DCE (Guo et al. 2020)</cell><cell>20.54</cell><cell>0.7786</cell><cell>0.3312</cell></row><row><cell>KinD (Zhang et al. 2019)</cell><cell>22.15</cell><cell>0.8535</cell><cell>0.2576</cell></row><row><cell>LLFlow (Ours)</cell><cell>26.02</cell><cell>0.9266</cell><cell>0.0996</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>The differences under l 1 and negative log likelihood (NLL) estimated by our method for images with brightness variance and strong noise. The mean values among the test set of the LOL dataset are reported in the table. The random noise r has the same shape with the reference image and the mean value and mean absolute value of r are 0 and 20 respectively.</figDesc><table><row><cell>Degradation</cell><cell>NLL estimated by our method</cell><cell>l 1</cell></row><row><cell>Reference</cell><cell>-6.09</cell><cell>N/A</cell></row><row><cell>Brightness reduced by 20 1</cell><cell>-5.95</cell><cell>20</cell></row><row><cell>Brightness increased by 20</cell><cell>-6.15</cell><cell>20</cell></row><row><cell>Reference + random noise r 2</cell><cell>4.84</cell><cell>20</cell></row><row><cell cols="2">1 The range of pixel value is 0 ? 255.</cell><cell></cell></row></table><note>2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Quantitative comparison between training the model with l 1 and NLL loss on the LOL dataset. ? (?) denotes that, larger (smaller) values lead to better quality.</figDesc><table><row><cell>Loss</cell><cell cols="3">PSNR ? SSIM ? LPIPS ?</cell></row><row><cell>Only encoder (L1 loss)</cell><cell>21.90</cell><cell>0.8587</cell><cell>0.1672</cell></row><row><cell>LLFlow (L1 loss)</cell><cell>22.68</cell><cell>0.8391</cell><cell>0.2038</cell></row><row><cell>LLFlow (Ours)</cell><cell>25.19</cell><cell>0.9252</cell><cell>0.1131</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The effect of different latent feature distributions. ? (?) denotes that, larger (smaller) values lead to better quality.</figDesc><table><row><cell>Latent Distribution</cell><cell cols="3">PSNR ? SSIM ? LPIPS ?</cell></row><row><cell>LLFlow w/o color map</cell><cell>24.46</cell><cell>0.9235</cell><cell>0.1146</cell></row><row><cell>LLFlow w/ color map, p = 0.5</cell><cell>24.85</cell><cell>0.9232</cell><cell>0.1192</cell></row><row><cell>LLFlow w/ color map, p = 0.2</cell><cell>25.19</cell><cell>0.9252</cell><cell>0.1131</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Noise flow: Noise modeling with conditional normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ardizzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>L?th</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kruse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>K?the</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02392</idno>
		<title level="m">Guided image generation with conditional invertible neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning a deep single image contrast enhancer from multi-exposure images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2049" to="2062" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to see in the dark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3291" to="3300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Nice: Nonlinear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A weighted variational model for simultaneous reflectance and illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2782" to="2790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Zero-reference deep curve estimation for lowlight image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1780" to="1789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LIME: Low-light image enhancement via illumination map estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="982" to="993" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tails of lipschitz triangular flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4673" to="4681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Enlightengan: Deep light enhancement without paired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2340" to="2349" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03039</idno>
		<title level="m">Glow: Generative flow with invertible 1x1 convolutions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Why normalizing flows fail to detect out-of-distribution data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kirichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08545</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Normalizing flows: An introduction and review of current methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kobyzev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brubaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Benchmarking Low-Light Image Enhancement and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1153" to="1184" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional adversarial generative flow for controllable image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7992" to="8001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Invertible Denoising Network: A Light Solution for Real Noise Removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13365" to="13374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">LLNet: A deep autoencoder approach to natural low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Lore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="650" to="662" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Srflow: Learning the super-resolution space with normalizing flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="715" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention guided low-light image enhancement with a large scale low-light simulation dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">C-flow: Conditional generative flow models for images and 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pumarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7949" to="7958" />
		</imprint>
	</monogr>
	<note>BMVC, 220</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Low-light image enhancement via a deep hybrid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4364" to="4375" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint enhancement and denoising method via sequential decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Retinex-inspired Unrolling with Cooperative Prior Architecture Search for Low-light Image Enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Risheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiaao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhongxuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Msr-net: Low-light image enhancement using deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02488</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Visual Communications and Image Processing</title>
		<imprint>
			<publisher>VCIP</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>LLCNN: A convolutional neural network for low-light image enhancement</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Trippe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04908</idno>
		<title level="m">Conditional density estimation with bayesian normalising flows</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Underexposed photo enhancement using deep illumination estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6849" to="6857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV) Workshops</title>
		<meeting>the European Conference on Computer Vision (ECCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Progressive retinex: Mutually reinforced illumination-noise perception network for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2015" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deep retinex decomposition for low-light enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.04560</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoogeboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00042</idno>
		<title level="m">Learning likelihoods with conditional normalizing flows</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deflow: Learning complex image degradations from unpaired data with conditional flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lugmayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="94" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Invertible image rescaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="126" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointflow: 3d point cloud generation with continuous normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4541" to="4550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3063" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">A bio-inspired multiexposure fusion framework for low-light image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00591</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06792</idno>
		<title level="m">Learning enriched features for real image restoration and enhancement</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Beyond Brightening Low-light Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1013" to="1037" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kindling the darkness: A practical low-light image enhancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
