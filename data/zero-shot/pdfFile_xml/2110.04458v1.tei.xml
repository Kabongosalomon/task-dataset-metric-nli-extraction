<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Vision Transformer based COVID-19 Detection using Chest X-rays</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivarama</forename><surname>Koushik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivarama</forename><surname>Karthik</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
						</author>
						<title level="a" type="main">Vision Transformer based COVID-19 Detection using Chest X-rays</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-COVID-19</term>
					<term>Chest X-rays</term>
					<term>Vision Transformer (ViT)</term>
					<term>Transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>COVID-19 is a global pandemic, and detecting them is a momentous task for medical professionals today due to its rapid mutations. Current methods of examining chest X-rays and CT scan requires profound knowledge and are time consuming, which suggests that it shrinks the precious time of medical practitioners when people's lives are at stake. This study tries to assist this process by achieving state-of-the-art performance in classifying chest X-rays by fine-tuning Vision Transformer(ViT). The proposed approach uses pretrained models, fine-tuned for detecting the presence of COVID-19 disease on chest X-rays. This approach achieves an accuracy score of 97.61%, precision score of 95.34%, recall score of 93.84% and, f1-score of 94.58%. This result signifies the performance of transformer-based models on chest X-ray.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The term Global Pandemic applies to an epidemic of an infectious disease that is spread all across the globe. The world has not seen a global pandemic after the Spanish flu up until the year 2019. In late 2019, a new strain of corona virus disease was found in Wuhan district of Hubei Province in China. Since this strain of coronavirus disease was not previously identified in human beings, World Health Organization named this strain as "Novel" coronavirus disease. The Novel Coronavirus Disease of 2019 was officially given an abbreviated name of COVID-19 by the World Health Organization, where the acronym CO stands for Corona, VI stands for Virus, D stands for Disease followed by the year 2019 represented as 19.</p><p>This disease abruptly spread all across the globe affecting millions of people and created global tension across all countries. Covid-19 is caused by the Severe Acute Respiratory Syndrome Coronavirus 2 (SARS-CoV-2) <ref type="bibr" target="#b17">[17]</ref>. This virus reside within the same clan of Severe Acute Respiratory Syndrome (SARS) and Middle East Respiratory Syndrome (MERS).The commonly identified symptoms for this disease include coughing, shortness of breath, fever, pneumonia, and severe respiratory distress. The symptomatology of this disease is so diverse and are usually identified within day one to day fourteen after the person is being exposed to this contagious virus.</p><p>After the initial identification of the presence of covid 19 disease in China, the cases started expanding exponentially, affecting millions of people. This sudden burst of cases, forced all countries to take severe measures to keep the outbreak in control. <ref type="figure">Figure  1</ref> shows the chart of new cases from January 2020 to July 2021 published in World Health Organization website.</p><p>With an exponential increase in covid 19 cases,there is a high demand for rapid testing. Baseline gold standard approach used to test this coronavirus disease 2019 is reverse transcription polymerase chain reaction (RT-PCR). The results for this test takes somewhere between 48 to 72 hours of wait time for people to know if they have <ref type="figure">Fig. 1</ref>: COVID-19 cases worldwide published by WHO <ref type="bibr" target="#b0">[1]</ref> been affected by this contagious virus or not. Other testing methods include Antigen testing for coronavirus disease. The results of this test usually takes around 15 minutes to 30 minutes of wait time for people to know if they have been exposed to the contagious virus or not. Typically, patients experiencing symptoms of this coronavirus disease 2019 are asked to take Chest CT scans or X-ray scans. Then a radiologist expert examines the scans to confirm if there is a presence of this disease in the chest area. This process is time consuming and is hardly inefficient considering the rise in cases every minute. Rapid and rigorous testing is required in-order to identify as much as cases possible and prevent them from spreading this virus to others. This calls for the need of artificial intelligence to help identify covid-19 patients with minimum time consumption and maximum efficiency.</p><p>Artificial Intelligence and Machine learning/Deep learning are currently the state of the art in predicting results in almost all fields. The AI based systems are showing massive results in healthcare domain and this would be helpful in mitigating the time delay for covid-19 virus identification. The vision transformer is the latest state-of-theart deep learning architecture as it achieves similar performance in benchmark datasets compared to CNN-based models and sometimes even outperforms them in some tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Deep Learning has become a great area of interest for researchers in recent days. Many have successfully applied deep learning / machine learning techniques <ref type="bibr" target="#b5">[6]</ref> for medical classification and obtained promising results. The transfer learning-based approaches have shown remarkable performance boosts in almost all Computer Vision and Natural Language Processing tasks. This approach saves a lot of computational time and can be trained with a fraction of the data. arXiv:2110.04458v1 [eess.IV] 9 Oct 2021</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Transfer Learning</head><p>Transfer Learning has become a common practice in almost all Natural Language Processing and Computer Vision tasks, given the enormous computational power required to train them. This approach is a two-step process. At first, you train the model on large volumes of benchmark datasets. Then you fine-tune them by freezing the initial layers and replacing the last layer based on the task at hand, thus using the previously learnt features for representation. This approach requires a fraction of the data required to train a model from scratch and can be trained in a shorter duration, with less computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Transformers</head><p>The Transformer architecture <ref type="bibr" target="#b6">[7]</ref> was initially proposed in 2017 for neural machine translation. This opened up a wide range of new possibilities in the domain of Natural Language Processing. Since then, several variants and advancements were proposed in transformer-based architectures and all of them are generally pretrained on a large corpus of text data. This architecture proved that combining self-attention with linear layers outperformed the traditional sequence-to-sequence LSTM-RNN based approaches in Neural Machine Translation and other Natural Language Processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Vision Transformers</head><p>Since the introduction of transformer architecture <ref type="bibr" target="#b6">[7]</ref>, many researchers have tried to apply them to Computer Vision tasks. Many even added CNN before self-attention to extract features from images, but most of these approaches could not be applied to real-world Computer Vision problems. The successful utilization of transformer architecture for images was proposed by Dosovitskiy et al. <ref type="bibr" target="#b18">[18]</ref>. They managed to get higher accuracy with less computational time for training. This approach also has a much less image-specific bias when compared to CNN-based models. In this approach, they split the images into small patches and stacked them linearly to pass them as input to the transformer model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transfer learning based approaches</head><p>Talha Anwar and others. <ref type="bibr" target="#b12">[12]</ref> used the pretrained EfficientNet family of architectures to detect COVID-19 in CT scans and managed to get an accuracy score of 89.7%, an F1 score of 89.6%, and an AUC score of 89.5%.</p><p>The authors, Rahaman and others. <ref type="bibr" target="#b16">[16]</ref> have developed a CAD system for detecting COVID-19 X-rays from others by fine-tuning the pretrained VGG19 model. They managed to obtain a precision score of 97.5% and a recall score of 82.0%. This study experimented with various pretrained CNN-based models and found VGG19 outperforming other latest models.</p><p>Sethy et al. <ref type="bibr" target="#b21">[21]</ref> suggested a classification model that combined both pretrained ResNet50 and SVM. Here, the author used the Convolutional Neural Network to extract feature from images. This extraced features is then passed into SVM. They showed that this approach is superior to using just a CNN-based network. They achieved an FPR of 95.38%, F1 score of 95.52%, MCC score of 91.41%, and Kappa score of 90.76% for detecting COVID-19 in X-ray images. The authors of <ref type="bibr" target="#b10">[11]</ref> also used convolutional neural networks as a feature extraction technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset</head><p>To test how good our proposed method is, we used chest X-rays of COVID19 patients, healthy patients, patients with opaque lungs, and patients with viral pneumonia. We collected images from two publicly available data sets. COVID19 X-ray database dataset <ref type="bibr" target="#b14">[14]</ref>  <ref type="bibr" target="#b19">[19]</ref>. This was created by a team of researchers from various universities, in cooperation with doctors from Malaysia. They examined various X-rays and classified them into COVID-19, Viral Pneumonia, Lung Opacity and Healthy chest X-rays.</p><p>The COVID19 Pneumonia Normal Chest X-ray PA Dataset from Kaggle was used as the test set.</p><p>COVID-19 image from the COVID19 X-ray database <ref type="bibr" target="#b14">[14]</ref> [19] was originally around 3500. We upsampled it to 7,000 by using various image augmentation techniques. We then took equal samples of images from Viral Pneumonia, Lung Opacity and Healthy chest X-rays for the Non-COVID class. <ref type="table">Dataset  COVID  NON-COVID TOTAL  Train  6880  6980  13,860  Validation  350  369  719  Test  2313  2313  4626  Total  9443  9662</ref> 19,105</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Dataset Split</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Preprocessing</head><p>The COVID-19 images are up-sampled using image augmentation techniques with the help of albumentations <ref type="bibr" target="#b13">[13]</ref> library. The chest X-ray is flipped vertically or horizontally, then randomly rotated to the limit of 270 degrees with constant edges, and the brightness and contrast are arbitrarily adjusted to the limit of 0.4. chest X-rays are of different sizes from 447 ? 530 to 4200 ? 3290 pixels. Therefore, we made the target size of images as 224 x 244 pixels. As the models were pretrained on RGB images, we created fake RGB images by    <ref type="bibr" target="#b13">[13]</ref> is used as an image enhancement method. CLAHE is a revision of Adaptive Histogram Equalization (AHE) that avoids excessive contrast enhancement in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND ANALYSIS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Setup</head><p>In this research, we have used DenseNet <ref type="bibr" target="#b7">[8]</ref>, InceptionV3 <ref type="bibr" target="#b3">[4]</ref>, WideResNet101 <ref type="bibr" target="#b4">[5]</ref> and, Vision Transformer(ViT-B/32) <ref type="bibr" target="#b18">[18]</ref> models in the transfer learning process, where the model is pre-trained on the ImageNet <ref type="bibr" target="#b1">[2]</ref> dataset. The bottom layer of the architecture is frozen, as it only extracts general features. We replaced the top layer of the model so that it returns dataset-specific output from the linear layer to match our dataset.</p><p>In ViT-B/32 <ref type="bibr" target="#b18">[18]</ref> pretrained model, the images are split into 32x32 patches and passed into an embedding layer to get the patch embeddings. As the transformers are permutationally invariant, 1-D position embeddings extracted from the images are added to the patch embeddings. This resultant embedding vector is then fed into the encoder part of ViT <ref type="bibr" target="#b18">[18]</ref>. The encoder of ViT <ref type="bibr" target="#b18">[18]</ref> consists of alternating Layer Normalization, Multiheaded Self-Attention and, MLP <ref type="bibr" target="#b20">[20]</ref> blocks. Skip connections are applied after every block. The logits from the linear layer pass through a sigmoid activation function, which returns the predicted probabilities. The target size of the image was set as 224 x 224 x 3 for all models, with an initial learning rate of 1e-4. We used Adam <ref type="bibr" target="#b2">[3]</ref> optimizer for CNN based models and RectifiedAdam <ref type="bibr" target="#b15">[15]</ref> optimizer for ViT-B/32 <ref type="bibr" target="#b18">[18]</ref> with the "ReduceLROnPlateau" and "EarlyStopping" schedulers. The learning rate and optimizer were found using hyperparameter optimization framework Optuna <ref type="bibr" target="#b9">[10]</ref>.</p><p>We used Optuna <ref type="bibr" target="#b9">[10]</ref> for 50 trials, using various optimizers and learning rates from 1e-3 to 1e-6 and found Adam optimizer works well for all pre-trained CNN models plus 1e-4 as the best initial learning rate. We used the RectifiedAdam <ref type="bibr" target="#b15">[15]</ref> optimizer as it showed considerable performance improvement over vanilla Adam <ref type="bibr" target="#b2">[3]</ref> optimizer for the ViT <ref type="bibr" target="#b18">[18]</ref> pre-trained model. ReduceLROnPlateau scheduler monitors the validation accuracy and reduces the learning rate by a factor of 0.2 if it does not improve for more than three epochs. EarlyStopping scheduler monitors validation accuracy and stops training if the monitored metric does not improve.</p><p>InceptionV3 <ref type="bibr" target="#b3">[4]</ref>, DenseNet <ref type="bibr" target="#b7">[8]</ref> and, WideResNet101 <ref type="bibr" target="#b4">[5]</ref> were trained on Google Colaboratory with a batch size of 32 for 25 epochs. ViT-B/32 <ref type="bibr" target="#b18">[18]</ref> model was trained on NVIDIA RTX 2060 mobile GPU with a batch size of 16 and was early stopped at an epoch of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Criteria</head><p>In this study, we used Accuracy, Precision, Recall, and f1score as scoring metrics. The accuracy gives a basic idea of the model's performance and is simply a ratio of the correctly predicted observation to the overall observations. The ratio of the correctly predicted positive output and the entire predicted positive output is known as precision score. The recall score is the ratio of the correctly predicted positive output and the total output of the actual positive observation. The weighted average of precision score and recall score is referred to as the F1 score, where the maximum value is 1 and the minimum value is 0.</p><formula xml:id="formula_0">Accuracy = T P + T N T P + FP + FN + T N (1) Precision = T P T P + FP (2) Recall = T P T P + FN (3) F1 ? score = 2 * Recall * Precision Recall + Precision<label>(4)</label></formula><p>V. RESULTS</p><p>To derive conclusions, we compared the accuracy, precision, recall and F1-score of each model. <ref type="table">Table   Model</ref> A Among all the tested models, InceptionV3 <ref type="bibr" target="#b3">[4]</ref> and WideResNet101 <ref type="bibr" target="#b4">[5]</ref> attained the least accuracy of 88% and 85% respectively, while ViT-B/32 <ref type="bibr" target="#b18">[18]</ref> and DenseNet <ref type="bibr" target="#b7">[8]</ref> were the best performers with accuracy of 97.6% and 92%. The ViT <ref type="bibr" target="#b18">[18]</ref> model took around 10 minutes to train while other CNN based models took around 35 minutes. This proves that transformer based approaches consume less time when compared to CNN based approaches. Thus ViT <ref type="bibr" target="#b18">[18]</ref> baseline model with 32x32 as input patch size is selected as the best model for classifying COVID-19 X-ray images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II Performance Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>The COVID-19 viral infection is a global pandemic and is rapidly mutating. The lungs of the infected people are inflamed due to this viral infection. Hence, examining chest X-rays is one of the possible approaches in the detection of COVID-19. In this study, we have proposed an automated and accurate technique for distinguishing COVID-19 cases from Viral Pneumonia, Lung Opacity, and Healthy chest X-rays. We experimented with four different transfer learning-based architectures and, their performance is evaluated based on four performance metrics. The results of ViT-B/32 <ref type="bibr" target="#b18">[18]</ref> confirm that transformer-based models are on par with professional radiologists.</p><p>Though this model achieves leading results in classifying COVID-19 chest X-rays, there is still scope for development. Noise is a key factor in radiography that affects the model's performance. Applying Generative Adversarial Network(GAN) based noise reduction <ref type="bibr" target="#b8">[9]</ref> techniques on the dataset can greatly improve the performance of our model. An ensemble learning-based approach can enhance the final performance. Using a large version of ViT <ref type="bibr" target="#b18">[18]</ref> with a larger dataset can surely reach higher performance metrics. We are also planning to extend our work to the segmentation of COVID-19 chest X-rays &amp; CT scans to give even more insights for the radiologists.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>(a) COVID Fig. 3: (b) Lung Opacity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>(c) Viral Pneumonia</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>(d) Normal stacking the channel over itself. Contrast Limited Adaptive Histogram Equalization (CLAHE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Model Architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The above table illustrates the performance of different deep learning models for detection of COVID-19 from chest X-rays. The test accuracy gives us a general idea about the model's real world performance. It can be observed from the above table that the highest avg. precision, avg. recall, avg. F1-score and, test accuracy of 95.3%, 93.8%, 94.6%, 97.6% respectively was achieved by ViT-B/32[18] model, followed by DenseNet[8] with value of 91.0%, 92.2%, 91.5% and, 92% respectively. The WideResNet101[5] model attained the lowest avg. precision, avg. recall, avg. F1-score and, test accuracy of 86.5%, 85.5%, 84.5%, 85.0% respectively.</figDesc><table><row><cell></cell><cell cols="3">.Precision A.Recall A.F1</cell><cell>ACC(%)</cell></row><row><cell>InceptionV3</cell><cell>0.875</cell><cell>0.870</cell><cell>0.872</cell><cell>0.88</cell></row><row><cell>DenseNet</cell><cell>0.910</cell><cell>0.922</cell><cell>0.915</cell><cell>0.92</cell></row><row><cell cols="2">WideResNet101 0.865</cell><cell>0.855</cell><cell>0.845</cell><cell>0.85</cell></row><row><cell>ViT-B/32</cell><cell>0.953</cell><cell>0.938</cell><cell>0.946</cell><cell>0.976</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Explorer. Geneva: World Health Organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Who Covid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2009.5206848</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno>arxiv:1412.6980Comment</idno>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">the 3rd International Conference for Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Published as a conference paper at</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00567</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<ptr target="http://arxiv.org/abs/1605.07146" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recognition of human arm gestures using Myo armband for the game of hand cricket</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karthik Sivarama Krishnan</surname></persName>
		</author>
		<idno type="DOI">10.1109/IRIS.2017.8250154</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Symposium on Robotics and Intelligent Sensors (IRIS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="389" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Digital radiography image denoising using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuewen</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.3233/XST-17356</idno>
		<ptr target="https://doi.org/10.3233/XST-17356" />
	</analytic>
	<monogr>
		<title level="j">Journal of X-Ray Science and Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="523" to="534" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Optuna: A Next-generation Hyperparameter Optimization Framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10902</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">ORB-DeepOdometry -A Feature-Based Deep Learning Approach to Monocular Visual Odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferat</forename><surname>Karthik Sivarama Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sahin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/SYSOSE.2019.8753848</idno>
		<idno>DOI: 10 . 1109 / SYSOSE . 2019.8753848</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="296" to="301" />
		</imprint>
		<respStmt>
			<orgName>Annual Conference System of Systems Engineering (SoSE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep learning based diagnosis of COVID-19 using chest CT-scan images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Talha</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seemab</forename><surname>Zakir</surname></persName>
		</author>
		<idno type="DOI">10.1109/INMIC50486.2020.9318212</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 23rd International Multitopic Conference (INMIC). 2020</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Albumentations: Fast and Flexible Image Augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Buslaev</surname></persName>
		</author>
		<idno type="DOI">10.3390/info11020125</idno>
		<ptr target="https://www.mdpi.com/2078-2489/11/2/125" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Can AI Help in Screening Viral and COVID-19 Pneumonia?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chowdhury</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3010287</idno>
		<idno>DOI: 10 . 1109 / ACCESS.2020.3010287</idno>
	</analytic>
	<monogr>
		<title level="j">In: IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="132665" to="132676" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the Variance of the Adaptive Learning Rate and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03265</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identification of COVID-19 samples from chest X-Ray images using deep learning: A comparison of transfer learning approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Md Mamunur Rahaman</surname></persName>
		</author>
		<idno type="DOI">10.3233/XST-200715</idno>
		<ptr target="https://doi.org/10.3233/XST-200715" />
	</analytic>
	<monogr>
		<title level="j">Journal of X-Ray Science and Technology</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="821" to="839" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure, function, and antigenicity of the SARS-CoV-2 spike glycoprotein</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra C</forename><surname>Walls</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<biblScope unit="volume">181</biblScope>
			<biblScope unit="page" from="281" to="292" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. 2021</title>
		<imprint/>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Exploring the effect of image enhancement techniques on COVID-19 detection using chest X-ray images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tawsifur</forename><surname>Rahman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.compbiomed.2021.104319</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S001048252100113X" />
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="10" to="4825" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">MLP-Mixer: An all-MLP Architecture for Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Detection of coronavirus disease (covid-19) based on deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prabira</forename><surname>Kumar Sethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santi Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Behera</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In: (</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
