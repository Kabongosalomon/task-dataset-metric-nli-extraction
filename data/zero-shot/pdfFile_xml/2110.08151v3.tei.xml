<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryokan</forename><surname>Ri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya@ousia</forename><surname>Jp</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshimasa</forename><surname>Tsuruoka</surname></persName>
							<email>tsuruoka@logos.t.u-tokyo.ac.jp</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Tokyo</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities. However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks. In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks. We train a multilingual language model with 24 languages with entity representations and show the model consistently outperforms word-based pretrained models in various crosslingual transfer tasks. We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features. We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset. We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations. Our source code and pretrained models are available at https: //github.com/studio-ousia/luke.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pretrained language models have become crucial for achieving state-of-the-art performance in modern natural language processing. In particular, multilingual language models <ref type="bibr" target="#b5">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b4">Conneau et al., 2020a;</ref><ref type="bibr">Doddapaneni et al., 2021)</ref> have attracted considerable attention particularly due to their utility in cross-lingual transfer.</p><p>In zero-shot cross-lingual transfer, a pretrained encoder is fine-tuned in a single resource-rich language (typically English), and then evaluated on other languages never seen during fine-tuning. A key to solving cross-lingual transfer tasks is to obtain representations that generalize well across languages. Several studies aim to improve multilingual models with cross-lingual supervision such as * Work done as an intern at Studio Ousia. bilingual word dictionaries <ref type="bibr" target="#b6">(Conneau et al., 2020b)</ref> or parallel sentences <ref type="bibr" target="#b5">(Conneau and Lample, 2019)</ref>.</p><p>Another source of such information is the crosslingual mappings of Wikipedia entities (articles). Wikipedia entities are aligned across languages via inter-language links and the text contains numerous entity annotations (hyperlinks). With these data, models can learn cross-lingual correspondence such as the words Tokyo (English) and ? ? (Japanese) refers to the same entity. Wikipedia entity annotations have been shown to provide rich cross-lingual alignment information to improve multilingual language models <ref type="bibr" target="#b2">(Calixto et al., 2021;</ref><ref type="bibr" target="#b12">Jiang et al., 2022)</ref>. However, previous studies only incorporate entity information through an auxiliary loss function during pretraining, and the models do not explicitly have entity representations used for downstream tasks.</p><p>In this study, we investigate the effectiveness of entity representations in multilingual language models. Entity representations are known to enhance language models in mono-lingual settings <ref type="bibr" target="#b34">(Zhang et al., 2019;</ref><ref type="bibr" target="#b22">Peters et al., 2019;</ref><ref type="bibr" target="#b30">Wang et al., 2021;</ref><ref type="bibr" target="#b31">Xiong et al., 2020;</ref><ref type="bibr" target="#b32">Yamada et al., 2020)</ref> presumably by introducing real-world knowledge. We show that using entity representations facilitates cross-lingual transfer by providing languageindependent features. To this end, we present a multilingual extension of LUKE <ref type="bibr" target="#b32">(Yamada et al., 2020)</ref>. The model is trained with the multilingual masked language modeling (MLM) task as well as the masked entity prediction (MEP) task with Wikipedia entity embeddings.</p><p>We investigate two ways of using the entity representations in cross-lingual transfer tasks: (1) perform entity linking for the input text, and append the detected entity tokens to the input sequence. The entity tokens are expected to provide languageindependent features to the model. We evaluate this approach with cross-lingual question answering (QA) datasets: XQuAD <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref> and MLQA ; (2) use the entity [MASK] token from the MEP task as a languageindependent feature extractor. In the MEP task, word tokens in a mention span are associated with an entity [MASK] token, the contextualized representation of which is used to train the model to predict its original identity. Here, we apply similar input formulations to tasks involving mention-span classification, relation extraction (RE) and named entity recognition (NER): the attribute of a mention or a pair of mentions is predicted using their contextualized entity [MASK] feature. We evaluate this approach with the RELX <ref type="bibr">(K?ksal and ?zg?r, 2020)</ref> and CoNLL NER <ref type="bibr" target="#b27">(Tjong Kim Sang, 2002;</ref><ref type="bibr" target="#b28">Tjong Kim Sang and De Meulder, 2003)</ref> datasets.</p><p>The experimental results show that these entitybased approaches consistently outperform wordbased baselines. Our analysis reveals that entity representations provide more language-agnostic features to solve the downstream tasks.</p><p>We also explore solving a multilingual zero-shot cloze prompt task  with the entity [MASK] token. Recent studies have shown that we can address various downstream tasks by querying a language model for blanks in prompts <ref type="bibr" target="#b23">(Petroni et al., 2019;</ref><ref type="bibr" target="#b7">Cui et al., 2021)</ref>. Typically, the answer tokens are predicted from the model's word-piece vocabulary but here we incorporate the prediction from the entity vocabulary queried by the entity [MASK] token. We evaluate our approach with the mLAMA dataset <ref type="bibr" target="#b14">(Kassner et al., 2021)</ref> in various languages and show that using the entity [MASK] token reduces language bias and elicits correct factual knowledge more likely than using only the word [MASK] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multilingual Language Models with</head><p>Entity Representations</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Model: mulitlingual LUKE</head><p>To evaluate the effectiveness of entity representations for cross-lingual downstream tasks, we introduce a new multilingual language model based on a bidirectional transformer encoder: Multilingual LUKE (mLUKE), a multilingual extension of LUKE <ref type="bibr" target="#b32">(Yamada et al., 2020)</ref>. The model is trained with the masked language modeling (MLM) task <ref type="bibr" target="#b29">(Vaswani et al., 2017)</ref> as well as the masked entity prediction (MEP) task. In MEP, some of the input entity tokens are randomly masked with the special entity <ref type="bibr">[MASK]</ref> token, and the model is trained to predict the original entities. Note that the entity [MASK] token is different from the word [MASK] token for MLM. The model takes as input a tokenized text (w 1 , w 2 , ..., w m ) and the entities appearing in the text (e 1 , e 2 , ..., e n ), and compute the contextualized representation for each token (h w 1 , h w 2 , ..., h wm and h e 1 , h e 2 , ..., h en ). The word and entity tokens equally undergo self-attention computation (i.e., no entity-aware self-attention in <ref type="bibr" target="#b32">Yamada et al. (2020)</ref>) after embedding layers.</p><p>The word and entity embeddings are computed as the summation of the following three embeddings: token embeddings, type embeddings, and position embeddings <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref>. The entity tokens are associated with the word tokens through position embeddings: the position of an entity token is defined as the positions of its corresponding word tokens, and the entity position embeddings are summed over the positions. Model Configuration. The model configurations of mLUKE follow the base and large configurations of XLM-RoBERTa <ref type="bibr" target="#b4">(Conneau et al., 2020a)</ref>, a variant of BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> trained with CommonCrawl data from 100 languages. Before pretraining, the parameters in common (e.g., the weights of the transformer encoder and the word embeddings) are initialized using the checkpoint from the Transformers library. <ref type="bibr">1</ref> The size of the entity embeddings is set to 256 and they are projected to the size of the word embeddings before being fed into the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Corpus: Wikipedia</head><p>We use Wikipedia dumps in 24 languages (Appendix A) as the training data. These languages are selected to cover reasonable numbers of languages that appear in downstream cross-lingual datasets. We generate input sequences by splitting the content of each page into sequences of sentences comprising ? 512 words with their entity annotations (i.e., hyperlinks). During training, data are sampled from each language with n i items with the following multinomial distribution:</p><formula xml:id="formula_0">p i = n ? i N k=1 n ? k ,<label>(1)</label></formula><p>where ? is a smoothing parameter and set to 0.7 following multilingual BERT. 2 Entity Vocabulary. Entities used in mLUKE are defined as Wikipedia articles. The articles from different languages are aligned through inter-language links 3 and the aligned articles are treated as a single entity. We include in the vocabulary the most frequent 1.2M entities in terms of the number of hyperlinks that appear across at least three languages to facilitate cross-lingual learning.</p><p>Optimization. We optimize the models with a batch size of 2048 for 1M steps in total using AdamW <ref type="bibr" target="#b21">(Loshchilov and Hutter, 2019)</ref> with warmup and linear decay of the learning rate. To stabilize training, we perform pretraining in two stages: (1) in the first 500K steps, we update only those parameters that are randomly initialized (e.g., entity embeddings); (2) we update all parameters in the remaining 500K steps. The learning rate scheduler is reset at each training stage. For further details on hyperparameters, see Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Baseline Models</head><p>We compare the primary model that we investigate, multilingual LUKE used with entity representations (mLUKE-E), against several baselines pretrained models and an ablation model based on word representations: mBERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> is one of the earliest multilingual language models. We provide these results as a reference. XLM-R <ref type="bibr" target="#b4">(Conneau et al., 2020a)</ref> is the model that mLUKE is built on. This result indicates how our additional pretraining step and entity representa-tion impact the performance. Since earlier studies <ref type="bibr" target="#b16">Lan et al., 2020)</ref> indicated longer pretraining would simply improve performance, we train another model based on XLM-R base with extra MLM pretraining following the same configuration of mLUKE. mLUKE-W is an ablation model of mLUKE-E. This model discards the entity embeddings learned during pretraining and only takes word tokens as input as with the other baseline models. The results from this model indicate the effect of MEP only as an auxiliary task in pretraining, and the comparison with this model will highlight the effect of using entity representations for downstream tasks in mLUKE-E. The above models are fine-tuned with the same hyperparameter search space and computational budget as described in Appendix B.</p><p>We also present the results of XLM-K <ref type="bibr" target="#b12">(Jiang et al., 2022)</ref> for ease of reference. XLM-K is based on XLM-R base and trained with entity information from Wikipedia but does not use entity representations in downstream tasks. Notice that their results are not strictly comparable to ours, because the pretraining and fine-tuning settings are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Adding Entities as Language-Agnostic Features in QA</head><p>We evaluate the approach of adding entity embeddings to the input of mLUKE-E with cross-lingual extractive QA tasks. The task is, given a question and a context passage, to extract the answer span from the context. The entity embeddings provide language-agnostic features and thus should facilitate cross-lingual transfer learning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Main Experiments</head><p>Datasets. We fine-tune the pretrained models with the SQuAD 1.1 dataset <ref type="bibr" target="#b24">(Rajpurkar et al., 2016)</ref>, and evaluate them with the two multilingual datasets: XQuAD <ref type="bibr" target="#b1">(Artetxe et al., 2020)</ref> and MLQA . XQuAD is created by translating a subset of the SQuAD development set while the source of MLQA is natural text in Wikipedia. Besides multiple monolingual evaluation data splits, MLQA also offers data to evaluate generalized cross-lingual transfer (G-XLT), where the question and context texts are in different languages. Models. All QA models used in this experiment follow <ref type="bibr" target="#b8">Devlin et al. (2019)</ref>. The model takes the question and context word tokens as input and predicts a score for each span of the context word tokens. The span with the highest score is predicted as the answer to the question. mLUKE-E takes entity tokens as additional features in the input <ref type="figure" target="#fig_0">(Figure 1</ref>) to enrich word representations. The entities are automatically detected using a heuristic string matching based on the original Wikipedia article from which the dataset instance is created. See Appendix C for more details. Results. <ref type="table" target="#tab_1">Table 1</ref> summarizes the model's F1 scores for each language. First, we discuss the base models. On the effectiveness of entity representations, mLUKE-E base performs better than its word-based counterpart mLUKE-W base (0.6 average points improvement in the XQuAD average score, 0.1 points in MLQA) and XLM-K (0.2 points improvement in MLQA), which indicates the input entity tokens provide useful features to facilitate cross-lingual transfer. The usefulness of entities is demonstrated especially in the MLQA's G-XLT setting (full results available in Appendix F); mLUKE-E base exhibits a substantial 1.6 point improvement in the G-XLT average score over mLUKE-W base . This suggests that entity representations are beneficial in a challenging situation where the model needs to capture language-agnostic semantics from text segments in different languages.</p><p>We also observe that XLM-R base benefits from extra training (0.4 points improvement in the average score on XQuAD and 2.1 points in MLQA). The mLUKE-W base model further improves the average score from XLM-R base with extra training (1.2 points improvement in XQuAD and 2.1 points in MLQA), showing the effectiveness of the MEP task for cross-lingual QA.</p><p>By comparing large models, we still observe substantial improvements from XLM-R large to the mLUKE models. Also we can see that mLUKE-E large overall provides better results than mLUKE-W large (0.4 and 0.3 points improvements in the MLQA average and G-XLT scores; comparable scores in XQuAD), confirming the effectiveness of entity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis</head><p>How do the entity representations help the model in cross-lingual transfer? In the mLUKE-E model, the input entity tokens annotate mention spans on which the model performs prediction. We hypothesize that this allows the encoder to inject languageagnostic entity knowledge into span representations, which help better align representations across languages. To support this hypothesis, we compare the degree of alignment between span representations before and after adding entity embeddings in the input, i.e., mLUKE-W and mLUKE-E. Task. We quantify the degree of alignment as performance on the contextualized word retrieval (CWR) task <ref type="bibr" target="#b3">(Cao et al., 2020)</ref>. The task is, given a word within a sentence in the query language, to find the word with the same meaning in the context from a candidate pool in the target language. Dataset. We use the MLQA dev set . As MLQA is constructed from parallel sentences mined from Wikipedia, some sentences and answer spans are aligned and thus the dataset can be easily adapted for the CWR task. As the query and target word, we use the answer span 4 annotated in the dataset, which is also parallel across the languages. We use the English dataset as the query language and other languages as the target. We discard query instances that do not have their parallel data in the target language. The candidate pool is all answer spans in the target language data. Models. We evaluate the mLUKE-W base and mLUKE-E base models without fine-tuning. The retrieval is performed by ranking the cosine similarity of contextualized span representations, which is computed by mean-pooling the output word vectors in the span. Results. <ref type="table" target="#tab_3">Table 2</ref> shows the retrieval performance in terms of the mean reciprocal rank score. We observe that the scores of mLUKE-E base are higher than mLUKE-W base across all the languages. This demonstrates that adding entities improves the degree of alignment of span representations, which may explain the improvement of mLUKE-E in the cross-lingual QA task.  In this section, we evaluate the approach of using the entity [MASK] token to extract features from mLUKE-E for two entity-related tasks: relation extraction and named entity recognition. We formulate both tasks as the classification of mention spans. The baseline models extract the feature of spans as the contextualized representations of word tokens, while mLUKE-E extracts the feature as the contextualized representations of the special language-independent entity tokens associated with the mentions <ref type="figure" target="#fig_0">(Figure 1</ref>). We demonstrate that this approach consistently improves the performance in cross-lingual transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Relation Extraction</head><p>Relation Extraction (RE) is a task to determine the correct relation between the two (head and tail) entities in a sentence. Adding entity type features have been shown to be effective to cross-lingual transfer in RE <ref type="bibr" target="#b26">(Subburathinam et al., 2019;</ref><ref type="bibr" target="#b0">Ahmad et al., 2021)</ref>, but here we investigate an approach that does not require predefined entity types but utilize special entity embeddings learned in pretraining. Datasets. We fine-tune the models with the English KBP-37 dataset <ref type="bibr" target="#b33">(Zhang and Wang, 2015)</ref> and evaluate the models with the RELX dataset <ref type="bibr">(K?ksal and ?zg?r, 2020)</ref>, which is created by translating a subset of 502 sentences from KBP-37's test set into four different languages. Following <ref type="bibr">K?ksal and ?zg?r (2020)</ref>, we report the macro average of F1 scores of the 18 relations. Models. In the input text, the head and tail entities are surrounded with special markers (&lt;ent&gt;, &lt;ent2&gt;). The baseline models extract the feature vectors for the entities as the contextualized vector of the first marker followed by their mentions. The two entity features are concatenated and fed into a linear classifier to predict their relation.</p><p>For mLUKE-E, we introduce two special entities, [HEAD] and [TAIL], to represent the head and tail entities <ref type="bibr" target="#b32">(Yamada et al., 2020)</ref>. Their embeddings are initialized with the entity [MASK] embedding. They are added to the input sequence being associated with the entity mentions in the input, and their contextualized representations are extracted as the feature vectors. As with the wordbased models, the features are concatenated and input to a linear classifier.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Named Entity Recognition</head><p>Named Entity Recognition (NER) is the task to detect entities in a sentence and classify their type. We use the CoNLL-2003 English dataset <ref type="bibr" target="#b28">(Tjong Kim Sang and De Meulder, 2003)</ref> as the training data, and evaluate the models with the CoNLL-2003 German dataset and the CoNLL-2002 Spanish and Dutch dataset <ref type="bibr" target="#b27">(Tjong Kim Sang, 2002)</ref>.</p><p>Models. We adopt the model of Sohrab and Miwa <ref type="formula" target="#formula_0">(2018)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. The mLUKE-E models outperform their word-based counterparts mLUKE-W in the average score in all the comparable settings (the base and large settings; the RE and NER tasks), which shows entity-based features are useful in cross-lingual tasks. We also observe that XLM-R base benefits from extra training (1.8 average points improvement in RE and 0.3 points in NER), but mLUKE-E still outperforms the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>The  token extracts better features for predicting entity attributes because it resembles how mLUKE is pretrained with the MEP task. We hypothesize that there exists another factor for the improvement in cross-lingual performance: language neutrality of representations.</p><p>The entity [MASK] token is shared across languages and their contextualized representations may be less affected by the difference of input languages, resulting in features that generalize well for cross-lingual transfer. To find out if the entity-based features are actually more languageindependent than word-based features, we evaluate the modularity <ref type="bibr" target="#b11">(Fujinuma et al., 2019)</ref> of the features extracted for the RELX dataset.</p><p>Modularity is computed for the k-nearest neighbor graph of embeddings and measures the degree to which embeddings tend to form clusters within the same language. We refer readers to <ref type="bibr" target="#b11">Fujinuma et al. (2019)</ref> for how to compute the metric. Note that the maximum value of modularity is 1, and 0 means the embeddings are completely randomly distributed regardless of language.</p><p>We compare the modularity of the word features from mLUKE-W base and entity features from mLUKE-E base before fine-tuning. Note that the features here are concatenated vectors of head and tail features.  demonstrating that entity-based features are more language-neutral. However, with entity-based features, the modularities are still greater than zero. In particular, the modularity computed with Turkish, which is the most distant language from English here, is significantly higher than the others, indicating that the contextualized entity-based features are still somewhat language-dependent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Cloze Prompt Task with Entity Representations</head><p>In this section, we show that using the entity representations is effective in a cloze prompt task  with the mLAMA dataset <ref type="bibr" target="#b14">(Kassner et al., 2021)</ref>. The task is, given a cloze template such as "[X] was born in [Y]" with [X] filled with an entity (e.g., Mozart), to predict a correct entity in [Y] (e.g., Austria). We adopt the typed querying setting <ref type="bibr" target="#b14">(Kassner et al., 2021)</ref>, where a template has a set of candidate answer entities and the prediction becomes the one with the highest score assigned by the language model. Model. As in <ref type="bibr" target="#b14">Kassner et al. (2021)</ref>, the word-based baseline models compute the candidate score as the log-probability from the MLM classifier. When a candidate entity in [Y] is tokenized into multiple tokens, the same number of the word [MASK] tokens are placed in the input sequence, and the score is computed by taking the average of the logprobabilities for its individual tokens. On the other hand, mLUKE-E computes the logprobability of the candidate entity in [Y] with the entity [MASK] token. Each candidate entity is associated with an entity in mLUKE's entity vocabulary via string matching. The input sequence has the entity [MASK] token associated with the word [MASK] tokens in [Y], and the candidate score is computed as the log-probability from the MEP classifier. We also try additionally appending the entity token of [X] to the input sequence if the entity is found in the vocabulary.</p><p>To accurately measure the difference between word-based and entity-based prediction, we restrict the candidate entities to the ones found in the entity vocabulary and exclude the questions if their answers are not included in the candidates (results with full candidates and questions in the dataset are in Appendix G).</p><p>Results. We experiment in total with 16 languages which are available both in the mLAMA dataset and the mLUKE's entity vocabulary. Here we only present the top-1 accuracy results from 9 languages on <ref type="table" target="#tab_9">Table 5</ref>, as we can make similar observations with the other languages.</p><p>We observe that XLM-R base performs notably worse than mBERT as mentioned in <ref type="bibr" target="#b14">Kassner et al. (2021)</ref>. However, with extra training with the Wikipedia corpus, XLM-R base shows a significant 9.3 points improvement in the average score and outperforms <ref type="bibr">mBERT (27.8 vs. 27.2)</ref>. We conjecture that this shows the importance of the training corpus for this task. The original XLM-R is only trained with the CommonCrawl corpus <ref type="bibr" target="#b4">(Conneau et al., 2020a)</ref>, text scraped from a wide variety of web pages, while mBERT and XLM-R + training are trained on Wikipedia. The performance gaps indicate that Wikipedia is particularly useful for the model to learn factual knowledge.</p><p>The mLUKE-W base model lags behind XLM-R base + extra training by 1.7 average points but we can see 5.4 points improvement from XLM-R base + extra training to mLUKE-E base ([Y]), indicating entity representations are more suitable to elicit correct factual knowledge from mLUKE than word representations. Adding the entity corresponding to [X] to the input (mLUKE-E base ([X] &amp; [Y])) further pushes the performance by 11.7 points to 44.9 %, which further demonstrates the effectiveness of entity representations. Analysis of Language Bias. <ref type="bibr" target="#b14">Kassner et al. (2021)</ref> notes that the prediction of mBERT is biased by the input language. For example, when queried in Italian (e.g., "[X] e stato creato in <ref type="bibr">[MASK]</ref>."), the model tends to predict entities that often appear in Italian text (e.g., Italy) for any question to answer en ja fr  location. We expect that using entity representations would reduce language bias because entities are shared among languages and less affected by the frequency in the language of questions. We qualitatively assess the degree of language bias in the models looking at their incorrect predictions. We show the top incorrect prediction for the template "[X] was founded in <ref type="bibr">[Y]</ref>." for each model in <ref type="table" target="#tab_11">Table 6</ref>, together with the top-1 incorrect ratio, that is, the ratio of the number of the most common incorrect prediction to the total false predictions, which indicates how much the false predictions are dominated by few frequent entities.</p><p>The examples show that the different models exhibit bias towards different entities as in English and French, although in Japanese the model consistently tends to predict Japan. Looking at the degree of language bias, mLUKE-E base ([X] &amp; [Y]) exhibits lower top-1 incorrect ratios overall (27% in fr, 44% in ja, and 30% in fr), which indicates using entity representations reduces language bias. However, lower language bias does not necessarily mean better performance: in French (fr), mLUKE-</p><formula xml:id="formula_1">E base ([X] &amp; [Y])</formula><p>gives a lower top-1 incorrect ratio than mBERT (30% vs. 71%) but their numbers of total false predictions are the same (895). Language bias is only one of several factors in the performance bottleneck.</p><p>6 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Multilingual Pretrained Language Models</head><p>Multilingual pretrained language models have recently seen a surge of interest due to their effectiveness in cross-lingual transfer learning <ref type="bibr" target="#b5">(Conneau and Lample, 2019;</ref><ref type="bibr" target="#b20">Liu et al., 2020)</ref>. A straightforward way to train such models is multilingual masked language modeling (mMLM) <ref type="bibr" target="#b8">(Devlin et al., 2019;</ref><ref type="bibr" target="#b4">Conneau et al., 2020a)</ref>, i.e., training a single model with a collection of monolingual corpora in multiple languages. Although models trained with mMLM exhibit a strong cross-lingual ability without any cross-lingual supervision <ref type="bibr" target="#b13">(K et al., 2020;</ref><ref type="bibr" target="#b6">Conneau et al., 2020b)</ref>, several studies aim to develop better multilingual models with explicit cross-lingual supervision such as bilingual word dictionaries <ref type="bibr" target="#b6">(Conneau et al., 2020b)</ref> or parallel sentences <ref type="bibr" target="#b5">(Conneau and Lample, 2019)</ref>. In this study, we build a multilingual pretrained language model on the basis of XLM-RoBERTa <ref type="bibr" target="#b4">(Conneau et al., 2020a)</ref>, trained with mMLM as well as the masked entity prediction (MEP) <ref type="bibr" target="#b32">(Yamada et al., 2020)</ref> with entity representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Pretrained Language Models with Entity Knowledge</head><p>Language models trained with a large corpus contain knowledge about real-world entities, which is useful for entity-related downstream tasks such as relation classification, named entity recognition, and question answering. Previous studies have shown that we can improve language models for such tasks by incorporating entity information into the model <ref type="bibr" target="#b34">(Zhang et al., 2019;</ref><ref type="bibr" target="#b22">Peters et al., 2019;</ref><ref type="bibr" target="#b30">Wang et al., 2021;</ref><ref type="bibr" target="#b31">Xiong et al., 2020;</ref><ref type="bibr" target="#b10">F?vry et al., 2020;</ref><ref type="bibr" target="#b32">Yamada et al., 2020)</ref>. When incorporated into multilingual language models, entity information can bring another benefit: entities may serve as anchors for the model to align representations across languages. Multilingual knowledge bases such as Wikipedia often offer mappings between different surface forms across languages for the same entity. <ref type="bibr" target="#b2">Calixto et al. (2021)</ref> fine-tuned the top two layers of multilingual BERT by predicting language-agnostic entity ID from hyperlinks in Wikipedia articles. As our concurrent work, <ref type="bibr" target="#b12">Jiang et al. (2022)</ref> trained a model based on XLM-RoBERTa with an entity prediction task along with an object entailment prediction task. While the previous studies focus on improving cross-lingual language representations by pretraining with entity information, our work investigates a multilingual model not only pretrained with entities but also explicitly having entity representations and how to extract better features from such model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We investigated the effectiveness of entity representations in multilingual language models. Our pretrained model, mLUKE, not only exhibits strong empirical results with the word inputs (mLUKE-W) but also shows even better performance with the entity representations (mLUKE-E) in cross-lingual transfer tasks. We also show that a cloze-promptstyle fact completion task can effectively be solved with the query and answer space in the entity vocabulary. Our results suggest a promising direction to pursue further on how to leverage entity representations in multilingual tasks. Also, in the current model, entities are represented as individual vectors, which may incur a large memory footprint in practice. One can investigate an efficient way of having entity representations.  Optimization. We optimize the mLUKE models for 1M steps in total using AdamW <ref type="bibr" target="#b21">(Loshchilov and Hutter, 2019)</ref> with learning rate warmup and linear decay of the learning rate. The pretraining consists of two stages: (1) in the first 500K steps, we update only those parameters that are randomly initialized (e.g., entity embeddings); (2) we update all parameters in the remaining 500K steps. The learning rate scheduler is reset at each training stage. The detailed hyper-parameters are shown in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Details of Downstream Experiments</head><p>Hyperparameter Search. For each downstream task, we perform hyperparameter searching for all the models with the same computational budget to ensure a fair comparison. For each task, we use the final evaluation metric on the validation split of the training English corpus as the validation score. The models are optimized with the AdamW optimizer <ref type="bibr" target="#b21">(Loshchilov and Hutter, 2019)</ref> with the weight decay term set to 0.01 and a linear warmup scheduler. The learning rate is linearly increased to a specified value in the first 6 % of training steps, and then gradually decreased to zero towards the end.  Computing Infrastructure. We run the fine-tuning on a server with a Intel(R) Core(TM) i7-6950X CPU and 4 NVIDIA GeForce RTX 3090 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Detecting Entities in the QA datasets</head><p>For each question-passage pair in the QA datasets, we first create a mapping from the entity mention strings (e.g., "U.S.") to their referent Wikipedia entities (e.g., United States) using the entity hyperlinks on the source Wikipedia page of the passage. We then perform simple string matching to extract all entity names in the question and the passage and treat all matched entity names as entity annotations for their referent entities. We ignore an entity name if the name refers to multiple entities on the page. Further, to reduce noise, we also exclude an entity name if its link probability, the probability that the name appears as a hyperlink in Wikipedia, is lower than 1%. The XQuAD datasets are created by translating English Wikipedia articles into target languages. For each translated article, we create the mention-entity mapping from the source English article by the following procedure: for all the entities found in the source article, we find the corresponding entity in the target language through inter-language links, and then collect its possible mention strings (i.e., hyperlinks to the entity) from a Wikipedia dump of the target language; the entity and the collected mention strings form the mention-entity mapping for the translated article.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The Model Size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Ablation Study of Entity Embeddings</head><p>In Section 3 and 4, we have shown that using entity representations in mLUKE improves the crosslingual transfer performance in QA, RE, and NER. Here we conduct an additional ablation study to investigate whether the learned entity embeddings are crucial to the success of our approach. We train an ablated model of mLUKE-E whose entity embeddings are re-initialized randomly before fine-tuning (ablation). <ref type="table" target="#tab_1">Table 11</ref> and <ref type="table" target="#tab_1">Table 12</ref> show that the ablated model performs significantly worse than the full model (mLUKE-E), indicating that using pretrained entity embeddings is crucial rather than applying our approach during fine-tuning in an ad-hoc manner without entity-aware pretraining.            <ref type="table" target="#tab_9">Table 5</ref> shows the results from the setting where the entity candidates not in the mLUKE's entity vocabulary are excluded. Here we provide in <ref type="table" target="#tab_1">Table 21</ref> the results with the full candidate set provided in the dataset for ease of comparison with other literature. When the candidate entity is not found in the mLUKE's entity vocabulary, the log-probability from the word [MASK] tokens are used instead.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>How to use entity representations in downstream tasks. The input entity embeddings are associated with their mentions (indicated by dotted lines) via positional embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>as the baseline model, which enumerates all possible spans in a sentence and classifies them into the target entity types or non-entity type. In this experiment, we enumerate spans with at most 16 tokens. For the baseline models, the span features are computed as the concatenation of the word representations of the first and last tokens. The span features are fed into a linear classifier to predict their entity type.The input of mLUKE-E contains the entity [MASK] tokens associated with all possible spans. The span features are computed as the contextualized representations of the entity [MASK] tokens. The features are input to a linear classifier as with the word-based models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>large 89.0 83.1 82.4 81.3 81.3 75.3 77.9 81.2 75.1 71.5 77.3 79.6 mLUKE-E large 88.6 83.0 81.7 81.4 80.8 75.8 77.7 81.9 75.4 71.9 77.5 79.6</figDesc><table><row><cell>XQuAD</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>el</cell><cell>ru</cell><cell>tr</cell><cell>ar</cell><cell>vi</cell><cell>th</cell><cell>zh</cell><cell>hi</cell><cell>avg.</cell></row><row><cell>mBERT</cell><cell cols="12">84.5 76.1 73.1 59.0 70.2 53.2 62.1 68.5 40.7 58.3 57.0 63.9</cell></row><row><cell>XLM-R base</cell><cell cols="12">84.0 76.5 76.4 73.9 74.4 67.8 68.1 74.2 66.8 61.5 68.7 72.0</cell></row><row><cell>+ extra training</cell><cell cols="12">86.1 76.9 76.5 73.7 74.7 66.3 68.2 74.5 67.7 64.7 66.6 72.4</cell></row><row><cell>mLUKE-W base</cell><cell cols="12">85.7 78.0 77.4 74.7 75.7 68.3 71.7 75.9 67.1 65.1 69.9 73.6</cell></row><row><cell>mLUKE-E base</cell><cell cols="12">86.3 78.9 78.9 73.9 76.0 68.8 71.4 76.4 67.5 65.9 72.2 74.2</cell></row><row><cell>XLM-R large</cell><cell cols="12">88.5 82.4 82.0 81.4 81.2 75.5 75.9 80.7 72.3 67.6 77.2 78.6</cell></row><row><cell>mLUKE-W MLQA</cell><cell></cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell><cell cols="4">avg. G-XLT avg.</cell></row><row><cell>mBERT</cell><cell></cell><cell cols="8">79.1 65.9 58.6 48.6 44.8 58.5 58.1 59.1</cell><cell></cell><cell>40.9</cell><cell></cell></row><row><cell>XLM-R base</cell><cell></cell><cell cols="8">79.7 67.7 62.2 55.8 59.9 65.3 62.5 64.7</cell><cell></cell><cell>33.4</cell><cell></cell></row><row><cell>+ extra training</cell><cell></cell><cell cols="8">81.3 69.8 65.0 54.8 59.3 65.6 64.2 65.7</cell><cell></cell><cell>50.2</cell><cell></cell></row><row><cell>mLUKE-W base</cell><cell></cell><cell cols="8">81.3 69.7 65.4 60.4 63.2 68.3 66.1 67.8</cell><cell></cell><cell>54.0</cell><cell></cell></row><row><cell>mLUKE-E base</cell><cell></cell><cell cols="8">80.8 70.0 65.5 60.8 63.7 68.4 66.2 67.9</cell><cell></cell><cell>55.6</cell><cell></cell></row><row><cell cols="10">XLM-K (Jiang et al., 2022) 80.8 69.2 63.8 60.0 65.3 70.1 63.8 67.7</cell><cell></cell><cell>-</cell><cell></cell></row><row><cell>XLM-R large</cell><cell></cell><cell cols="8">83.9 74.7 69.9 64.9 69.9 73.3 70.3 72.4</cell><cell></cell><cell>65.3</cell><cell></cell></row><row><cell>mLUKE-W large</cell><cell></cell><cell cols="8">84.0 74.3 70.3 66.2 70.2 74.2 69.7 72.7</cell><cell></cell><cell>67.4</cell><cell></cell></row><row><cell>mLUKE-E large</cell><cell></cell><cell cols="8">84.1 74.5 70.5 66.2 71.4 74.3 70.5 73.1</cell><cell></cell><cell>67.7</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>F1 scores on the XQuAD and MLQA dataset in the cross-lingual transfer settings. The scores without reference are from the best model tuned with the English development data.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>The mean reciprocal rank score of the CWR task with the MLQA dev set.4 The Entity MASK Token as FeatureExtractor in RE and NER</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>57.3 61.6 58.9 56.2 59.8 89.7 70.0 75.2 77.1 78.0 XLM-R base 66.5 60.8 62.9 60.9 57.7 61.7 91.5 74.3 80.7 79.8 81.6 + extra training 67.0 61.3 62.9 64.3 61.9 63.5 91.8 75.7 80.3 79.8 81.9 mLUKE-W base 68.7 64.3 65.8 62.1 65.0 65.2 91.6 75.1 80.2 79.2 81.5 mLUKE-E base 69.3 64.5 65.2 64.7 68.7 66.5 93.6 77.2 81.8 77.7 82.6</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>RE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NER</cell><cell></cell></row><row><cell></cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>tr</cell><cell>avg.</cell><cell>en</cell><cell>de</cell><cell>nl</cell><cell>es</cell><cell>avg.</cell></row><row><cell cols="2">mBERT 65.0 XLM-K (Jiang et al., 2022) -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="5">90.7 73.3 80.0 76.6 80.1</cell></row><row><cell>XLM-R large</cell><cell cols="11">68.0 65.3 65.0 63.3 64.1 65.1 92.5 75.1 82.9 80.5 82.8</cell></row><row><cell>mLUKE-W large</cell><cell cols="11">66.2 65.3 68.1 66.5 64.7 66.2 92.3 76.5 82.6 80.7 83.0</cell></row><row><cell>mLUKE-E large</cell><cell cols="11">68.1 65.8 67.8 66.4 64.4 66.5 94.0 78.3 83.5 81.4 84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>F1 scores on relation extraction (RE) and named entity recognition (NER).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>performance gain of mLUKE-E over mLUKE-W can be partly explained as the entity [MASK]</figDesc><table><row><cell>de</cell><cell>es</cell><cell>fr</cell><cell>tr</cell></row><row><cell cols="4">mLUKE-W base 0.71 0.74 0.74 0.84</cell></row><row><cell cols="4">mLUKE-E base 0.25 0.28 0.24 0.36</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>The modularity of word and entity features computed with the same mLUKE model. The data are from pairs of English and the other languages in the RELX dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>shows that the modularity of mLUKE-E base is much lower than mLUKE-W base ,</figDesc><table><row><cell></cell><cell>ar</cell><cell>en</cell><cell>fi</cell><cell>fr</cell><cell>id</cell><cell>ja</cell><cell>ru</cell><cell>vi</cell><cell>zh</cell><cell>avg.</cell></row><row><cell>mBERT</cell><cell cols="10">17.1 36.8 24.0 24.3 42.9 14.3 19.5 39.4 26.2 27.2</cell></row><row><cell>XLM-R base</cell><cell cols="10">14.2 27.2 16.2 14.9 28.2 11.9 11.7 25.1 17.6 18.5</cell></row><row><cell>+ extra training</cell><cell cols="10">21.2 35.0 23.0 22.2 46.8 19.6 17.5 34.4 30.7 27.8</cell></row><row><cell>mLUKE-W base</cell><cell cols="10">22.3 31.3 18.4 19.6 46.7 18.4 16.7 31.9 29.3 26.1</cell></row><row><cell>mLUKE-E base ([Y])</cell><cell cols="10">27.8 37.5 30.4 28.4 44.2 28.9 25.8 42.1 33.4 33.2</cell></row><row><cell cols="11">mLUKE-E base ([X] &amp; [Y]) 42.4 47.5 44.2 35.9 56.2 40.3 35.5 55.2 46.7 44.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>The top-1 accuracies from 9 languages from the mLAMA dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>The top incorrect predictions in three languages for the template "[X] was founded in[Y]." for each model. The predictions in the original language are translated into English.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Appendix for "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models"A Details of Pretraining Dataset. We download the Wikipedia dumps from December 1st, 2020. We show the 24 languages included in the dataset onTable 7, along with the data size and the number of entities in the vocabulary.</figDesc><table><row><cell cols="6">Language Code Size # entities in vocab Language Code Size # entities in vocab</cell></row><row><cell>ar</cell><cell>851M</cell><cell>427,460</cell><cell>ko</cell><cell>537M</cell><cell>378,399</cell></row><row><cell>bn</cell><cell>117M</cell><cell>62,595</cell><cell>nl</cell><cell>1.1G</cell><cell>483,277</cell></row><row><cell>de</cell><cell>3.5G</cell><cell>540,347</cell><cell>pl</cell><cell>1.3G</cell><cell>489,109</cell></row><row><cell>el</cell><cell>315M</cell><cell>135,277</cell><cell>pt</cell><cell>1.0G</cell><cell>537,028</cell></row><row><cell>en</cell><cell>6.9G</cell><cell>613,718</cell><cell>ru</cell><cell>2.5G</cell><cell>529,171</cell></row><row><cell>es</cell><cell>2.1G</cell><cell>587,525</cell><cell>sv</cell><cell>1.1G</cell><cell>390,313</cell></row><row><cell>fi</cell><cell>480M</cell><cell>300,333</cell><cell>sw</cell><cell>27M</cell><cell>30,129</cell></row><row><cell>fr</cell><cell>3.1G</cell><cell>630,355</cell><cell>te</cell><cell>66M</cell><cell>14,368</cell></row><row><cell>hi</cell><cell>90M</cell><cell>54,038</cell><cell>th</cell><cell>153M</cell><cell>100,231</cell></row><row><cell>id</cell><cell>327M</cell><cell>217,758</cell><cell>tr</cell><cell>326M</cell><cell>297,280</cell></row><row><cell>it</cell><cell>1.9G</cell><cell>590,147</cell><cell>vi</cell><cell>516M</cell><cell>263,424</cell></row><row><cell>ja</cell><cell>2.3G</cell><cell>369,470</cell><cell>zh</cell><cell>955M</cell><cell>332,970</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Total</cell><cell>31.4G</cell><cell>8,374,722</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>Training Data Statistics: the size of training data, and the number of entities found in the 1.2M entity vocabulary.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Hyper-parameters used to pretrain mLUKE.</figDesc><table><row><cell>Computing Infrastructure. We run the pretraining on NVIDIA's PyTorch Docker container 19.02 hosted</cell></row><row><cell>on a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training</cell></row><row><cell>takes approximately 2 months.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>summarizes the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 9 :</head><label>9</label><figDesc>The hyperparameters search spaces and other details of downstream experiments.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc># of layers hidden size # of heads vocabulary size # of parameters</figDesc><table><row><cell>mBERT</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>120K</cell><cell>177M</cell></row><row><cell>XLM-R base</cell><cell>12</cell><cell>768</cell><cell>8</cell><cell>250K</cell><cell>278M</cell></row><row><cell>mLUKE-E base</cell><cell>12</cell><cell>768</cell><cell>8</cell><cell>250K</cell><cell>585M</cell></row><row><cell>XLM-R large</cell><cell>24</cell><cell>1024</cell><cell>16</cell><cell>250K</cell><cell>559M</cell></row><row><cell>mLUKE-E large</cell><cell>24</cell><cell>1024</cell><cell>16</cell><cell>250K</cell><cell>867M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 10 :</head><label>10</label><figDesc>The model sizes of the pretrained models.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 11 :</head><label>11</label><figDesc>F1 scores on the XQuAD and MLQA datasets in the cross-lingual transfer settings.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>RE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NER</cell><cell></cell></row><row><cell></cell><cell>en</cell><cell>de</cell><cell>es</cell><cell>fr</cell><cell>tr</cell><cell>avg.</cell><cell>en</cell><cell>de</cell><cell>du</cell><cell>es</cell><cell>avg.</cell></row><row><cell cols="12">mLUKE-E base 69.3 64.5 65.2 64.7 68.7 66.5 93.6 77.2 81.8 77.7 82.6</cell></row><row><cell>-ablation</cell><cell cols="11">62.5 59.3 60.7 61.0 60.5 50.8 93.0 76.3 80.8 76.1 81.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 12 :</head><label>12</label><figDesc>F1 scores on relation extraction (RE) and named entity recognition (NER).</figDesc><table><row><cell cols="5">F Full Results of MLQA</cell><cell></cell><cell></cell><cell></cell></row><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 79.1 65.4 63.4 37.9 29.7 47.1 43.2</cell></row><row><cell>es</cell><cell cols="7">67.7 65.9 58.2 38.2 24.4 43.6 39.5</cell></row><row><cell cols="8">de 61.7 55.9 58.6 32.3 29.7 38.4 36.8</cell></row><row><cell>ar</cell><cell cols="7">49.9 43.2 44.6 48.6 23.4 29.4 27.1</cell></row><row><cell>hi</cell><cell cols="7">47.0 37.8 39.1 26.2 44.8 28.0 23.0</cell></row><row><cell>vi</cell><cell cols="7">59.9 49.4 48.6 26.7 25.6 58.5 40.7</cell></row><row><cell cols="8">zh 55.3 44.2 45.3 28.3 22.7 38.7 58.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 13 :</head><label>13</label><figDesc>MLQA full results of mBERT</figDesc><table><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 79.6 52.3 59.6 30.8 43.2 40.0 36.0</cell></row><row><cell>es</cell><cell cols="7">67.0 67.7 52.0 25.2 31.8 32.9 31.5</cell></row><row><cell cols="8">de 59.5 41.7 62.1 22.2 27.8 29.2 29.5</cell></row><row><cell>ar</cell><cell cols="7">49.6 23.2 30.9 55.8 10.6 11.6 10.3</cell></row><row><cell>hi</cell><cell cols="7">58.5 34.6 42.3 17.8 59.8 22.4 23.0</cell></row><row><cell>vi</cell><cell cols="7">61.1 28.1 39.5 17.0 27.5 65.2 26.5</cell></row><row><cell cols="8">zh 55.2 22.7 28.1 9.26 21.1 17.5 62.4</cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 14 :</head><label>14</label><figDesc>MLQA full results of XLM-R base</figDesc><table><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 81.3 71.2 70.1 40.6 52.3 54.8 48.2</cell></row><row><cell>es</cell><cell cols="7">70.6 69.8 66.2 43.3 47.9 52.8 49.0</cell></row><row><cell cols="8">de 64.4 60.4 64.9 36.8 42.3 44.3 42.9</cell></row><row><cell>ar</cell><cell cols="7">59.3 52.3 52.2 54.8 30.3 37.1 31.5</cell></row><row><cell>hi</cell><cell cols="7">65.0 56.5 56.8 33.8 59.3 43.0 39.9</cell></row><row><cell>vi</cell><cell cols="7">67.0 57.1 58.2 31.7 43.8 65.5 44.0</cell></row><row><cell cols="8">zh 62.4 53.7 54.2 33.3 40.2 44.8 64.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 15 :</head><label>15</label><figDesc>MLQA full results of XLM-R base + training</figDesc><table><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 81.2 69.5 69.1 53.6 60.8 60.4 58.4</cell></row><row><cell>es</cell><cell cols="7">70.3 69.6 65.5 52.1 52.9 56.1 56.4</cell></row><row><cell cols="8">de 64.7 59.8 65.3 45.4 48.9 49.9 49.3</cell></row><row><cell>ar</cell><cell cols="7">60.4 52.3 54.3 60.3 34.0 43.4 41.3</cell></row><row><cell>hi</cell><cell cols="7">65.5 56.9 58.3 35.4 63.1 49.0 44.6</cell></row><row><cell>vi</cell><cell cols="7">66.8 54.4 57.1 39.7 49.3 68.3 52.4</cell></row><row><cell cols="8">zh 63.2 55.1 56.6 39.8 43.3 49.6 66.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 16 :</head><label>16</label><figDesc>MLQA full results of mLUKE-W base</figDesc><table><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 80.8 71.3 69.9 55.9 61.9 62.8 62.1</cell></row><row><cell>es</cell><cell cols="7">70.6 69.9 66.4 52.6 53.7 57.6 58.0</cell></row><row><cell cols="8">de 65.2 61.2 65.4 47.2 49.3 51.8 51.7</cell></row><row><cell>ar</cell><cell cols="7">61.1 54.6 56.9 60.7 39.5 47.0 44.8</cell></row><row><cell>hi</cell><cell cols="7">65.1 58.4 59.2 38.3 63.7 50.5 46.2</cell></row><row><cell>vi</cell><cell cols="7">66.7 56.5 59.5 44.3 51.1 68.4 54.2</cell></row><row><cell cols="8">zh 62.7 56.3 56.2 41.1 44.3 51.7 66.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 17 :</head><label>17</label><figDesc>MLQA full results of mLUKE-E base</figDesc><table><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 83.9 79.6 79.0 62.0 70.6 70.5 69.5</cell></row><row><cell>es</cell><cell cols="7">75.2 74.7 73.0 60.3 63.4 66.6 65.9</cell></row><row><cell cols="8">de 69.4 69.0 69.9 58.9 59.7 62.0 60.6</cell></row><row><cell>ar</cell><cell cols="7">67.0 63.6 66.2 64.9 54.5 58.9 57.7</cell></row><row><cell>hi</cell><cell cols="7">72.1 67.3 67.2 56.1 69.9 61.0 62.1</cell></row><row><cell>vi</cell><cell cols="7">73.5 69.6 70.7 57.1 63.0 73.3 64.5</cell></row><row><cell cols="8">zh 69.1 64.0 65.7 53.4 58.2 62.7 70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 18 :</head><label>18</label><figDesc>MLQA full results of XLM-R large</figDesc><table><row><cell>.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 19 :</head><label>19</label><figDesc>MLQA full results of mLUKE-W large</figDesc><table><row><cell>c/q</cell><cell>en</cell><cell>es</cell><cell>de</cell><cell>ar</cell><cell>hi</cell><cell>vi</cell><cell>zh</cell></row><row><cell cols="8">en 84.1 80.5 80.2 70.0 75.0 75.0 73.5</cell></row><row><cell>es</cell><cell cols="7">75.2 74.5 74.8 62.4 65.3 67.6 66.5</cell></row><row><cell cols="8">de 71.1 70.2 70.5 62.2 61.0 63.5 62.3</cell></row><row><cell>ar</cell><cell cols="7">68.4 65.6 68.4 66.2 57.7 62.3 58.0</cell></row><row><cell>hi</cell><cell cols="7">72.9 70.9 71.6 59.1 71.4 65.6 62.1</cell></row><row><cell>vi</cell><cell cols="7">74.7 71.0 73.1 61.7 64.7 74.3 66.8</cell></row><row><cell cols="8">zh 70.1 66.1 68.8 59.2 60.9 66.3 70.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head>Table 20 :</head><label>20</label><figDesc>MLQA full results of mLUKE-E large G Full Results of mLAMA</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>base ([X] &amp; [Y]) 47.6 37.7 41.6 37.7 44.8 31.4 50.1 41.6 39.3</figDesc><table><row><cell></cell><cell>ar</cell><cell>bn</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>fi</cell><cell>fr</cell></row><row><cell>mBERT</cell><cell cols="8">15.1 12.7 28.6 19.4 34.8 30.2 19.2 27.1</cell></row><row><cell>XLM-R base</cell><cell cols="8">14.9 7.5 18.4 12.7 24.2 18.5 14.5 16.1</cell></row><row><cell>+ extra training</cell><cell cols="8">20.7 14.0 29.3 18.2 31.6 26.4 19.2 25.0</cell></row><row><cell>mLUKE-W base</cell><cell cols="8">21.3 12.9 25.7 17.5 27.1 23.3 15.9 23.0</cell></row><row><cell>mLUKE-E base ([Y])</cell><cell cols="8">25.6 21.6 32.9 25.2 34.9 28.5 24.7 27.7</cell></row><row><cell cols="9">mLUKE-E base ([X] &amp; [Y]) 37.3 32.3 43.7 34.4 43.2 36.4 35.3 34.2</cell></row><row><cell></cell><cell>id</cell><cell>ja</cell><cell>ko</cell><cell>pl</cell><cell>pt</cell><cell>ru</cell><cell>vi</cell><cell>zh</cell><cell>avg.</cell></row><row><cell>mBERT</cell><cell cols="9">37.4 14.2 17.8 21.9 32.0 17.4 36.5 24.2 24.3</cell></row><row><cell>XLM-R base</cell><cell cols="9">24.6 11.4 10.9 16.6 22.2 12.6 23.0 15.5 16.5</cell></row><row><cell>+ extra training</cell><cell cols="9">38.2 19.1 21.4 20.5 29.6 20.6 33.8 28.1 24.7</cell></row><row><cell>mLUKE-W base</cell><cell cols="9">36.6 18.0 17.9 20.2 29.4 19.6 31.0 26.9 22.9</cell></row><row><cell>mLUKE-E base ([Y])</cell><cell cols="9">35.3 27.2 26.3 25.7 34.7 23.8 39.1 29.5 28.9</cell></row><row><cell>mLUKE-E</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 21 :</head><label>21</label><figDesc>The average of Top-1 accuracies from 16 languages from the mLAMA dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://huggingface.co/transformers/ 2 https://github.com/google-research/ bert/blob/master/multilingual.md</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://en.wikipedia.org/wiki/Help: Interlanguage_links. We build an inter-language database from the wikidatawiki dump from November 30, 2020.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Answer spans are not necessarily a word, but here we generalize the task as span retrieval for our purpose.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gate: Graph attention transformer encoder for crosslingual relation and event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wasi</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Fifth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the Cross-lingual Transferability of Monolingual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.421</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wikipedia Entities as Rendezvous across Languages: Grounding Multilingual Language Models by Predicting Wikipedia Hyperlinks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacer</forename><surname>Calixto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Pasini</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.286</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multilingual Alignment of Contextual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised Cross-lingual Representation Learning at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Guzm?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.747</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Crosslingual Language Model Pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Emerging Cross-lingual Structure in Pretrained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.536</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Template-Based Named Entity Recognition Using BART</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.161</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumanth</forename><surname>Doddapaneni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gowtham</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Kunchukuttan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratyush</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitesh</forename><forename type="middle">M</forename><surname>Khapra</surname></persName>
		</author>
		<idno>abs/2107.00676</idno>
	</analytic>
	<monogr>
		<title level="m">2021. A Primer on Pretrained Multilingual Language Models</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Entities as Experts: Sparse Memory Access with Entity Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baldini</forename><surname>Livio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.400</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Resource-Free Evaluation Metric for Cross-Lingual Word Embeddings Based on Graph Modularity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshinari</forename><surname>Fujinuma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Paul</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1489</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">XLM-K: Improving Cross-Lingual Language Model Pre-Training with Multilingual Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoze</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaobo</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Sixth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-Lingual Ability of Multilingual BERT: An Empirical Study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multilingual LAMA: Investigating Knowledge in Multilingual Pretrained Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Kassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Dufter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2020. The RELX Dataset and Matching the Multilingual Blanks for Cross-Lingual Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdullatif</forename><surname>K?ksal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzucan</forename><surname>?zg?r</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.32</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MLQA: Evaluating Cross-lingual Extractive Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barlas</forename><surname>Oguz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruty</forename><surname>Rinott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.653</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pretrain, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno>abs/2107.13586</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
	</analytic>
	<monogr>
		<title level="j">RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multilingual Denoising Pre-training for Neural Machine Translation. Transactions of the Association for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00343</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="726" to="742" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Contextual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language Models as Knowledge Bases?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rockt?schel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ Questions for Machine Comprehension of Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D16-1264</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep Exhaustive Model for Nested Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golam</forename><surname>Mohammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miwa</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1309</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-lingual structure transfer for relation and event extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Subburathinam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>May</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Sil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><surname>Voss</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 Shared Task: Language-Independent Named Entity Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename><surname>Tjong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fien</forename><surname>De Meulder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL</title>
		<meeting>the Seventh Conference on Natural Language Learning at HLT-NAACL</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention Is All You Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan-Zi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00360</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="176" to="194" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">LUKE: Deep Contextualized Entity Representations with Entityaware Self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Relation Classification via Recurrent Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/1508.01006</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ERNIE: Enhanced Language Representation with Informative Entities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qun</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1139</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
