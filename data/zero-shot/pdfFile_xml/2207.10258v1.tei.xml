<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Region Aware Video Object Segmentation with Deep Motion Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Miao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Mohammed</forename><forename type="middle">Bennamoun</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yongsheng</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Ajmal</forename><forename type="middle">Mian</forename></persName>
						</author>
						<title level="a" type="main">Region Aware Video Object Segmentation with Deep Motion Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Video object segmentation</term>
					<term>multi-object dense tracking</term>
					<term>feature matching</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current semi-supervised video object segmentation (VOS) methods usually leverage the entire features of one frame to predict object masks and update memory. This introduces significant redundant computations. To reduce redundancy, we present a Region Aware Video Object Segmentation (RAVOS) approach that predicts regions of interest (ROIs) for efficient object segmentation and memory storage. RAVOS includes a fast object motion tracker to predict their ROIs in the next frame. For efficient segmentation, object features are extracted according to the ROIs, and an object decoder is designed for object-level segmentation. For efficient memory storage, we propose motion path memory to filter out redundant context by memorizing the features within the motion path of objects between two frames. Besides RAVOS, we also propose a large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new OVOS dataset show that our method achieves stateof-the-art performance with significantly faster inference time, e.g., 86.1 J &amp;F at 42 FPS on DAVIS and 84.4 J &amp;F at 23 FPS on YouTube-VOS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Current semi-supervised video object segmentation (VOS) methods usually leverage the entire features of one frame to predict object masks and update memory. This introduces significant redundant computations. To reduce redundancy, we present a Region Aware Video Object Segmentation (RAVOS) approach that predicts regions of interest (ROIs) for efficient object segmentation and memory storage. RAVOS includes a fast object motion tracker to predict their ROIs in the next frame. For efficient segmentation, object features are extracted according to the ROIs, and an object decoder is designed for object-level segmentation. For efficient memory storage, we propose motion path memory to filter out redundant context by memorizing the features within the motion path of objects between two frames. Besides RAVOS, we also propose a large-scale dataset, dubbed OVOS, to benchmark the performance of VOS models under occlusions. Evaluation on DAVIS and YouTube-VOS benchmarks and our new OVOS dataset show that our method achieves stateof-the-art performance with significantly faster inference time, e.g., 86.1 J &amp;F at 42 FPS on DAVIS and 84.4 J &amp;F at 23 FPS on YouTube-VOS.</p><p>Index Terms-Video object segmentation, multi-object dense tracking, feature matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>V IDEO object segmentation (VOS) is a fundamental research topic in visual understanding, with the aim to segment target objects in video sequences. VOS enables machines to sense the motion pattern, location, and boundaries of the objects of interest in videos <ref type="bibr" target="#b0">[1]</ref>, which can foster a wide range of applications, e.g., augmented reality, video editing, and robotics. This work focuses on semi-supervised VOS, where object segmentations given on the first-frame are leveraged to segment and track objects in subsequent frames. A practical semi-supervised VOS method should be able to segment the objects of interest efficiently and accurately under challenging scenarios, such as occlusions, large deformations, similar appearances, background confusion, and scale variations.</p><p>Recent semi-supervised VOS methods mainly follow one of two paradigms: detection-based <ref type="bibr" target="#b1">[2]</ref>- <ref type="bibr" target="#b3">[4]</ref> and memorybased <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref>. Detection-based methods usually rely on online B. Miao, M. Bennamoun, and A. Mian are with the Department of Computer Science and Software Engineering, The University of Western Australia, Perth, Crawley, WA 6009, Australia (e-mail: bo.miao, mohammed.bennamoun, ajmal.mian@uwa.edu.au).</p><p>Y. Gao is with the School of Engineering, Griffith University, Brisbane, QLD 4111, Australia (e-mail: yongsheng.gao@griffith.edu.au).</p><p>This research was funded by the Australian Research Council Industrial Transformation Research Hub IH180100002. Professor Ajmal Mian is the recipient of an Australian Research Council Future Fellowship Award (project number FT210100268) funded by the Australian Government.</p><p>adaption to make the model object-specific, while memorybased methods adopt memory networks to memorize and propagate spatio-temporal features across frames for object segmentation. Methods in the latter paradigm have recently drawn significant research attention due to their exceptional accuracy. These methods either perform non-local matching <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref> or local-matching <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref> for mask propagation.</p><p>Although current memory-based methods have shown promising performance, memorizing and segmenting the entire features of one frame inevitably introduces redundant computations and slows down the process. Some methods have attempted to accelerate VOS by introducing additional instance segmentation or detection networks <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, template matching modules <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b14">[15]</ref>, or optical flow <ref type="bibr" target="#b15">[16]</ref> to create regions of interest (ROIs) and then performing local segmentation. However, these local segmentation methods are either not accurate enough or still time-consuming given the additional computational overhead. Therefore, developing an effective method that avoids redundant computations and memory storage, while maintaining high segmentation accuracy is significant for improving the overall semi-supervised VOS performance.</p><p>In this paper, we propose a novel Region Aware Video Object Segmentation (RAVOS) approach, which enables multiobject tracking and ROI prediction to achieve fast and accurate semi-supervised VOS with less memory burden. First, a lightweight object motion tracker (OMT) is proposed to estimate the parameters of motion functions using the position information of instances in past frames for object tracking and ROI prediction, as shown in <ref type="figure">Fig. 1</ref>. Since the position features, rather than costly image features, are used for tracking, OMT achieves about 5000 FPS on a single GPU. To enable efficient object segmentation, we extract object features based on the predicted ROIs and adopt a designed object decoder that uses object skip connections for object-level segmentation. Second, we propose motion path memory (MPM) to filter out redundant context by memorizing the features within the motion path of objects between two frames. Hence, redundant segmentation and memory storage are alleviated significantly.</p><p>Occlusion is a challenging scenario for matching-based VOS methods due to the similar appearance and position of objects. Currently, no large-scale datasets are designed to evaluate semi-supervised VOS models under occlusions specifically. To fill this gap, we create a large-scale occluded video object segmentation dataset, coined OVOS, based on the OVIS dataset <ref type="bibr" target="#b16">[17]</ref>. We further evaluate our method and the state-of-the-art STCN <ref type="bibr" target="#b7">[8]</ref> on the OVOS dataset to verify Fig1. Concept of our Coordinate Tracker: it only leverages the object coordinate information in two adjacent previous frames to predict the location of each object in frame t.</p><p>Object Motion Tracker C <ref type="figure">Fig. 1</ref>: Concept of our object motion tracker. Each object is tracked across frames by predicting the parameters of motion functions using the position features of the object in the previous frames.</p><p>their ability in occlusion scenarios. We perform extensive experiments on benchmark datasets, i.e., DAVIS and YouTube-VOS, and our new OVOS dataset to evaluate the performance of our method. RAVOS achieves state-of-the-art overall performance compared to existing methods. For instance, it achieves 86.1 J &amp;F with 42 FPS on DAVIS 2017 validation set, outperforming current methods in both accuracy and inference speed. Our main contributions are summarized as follows:</p><p>? We propose motion path memory (MPM), which memorizes the features within the motion path of objects to mitigate redundant memory storage and to accelerate feature matching and propagation. ? We propose a fast (5000 FPS) object motion tracker to track objects across frames by predicting the parameters of motion functions. This enables object-level segmentation with the help of our designed object decoder, which leverages object skip connections. ? We create an occluded video object segmentation (OVOS) dataset and compare the performance of RAVOS with an existing method on it. To the best of our knowledge, this is the first time a semi-supervised VOS method is evaluated on a large-scale dataset with occlusions. The dataset is available at http://ieee-dataport.org/9608. ? Experiments on DAVIS, YouTube-VOS, and OVOS datasets show that our method achieves state-of-the-art performance while running twice as fast as existing ones.</p><p>II. RELATED WORK Semi-supervised Video Object Segmentation. Semisupervised VOS aims to leverage the ground truth object masks provided (only) for the first frame to segment the entire video sequence at pixel level. Before the rise of deep learning, traditional methods usually adopted graphical models <ref type="bibr" target="#b17">[18]</ref> or optical flow <ref type="bibr" target="#b18">[19]</ref> for video segmentation. Recent studies of semi-supervised VOS mainly focus on deep neural networks because of their unmatched performance.</p><p>Early deep learning-based methods often fine-tune the networks on each video sequence during inference, making them focus on different target objects <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. For example, OSVOS <ref type="bibr" target="#b1">[2]</ref> and its variants <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref> fine-tune their networks on the first frame or confident middle frames. Lucid Tracker <ref type="bibr" target="#b24">[25]</ref> and PReMVOS <ref type="bibr" target="#b25">[26]</ref> use data augmentation to generate plenty of synthetic frames for online fine-tuning. Despite their satisfying results, online fine-tuning severely limits the inference speed of networks and leads to overfitting. To accelerate VOS, DMN-AOA <ref type="bibr" target="#b10">[11]</ref> adopts instance segmentation network to generate plenty of ROIs and then perform local segmentation after non-maximum suppression operation. SAT <ref type="bibr" target="#b13">[14]</ref> incorporates template matching for object localization.</p><p>To achieve higher segmentation accuracy, recent works aim to leverage spatio-temporal feature propagation <ref type="bibr" target="#b26">[27]</ref>- <ref type="bibr" target="#b30">[31]</ref> or pixel-wise feature matching <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b36">[37]</ref> to guide VOS. The former propagates spatio-temporal features implicitly across frames. Among them, RVOS <ref type="bibr" target="#b37">[38]</ref> and DyeNet <ref type="bibr" target="#b38">[39]</ref> adopt recurrent neural networks to propagate spatio-temporal features. AGAME <ref type="bibr" target="#b39">[40]</ref> proposes a fusion module that integrates spatio-temporal features with the appearance features of the current frame. The latter computes spatio-temporal correspondences for mask propagation. PML <ref type="bibr" target="#b31">[32]</ref> adopts pixel-wise metric learning and classifies pixels based on a nearest-neighbor method. STM <ref type="bibr" target="#b4">[5]</ref> and its variants <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref> memorize spatio-temporal features and perform nonlocal spatio-temporal matching for temporal association. To enable unsupervised training, MAST <ref type="bibr" target="#b8">[9]</ref> and MAMP [10] use a self-supervised photometric reconstruction task to learn to construct spatio-temporal correspondences without any mask annotations. To accelerate association, RMNet <ref type="bibr" target="#b15">[16]</ref> leverages optical flow to perform regional matching. Based on the observation that the dot product affinity leads to poor memory usage, STCN <ref type="bibr" target="#b7">[8]</ref> adopts L2 similarity for affinity measurement.</p><p>The above methods achieve good performance on semisupervised VOS. However, they either require to segment and memorize the entire features of one frame, which leads to redundant computations and memory storage, or rely on extra time-consuming networks, e.g., optical flow, to locate ROIs. These problems restrict the deployment of VOS in memory-constrained real-time applications. Hence, an effective ROI localization and segmentation method is needed for fast and accurate VOS. We propose RAVOS, which contains an extremely fast object motion tracker to predict ROIs and leverages object-level segmentation and motion path memory for efficient segmentation and memorization.</p><p>Multi-Object Tracking. Multi-object tracking (MOT) aims to continuously estimate the trajectories of target objects across frames. Object detection, association, and motion estimation are three key components of MOT. Among them, CenterTrack <ref type="bibr" target="#b42">[43]</ref> adopts a detection network to detect object centers and predict motion offsets for tracking. TraDeS <ref type="bibr" target="#b43">[44]</ref> estimates motion offsets to track objects, and combines the tracking results with detection results for MOT. DMMNet <ref type="bibr" target="#b44">[45]</ref> leverages spatio-temporal features to predict tracklets for tracking. TT17 <ref type="bibr" target="#b45">[46]</ref> proposes an iterative clustering method to generate multiple high confidence tracklets for objects. Byte-Track <ref type="bibr" target="#b46">[47]</ref> incorporates low-confident boxes for association to dig out objects. DAN <ref type="bibr" target="#b47">[48]</ref> proposes an affinity refinement module for more comprehensive associations.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame t</head><p>Obj. Mot. Track.</p><p>(OMT)</p><p>Frame t-1  With the help of object trackers, we can locate ROIs for region aware segmentation and memorization. However, directly using existing MOT methods for tracking in VOS will introduce redundant architectures and computations, violating the lightweight and real-time VOS performance requirements. In this work, we propose OMT to meet the lightweight and real-time processing requirements. Instead of using image features for tracking, OMT leverages the object position information in previous frames to predict the parameters of motion functions.</p><formula xml:id="formula_0">[ 1, ?2 , 1, ?2 , 2, ?2 , 2, ?2 ] [ 1, ?1 , 1, ?1 , 2, ?1 , 2, ?1 ] C [ ? 1, , ? 1, ] [ ? 2, , ? 2, ]</formula><p>Memory Networks. Memory networks aim to capture the long-term dependencies by storing temporal features or different categories of features in a memory module. LSTM <ref type="bibr" target="#b48">[49]</ref> and GRU <ref type="bibr" target="#b49">[50]</ref> implicitly represent spatio-temporal features with local memory cells in a highly compressed way limiting the representation ability. Memory networks <ref type="bibr" target="#b50">[51]</ref> were introduced to explicitly store the important features. A classical memory network-based VOS method is STM <ref type="bibr" target="#b4">[5]</ref> which incrementally adds uncompressed features of past frames to the memory bank, and performs non-local matching to propagate spatio-temporal features. However, the background features are highly redundant. In this work, we introduce motion path memory which filters redundant context (background far from objects) while still keeping important context (foreground and nearby background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head><p>We propose RAVOS, an efficient and accurate semisupervised VOS method as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In a nut-shell, RAVOS is developed based on matching-based VOS framework and contains five parts: feature extraction, ROI prediction, memory storage, memory propagation, and object segmentation.</p><p>RAVOS adopts ResNet-50 and ResNet-18 <ref type="bibr" target="#b51">[52]</ref> to encode image features (key and query) and mask features (value), separately. After extracting features via encoders, RAVOS uses the proposed OMT to track objects and predict their ROIs in frame t, i.e.,R t ? [x 1,t ,? 1,t ,x 2,t ,? 2,t ]. Next, R t?1 and the predicted object ROIsR t are forwarded to the motion path ROI generator to generate memory ROIs in frame t-1, and MPM is updated by the features within memory ROIs. Then, object-level features of frame t are extracted according to the predicted object ROIsR t and their corresponding spatiotemporal features are retrieved from the memory bank. Finally, an object decoder with object skip connections is used to segment each object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Feature Extraction</head><p>Following previous works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we adopt ResNet-50 and ResNet-18 (excluding their last stages) <ref type="bibr" target="#b51">[52]</ref> as the image and mask encoders, respectively. Inputs are downsampled by 1/16 via encoders. For the image encoder, one additional 3?3 convolutional layer is used on top of the res4 features to extract key K ? R HW ?64 or query Q ? R HW ?64 for matching, and another 3?3 convolutional layer is leveraged on top of the res4 features to compute appearance features F ? R HW ?512 to assist object segmentation. Image features time <ref type="figure">Fig. 3</ref>: Visualization of the tracking results predicted by OMT. The first column denotes the reference frames for mask propagation, and the segmentation is performed within the object ROIs. at the middle layers are also saved to extract object skip connections for the object decoder. For the mask encoder, two residual blocks as well as one CBAM block <ref type="bibr" target="#b52">[53]</ref> are used on top of the res4 features to extract value V ? R HW ?512 for each object.</p><formula xml:id="formula_1">? 1, ? 1, ? 2, ? 2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Object Motion Tracker</head><p>An effective tracker is imperative for ROI prediction and efficient regional semi-supervised VOS. Existing deep learningbased MOT methods use appearance features for tracking. Although appearance features lead to good MOT performance, directly incorporating such techniques into VOS is difficult to cater for the lightweight and real-time processing requirements.</p><p>To address the problem, we propose a novel object motion tracker (OMT) that leverages object position features in previous frames to predict the parameters of instantaneous motion functions for MOT, as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Specifically, given the normalized position information of the ith object in the previous frames, e.g.,</p><formula xml:id="formula_2">R i t?2 ? [x i 1,t?2 , y i 1,t?2 , x i 2,t?2 , y i 2,t?2 ] and R i t?1 ? [x i 1,t?1 , y i 1,t?1 , x i 2,t?1 , y i 2,t?1 ].</formula><p>Where [x 1 , y 1 ] denotes the top-left and [x 2 , y 2 ] denotes the bottom right corner. OMT uses a deep motion estimator to aggregate the position features of the object in previous frames and to predict the parameters of motion functions for each corner. In this work, we choose the simple yet effective quadratic function as the motion function template. Next, the object ROIR i t in the current frame is predicted by plugging R i t?1 into the estimated motion functions:</p><formula xml:id="formula_3">x 1,t = ? x1 x 1,t?1 2 + ? x1 x 1,t?1 + x1 ? ? y 1,t = ? y1 y 1,t?1 2 + ? y1 y 1,t?1 + y1 ? ? x 2,t = ? x2 x 2,t?1 2 + ? x2 x 2,t?1 + x2 + ? y 2,t = ? y2 y 2,t?1 2 + ? y2 y 2,t?1 + y2 + ?<label>(1)</label></formula><p>where ?, ?, and are the predicted parameters of motion functions, and ? denotes the padding of bounding boxes. Finally, the segmentation operates within the object ROIR i t to reduce computation and update the ROI. Different from previous template matching-based methods <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, OMT does not rely on feature matching that struggles to handle objects with similar appearances. Unlike optical flow-based methods <ref type="bibr" target="#b15">[16]</ref>, OMT leverages the lightweight but crucial position features rather than costly appearance features to estimate ROIs. The lightweight framework of OMT enables it to perform at 5000 FPS on a single GPU which is about  100? faster than the prevalent RAFT optical flow <ref type="bibr" target="#b53">[54]</ref>. Moreover, OMT predicts motion functions to generate a definite ROI for each object rather than generating many proposals using additional detection networks like <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> making the non-maximum suppression (NMS) operation redundant. The visualization of the tracking results shows that our efficient OMT can generate sufficiently accurate ROIs for regional VOS (see <ref type="figure">Fig. 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Motion Path Memory</head><p>Previous methods <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b9">[10]</ref> have shown that only a few positions in the memory are helpful for the association of a query point. Moreover, the proposed RAVOS only segments the ROIs generated by OMT. Therefore, memorizing the entire features of one frame will include redundant background context, which is far from foreground objects. The redundant context impedes the deployment of efficient VOS.</p><p>For redundancy reduction, we present motion path memory (MPM) to memorize the critical context, i.e., the foreground and nearby background. As shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, MPM generates the motion path of each object between two frames and then memorizes features within the united motion paths to filter redundant context. Specifically, before memorizing the features of frame t-1, we first predict the ROIs of object? R t = {?R i t , i ? [1, ..., N ]} in the next frame t using OMT. The motion path of each object between two frames is then extracted according to the position of the object in the two frames. Finally, the union of all motion paths is created as the memory ROIs:</p><formula xml:id="formula_4">ROI = U ({?U (R i t?1 ,R i t ), i ? [1, ..., N ]})<label>(2)</label></formula><p>where U denotes the union operation and N is the number of objects. In that case, redundant background features outside the memory ROI will not be used to update the memory. Therefore, the proposed MPM not only accelerates feature matching and propagation, but also reduces the memory footprint for efficient deployment.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Memory Propagation</head><p>As in common matching-based VOS methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, the memory module propagates mask features across frames for segmentation according to the pixel-wise affinity between all query and memory pixels. In this work, we perform regional matching using L2 similarity to compute the affinity.</p><p>To illustrate, we first define Q ? R HW ?64 , K ? R N ?64 , V ? R N ?512 as the query of the current frame, key of the memory, and value of the memory, respectively. Where H and W are the feature height and width, and N T HW denotes the number of positions in the memory. For each object, after predicting the object ROIR t at frame t, we crop query to obtain object query Q obj ? R S1S2?64 , where S1 and S2 denote the height and width ofR t at feature scale and S1S2 HW . Then, the affinity between Q obj and K is computed as:</p><formula xml:id="formula_5">W i,j = exp( Q i obj , K j ) j exp( Q i obj , K j )<label>(3)</label></formula><p>where i and j are the locations in Q obj and K, respectively. ?, ? denotes the L2 similarity between two vectors. Finally, each query position retrieves spatio-temporal features from the memory based on the computed affinity:</p><formula xml:id="formula_6">V i st = j W i,j V j<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Object-level Segmentation</head><p>To reduce redundant computations without losing important information, we predict object ROIs and decode object features. As shown in <ref type="figure" target="#fig_6">Fig. 6</ref>, residual blocks with object skip connections are leveraged to build the object decoder. Specifically, the object decoder concatenates the object appearance features of the current frame F obj and their corresponding spatiotemporal features V st obtained from the memory as input, and progressively increases the object feature resolution from 1/16 to 1/4. The decoded object features are used to predict object probability maps and are up-sampled to the input resolution. Finally, the object probability maps are projected on the image probability map, and Argmax is used for segmentation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Occluded Video Object Segmentation Dataset</head><p>In this work, we introduce occluded video object segmentation (OVOS) dataset to evaluate the performance of VOS models under occlusions. OVOS is an extension of the training set of OVIS dataset <ref type="bibr" target="#b16">[17]</ref> in video instance segmentation since the segmentation of the first frame is not available for the validation set. To meet the format of DAVIS for convenient evaluation, we only select the objects that appear in the first frame as targets and resize videos to make their shortest size 480 pixels. An example is shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, OVOS comes with accurate annotations and includes severe object occlusions. The presented OVOS dataset contains 607 video sequences with a total of 42149 frames and 2034 objects, which is larger than the current largest YouTube-VOS 2019 validation set (507 videos with a total of 13710 frames). The dataset is available at http://ieee-dataport.org/9608.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. IMPLEMENTATION DETAILS</head><p>Training. Exactly following previous works <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b62">[63]</ref>, we pre-train the models on static image datasets [64]- <ref type="bibr" target="#b67">[68]</ref> and perform the main training on the synthetic dataset <ref type="bibr" target="#b62">[63]</ref> as well as DAVIS <ref type="bibr" target="#b68">[69]</ref> and YouTube-VOS <ref type="bibr" target="#b69">[70]</ref>. In the former stage, three synthetic frames are generated from one static image by applying random augmentation. During main training, all the video frames are resized to 480p, and three neighboring frames are randomly sampled with the maximum sampling interval ranging from 5 to 25.</p><p>In this work, all experiments are conducted in PyTorch [71] using a single 3090 GPU. Adam optimizer <ref type="bibr" target="#b71">[72]</ref> is used to optimize the parameters. We adopt bootstrapped cross-entropy loss L seg to train the segmentation model and mean squared error loss L track to train the OMT:</p><formula xml:id="formula_7">L seg = ? 1 n n i=1 c j=1 y i,j log f j (x i ; ?)<label>(5)</label></formula><formula xml:id="formula_8">L track = 1 n n i=1 (v i ?v i ) 2 , v ? [x 1 , y 1 , x 2 , y 2 ]<label>(6)</label></formula><p>Inference. We segment all videos at 480p during inference. Unless specified otherwise, RAVOS updates the memory every three frames for DAVIS and five frames for YouTube-VOS. We adopt only the top 20 matches for feature propagation as in <ref type="bibr" target="#b7">[8]</ref>. For video segmentation, we segment entire features on the second frame and start to track objects on the third frame using the positional information of objects in the two past frames. The minimum object ROI to feature area ratio is set to 0.2 to avoid object features being too small to decode. The object ROI is expanded to the whole image when OMT senses the disappearance of objects, and RAVOS performs regional segmentation again when the objects appear again in subsequent frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>We evaluate RAVOS on the popular DAVIS and YouTube-VOS benchmark datasets as well as our newly created OVOS dataset. Region Similarity J (average IoU score between the segmentation and ground truth), Contour Accuracy F (average boundary similarity between the segmentation and ground truth), and their mean value J &amp;F are used as the evaluation metrics. All results are evaluated using the official evaluation tools or servers and, unless specified otherwise, FPS is measured without automatic mixed precision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Results</head><p>DAVIS 2016 <ref type="bibr" target="#b73">[74]</ref> is a popular single-object benchmark dataset that contains 20 videos in the validation set. For a fair comparison, we re-time the FPS for the nearest competitors on our machine, i.e., STM <ref type="bibr" target="#b4">[5]</ref>, R50-AOT <ref type="bibr" target="#b6">[7]</ref>, and STCN <ref type="bibr" target="#b7">[8]</ref>. As shown in <ref type="table" target="#tab_2">Table I</ref>, RAVOS achieves 91.7 J &amp;F with 58 FPS on DAVIS 2016 validation set, surpassing the above competitors in both accuracy and inference speed. RAVOS even outperforms STCN with significant faster inference speed (2.6? faster).</p><p>DAVIS 2017 <ref type="bibr" target="#b68">[69]</ref> is a multi-object extension of DAVIS 2016, which contains 30 videos in the validation and testdev split, separately. As shown in <ref type="table" target="#tab_2">Table I</ref>, although the proposed RAVOS aims at reducing redundant segmentation and memorization, it achieves 86.1 J &amp;F with 42 FPS, leading all present methods. Compared with the nearest regional segmentation competitor DMN-AOA, RAVOS achieves better performance (86.1 vs 84.0 J &amp;F) and runs more than 5? faster. Compared with the nearest competitor STCN, RAVOS surpasses it by 0.5% and runs about 2.1? faster (42 vs 20 FPS). We further evaluate our method on DAVIS 2017 testdev split. As shown in <ref type="table" target="#tab_2">Table II</ref>, RAVOS achieves 80.8 J &amp;F and outperforms current state-of-the-art STCN by 0.9%.</p><p>YouTube-VOS <ref type="bibr" target="#b69">[70]</ref> is currently the largest dataset for VOS, containing 3471 videos in the training set and 474/507 videos in the 2018/2019 validation set. YouTube-VOS splits the validation videos into seen categories and unseen categories based TABLE III: Quantitative evaluation on YouTube-VOS 2018 and 2019 validation sets. J &amp;F is the overall performance on "seen" and "unseen" categories. RS: regional segmentation.</p><p>* : re-timed on our machine for fair-comparison.  on whether the objects of a category appear in the training videos or not. The performance on unseen categories is used to evaluate the generalization ability of models. As shown in <ref type="table" target="#tab_2">Table III</ref>  the fastest inference time.</p><p>OVOS is a large-scale occluded VOS dataset that contains 607 video sequences with severe object occlusions for validation. More details of the dataset are included in Section III-F. We directly evaluate RAVOS on OVOS dataset without retraining to verify its performance in the occlusion scenario. It is noteworthy that OVOS dataset is only used for evaluation and, to the best of our knowledge, this is the first time a semi-supervised VOS method is evaluated on this large-scale dataset. We also evaluate the state-ofthe-art STCN on OVOS for comparison. Automatic mixed precision is used for both methods since OVOS contains some long video sequences, which cause large memory burden and out-of-memory problems for STCN. As shown in <ref type="table" target="#tab_2">Table IV</ref>, RAVOS outperforms STCN by 1.0% since our method performs regional segmentation to reduce the risk of false positives caused by same class object occlusions. More importantly, thanks to the efficient regional segmentation and memorization approaches, our method runs about 2.5? faster than STCN. The results also indicate that precisely localizing and reasoning under occlusions is still challenging for existing VOS models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Ablation Studies</head><p>Object-level segmentation with object motion tracker. <ref type="table" target="#tab_6">Table V</ref> shows the ablation study for object-level segmentation. RAVOS outperforms baseline by 0.5% by segmenting the predicted object ROIs because of the reduced risk of false positives on background regions. Moreover, object-level segmentation significantly accelerates the feature matching time (5.9 vs 12.2 ms) as well as feature decoding time (3.9 vs 7.5 ms) due to the less redundant computations.    Motion path memory. As shown in <ref type="table" target="#tab_6">Table V</ref>, by memorizing the important motion path regions instead of the entire features, J &amp;F drops by 0.2 on DAVIS 2017 validation set. This is because MPM filters out most of the background regions far from objects before updating the memory, resulting in the loss of some prior information of backgrounds when performing global segmentation. However, leveraging MPM for object-level segmentation does not drop the performance of the model since object-level segmentation avoids segmenting these redundant background areas. Most importantly, MPM significantly reduces the feature matching time by about 3.8? (3.2 vs 12.2 ms) and memory size by about 1.9? (19.2 vs 36.3 K). This is important when segmenting long video sequences, i.e., when the method is deployed on autonomous systems. In a nutshell, OMT reduces redundant segmentation to accelerate object segmentation, and MPM reduces redundant memorization to accelerate spatio-temporal feature matching and propagation. OMT and MPM are complementary, and the combination of the two modules achieves the best performance.</p><p>Different trackers. We compare the performance of OMT with the traditional Lucas-Kanade optical flow <ref type="bibr" target="#b76">[77]</ref> and the cutting-edge RAFT optical flow <ref type="bibr" target="#b53">[54]</ref>. As shown in <ref type="table" target="#tab_2">Table VI</ref>, OMT slightly outperforms the two methods in regional VOS. Most importantly, OMT only requires 0.2 ms for single object tracking, which is about 100? faster than RAFT. These results indicate that OMT is more suitable for object tracking in efficient VOS.</p><p>Different memory regions. We compare our MPM with foreground bounding boxes only memory, which does not include the motion path. As shown in <ref type="table" target="#tab_2">Table VII</ref>, MPM outperforms foreground bounding boxes only memory by 0.8% since MPM also contains some useful background features for the ROIs in next frame.</p><p>Different tracking functions. We use different motion functions to verify the performance of OMT. As shown in <ref type="table" target="#tab_2">Table VIII</ref>, all motion functions have good performance and the quadratic motion function obtains the best performance among the three. The results indicate the efficacy of the motion estimator in OMT in predicting the parameters of motion functions.</p><p>Inference time analysis. We first compute the distribution of object area ratio on YouTube-VOS 2018 validation set, where the size of an object is determined by the given mask of the first frame. As shown in <ref type="figure" target="#fig_10">Fig. 9 (c)</ref>, nearly 70% of objects are smaller than 10% of the image area in YouTube-VOS 2018 validation set. We then analyze the single object processing time of feature matching and propagation as well as feature decoding to observe the efficiency of our method on different object sizes. As shown in <ref type="figure" target="#fig_10">Fig. 9</ref> (a) and (b), RAVOS significantly accelerates the feature matching and propagation as well as feature decoding time on small objects. That is because the smaller the objects, the smaller is their ROIs, and the faster our method executes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>In this paper, we presented a novel segmentation-bytracking approach for region aware semi-supervised VOS. Our method outperformed existing techniques on multiple benchmark datasets in accuracy with the added advantage of faster inference time. We proposed OMT which meets the requirements of fast processing and minimal redundancy to achieve a very high frame rate of 5000 FPS for object tracking and ROI prediction. On top of OMT, we designed object-level segmentation and MPM to accelerate VOS and reduce memory size by a large margin. Moreover, we evaluated RAVOS on a newly created OVOS dataset for the first time in the community of semi-supervised VOS. We hope our RAVOS can serve as a fundamental baseline for efficient VOS and help in the advancement of research and deployment of efficient video object segmentation, video instance segmentation, and multiple object tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>/</head><label></label><figDesc>Object Feature GeneratorFig2. Framework of our used to estimate the roug frame t. Each object is th</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Frame t- 2</head><label>2</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>RAVOS architecture. Q, K, V , and F denote the query of frame t, key of the memory, value of the memory, and appearance features of frame t, respectively. The proposed OMT estimates the ROIs of target objects. Each object is then decoded and segmented at object level according to the object ROIs, MPM, and object decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Object motion tracker. The proposed tracker encodes the position of each object in previous frames into the parameters of quadratic motion functions for tracking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 4 .</head><label>4</label><figDesc>Motion path where the yellow b of the object at fra refers to the predic frame t, and the red generated memory total motion path o frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FrameFig. 5 :</head><label>5</label><figDesc>Motion path ROI generation. The yellow and green bounding boxes denote the object ROIs of the sailboat and person in two frames, respectively. The red bounding box refers to the generated motion path ROI for frame t-1, which covers the motion path of all objects between two frames and filters redundant background far from objects. All bounding boxes in frame t are predicted by OMT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Object decoder. F 16,obj and V st denote the appearance features of current frame and the queried spatio-temporal features, respectively. F 8,obj and F 4,obj are object skip connections extracted from the middle layer features of the image encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>timeFig. 7 :</head><label>7</label><figDesc>Examples of video sequences in OVOS dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 :</head><label>8</label><figDesc>, RAVOS achieves 84.4/84.2 J &amp;F and 23/20 FPS on YouTube-VOS 2018/2019 validation set. Compared with the nearest regional segmentation competitor DMN-AOA, RAVOS outperforms it by 1.9%. Compared with the nearest competitor STCN, RAVOS has competitive performance and runs about 2? faster (23/20 vs 12/11 FPS on YouTube-VOS 2018/2019).Overall, RAVOS achieves state-of-the-art performance with Qualitative results. The first column denotes the reference frames for mask propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8</head><label>8</label><figDesc>shows qualitative results of RAVOS compared with STM, R50-AOT, and STCN. RAVOS performs better when multiple objects overlap with each other because it only segments the region within ROIs for each object. This reduces the risk of false positives on redundant context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Processing time for different object sizes on YouTube-VOS 2018 validation set. (a) Average spatio-temporal feature matching and propagation time (ms) on different object sizes; (b) Average feature decoding time (ms) on different object sizes; (c) Distribution of object area ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Quantitative evaluation on DAVIS 2016 and 2017 validation sets. RS: regional segmentation. : re-timed on our machine for fair-comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">DAVIS 2017</cell><cell></cell><cell></cell><cell cols="2">DAVIS 2016</cell></row><row><cell>Method</cell><cell cols="2">RS J &amp;F</cell><cell>J</cell><cell>F</cell><cell>FPS</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>FPS</cell></row><row><cell>OSVOS [2]</cell><cell>?</cell><cell>60.3</cell><cell cols="2">56.6 63.9</cell><cell>&lt;1</cell><cell>80.2</cell><cell cols="2">79.8 80.6</cell><cell>&lt;1</cell></row><row><cell>RGMP [28]</cell><cell>?</cell><cell>66.7</cell><cell cols="2">64.8 68.6</cell><cell>&lt;7.7</cell><cell>81.8</cell><cell cols="2">81.5 82.0</cell><cell>7.7</cell></row><row><cell>FEELVOS [55]</cell><cell>?</cell><cell>71.5</cell><cell cols="2">69.1 74.0</cell><cell>2.0</cell><cell>81.7</cell><cell cols="2">81.1 82.2</cell><cell>2.2</cell></row><row><cell>GC [42]</cell><cell>?</cell><cell>71.4</cell><cell cols="2">69.3 73.5</cell><cell>&lt;25</cell><cell>86.6</cell><cell cols="2">87.6 85.7</cell><cell>25</cell></row><row><cell>AFB-URR [41]</cell><cell>?</cell><cell>74.6</cell><cell cols="2">73.0 76.1</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>KMN [56]</cell><cell>?</cell><cell>82.8</cell><cell cols="2">80.0 85.6</cell><cell>&lt;8.3</cell><cell>90.5</cell><cell cols="2">89.5 91.5</cell><cell>8.3</cell></row><row><cell>CFBI+ [57]</cell><cell>?</cell><cell>82.9</cell><cell cols="2">80.1 85.7</cell><cell>5.6</cell><cell>89.9</cell><cell cols="2">88.7 91.1</cell><cell>5.9</cell></row><row><cell>SwiftNet [58]</cell><cell>?</cell><cell>81.1</cell><cell cols="2">78.3 83.9</cell><cell>&lt;25</cell><cell>90.4</cell><cell cols="2">90.5 90.3</cell><cell>25</cell></row><row><cell>ASRF [59]</cell><cell>?</cell><cell>83.2</cell><cell cols="2">80.3 86.1</cell><cell>-</cell><cell>90.9</cell><cell cols="2">90.1 91.7</cell><cell>-</cell></row><row><cell>LCM [37]</cell><cell>?</cell><cell>83.5</cell><cell cols="2">80.5 86.5</cell><cell>&lt;8.5</cell><cell>90.7</cell><cell cols="2">89.9 91.4</cell><cell>8.5</cell></row><row><cell>JOINT [60]</cell><cell>?</cell><cell>83.5</cell><cell cols="2">80.8 86.2</cell><cell>4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HMMN [61]</cell><cell>?</cell><cell>84.7</cell><cell cols="2">81.9 87.5</cell><cell>&lt;10</cell><cell>90.8</cell><cell cols="2">89.6 92.0</cell><cell>10</cell></row><row><cell>STM [5]</cell><cell>?</cell><cell>81.8</cell><cell cols="2">79.2 84.3</cell><cell>19  *</cell><cell>89.3</cell><cell cols="2">88.7 89.9</cell><cell>23  *</cell></row><row><cell>R50-AOT [7]</cell><cell>?</cell><cell>84.9</cell><cell cols="2">82.3 87.5</cell><cell>24  *</cell><cell>91.1</cell><cell cols="2">90.1 92.1</cell><cell>24  *</cell></row><row><cell>STCN [8]</cell><cell>?</cell><cell>85.6</cell><cell cols="2">82.5 88.7</cell><cell>20  *</cell><cell>91.6</cell><cell cols="2">90.7 92.5</cell><cell>22  *</cell></row><row><cell>FAVOS [15]</cell><cell></cell><cell>58.2</cell><cell cols="2">54.6 61.8</cell><cell>&lt;1</cell><cell>80.9</cell><cell cols="2">82.4 79.5</cell><cell>&lt;1</cell></row><row><cell>FTMU [62]</cell><cell></cell><cell>70.6</cell><cell>69.1</cell><cell>-</cell><cell>11.1</cell><cell>78.9</cell><cell>77.5</cell><cell>-</cell><cell>11.1</cell></row><row><cell>SAT [14]</cell><cell></cell><cell>72.3</cell><cell cols="2">68.6 76.0</cell><cell>&lt;39</cell><cell>83.1</cell><cell cols="2">82.6 83.6</cell><cell>39</cell></row><row><cell>TAN-DTTM [12]</cell><cell></cell><cell>75.9</cell><cell cols="2">72.3 79.4</cell><cell>7.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RMNet [16]</cell><cell></cell><cell>83.5</cell><cell cols="2">81.0 86.0</cell><cell>&lt;11.9</cell><cell>88.8</cell><cell cols="3">88.9 88.7 11.9</cell></row><row><cell>DMN-AOA [11]</cell><cell></cell><cell>84.0</cell><cell cols="2">81.0 87.0</cell><cell>6.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RAVOS (Ours)</cell><cell></cell><cell>86.1</cell><cell cols="2">82.9 89.3</cell><cell>42</cell><cell>91.7</cell><cell cols="2">90.8 92.6</cell><cell>58</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II :</head><label>II</label><figDesc>Evaluation on DAVIS 2017 test-dev split. STM [5] CFBI [6] KMN [56] RMNet [16] GIEL [73] R50-AOT [7] STCN [8] RAVOS (Ours)</figDesc><table><row><cell>J &amp;F</cell><cell>72.2</cell><cell>75.0</cell><cell>77.2</cell><cell>75.0</cell><cell>75.2</cell><cell>79.6</cell><cell>79.9</cell><cell>80.8</cell></row><row><cell>J</cell><cell>69.3</cell><cell>71.4</cell><cell>74.1</cell><cell>71.9</cell><cell>72.0</cell><cell>75.9</cell><cell>76.3</cell><cell>77.1</cell></row><row><cell>F</cell><cell>75.2</cell><cell>78.7</cell><cell>80.3</cell><cell>78.1</cell><cell>78.3</cell><cell>83.3</cell><cell>83.5</cell><cell>84.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV :</head><label>IV</label><figDesc>Evaluation on OVOS dataset.</figDesc><table><row><cell>Method</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell>FPS</cell></row><row><cell>STCN</cell><cell cols="4">61.5 57.3 65.6 5.7</cell></row><row><cell cols="5">RAVOS (Ours) 62.5 58.3 66.6 14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE V :</head><label>V</label><figDesc>Ablation of motion path memory (M) and object-level segmentation (O) on DAVIS 2017 validation set.</figDesc><table><row><cell cols="2">M O</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell><cell cols="4">Matching (ms) Decoding (ms) Mem. Size (K) FPS</cell></row><row><cell>?</cell><cell>?</cell><cell>85.6</cell><cell cols="2">82.5 88.7</cell><cell>12.2</cell><cell>7.5</cell><cell>36.3</cell><cell>20</cell></row><row><cell></cell><cell>?</cell><cell>85.4</cell><cell cols="2">82.3 88.5</cell><cell>3.2</cell><cell>7.1</cell><cell>19.2</cell><cell>30</cell></row><row><cell>?</cell><cell></cell><cell>86.1</cell><cell cols="2">82.9 89.3</cell><cell>5.9</cell><cell>3.9</cell><cell>36.3</cell><cell>31</cell></row><row><cell></cell><cell></cell><cell>86.1</cell><cell cols="2">82.9 89.3</cell><cell>2.2</cell><cell>3.8</cell><cell>18.6</cell><cell>42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="3">: RAVOS performance us-</cell></row><row><cell cols="2">ing different trackers.</cell><cell></cell></row><row><cell>Tracker</cell><cell>J &amp;F</cell><cell>Time (ms)</cell></row><row><cell>Lucas-Kanade [77]</cell><cell>80.0</cell><cell>217.4</cell></row><row><cell>RAFT [54]</cell><cell>86.0</cell><cell>20.3</cell></row><row><cell>OMT (Ours)</cell><cell>86.1</cell><cell>0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell cols="4">: RAVOS performance us-</cell></row><row><cell cols="3">ing different memory regions.</cell><cell></cell></row><row><cell>Memory</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell></row><row><cell>Foreground</cell><cell>85.3</cell><cell cols="2">82.1 88.5</cell></row><row><cell>Motion Path</cell><cell>86.1</cell><cell cols="2">82.9 89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VIII</head><label>VIII</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">: RAVOS performance</cell></row><row><cell cols="4">using different motion functions.</cell></row><row><cell>Function</cell><cell>J &amp;F</cell><cell>J</cell><cell>F</cell></row><row><cell>Linear</cell><cell>86.0</cell><cell>82.8</cell><cell>89.3</cell></row><row><cell>Quadratic</cell><cell>86.1</cell><cell cols="2">82.9 89.3</cell></row><row><cell>Cubic</cell><cell>86.0</cell><cell cols="2">82.8 89.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Matnet: Motion-attentive transition network for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="8326" to="8338" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="221" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
		<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Make one-shot video object segmentation efficient again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="332" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Associating objects with transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2491" to="2502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rethinking space-time networks with improved memory coverage for efficient video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11" to="781" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mast: A memory-augmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised video object segmentation by motion-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Multimedia Expo. (ICME)</title>
		<meeting>IEEE Int. Conf. Multimedia Expo. (ICME)</meeting>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video object segmentation with dynamic memory networks and adaptive object alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8065" to="8074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast video object segmentation with temporal aggregation network and dynamic template matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8879" to="8889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast online object tracking and segmentation: A unifying approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1328" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">State-aware tracker for real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9384" to="9393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7415" to="7424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient regional memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1286" to="1295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Occluded video instance segmentation: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01558</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Label propagation in complex video sequences using semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Brit. Mach. Vis. Conf. (BMVC)</title>
		<meeting>Brit. Mach. Vis. Conf. (BMVC)</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">2257</biblScope>
			<biblScope unit="page" from="2258" to="2259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video object segmentation via dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2225" to="2234" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2663" to="2672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Monet: Deep motion exploitation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1140" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mhp-vos: Multiple hypotheses propagation for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="314" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning fast and robust target models for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7406" to="7415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1515" to="1530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Lucid data dreaming for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1175" to="1197" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comput. Vis. (ACCV)</title>
		<meeting>Asian Conf. Comput. Vis. (ACCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="565" to="580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pixel-level matching for video object segmentation using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2167" to="2176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by reference-guided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7376" to="7385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Full-duplex strategy for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4922" to="4933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning dynamic network using a reuse gate function in semi-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8405" to="8414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Capsulevos: Semi-supervised video object segmentation using capsule routing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8480" to="8489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Blazingly fast video object segmentation with pixel-wise metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1189" to="1198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ranet: Ranking attention network for fast video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3978" to="3987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A transductive approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6949" to="6958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sstvos: Sparse spatiotemporal transformers for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Aarabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5912" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning position and target consistency for memory-based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4144" to="4154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rvos: End-to-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5277" to="5286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint reidentification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8953" to="8962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Video object segmentation with adaptive feature bank and uncertain-region refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<meeting>Adv. Neural Inf. ess. Syst. (NIPS)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3430" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Fast video object segmentation using the global context module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tracking objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="474" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Track to detect and segment: An online multi-object tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Simultaneous detection and tracking with motion modelling for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="626" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Long-term tracking with deep tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="6694" to="6706" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Bytetrack: Multi-object tracking by associating every detection box</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06864</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep affinity network for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="119" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Raft: Recurrent all-pairs field transforms for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9481" to="9490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="629" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by multi-scale foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Swiftnet: Real-time video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1296" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Adaptive selection of reference frames for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1057" to="1071" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Joint inductive and transductive learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9670" to="9679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical memory matching network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12" to="889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Fast template matching and update for video object tracking and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Modular interactive video object segmentation: Interaction-to-mask, propagation and differenceaware fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5559" to="5568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hierarchical image saliency detection on extended cssd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Towards high-resolution salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7234" to="7243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Cascadepsp: Toward class-agnostic and very high-resolution segmentation via global and local refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8890" to="8899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Youtube-vos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.03327</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Adv. Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Video object segmentation using global and instance embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16" to="836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="661" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Eur. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="777" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Underst. Workshop. Vancouver</title>
		<imprint>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
