<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PARTICULAR OBJECT RETRIEVAL WITH INTEGRAL MAX-POOLING OF CNN ACTIVATIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Machine Perception FEE CTU Prague</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Irisa Rennes</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PARTICULAR OBJECT RETRIEVAL WITH INTEGRAL MAX-POOLING OF CNN ACTIVATIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2016</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, image representation built upon Convolutional Neural Network (CNN)   has been shown to provide effective descriptors for image search, outperforming pre-CNN features as short-vector representations. Yet such models are not compatible with geometry-aware re-ranking methods and still outperformed, on some particular object retrieval benchmarks, by traditional image search systems relying on precise descriptor matching, geometric re-ranking, or query expansion. This work revisits both retrieval stages, namely initial search and re-ranking, by employing the same primitive information derived from the CNN. We build compact feature vectors that encode several image regions without the need to feed multiple inputs to the network. Furthermore, we extend integral images to handle max-pooling on convolutional layer activations, allowing us to efficiently localize matching objects. The resulting bounding box is finally used for image reranking. As a result, this paper significantly improves existing CNN-based recognition pipeline: We report for the first time results competing with traditional methods on the challenging Oxford5k and Paris6k datasets. * Research partially conducted while G. Tolias and H. J?gou were at Inria. We would like to thank Florent Perronnin for his valuable feedback.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>C ONTENT based image retrieval has received a sustained attention over the last decade, leading to mature systems for tasks like visual instance retrieval. Current state-of-the-art approaches are derived from the Bag-of-Words model of <ref type="bibr" target="#b42">Sivic &amp; Zisserman (2003)</ref> and mainly owe their success to locally invariant features <ref type="bibr" target="#b29">(Lowe, 2004)</ref> and large visual codebooks <ref type="bibr" target="#b33">(Philbin et al., 2007)</ref>. These methods are typically composed of an initial filtering stage where all database images are ranked in terms of similarity to a query image and a second re-ranking stage, which refines the search results of the top-ranked elements. The filtering stage is improved in several ways, such as incorporating weak geometric information <ref type="bibr" target="#b22">(J?gou et al., 2010)</ref>, employing compact approximations of the local descriptors <ref type="bibr" target="#b22">(J?gou et al., 2010)</ref>, or learning smart codebooks <ref type="bibr" target="#b30">(Mikulik et al., 2013;</ref><ref type="bibr" target="#b5">Avrithis &amp; Kalantidis, 2012)</ref>. In such cases, local descriptors are individually matched and selective matching functions <ref type="bibr" target="#b44">(Tolias et al., 2015;</ref><ref type="bibr" target="#b43">Tao et al., 2014)</ref> improve the search quality. Geometric matching models <ref type="bibr" target="#b33">(Philbin et al., 2007;</ref><ref type="bibr" target="#b6">Avrithis &amp; Tolias, 2014)</ref> are typically applied in a pairwise manner during the re-ranking stage of a short-list of images. Query expansion approaches significantly increase the performance <ref type="bibr" target="#b12">(Chum et al., 2011)</ref>, at the cost of larger query times.</p><p>The recent advances achieved by Convolutional Neural Networks (CNN) and the use of intermediate layer activations as feature vectors <ref type="bibr" target="#b15">(Donahue et al., 2013)</ref> create opportunities for representations that are competitive for image or particular object retrieval, and not only classification tasks. Several works have already investigated this research direction, such as global or local representations based on either fully connected <ref type="bibr" target="#b9">(Babenko et al., 2014;</ref><ref type="bibr" target="#b18">Gong et al., 2014)</ref> or convolutional layers <ref type="bibr" target="#b37">(Razavian et al., 2014b;</ref><ref type="bibr" target="#b7">Azizpour et al., 2014;</ref><ref type="bibr" target="#b8">Babenko &amp; Lempitsky, 2015)</ref>. The performance of CNN-based features has rapidly improved to the point of competing and even outperforming pre-CNN works that aggregate local features <ref type="bibr" target="#b35">Radenovi? et al., 2015)</ref>. In particular, activations of convolutional layers followed by a global max-pooling operation <ref type="bibr" target="#b7">(Azizpour et al., 2014)</ref> produce highly competitive compact image representations. One limitation is that such approaches are not compatible with the geometric-aware models involved in the final re-ranking stages. We visualize the patches that contribute the highest to the image similarity score. Displayed patches correspond to the receptive field of CNN activations. Object localization is displayed in magenta, while different colors are used for patches in correspondence.</p><p>This work revisits both filtering and re-ranking stages with CNN-based features. We make the three following contributions.</p><p>? First, we propose a compact image representation derived from the convolutional layer activations that encodes multiple image regions without the need to re-feed multiple inputs to the network, in spirit of recent Fast-RCNN <ref type="bibr" target="#b16">(Girshick, 2015)</ref> and Faster-RCNN <ref type="bibr" target="#b38">(Ren et al., 2015)</ref> methods but here targeting particular object retrieval. The underlying primitive representation is used in all stages (initial retrieval and re-ranking). ? Second, we employ the generalized mean <ref type="bibr" target="#b14">(Doll?r et al., 2009)</ref> to enable the use of integral images along with max-pooling. This efficient method is exploited for particular object localization (see <ref type="figure" target="#fig_0">Figure 1</ref>) directly in the 2D maps of CNN activations. ? Third, our localization approach is used for image re-ranking and leads us to define a simple yet effective query expansion method.</p><p>These approaches are complementary and, when combined, produce for the first time a system which compete on the Oxford and Paris building benchmarks with state-of-the-art re-ranking approaches based on local features. Our approach outperforms by a large margin previous methods based on CNN, while being more efficient in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>CNN based representation. A typical CNN consists of several convolutional layers, followed by fully connected layers and ends with a softmax layer producing a distribution over the training classes. Instead of using this inherent classifier, one can consider the activations of the intermediate layers to train a classifier. In particular, the activations of the fully connected layers have been shown to be very effective and capable of adaptation to various domains <ref type="bibr" target="#b31">(Oquab et al., 2014)</ref>, such as scene recognition <ref type="bibr" target="#b15">(Donahue et al., 2013;</ref><ref type="bibr" target="#b40">Sicre &amp; Jurie, 2015)</ref>, object detection <ref type="bibr" target="#b19">(Iandola et al., 2014)</ref>, and semantic segmentation . In the case of image retrieval, fully connected layers are used as global descriptors followed by dimensionality reduction <ref type="bibr" target="#b9">(Babenko et al., 2014)</ref>. They are also employed as region descriptors to be compared to database descriptors <ref type="bibr" target="#b36">(Razavian et al., 2014a)</ref> or aggregated in a VLAD manner <ref type="bibr" target="#b18">(Gong et al., 2014)</ref>.</p><p>Recent works derive visual representations from the activations of the convolutional layers. This is achieved either by stacking activations  or by performing spatial maxpooling <ref type="bibr" target="#b7">(Azizpour et al., 2014)</ref> or sum-pooling <ref type="bibr" target="#b8">(Babenko &amp; Lempitsky, 2015)</ref> for each feature channel. According to <ref type="bibr" target="#b7">Azizpour et al. (2014)</ref> such representation offers better generalization properties for test data that are far from the source (training) data. Noticeably, higher performance in particular object or scene retrieval is obtained by using convolutional layers rather than fully connected ones. The very recent work of <ref type="bibr" target="#b8">Babenko &amp; Lempitsky (2015)</ref> shows that sum-pooling performs better than max-pooling when the image representation is whitened. In addition to be a costly choice, we will show that this is not optimal in our context of object localization (see Section 8). Finally, <ref type="bibr" target="#b24">Kalantidis et al. (2015)</ref> propose spatial and feature channel weighting that significantly improves performance. Their approach is complementary to what we propose for the filtering and the re-ranking stage.</p><p>Recent examples utilize information from fully connected layers to perform generic object detection <ref type="bibr" target="#b19">(Iandola et al., 2014;</ref><ref type="bibr" target="#b32">Papandreou et al., 2014)</ref>. Such approaches are prohibitive for the reranking purposes of large scale image retrieval. They have high computational cost and the inherent features are not optimal for particular object matching. <ref type="figure">Figure 2</ref>: We visualize the receptive fields related to the 5 MAC components that contribute the most to the image similarity. Each displayed receptive field corresponds to the maximum response of a feature channel. A different color is used for each feature channel, while different feature channels are shown for each image pair.</p><p>Localization. In the recent years, the sliding window principle has been quite successful for many object localization methods. Due to the large number of possible windows, exhaustive search is extremely costly. However, integral images <ref type="bibr" target="#b48">(Viola &amp; Jones, 2001</ref>) offer a constant cost solution to the evaluation of a single region. This attractive alternative is applicable for feature vectors constructed via a sum-pooling operation.</p><p>A globally optimal solution is given by Efficient Subwindow Search (ESS) of <ref type="bibr" target="#b27">Lampert et al. (2009)</ref>, who use branch-and-bound search to avoid exhaustive search. Their work employs integral images, which are also used in later improvements of ESS ). <ref type="bibr" target="#b1">An et al. (2009)</ref> formalize localization as a maximum sub-array problem and similarly to <ref type="bibr" target="#b11">Chen et al. (2013)</ref> they employ Bentley's algorithm <ref type="bibr" target="#b10">(Bentley, 1999)</ref>. Integral images facilitate the evaluation of many region candidates (Uijlings et al., 2013) based on VLAD or Fisher vectors <ref type="bibr" target="#b46">(Van de Sande et al., 2014)</ref>. All aforementioned approaches take advantage of integral images due to the inherent sum-pooling operation in the given representation. In this paper, we extend integral images to perform max-pooling over CNN activation maps, which is shown to be a better choice for describing regions (as opposed to the entire image).</p><p>Several object localization techniques have been proposed in the context of image retrieval as well. <ref type="bibr" target="#b26">Lampert (2009)</ref> propose a two layer branch-and-bound method that alternates between regions and images. Integral images offer a significant speed-up in the work of <ref type="bibr" target="#b28">Lin &amp; Brandt (2010)</ref> to perform localization through Bag-of-Words. The overall idea bears similarities with our work. However, we differentiate by employing CNN-based representation with max-pooling. Some approaches <ref type="bibr" target="#b43">(Tao et al., 2014;</ref><ref type="bibr" target="#b39">Shen et al., 2014)</ref> individually index local features for localization. In our case, the localization method is built on top of a compact representation, initially used for the filtering stage. Finally, <ref type="bibr" target="#b3">Arandjelovic &amp; Zisserman (2013)</ref> propose a localization strategy based on VLAD, where similarity is computed for multiple image regions, giving a more precise localization via regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>We consider a pre-trained CNN and discard all the fully connected layers. Given an input image I of size W I ? H I , the activations (responses) of a convolutional layer form a 3D tensor of W ? H ? K dimensions, where K is the number of output feature channels, i.e. multi-dimensional filters. The spatial resolution W ? H depends on the network architecture, the layer examined, and the input image resolution. We assume that Rectified Linear Units (ReLU) are applied as a last step, guaranteeing that all elements are non-negative.</p><p>We represent this 3D tensor of responses as a set of 2D feature channel responses X = {X i }, i = 1 . . . K, where X i is the 2D tensor representing the responses of the i th feature channel over the set ? of valid spatial locations, and X i (p) is the response at a particular position p. Therefore, the feature vector constructed by a spatial max-pooling over all locations <ref type="bibr" target="#b7">(Azizpour et al., 2014)</ref> is given by</p><formula xml:id="formula_0">f ? = [f ?,1 . . . f ?,i . . . f ?,K ] , with f ?,i = max p?? X i (p).<label>(1)</label></formula><p>Maximum activations of convolutions (MAC). Two images are compared with the cosine similarity of the K-dimensional vectors produced as described above. This representation, referred to as MAC, does not encode the location of the activations (unlike activations of fully connected layers), due to the max-pooling operated over a single region of size W ? H. It encodes the maximum "local" response of each of the convolutional filters and is therefore translation invariant. In all the following, we consider the last convolutional layer of the examined networks. <ref type="figure">Figure 2</ref> visualizes the patches that contribute the most to the image similarity. They correspond either to the same object part or similar parts due to repeated structures. We extract MAC from input images of any resolution or aspect ratio by simply subtracting the mean pixel value <ref type="bibr" target="#b19">(Iandola et al., 2014)</ref> from the input images. No crop or change of aspect ratio is required <ref type="bibr" target="#b7">(Azizpour et al., 2014)</ref>.</p><p>The max pooling operation that is performed over a single cell offers translation invariance to the resulting representation. This is in contrast to representation derived from the fully connected layers that requires objects to be aligned. In our case, we assume that objects are up-right and we simply benefit from the rotation tolerance provided by the CNN due to the training data used. The same stands for the tolerance to scale changes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">ENCODING REGIONS INTO SHORT VECTORS</head><p>This section describes how we exploit the activations of the CNN convolutional layers to derive representations for image regions. Region vectors are aggregated to produce a short signature used in the filtering stage of image retrieval.</p><p>Region feature vector. The feature vector f ? described in Section 3 is a representation for the whole image I. Now, we consider a rectangular region R ? ? = [1, W ] ? [1, H], and define the regional feature vector</p><formula xml:id="formula_1">f R = [f R,1 . . . f R,i . . . f R,K ]<label>(2)</label></formula><p>where f R,i = max p?R X i (p) is the maximum activation of the i th channel on the considered region.</p><p>The regions R are defined on the space ? of all valid positions for the considered feature map (and not on the input image plane). A region of size 1 corresponds to a feature vector consisting of a single activation at a particular location. We are now able to construct a representation for multiple regions without re-feeding additional input to the CNN, similarly to recent RNN variants <ref type="bibr" target="#b38">(Ren et al., 2015;</ref><ref type="bibr" target="#b16">Girshick, 2015)</ref>, which drastically reduces the processing cost.</p><p>Now assume a linear mapping of a given region R back to the original image. The proposed region vector captures a larger image region than the back-projected one, due to the large receptive field. A similar effect occurs in the context of object detection <ref type="bibr" target="#b19">(Iandola et al., 2014)</ref>, where fully connected layers are applied in a sliding window fashion.</p><p>R-MAC: regional maximum activation of convolutions. We now consider a set of R regions of different sizes. The structure of the regions is similar to the one proposed by <ref type="bibr" target="#b37">Razavian et al. (2014b)</ref>, but we define them on the CNN response maps and not on the original image. We sample square regions at L different scales. At the largest scale (l = 1), the region size is determined to be as large as possible, i.e., its height and width are both equal to min(W, H). The regions are sampled uniformly such that the overlap between consecutive regions is as close as possible to 40%. Remark that the aspect ratio of the original image has an influence on the number m of regions that we extract (1 region only if the input image is square). At every other scale l we uniformly sample l ? (l + m ? 1) regions of width 2 min(W, H)/(l + 1), as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref> (left).</p><p>Then we calculate the feature vector associated with each region, and post-process it with 2normalization, PCA-whitening <ref type="bibr" target="#b20">(J?gou &amp; Chum, 2012</ref>) and 2 -normalization. We combine the collection of regional feature vectors into a single image vector by summing them and 2 -normalizing in the end. This choice keeps the dimensionality low which is equal to the number of feature channels. However, we show in our experiments that the resulting representation, referred to as R-MAC, offers a significant better performance than the corresponding MAC with same dimensionality. Note, the aggregation of the region vectors can be seen as a simple kernel that cross matches all possible regions, including across different scale.  </p><formula xml:id="formula_2">|X i | |f R,i ? f R,i | ? = 5 ? = 10 ? = 20 ? = 50 0.98 0.99 1 f R f R /( f R f R ) ? = 5 ? = 10</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">OBJECT LOCALIZATION</head><p>In this section we propose an extension of integral images to perform approximate max-pooling over a set X of 2D feature channel response maps, which provide a rough yet efficient localization to our CNN-based method.</p><p>Approximate integral max-pooling. Noticing that the responses X i are non-negative, we exploit the generalized mean <ref type="bibr" target="#b14">(Doll?r et al., 2009)</ref> to approximate each feature value f R,i associated with a given region R by the estimat?</p><formula xml:id="formula_3">f R,i = ? ? p?R X i (p) ? ? ? 1 ? ? max p?R X i (p) = f R,i ,<label>(3)</label></formula><p>where the parameter ? &gt; 1 is such thatf i ? f i when ? ? +?.</p><p>Figure 3 (middle) shows the average approximation error |f R,i ? f R,i | estimated over several image regions. We report the approximation error as a function of the size of the corresponding response set on which the maximum value is computed. The various sizes of response sets are an outcome of using all possible regions. A high value of the exponent ? leads to a better approximation, while applying on more elements makes the approximation less precise.</p><p>By approximating the maximum in this manner, we can now use integral images <ref type="bibr" target="#b48">(Viola &amp; Jones, 2001)</ref> to approximate the regional feature vector f R defined on any rectangular region R. For each channel, we construct the integral image of the 2D tensor whose value at position p is equal to X i (p) ? , p ? R. Then, the sum of Equation <ref type="formula" target="#formula_3">(3)</ref> is simply given by the sum of 4 terms <ref type="bibr" target="#b48">(Viola &amp; Jones, 2001)</ref>. This allow us to efficiently compute max-pooling for many regions and therefore to construct the corresponding feature vectors. This is in contrast to the explicit construction of many regions with representation derived from fully connected layers, which is prohibitive due to the need to resize/crop and re-feed each region to the network.</p><p>We evaluate the approximation quality by measuring the cosine similarity between the exact vector and its approximate counterpart. The distribution of this similarity is presented in <ref type="figure" target="#fig_2">Figure 3</ref> (right) and is measured on all possible regions of 10 randomly selected images. The proposed approximation is very precise even for moderate values of ?. We set ? equal to 10 in all of our experiments.</p><p>Window detection. Let us now assume that there is another image Q depicting a single object, i.e. cropped via a bounding box defining the object of interest. We denote as q the corresponding MAC feature vector. The 2D region, defined on the CNN activations X of image I, that maximizes the similarity to q is computed asR</p><formula xml:id="formula_4">= arg max R??f R q f R q .<label>(4)</label></formula><p>The regionR maximizing the similarity is mapped back to the original image I with a precision of ( W W I , H H I ) pixels, providing a rough localization of the object depicted in Q. The corresponding similarity does not take into account all the visual content of image I and is therefore free from the influence of background clutter. The brute-force detection of the optimal region by exhaustive search is expensive, as the number of possible regions is in O(W 2 H 2 ). In preliminary tests, we have evaluated a globally optimal solution based on branch and bound search, as in ESS . The necessary bounds are trivially derived for our representation. The search is not significantly sped up in our case: The maxima are not distinct enough and a large number of regions are considered, while the overhead of maintaining the priority queue is high.</p><p>AML: approximate max-pooling localization. Instead, we restrict the number of regions that we evaluate and locally refine the best ones with simple heuristics. Candidate regions are uniformly sampled with a search step equal to t. In addition, regions having an aspect ratio larger than s times that of the query region are discarded. The parameters of the best region are refined in a coordinate descent manner, while allowing a maximum change of 3 units. The refinement process is repeated up to 5 times. Experiments show that the overlap of the detected region to the optimal one is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">RETRIEVAL, LOCALIZATION AND RE-RANKING</head><p>Initial retrieval. The MAC or R-MAC feature vector is computed for all databases images. Similarly, at query time we process the query image and extract the corresponding feature vector. During the filtering stage we directly evaluate cosine similarity between the query and all the database vectors. Therefore, we obtain the initial ranking based on the similarity of MAC or R-MAC vectors.</p><p>Re-ranking. We consider a second re-ranking stage, as typically performed in spatial verification <ref type="bibr" target="#b33">(Philbin et al., 2007)</ref> with local features. A short-list of N top-ranked images is considered and AML, as described in Section 5, is applied on pairs of query and database images. Note that the query is now represented by the MAC vector, since this is used in AML, while the database image is represented by X . For each re-ranked image we obtain a score given by the region that maximizes the similarity to the query. This similarity is used to re-rank the elements of the short-list. Furthermore, a rough localization of the query object is available.</p><p>Remarks: At the filtering stage, whitened MAC (whitening as described in Section 8) or R-MAC can be used, while the localization procedure employs similarity with respect to 2 -normalized MAC. However, once the query object is localized, then, similarity between the query and the detected region is computed via whitened MAC or R-MAC, depending on the chosen filtering method. This similarity score is used to perform re-ranking. The required representation is constructed on query time only for the detected region and is acquired efficiently with integral images.</p><p>Query expansion (QE). Re-ranking brings positive images at the very top ranked positions. Then, we collect the 5 top-ranked images, merge them with the query vector, and compute their mean. Finally, the similarity to this mean vector is adopted to re-rank once more the top N images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">IMPLEMENTATION DETAILS</head><p>We observe that thresholding the response values of X which are larger than 128 (0.001% of all responses) and mapping each value to the closest smaller integer (floor operation) leads to insignificant losses. This allows the computation of ?-th power with a lookup table and speeds-up the construction of integral images. Moreover, we approximate the ?-th root of Equation (3) by performing binary search on the same lookup table of ?-th power. This process allows the optimal window search to be more efficient.</p><p>The response maps represented by X are sparse <ref type="bibr" target="#b0">(Agrawal et al., 2014)</ref>. In particular, using the network of <ref type="bibr" target="#b25">Krizhevsky et al. (2012)</ref> on Oxford buildings dataset <ref type="bibr" target="#b33">(Philbin et al., 2007)</ref> results in 81% of response values being zero, which is convenient for storage purpose. We further decrease the memory requirements by uniformly quantizing the responses into 8 values. This results in more elements mapped to the same value. Therefore, we store the positions of non-zeros values with delta coding and use only 1 byte per non-zero element. Note that an image of resolution equal to 1024 ? 768 corresponds to feature channel response maps of size 30 ? 22 using the same network. Finally, an image requires around 32 kB of memory. At re-ranking time we construct one integral image at a time and use double precision (8 bytes) for its elements. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">EXPERIMENTS</head><p>This section presents the results of our compact representation for image retrieval, evaluate the localization accuracy AML, and finally employ it for retrieval re-ranking.</p><p>Experimental setup. We evaluate the proposed methods on Oxford Buildings dataset <ref type="bibr" target="#b33">(Philbin et al., 2007)</ref> and Paris dataset <ref type="bibr" target="#b34">(Philbin et al., 2008)</ref>, which are composed of 5063 and 6412 images, respectively. We refer to these datasets as Oxford5k and Paris6k. We additionally use 100k Flickr images <ref type="bibr" target="#b33">(Philbin et al., 2007)</ref> to compose Oxford105k and Paris106k, respectively. A distractor set of 1 million images from Flickr <ref type="bibr" target="#b22">(J?gou et al., 2010)</ref> is additionally used to go at larger scale. Retrieval performance is measured in terms of mean Average Precision (mAP). We follow the standard protocol and use the bounding boxes defined on the query images 1 . These bounding boxes are also employed to evaluate localization accuracy. PCA is learned on Paris6k when testing on Oxford5k and vice versa. In order to be fair, we directly compare our results only to previous methods that do not perform learning on the test set.</p><p>The focus of our work is not to train a CNN, but to extract visual descriptors from its convolutional layers. We use networks widely used in the literature: AlexNet by <ref type="bibr" target="#b25">Krizhevsky et al. (2012)</ref> and the very deep network (VGG16) by <ref type="bibr" target="#b41">Simonyan &amp; Zisserman (2014)</ref>. We choose VGG16 instead of VGG19 because we observe that the latter does not always attain better performance while it has higher feature extraction cost. Our representation is extracted from the last pooling layer, which has 256 feature channels for AlexNet and 512 for VGG16. MatConvNet <ref type="bibr" target="#b47">(Vedaldi &amp; Lenc, 2014</ref>) is used to extract the features.</p><p>Localization accuracy. To evaluate the accuracy of AML, we employ pairs of Oxford5k query images and their corresponding positive images. We first perform exhaustive search to detect the globally optimal window. Then, we apply our speeded-up detector that evaluates fewer regions and in the end refines the best one. In both cases the approximate max-pooling is used for each window evaluation. We report Intersection over Union (IoU) with the optimal window and the percentage of windows evaluated compared to the exhaustive case. Results are shown in <ref type="table" target="#tab_0">Table 1</ref> (left). We provide a large speed-up while maintaining a high overlap with the optimal detection. Recall that our purpose is to apply this detector for fast re-ranking. Measuring IoU provides evidence for localization accuracy, however we observed that it does not directly impact retrieval performance. We finally set s = 1.1 and t = 3 for re-ranking usage.</p><p>In order to evaluate the localization accuracy with respect to ground-truth annotation we crossmatch all 5 query images that exist per building. One of them is used as a query (cropped bounding box), while for the other we compare the detected region to the ground-truth annotation. Exhaustive evaluation achieves an IoU equal to 52.6% (52.9%) and the speeded-up approach achieves 51.3% (51.4%) on Oxford5k (Paris6k) datasets. The accuracy loss is limited, while the localization is approximately 180 times faster. AML provides a rough localization at low computational cost. Such   <ref type="bibr" target="#b9">Babenko et al. (2014)</ref> 128 55.7 -52.3 - <ref type="bibr" target="#b30">Mikulik et al. (2013)</ref> 84.9 82.4 79.5 77.3 <ref type="bibr" target="#b37">Razavian et al. (2014b)</ref> 256 53.3 67.0 48.9 - <ref type="bibr" target="#b39">Shen et al. (2014)</ref> 75.2 74.1 72.9 - <ref type="bibr" target="#b8">Babenko &amp; Lempitsky (2015)</ref>  a setup results in an average re-ranking query time of 2.9 sec using AlexNet, when re-ranking 1000 images with a single threaded implementation.</p><p>Retrieval and re-ranking. We evaluate retrieval performance using MAC and R-MAC compact representations. The MAC vectors are 2 -normalized, PCA-whitened and 2 -normalized once more, while the corresponding processing of the R-MAC is as described in Section 4. <ref type="table" target="#tab_0">Table 1</ref> (right) presents the results on Oxford5k. We evaluate different input image resolutions and observe that the original image size (1024) provides higher performance. Note that MAC is similar to the one proposed by <ref type="bibr" target="#b7">Azizpour et al. (2014)</ref>, however their process remains constrained by standard input size and aspect ratio. The proposed R-MAC gives a large performance improvement at no extra cost, as both feature vectors have exactly the same dimensionality. Regions of different scales are aggregated together, meaning that L = 3 combines regions at scales l = 1, l = 2, and l = 3. We set L = 3 in the following. In order to decompose the components of R-MAC, we construct R-MAC by aggregating only regions of l = 3. It achieves mAP equal to 63.0 on Oxford5k with VGG16. Aggregating both regions of l = 2 and l = 3 improves to 65.4. Finally, adding l = 1 (original R-MAC) performs 66.9 (see <ref type="table" target="#tab_0">Table 1</ref> right). Filtering time is 12 ms on average for Oxford105k.</p><p>Next, we employ AML for image re-ranking and conduct performance evaluation on Oxford105k by re-ranking up to 1000 images. The performance is consistently improved as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. R-MAC brings a larger benefit and VGG16 performs better than AlexNet. Query expansion, as described in Section 6, improves the performance at low extra cost, since similarity is re-computed only for the re-ranked short-list. Finally, we carry out experiment at larger scale with 1M distractor images and present results in <ref type="figure" target="#fig_3">Figure 4</ref>. AML improves the performance by 13% mAP.</p><p>Examples of ranking using MAC and re-ranking using AML are presented in <ref type="figure">Figure 5</ref>. Recall that we only provide a rough object localization, since our main goal is to obtain improved image similarity. Furthermore, the provided localization is accurate enough for re-ranking.</p><p>Comparison to the state of the art. We compare the proposed methods to state-of-the-art performance of compact representations and approaches based on local features that perform precise Discussion about other CNN-based approaches. <ref type="bibr" target="#b37">Razavian et al. (2014b)</ref> propose to perform region cross-matching and accumulate the maximum similarity per query region. We evaluate this cross-matching process on the collection of regional vectors used in R-MAC; we simply skip the final aggregation process and keep the regional vectors individually. The cross-matching achieves 75.2% mAP on Oxford5k as a filtering stage, while re-ranking with AML on top of this acts in a complementary way and increases the performance up to 78.1%. However, cross-matching has two drawbacks. Firstly, the region vectors have to be stored individually and increase the memory requirements by a factor of |R|, where |R| is the number of extracted regions. Secondly, the complexity cost is linear in the number of indexed images and quite high since it requires to compute |R| 2 (e.g. 1024 <ref type="bibr" target="#b37">(Razavian et al., 2014b)</ref>) inner products per image. The work of <ref type="bibr" target="#b37">Razavian et al. (2014b)</ref> follows a non-standard evaluation protocol by enlarging the provided query bounding boxes. In addition, the cost of their feature extraction is extremely high since they feed 32 images of resolution 576 ? 576 to the CNN. The recent work of <ref type="bibr" target="#b49">Xie et al. (2015)</ref> is quite similar to theirs and is applied on both retrieval and classification. <ref type="bibr" target="#b8">Babenko &amp; Lempitsky (2015)</ref> show that global sum-pooling on convolutional layer activations is better than max-pooling when the final image vectors are PCA-whitened. When whitening is not employed, then the latter is better. In the context of object localization we efficiently evaluate a large number of candidate regions on query time with AML. Performing whitening on each candidate region vector significantly increases the cost and is prohibitive for this task. We switch max-pooling to sum-pooling for both our proposed R-MAC and AML and test performance. Note that sum-pooling is a special case of our integral max-pooling with ? = 1. Switching to sum-pooling makes R-MAC perform 69.8 and R-MAC +AML +QE perform 76.9 on Paris106k. These scores are directly comparable to our scores in <ref type="table" target="#tab_1">Table 2</ref> and reveal that our choice is consistently better in all cases within our pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>In this work, we re-visit both filtering and re-ranking retrieval stages by employing CNN activations of convolutional layers. Our compact vector representation encodes several image regions with simple aggregation method and is shown to outperform state-of-the-art competitors. Our localization increases the performance of the retrieval system that is initially based on a compact representation. The same CNN information adopted during the filtering stage is employed for re-ranking as well. Our approach competes with state-of-the-art methods that employ costly geometric matching or query expansion and we achieve the highest performance on Paris dataset, and provided a much better performance than existing approaches built upon CNN features. A very recent work <ref type="bibr" target="#b4">(Arandjelovic et al., 2015)</ref> shows how MAC performance is improved by end-to-end fine tunning where the objective is based on MAC similarity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Query objects (left) and the corresponding localization in another image (right) are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Left: Sample regions extracted at 3 different scales (l = 1 . . . 3). We show the top-left region of each scale (gray colored region) and its neighboring regions towards each direction (dashed borders). We depict the centers of all regions with a cross. Middle: Approximation error of the maximum value versus the size of the response set for different values of exponent ?. Measurements are performed on 10 randomly selected images by evaluating all possible regions. The responses for this set of images take values in[0, 151]. Right: Empirical distribution of the cosine similarity value between the exact vector f R and its approximationf R . Measurements are collected by constructing the exact and approximate vectors of all possible regions on 10 randomly sampled images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Performance of retrieval with re-ranking by AML versus number of re-ranked images on Oxford105k and Oxford5k combined with 1M distractor images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Left: Comparison between the exhaustive sliding window and our alternative of window sampling and refinement. We report the average IoU w.r.t. the globally optimal window and the average percentage of windows evaluated w.r.t. to the exhaustive search (noted by %W). Measurements are conducted on all pairs of Oxford5k query images and their corresponding positive images. Right: Performance (mAP) of MAC and R-MAC on Oxford5k. Resol. corresponds to the input image resolution (maximum dimension).</figDesc><table><row><cell></cell><cell cols="3">Aspect ratio change threshold s</cell><cell></cell><cell></cell></row><row><cell>Search step t</cell><cell>1.1 IoU %W IoU %W IoU 1.5</cell><cell>2.0</cell><cell>%W</cell><cell cols="2">Network Resol. MAC</cell><cell>R-MAC L=1 L=2 L=3 L=4</cell></row><row><cell>1 2</cell><cell>81.8 8.9 88.7 27.5 93.7 79.9 0.5 83.8 2.0 86.6</cell><cell></cell><cell>46.3 3.6</cell><cell>AlexNet</cell><cell cols="2">1024 44.9 47.9 54.6 56.1 55.6 724 44.8 48.4 54.4 54.3 52.6</cell></row><row><cell>3 4</cell><cell>78.7 0.2 81.2 0.5 83.6 77.0 0.1 79.5 0.2 81.5</cell><cell></cell><cell>0.8 0.3</cell><cell>VGG16</cell><cell cols="2">1024 55.2 57.3 64.5 66.9 67.4 724 52.2 54.8 58.0 60.9 60.3</cell></row><row><cell>5</cell><cell>75.8 0.1 79.0 0.1 80.9</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison with state of the art. We report results for compact vector representations (left) and for retrieval approaches employing geometry, re-ranking, query expansion, or vector approximations (right). D = dimensionality. Our approaches are identified with bullets ?.</figDesc><table><row><cell>Method</cell><cell>D</cell><cell cols="5">Oxf5k Par6k Oxf105k Par106k Method</cell><cell cols="2">Oxf5k Par6k Oxf105k Par106k</cell></row><row><cell>J?gou &amp; Zisserman (2014)</cell><cell cols="2">1024 56.0</cell><cell>-</cell><cell>50.2</cell><cell>-</cell><cell>Chum et al. (2011)</cell><cell>82.7 80.5 76.7</cell><cell>71.0</cell></row><row><cell></cell><cell cols="2">128 43.3</cell><cell>-</cell><cell>35.3</cell><cell>-</cell><cell cols="2">Danfeng et al. (2011) 81.4 80.3 76.7</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Examples of top retrieved images before (top) and after (bottom) re-ranking with AML. On the left we show the query image and depict the bounding box in blue color. When re-ranking is used, we present the top ranked images and report for each image its initial and final ranking. The localization window is shown in magenta, while positive/negative/junk images are depicted with green/red/yellow border. descriptor matching, re-ranking or query expansion. Results are shown inTable 22 . AlexNex and VGG16 are used to produce the 256D and 512D vectors for R-MAC, respectively. Regarding the compact representations, our short-sized R-MAC outperforms all other approaches. The better performance on Paris is inherited by the nature of the pre-trained networks; the baseline MAC with VGG achieves 55.2 on Oxford5k and 74.7 on Paris6k.Unlike previous description schemes derived from CNN layers, our approach compete with the best approaches based on local features for geometric matching and query expansion. Our AML can even outperform them: while our results are lower on Oxford, we achieve the best performance on Paris and, to the best of our knowledge, outperform all published results on this benchmark. Higher scores on Paris6k are reported by Arandjelovic &amp; Zisserman (2012) (91.0) and byZhong et al.  (2015) (91.5). These are achieved by learning the codebook on Paris6k itself and by performing pre-processing of the indexed dataset.</figDesc><table><row><cell>Query 1 ? 1</cell><cell>21 ? 2</cell><cell cols="2">19 ? 3</cell><cell>13 ? 4</cell><cell></cell><cell>3 ? 5</cell><cell>25 ? 6</cell><cell>2 ? 7</cell><cell></cell><cell>8 ? 8</cell></row><row><cell cols="3">Query 1 ? 1 4 ? 2 220 ? 3</cell><cell>52 ? 4</cell><cell>15 ? 5</cell><cell></cell><cell>212 ? 6</cell><cell cols="3">10 ? 7 159 ? 8 26 ? 9</cell><cell>860 ? 10</cell></row><row><cell>Query</cell><cell>120 ? 1</cell><cell>118 ? 2</cell><cell cols="2">753 ? 3</cell><cell cols="2">467 ? 4</cell><cell>631 ? 5</cell><cell>82 ? 6</cell><cell></cell><cell>594 ? 7</cell></row><row><cell>Query 1 ? 1</cell><cell>2 ? 2</cell><cell>7 ? 3</cell><cell>5 ? 4</cell><cell cols="2">3 ? 5</cell><cell>4 ? 6</cell><cell>8 ? 7</cell><cell>6 ? 8</cell><cell cols="2">43 ? 9</cell><cell>9 ? 10</cell></row><row><cell>Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The query regions are cropped and then used as input to the CNN.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot"><ref type="bibr" target="#b21">J?gou &amp; Zisserman (2014)</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Small differences of scores compared to the first version of the manuscript on arxiv are due to a slightly different evaluation protocol used before. Now, the evaluation protocol is the standard one for these datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the performance of multilayer neural networks for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Efficient algorithms for subwindow search in object detection and localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peursum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanquan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svetha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">All about VLAD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Petr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Akihiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Approximate gaussian mixtures for large scale vocabularies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Hough pyramid matching: Speeded-up geometry re-ranking for large scale image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aggregating deep convolutional features for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural codes for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Artem</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Slesarev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><surname>Chigorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Programming Pearls, 2/E</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Bentley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Addison-Wesley Professional</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Efficient maximum appearance search for large-scale object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rogerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amitava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liangliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Total recall II: Query expansion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ondrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Danfeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Integral channel features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhuowen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liwei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Densenet: Implementing efficient convnet descriptor pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno>arxiv</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Negative evidences and co-occurences in image retrieval: The benefit of PCA and whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Triangulation embedding and democratic aggregation for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herv?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<date type="published" when="2012-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Cross-dimensional weighting for aggregated deep convolutional features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Mellina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.04065</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Detecting objects in large image collections and videos by efficient subimage retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient subwindow search: A branch and bound framework for object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2129" to="2142" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A local bag-of-features model for large-scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning vocabularies over a fine quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji??</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Untangling local and global deformations in deep convolutional networks for image classification and sliding window detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savalle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre-Andr?</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ondrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ondrej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple measurements and joint dimensionality reduction for large scale image search with short vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ondrej</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">CNN features off-the-shelf: An astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A baseline for visual instance retrieval with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shaoqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>arXiv</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatially-constrained similarity measure for large-scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. PAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1229" to="1241" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminative part model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurie</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frdric</forename></persName>
		</author>
		<idno>1077-3142</idno>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="28" to="37" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video Google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Locality in generic instance search from one example</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efstratios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Image search with selective match kernels: aggregation across single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fisher and VLAD with flair</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Cees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">M</forename><surname>Arnold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Matconvnet-convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="34" to="47" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Image classification and retrieval are one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Fast object retrieval using direct spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1391" to="1397" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
