<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><forename type="middle">Yu</forename><surname>Rippleai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snu</forename><surname>Seoul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghwan</forename><surname>Lee</surname></persName>
							<email>seunghwan@rippleai.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rippleai</forename><surname>Seoul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheol</forename><surname>Choi</surname></persName>
							<email>ycchoi@rippleai.co</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rippleai</forename><surname>Seoul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunheekim</forename><surname>Rippleai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snu</forename><surname>Seoul</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">CurlingNet: Compositional Learning between Images and Text for Fashion IQ Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present an approach named CurlingNet that can measure the semantic distance of composition of imagetext embedding. In order to learn an effective image-text composition for the data in the fashion domain, our model proposes two key components as follows. First, the Delivery makes the transition of a source image in an embedding space. Second, the Sweeping emphasizes query-related components of fashion images in the embedding space. We utilize a channel-wise gating mechanism to make it possible. Our single model outperforms previous state-of-the-art image-text composition models including TIRG <ref type="bibr" target="#b16">[17]</ref> and FiLM <ref type="bibr" target="#b13">[14]</ref>. We participate in the first fashion-IQ challenge [5] in ICCV 2019, for which ensemble of our model achieves one of the best performances.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Controllable image search based on user input (e.g. natural language query and category filter) is an important topic of research to technically study the multimodal embedding and practically promote user convenience of search. It can provide a significant contribution for developing an interactive conversational image search systems.</p><p>In this report, we aim at proposing an approach that can measure the semantic similarity between the composition embedding of images and text, and applying it to tackle controllable multimodal image retrieval for fashion data. Our model, named Curling Network, consists of two main components, Delivery and Sweeping as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The delivery path guides the image embedding into the imaginary center of candidate images, which will be an ideal target point. The sweeping path learns to make the distance closer to the target images with the same properties as described in the query text. That is, the sweeping component moves the target embedding point to emphasize key attributes in the query text (e.g. striped, covered in the neck). We summarize the contributions of this work as follows.</p><p>1. We propose the CurlingNet, which can measure the semantic differential relationship between images with respect to a query text. Compare to existing models such as TIRG <ref type="bibr" target="#b16">[17]</ref>, our model not only learns the image-text composition features for finding the most suitable target data, but also focuses on the queried attributes in target data for better ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>To validate our proposed model, we participate in Fashion-IQ challenge and our ensemble model achieves the second place on the leaderboard.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preprocessing</head><p>Fashion Attribute Experts. We use the image feature encoded by pre-trained image CNNs (e.g. ResNet-152 <ref type="bibr" target="#b6">[7]</ref>, DenseNet-169 <ref type="bibr" target="#b7">[8]</ref>) as a basic expert. In addition, we treat the Fashion IQ attributes of each category as another expert. as a result, a total of seven experts are used to en- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Curling Network</head><p>Projection <ref type="figure">Figure 2</ref>. The architecture of the image encoder using Collaborative Experts <ref type="bibr" target="#b10">[11]</ref>.</p><p>code the source and target images. We embedded the attributes using the pretrained word2vec <ref type="bibr" target="#b12">[13]</ref>. The number of attributes per data ranges from 0 to 19, so we pool them with a learnable pooling method, NetVLAD <ref type="bibr" target="#b0">[1]</ref>. It captures information about the statistics of local descriptors aggregated over the attributes. Then, six expert features are obtained by combining image features and the pooled attributes. These features are supposed to represent the attribute information in the image. We use Context Gating <ref type="bibr" target="#b11">[12]</ref> to combine image features and attributes. Finally, we obtain the encoding of the experts from Collaborative Expert (CE) gating <ref type="bibr" target="#b10">[11]</ref>.</p><p>Text Encoding. For text encoding, we use the threelevel encoding method <ref type="bibr" target="#b2">[3]</ref>: (1) global encoding by mean pooling, (2) temporal-aware encoding by biGRU and <ref type="formula">(3)</ref> Locally-enhanced encoding by biGRU-CNN. We concatenate the encodings of all levels to make the final text feature, so that the text encoding includes multi-level information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transition Filter Networks</head><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, suppose that the text query is less covered on the neck and is fully patterned for a given source image. However, there can be many other differences beyond these two query requests between the true source and taget image pair, such as shorter skirts, sticking to the body, and red and blue. In other words, the target image cannot be fully specified by text query; instead, only some center points of various valid targets can be specified with one source and text query. Therefore, we need two types of networks. The first one delivers the source image to the candidate cluster according to a given query in an embedding space (i.e. TIRG <ref type="bibr" target="#b16">[17]</ref>). The second network checks the attributes highlighted in the query and learn the path from the center of valid target candidates to the true target image. We design two networks named Delivery filters and Sweeping filters, whose descriptions are shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Multimodal Fusion Layers. Image expert features and text encodings are taken into the multimodal fusion layer. We use concatenation of these multimodal inputs and their output of the fusion function, for which we use a simple Hadamard product so that each of the two inputs is projected into a semantic space with the same dimension size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FusionLayer(A, B) = [A; B; A B]</head><p>For final ensemble, we use several variants of fusion functions such as MUTAN <ref type="bibr" target="#b1">[2]</ref> and MCB <ref type="bibr" target="#b3">[4]</ref>.</p><p>The Delivery Filter. We use the subsequent context gating to manipulate the vector transition after the multimodal fusion pooling. It is inspired by TIRG <ref type="bibr" target="#b16">[17]</ref> as an image-text composition technique that is successfully used for imagetext retrieval tasks. We add 3 dense projection layers with BatchNorm <ref type="bibr" target="#b8">[9]</ref> prior to the sigmoid gating function.</p><p>The Sweeping Filter. We use the channel-wise addition on target image encoding with visual-difference text query. As described in 2.1, we compute text feature and fusion with target expert features using <ref type="bibr" target="#b1">[2]</ref>. Then we obtain an adjusted feature after simple addition. The adjusted feature is summed up with the residual connection to target expert features.</p><p>External Dataset. We use fashion-200K <ref type="bibr" target="#b5">[6]</ref> and fashion-gen <ref type="bibr" target="#b14">[15]</ref> dataset as pretraining sources for Only-Resnet-Encoder model.</p><p>For fashion-gen dataset, we only use four category, SHIRTS, SWEATERS, TOPS, and DRESSES. Following fashion-200K post-processing strategy in <ref type="bibr" target="#b16">[17]</ref>, we create (source image, query text, target image) triplets from both fashion-200K and fashion-gen datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training and Evaluation</head><p>We use the additive margin softmax as our loss function <ref type="bibr" target="#b17">[18]</ref>. Each training batch consists of L triplets of (source image, query text, target image). We use batch shuffling in every training epoch. The total similarity between the source and target image combined with query text is obtained by the weighted sum of normalized dot products calculated by each expert. The weight for weighted sums can be learned from the text encoding. For data with no or only parts of six attributes, the weights for the missing attributes are set to 0 and renormailized so that they would not be used for the similarity calculation <ref type="bibr" target="#b10">[11]</ref>. We use the Adam <ref type="bibr" target="#b9">[10]</ref> optimizer. We set the Initial learning rate as lr = 0.0005 in our experiments, and use the exponential learning rate decay of 0.95 per step. For regularization, we apply batch normalization <ref type="bibr" target="#b8">[9]</ref> to every dense layer, and use dropout <ref type="bibr" target="#b15">[16]</ref> after dense layers. Our best model does not require any external data or pre-training other than Fashion IQ dataset. However, in order to improve ensemble performance, pretraining with external data was performed on variant models without attribute experts. In the pre-training phase, we train each pre-training dataset in 3 epochs. And then we fine-tune the model with the fashion-IQ training dataset.</p><p>In the test phase, we passed the sweeper filter for all possible target candidates. After that, calculating ranking using the distance with the Delivery Filter value</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We report the experimental results of CurlingNet model for the fashion-IQ challenge. The challenge provides an image retrieval dataset where the input query is specified in the form of a candidate image and two natural language expressions that describe the visual differences of the search target. The dataset contains 77,683 fashion images and 30,134 pair of relative captions. We strictly follow the evaluation protocols of the challenge.</p><p>Evaluation Metrics. Every participant is evaluated by the recall metrics on the test splits of the dataset. For each of the three fashion categories (dresses, tops, shirts), Re-call@10 and Recall@50 are computed on all test queries. The overall performance is evaluated based on the average of Recall@10 and Recall@50.</p><p>We defer more details and challenge rules to the challenge homepage 1 . <ref type="table" target="#tab_1">Table 1</ref>-2 summarize the quantitative results of our experiments. <ref type="table" target="#tab_1">Table 1</ref> show the public validation results on single (i.e. non-ensemble) models. They share the same training pipeline for fair comparison. For the Fashion-IQ <ref type="bibr" target="#b4">[5]</ref> challenge, we compare with the results on the test dataset in <ref type="table" target="#tab_2">Table 2</ref>, reported in the official evaluation server of Fashion-IQ as of the submission deadline (i.e. Sep 30th, 2019 UTC 23:59). We use the corresponding IDs and Team Name in the leaderboard to denote participants. In addition to the competitors in the leaderboard, we add a simple ablation baseline (Curling-concat), which conducts the simple fusion methods on a pair of source image and text query encoders. We also report the performance of state-of-the art models, FiLM <ref type="bibr" target="#b13">[14]</ref> and TIRG <ref type="bibr" target="#b16">[17]</ref> that exactly follow our setting of training environment, using the source codes provided by the original authors. <ref type="table" target="#tab_1">Table 1</ref> compares between different methods on the validation dataset. Our CurlingNet achieves the best retrieval performance with significant margins over the official baselines. The SUM baseline is based on the official start code, but shares the same image and text encoder setting with our model, trained with a triplet loss between (source image encoder + text encoder) and (target image encoder). FiLM and TiRG model follows the setting in <ref type="bibr" target="#b16">[17]</ref>. For Dual Encoder, we use their official code to train the model.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We proposed the CurlingNet model for learning differential relations between two image embeddings with respect to text query. The two key components of the model, Delivery and Sweeping filters, are easily adaptable in many deep metric learning tasks, including user adjustable retrieval or image recommendation systems. We demonstrated that our method significantly improved the performance of imagetext composition learning. Our method achieved the top performance in Fashion-IQ challenge, and outperformed many state-of-the-art models for image-text retrieval tasks. Moving forward, we plan to expand the applicability of the CurlingNet model; since our method is applicable to any visual-text pair data, we can explore other retrieval tasks on huge web data or multimodal conversational systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submission</head><p>Team Name </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The key intuition of the CurlingNet model. Given a triplet of (Source image, Language query, Target image), Our model effectively encodes the differential relationship between the two images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Schematics for the Deliverer filter and the Sweeper filter. The dense projection layer is omitted. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure</head><label></label><figDesc>Figure 4 illustrates qualitative results of our CurlingNet method. We display source images and text queries (Left) and the highest-scored retrieved images (Right) for some test examples. While maintaining the style of the source image, our model assigns a high score to the sample that well reflects the needs of the user query.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative examples of the image-text retrieval task in the Fashion-IQ dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>4 illustrates qualitative results of our CurlingNet method. We display source images and text queries (Left) and the highest-scored retrieved images (Right) for some test examples. While maintaining the style of the source image, our model assigns a high score to the sample that well reflects the needs of the user query. Performance comparison for the Fashion-IQ dataset validation split in terms fo Recall@10 and Recall@50 (higher is better). Note that our model does not use external datasets.</figDesc><table><row><cell></cell><cell>Category</cell><cell>Dress</cell><cell>Shirt</cell><cell>Toptee</cell><cell>Total</cell></row><row><cell></cell><cell>Metric</cell><cell cols="3">R@10 R@50 R@10 R@50 R@10 R@50</cell><cell>Avg</cell></row><row><cell></cell><cell>SUM</cell><cell cols="4">0.1120 0.2885 0.0873 0.2350 0.1086 0.2799 0.1852</cell></row><row><cell></cell><cell>FiLM [14]</cell><cell cols="4">0.1502 0.3748 0.1118 0.3076 0.1657 0.3941 0.2507</cell></row><row><cell></cell><cell>Dual Encoder</cell><cell cols="4">0.1720 0.4025 0.1398 0.3287 0.1815 0.3952 0.2699</cell></row><row><cell></cell><cell>TiRG [17]</cell><cell cols="4">0.1814 0.4253 0.1413 0.3415 0.1856 0.4171 0.2820</cell></row><row><cell></cell><cell cols="5">Only-Resnet-Encoder (Pretrain) 0.2419 0.4863 0.1761 0.4038 0.2345 0.5109 0.3423</cell></row><row><cell></cell><cell>Ours</cell><cell cols="4">0.2444 0.4769 0.1859 0.4057 0.2519 0.4966 0.3436</cell></row><row><cell></cell><cell>Ensemble</cell><cell cols="4">0.3302 0.6009 0.2591 0.5020 0.3391 0.6298 0.4435</cell></row><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>is blue dress</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>is dark blue and</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>solid</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(b)</cell><cell>is a lighter color</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>is white with a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>different graphic</cell><cell></cell><cell></cell><cell></cell></row><row><cell>(c)</cell><cell>is more revealing</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>and striped</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Is more revealing</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>and sexy</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison for final submission on TestPhase. We finished the second place in the challenge.</figDesc><table><row><cell>Avg R@(10,50)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://sites.google.com/view/lingir/ fashion-iq.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akira</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Huk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daylen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The fashion iq dataset: Retrieving images by combining side information and relative natural language feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yupeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic spatially-aware fashion concept discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phoenix</forename><forename type="middle">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Use what you have: Video retrieval using representations from collaborative experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learnable pooling with context gating for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fashion-Gen: The Generative Fashion Dataset and Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Stokowiec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<editor>ArXiv</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Composing text and image for image retrieval-an empirical odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving multilingual sentence embedding using bi-directional dual encoder with additive margin softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Hernandez Abrego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinlan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Strope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ray</forename><surname>Kurzweil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
