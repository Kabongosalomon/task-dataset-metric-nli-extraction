<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Answering Complex Open-domain Questions Through Iterative Query Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
							<email>pengqi@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department ? Symbolic Systems Program</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department ? Symbolic Systems Program</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Mehr</surname></persName>
							<email>leomehr@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department ? Symbolic Systems Program</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
							<email>zijwang@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department ? Symbolic Systems Program</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
							<email>manning@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science Department ? Symbolic Systems Program</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Answering Complex Open-domain Questions Through Iterative Query Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is challenging for current one-step retrieveand-read question answering (QA) systems to answer questions like "Which novel by the author of 'Armada' will be adapted as a feature film by Steven Spielberg?" because the question seldom contains retrievable clues about the missing entity (here, the author). Answering such a question requires multi-hop reasoning where one must gather information about the missing entity (or facts) to proceed with further reasoning. We present GOLDEN (Gold Entity) Retriever, which iterates between reading context and retrieving more supporting documents to answer open-domain multi-hop questions. Instead of using opaque and computationally expensive neural retrieval models, GOLDEN Retriever generates natural language search queries given the question and available context, and leverages off-the-shelf information retrieval systems to query for missing entities. This allows GOLDEN Retriever to scale up efficiently for open-domain multi-hop reasoning while maintaining interpretability. We evaluate GOLDEN Retriever on the recently proposed open-domain multi-hop QA dataset, HOTPOTQA, and demonstrate that it outperforms the best previously published model despite not using pretrained language models such as BERT. * Equal contribution, order decided by a random number generator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open-domain question answering (QA) is an important means for us to make use of knowledge in large text corpora and enables diverse queries without requiring a knowledge schema ahead of time. Enabling such systems to perform multistep inferencecan further expand our capability to explore the knowledge in these corpora (e.g., see <ref type="figure" target="#fig_0">Figure 1</ref>). Fueled by the recently proposed large-scale QA datasets such as SQuAD <ref type="bibr" target="#b3">(Rajpurkar et al., 2016</ref><ref type="bibr">(Rajpurkar et al., , 2018</ref> and <ref type="bibr">TriviaQA (Joshi et al., 2017)</ref>, much progress has been made in open-domain question answering. <ref type="bibr" target="#b1">Chen et al. (2017)</ref> proposed a twostage approach of retrieving relevant content with the question, then reading the paragraphs returned by the information retrieval (IR) component to arrive at the final answer. This "retrieve and read" approach has since been adopted and extended in various open-domain QA systems <ref type="bibr">(Nishida et al., 2018;</ref><ref type="bibr">Kratzwald and Feuerriegel, 2018)</ref>, but it is inherently limited to answering questions that do not require multi-hop/multi-step reasoning. This is because for many multi-hop questions, not all the relevant context can be obtained in a single retrieval step (e.g., "Ernest Cline" in <ref type="figure" target="#fig_0">Figure 1</ref>).</p><p>More recently, the emergence of multi-hop question answering datasets such as QAngaroo <ref type="bibr" target="#b7">(Welbl et al., 2018)</ref> and HOTPOTQA <ref type="bibr" target="#b8">(Yang et al., 2018)</ref> has sparked interest in multi-hop QA in the research community. Designed to be more challenging than SQuAD-like datasets, they feature questions that require context of more than one document to answer, testing QA systems' abilities to infer the answer in the presence of multiple pieces of evidence and to efficiently find the evidence in a large pool of candidate documents. However, since these datasets are still relatively new, most of the existing research focuses on the few-document setting where a relatively small set of context documents is given, which is guaranteed to contain the "gold" context documents, all those from which the answer comes <ref type="bibr">(De Cao et al., 2019;</ref><ref type="bibr" target="#b9">Zhong et al., 2019)</ref>.</p><p>In this paper, we present GOLDEN (Gold Entity) Retriever. Rather than relying purely on the original question to retrieve passages, the central innovation is that at each step the model also uses IR results from previous hops of reasoning to generate a new natural language query and retrieve new evidence to answer the original question. For the example in <ref type="figure" target="#fig_0">Figure 1</ref>, GOLDEN would first generate a query to retrieve Armada (novel) based on the question, then query for Ernest Cline based on newly gained knowledge in that article. This allows GOLDEN to leverage off-the-shelf, generalpurpose IR systems to scale open-domain multihop reasoning to millions of documents efficiently, and to do so in an interpretable manner. Combined with a QA module that extends BiDAF++ (Clark and Gardner, 2017), our final system outperforms the best previously published system on the open-domain (fullwiki) setting of HOTPOTQA without using powerful pretrained language models like <ref type="bibr">BERT (Devlin et al., 2019)</ref>.</p><p>The main contributions of this paper are: (a) a novel iterative retrieve-and-read framework capable of multi-hop reasoning in open-domain QA 1 (b) a natural language query generation approach that guarantees interpretability in the multi-hop evidence gathering process; (c) an efficient training procedure to enable query generation with minimal supervision signal that significantly boosts recall of gold supporting documents in retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Open-domain question answering (QA) Inspired by the series of TREC QA competitions, 2 <ref type="bibr" target="#b1">Chen et al. (2017)</ref> were among the first to adapt neural QA models to the open-domain setting.</p><p>They built a simple inverted index lookup with TF-IDF on the English Wikipedia, and used the question as the query to retrieve top 5 results for a reader model to produce answers with. Recent work on open-domain question answering largely follow this retrieve-and-read approach, and focus on improving the information retrieval component with question answering performance in consideration <ref type="bibr">(Nishida et al., 2018;</ref><ref type="bibr">Kratzwald and Feuerriegel, 2018;</ref><ref type="bibr">Nogueira et al., 2019)</ref>. However, these one-step retrieve-and-read approaches are fundamentally ill-equipped to address questions that require multi-hop reasoning, especially when necessary evidence is not readily retrievable with the question.</p><p>Multi-hop QA datasets QAngaroo <ref type="bibr" target="#b7">(Welbl et al., 2018)</ref> and HOTPOTQA <ref type="bibr" target="#b8">(Yang et al., 2018)</ref> are among the largest-scale multi-hop QA datasets to date. While the former is constructed around a knowledge base and the knowledge schema therein, the latter adopts a free-form question generation process in crowdsourcing and span-based evaluation. Both datasets feature a few-document setting where the gold supporting facts are provided along with a small set of distractors to ease the computational burden. However, researchers have shown that this sometimes results in gameable contexts, and thus does not always test the model's capability of multi-hop reasoning <ref type="bibr" target="#b2">(Chen and Durrett, 2019;</ref><ref type="bibr">Min et al., 2019a)</ref>. Therefore, in this work, we focus on the fullwiki setting of HOTPOTQA, which features a truly open-domain setting with more diverse questions.</p><p>Multi-hop QA systems At a broader level, the need for multi-step searches, query task decomposition, and subtask extraction has been clearly recognized in the IR community <ref type="bibr">(Hassan Awadallah et al., 2014;</ref><ref type="bibr">Mehrotra et al., 2016;</ref><ref type="bibr">Mehrotra and Yilmaz, 2017)</ref>, but multi-hop QA has only recently been studied closely with the release of large-scale datasets. Much research has focused on enabling multi-hop reasoning in question answering models in the few-document setting, e.g., by modeling entity graphs <ref type="bibr">(De Cao et al., 2019)</ref> or scoring answer candidates against the context <ref type="bibr" target="#b9">(Zhong et al., 2019)</ref>. These approaches, however, suffer from scalability issues when the number of supporting documents and/or answer candidates grow beyond a few dozen. <ref type="bibr">Ding et al. (2019)</ref> apply entity graph modeling to HOTPOTQA, where they expand a small entity graph starting from the question to arrive at the context for the QA model. However, centered around entity names, this model risks missing purely descriptive clues in the question. <ref type="bibr">Das et al. (2019)</ref> propose a neural retriever trained with distant supervision to bias towards paragraphs containing answers to the given questions, which is then used in a multi-step reader-reasoner framework. This does not fundamentally address the discoverability issue in opendomain multi-hop QA, however, because usually not all the evidence can be directly retrieved with the question. Besides, the neural retrieval model lacks explainability, which is crucial in real-world applications. <ref type="bibr" target="#b6">Talmor and Berant (2018)</ref>, instead, propose to answer multi-hop questions at scale by decomposing the question into sub-questions and perform iterative retrieval and question answering, which shares very similar motivations as our work. However, the questions studied in that work are based on logical forms of a fixed schema, which yields additional supervision for question decomposition but limits the diversity of questions. More recently, Min et al. (2019b) apply a similar idea to HOTPOTQA, but this approach similarly requires additional annotations for decomposition, and the authors did not apply it to iterative retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head><p>In this section, we formally define the problem of open-domain multi-hop question answering, and motivate the architecture of the proposed GOLDEN (Gold Entity) Retriever model. We then detail the query generation components as well as how to derive supervision signal for them, before concluding with the QA component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>We define the problem of open-domain multi-hop QA as one involving a question q, and S relevant (gold) supporting context documents d 1 , . . . , d S which contain the desired answer a 3 These supporting documents form a chain of reasoning necessary to arrive at the answer, and they come from a large corpus of documents D where |D| S. In this chain of reasoning, the supporting documents are usually connected via shared entities or textual similarities (e.g., they describe similar entities or events), but these connections do not necessarily conform to any predefined knowledge schema.</p><p>We contrast this to what we call the fewdocument setting of multi-hop QA, where the QA system is presented with a small set of documents D few-doc = {d 1 , . . . , d S , d 1 , . . . , d D }, where d 1 , . . . , d D comprise a small set of "distractor" documents that test whether the system is able to pick out the correct set of supporting documents in the presence of noise. This setting is suitable for testing QA systems' ability to perform multi-hop reasoning given the gold supporting documents with bounded computational budget, but we argue that it is far from a realistic one. In practice, an open-domain QA system has to locate all gold supporting documents from D on its own, and as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, this is often difficult for multi-hop questions based on the original question alone, as not all gold documents are easily retrievable given the question.</p><p>To address this gold context discoverability issue, we argue that it is necessary to move away from a single-hop retrieve-and-read approach where the original question serves as the search query. In the next section, we introduce GOLDEN Retriever, which addresses this problem by iterating between retrieving more documents and reading the context for multiple rounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Overview</head><p>Essentially, the challenge of open-domain multihop QA lies in the fact that the information need of the user (q ? a) cannot be readily satisfied by any information retrieval (IR) system that models merely the similarity between the question q and the documents. This is because the true information need will only unfold with progressive reasoning and discovery of supporting facts. Therefore, one cannot rely solely on a similarity-based IR system for such iterative reasoning, because the potential pool of relevant documents grows exponentially with the number of hops of reasoning.</p><p>To this end, we propose GOLDEN (Gold Entity) Retriever, which makes use of the gold document 4 information available in the QA dataset at training time to iteratively query for more relevant supporting documents during each hop of reasoning. Instead of relying on the original question as the search query to retrieve all supporting facts, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ready Player One</head><p>Which novel by the author of "Armada" will be adapted as a feature film by Steven Spielberg? or building computationally expensive search engines that are less interpretable to humans, we propose to leverage text-based IR engines for interpretability, and generate different search queries as each reasoning step unfolds. In the very first hop of reasoning, GOLDEN Retriever is presented the original question q, from which it generates a search query q 1 that retrieves supporting document d 1 5 Then for each of the subsequent reasoning steps (k = 2, . . . , S), GOLDEN Retriever generates a query q k from the question and the available context, (q, d 1 , . . . , d k?1 ). This formulation allows the model to generate queries based on information revealed in the supporting facts (see <ref type="figure" target="#fig_1">Figure 2</ref>, for example).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q Q Q</head><p>We note that GOLDEN Retriever is much more efficient, scalable, and interpretable at retrieving gold documents compared to its neural retrieval counterparts. This is because GOLDEN Retriever does not rely on a QA-specific IR engine tuned to a specific dataset, where adding new documents or question types into the index can be extremely inefficient. Further, GOLDEN Retriever generates queries in natural language, making it friendly to human interpretation and verification. One core challenge in GOLDEN Retriever, however, is to train query generation models in an efficient manner, because the search space for potential queries is enormous and off-the-shelf IR engines are not end-to-end differentiable. We outline our solution to this challenge in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Query Generation</head><p>For each reasoning step, we need to generate the search query given the original question q and 5 For notational simplicity, d k denotes the supporting document needed to complete the k-th step of reasoning. We also assume that the goal of each IR query is to retrieve one and only one gold supporting document in its top n results. some context of documents we have already retrieved (initially empty). This query generation problem is conceptually similar to the QA task in that they both map a question and some context to a target, only instead of an answer, the target here is a search query that helps retrieve the desired supporting document for the next reasoning step. Therefore, we formulate the query generation process as a question answering task.</p><p>To reduce the potentially large space of possible queries, we favor a QA model that extracts text spans from the context over one that generates free-form text as search queries. We therefore employ DrQA's Document Reader model <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>, which is a relatively light-weight recurrent neural network QA model that has demonstrated success in few-document QA. We adapt it to query generation as follows.</p><p>For each reasoning step k = 1, . . . , S, given a question q and some retrieval context C k which ideally contains the gold supporting documents d 1 , . . . , d k?1 , we aim to generate a search query q k that helps us retrieve d k for the next reasoning step. A Document Reader model is trained to select a span from C k as the query</p><formula xml:id="formula_0">q k = G k (q, C k ),</formula><p>where G k is the query generator. This query is then used to search for supporting documents, which are concatenated with the current retrieval context to update it</p><formula xml:id="formula_1">C k+1 = C k IR n (q k )</formula><p>where IR n (q k ) is the top n documents retrieved from the search engine using q k , and C 1 = q 6 At the end of the retrieval steps, we provide q as question along with C S as context to the final few- To train the query generators, we follow the steps above to construct the retrieval contexts, but during training time, when d k is not part of the IR result, we replace the lowest ranking document with d k before concatenating it with C k to make sure the downstream models have access to necessary context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Deriving Supervision Signal for Query Generation</head><p>When deriving supervision signal to train our query generators, the potential search space is enormous for each step of reasoning even if we constrain ourselves to predicting spans from the context. This is aggravated by multiple hops of reasoning required by the question. One solution to this issue is to train the query generators with reinforcement learning (RL) techniques (e.g., RE-INFORCE <ref type="bibr" target="#b5">(Sutton et al., 2000)</ref>), where (Nogueira and Cho, 2017) and <ref type="bibr" target="#b0">(Buck et al., 2018)</ref> are examples of one-step query generation with RL. However, it is computationally inefficient, and has high variance especially for the second reasoning step and forward, because the context depends greatly on what queries have been chosen previously and their search results. Instead, we propose to leverage the limited supervision we have about the gold supporting documents d 1 , . . . , d S to narrow down the search space.</p><p>The key insight we base our approach on is that at any step of open-domain multi-hop reasoning, there is some semantic overlap between the retrieval context and the next document(s) we wish to retrieve. For instance, in our Armada example, when the retrieval context contains only the question, this overlap is the novel itself; after we have expanded the retrieval context, this overlap becomes the name of the author, Ernest Cline. Finding this semantic overlap between the retrieval context and the desired documents not only reveals the chain of reasoning naturally, but also allows us to use it as the search query for retrieval.</p><p>Because off-the-shelf IR systems generally optimize for shallow lexical similarity between query and candidate documents in favor of efficiency, a good proxy for this overlap is locating spans of text that have high lexical overlap with the intended supporting documents. To this end, we propose a simple yet effective solution, employing several heuristics to generate candidate queries: computing the longest common string/sequence between the current retrieval context and the title/text of the intended paragraph ignoring stop words, then taking the contiguous span of text that corresponds to this overlap in the retrieval context. This allows us to not only make use of entity names, but also textual descriptions that better lead to the gold entities. It is also more generally applicable than question decomposition approaches <ref type="bibr" target="#b6">(Talmor and Berant, 2018;</ref><ref type="bibr">Min et al., 2019b)</ref>, and does not require additional annotation for decomposition.</p><p>Applying various heuristics results in a handful of candidate queries for each document, and we use our IR engine (detailed next) to rank them based on recall of the intended supporting document to choose one as the final oracle query we train our query generators to predict. This allows us to train the query generators in a fully supervised manner efficiently. Some examples of oracle queries on the HOTPOTQA dev set can be found in <ref type="table" target="#tab_1">Table 1</ref>. We refer the reader to Appendix B for more technical details about our heuristics and how the oracle queries are derived.</p><p>Oracle Query vs Single-hop Query We evaluate the oracle query against the single-hop query, i.e., querying with the original question, on the HOTPOTQA dev set. Specifically, we compare the recall of gold paragraphs, because the greater the recall, the fewer documents we need to pass into the expensive neural multi-hop QA component. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-hop d1</head><p>Single-hop d2 Oracle d1</p><p>Oracle d2 <ref type="figure">Figure 3</ref>: Recall comparison between single-hop queries and GOLDEN Retriever oracle queries for both supporting paragraphs on the HOTPOTQA dev set. Note that the oracle queries are much more effective than the original question (single-hop query) at retrieving target paragraphs in both hops.</p><p>We index the English Wikipedia dump with introductory paragraphs provided by the HOT-POTQA authors 7 with Elasticsearch 6.7 (Gormley and Tong, 2015), where we index the titles and document text in separate fields with bigram indexing enabled. This results in an index with 5,233,329 total documents. At retrieval time, we boost the scores of any search result whose title matches the search query better -this results in a better recall for entities with common names (e.g., "Armada" the novel). For more details about how the IR engine is set up and the effect of score boosting, please refer to Appendix A.</p><p>In <ref type="figure">Figure 3</ref>, we compare the recall of the two gold paragraphs required for each question in HOTPOTQA at various number of documents retrieved (R@n) for the single-hop query and the multi-hop queries generated from the oracle. Note that the oracle queries are much more effective at retrieving the gold paragraphs than the original question in both hops. For instance, if we combine R@5 of both oracles (which effectively retrieves 10 documents from two queries) and compare that to R@10 for the single-hop query, the oracle queries improve recall for d 1 by 6.68%, and that for d 2 by a significant margin of 49.09% 8 This 7 https://hotpotqa.github.io/ wiki-readme.html 8 Since HOTPOTQA does not provide the logical order its gold entities should be discovered, we simply call the document d1 which is more easily retrievable with the queries, and the other as d2. means that the final QA model will need to consider far fewer documents to arrive at a decent set of supporting facts that lead to the answer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Question Answering Component</head><p>The final QA component of GOLDEN Retriever is based on the baseline model presented in <ref type="bibr" target="#b8">(Yang et al., 2018)</ref>, which is in turn based on BiDAF++ (Clark and Gardner, 2017). We make two major changes to this model. <ref type="bibr" target="#b8">Yang et al. (2018)</ref> concatenated all context paragraphs into one long string to predict span begin and end offsets for the answer, which is potentially sensitive to the order in which these paragraphs are presented to the model. We instead process them separately with shared encoder RNN parameters to obtain paragraph order-insensitive representations for each paragraph. Span offset scores are predicted from each paragraph independently before finally aggregated and normalized with a global softmax operation to produce probabilities over spans. The second change is that we replace all attention mechanism in the original model with self attention layers over the concatenated question and context. To differentiate context paragraph representations from question representations in this self-attention mechanism, we indicate question and context tokens by concatenating a 0/1 feature at the input layer. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the QA model architecture.  <ref type="table">Table 2</ref>: End-to-end QA performance of baselines and our GOLDEN Retriever model on the HOTPOTQA fullwiki test set. Among systems that were not published at the time of submission of this paper, "BERT pip." was submitted to the official HOTPOTQA leaderboard on May 15 th (thus contemporaneous), while "Entity-centric BERT Pipeline" and "PR-Bert" were submitted after the paper submission deadline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data and Setup</head><p>We evaluate our models in the fullwiki setting of HOTPOTQA <ref type="bibr" target="#b8">(Yang et al., 2018)</ref>. HOTPOTQA is a question answering dataset collected on the English Wikipedia, containing about 113k crowdsourced questions that are constructed to require the introduction paragraphs of two Wikipedia articles to answer. Each question in the dataset comes with the two gold paragraphs, as well as a list of sentences in these paragraphs that crowdworkers identify as supporting facts necessary to answer the question. A diverse range of reasoning strategies are featured in HOTPOTQA, including questions involving missing entities in the question (our Armada example), intersection questions (What satisfies property A and property B?), and comparison questions, where two entities are compared by a common attribute, among others. In the few-document distractor setting, the QA models are given ten paragraphs in which the gold paragraphs are guaranteed to be found; in the opendomain fullwiki setting, which we focus on, the models are only given the question and the entire Wikipedia. Models are evaluated on their answer accuracy and explainability, where the former is measured as overlap between the predicted and gold answers with exact match (EM) and unigram F 1 , and the latter concerns how well the predicted supporting fact sentences match human annotation (Supporting Fact EM/F 1 ). A joint metric is also reported on this dataset, which encourages systems to perform well on both tasks simultaneously.</p><p>We use the Stanford CoreNLP toolkit <ref type="bibr">(Manning et al., 2014)</ref> to preprocess Wikipedia, as well as to generate POS/NER features for the query generators, following <ref type="bibr" target="#b8">(Yang et al., 2018)</ref> and <ref type="bibr" target="#b1">(Chen et al., 2017)</ref>. We always detokenize a generated search query before sending it to Elasticsearch, which has its own preprocessing pipeline. Since all questions in HOTPOTQA require exactly two supporting documents, we fix the number of retrieval steps of GOLDEN Retriever to S = 2. To accommodate arbitrary steps of reasoning in GOLDEN Retriever, a stopping criterion is required to determine when to stop retrieving for more supporting documents and perform few-document question answering, which we leave to future work. During training and evaluation, we set the number of retrieved documents added to the retrieval context to 5 for each retrieval step, so that the total number of paragraphs our final QA model considers is 10, for a fair comparison to <ref type="bibr" target="#b8">(Yang et al., 2018)</ref>.</p><p>We include hyperparameters and training details in Appendix C for reproducibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">End-to-end Question Answering</head><p>We compare the end-to-end performance of GOLDEN Retriever against several QA systems on the HOTPOTQA dataset: (1) the baseline presented in <ref type="bibr" target="#b8">(Yang et al., 2018)</ref>, (2) CogQA <ref type="bibr">(Ding et al., 2019)</ref>, the top-performing previously published system, and (3) other high-ranking systems on the leaderboard. As shown in <ref type="table">Table 2</ref>, GOLDEN Retriever is much better at locating the correct supporting facts from Wikipedia compared  <ref type="table">Table 3</ref>: Question answering and IR performance amongst different IR settings on the dev set. We observe that although improving the IR engine is helpful, most of the performance gain results from the iterative retrieve-and-read strategy of GOLDEN Retriever. (*: for GOLDEN Retriever, the 10 paragraphs are combined from both hops, 5 from each hop.)</p><p>to CogQA, as well as most of the top-ranking systems. However, the QA performance is handicapped because we do not make use of pretrained contextualization models (e.g., BERT) that these systems use. We expect a boost in QA performance from adopting these more powerful question answering models, especially ones that are tailored to perform few-document multi-hop reasoning. We leave this to future work.</p><p>To understand the contribution of GOLDEN Retriever's iterative retrieval process compared to that of the IR engine, we compare the performance of GOLDEN Retriever against two baseline systems on the dev set: one that retrieves 10 supporting paragraphs from Elasticsearch with the original question, and one that uses the IR engine presented in HOTPOTQA 9 In all cases, we use the QA component in GOLDEN Retriever for the final question answering step. As shown in <ref type="table">Table 3</ref>, replacing the hand-engineered IR engine in <ref type="bibr" target="#b8">(Yang et al., 2018)</ref> with Elasticsearch does result in some gain in recall of the gold documents, but that does not translate to a significant improvement in QA performance. Further inspection reveals that despite Elasticsearch improving overall recall of gold documents, it is only able to retrieve both gold documents for 36.91% of the dev set questions, in comparison to 28.21% from the IR engine in <ref type="bibr" target="#b8">(Yang et al., 2018)</ref>. In contrast, GOLDEN Retriever improves this percentage to 61.01%, almost doubling the recall over the single-hop baseline, providing the QA component a much better set of context documents to predict answers from.</p><p>Lastly, we perform an ablation study in which we replace our query generator models with our query oracles and observe the effect on end-to-end  performance. As can be seen in <ref type="table" target="#tab_5">Table 4</ref>, replacing G 1 with the oracle only slightly improves end-toend performance, but further substituting G 2 with the oracle yields a significant improvement. This illustrates that the performance loss is largely attributed to G 2 rather than G 1 , because G 2 solves a harder span selection problem from a longer retrieval context. In the next section, we examine the query generation models more closely by evaluating their performance without the QA component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of Query Generation</head><p>To evaluate the query generators, we begin by determining how well they emulate the oracles. We evaluate them using Exact Match (EM) and F 1 on the span prediction task, as well as compare their queries' retrieval performance against the oracle queries. As can be seen in <ref type="table" target="#tab_8">Table 6</ref>, the performance of G 2 is worse than that of G 1 in general, confirming our findings on the end-to-end pipeline. When we combine them into a pipeline, the generated queries perform only slightly better on d 1 when a total of 10 documents are retrieved (89.91% vs 87.85%), but are significantly more effective for d 2 (61.01% vs 36.91%). If we further zoom in on the retrieval performance on noncomparison questions for which finding the two entities involved is less trivial, we can see that the recall on d 2 improves from 27.88% to 53.23%, almost doubling the number of questions we have the complete gold context to answer. We note that the IR performance we report on the full pipeline is different to that when we evaluate the query generators separately. We attribute this difference to the fact that the generated queries sometimes retrieve both gold documents in one step. To better understand model behavior, we also randomly sampled some examples from the dev set to compare the oracle queries and the predicted queries. Aside from exact matches, we find that the predicted queries are usually small variations of the oracle ones. In some cases, the model selects spans that are more natural and informative   (Example (1) in <ref type="table" target="#tab_7">Table 5</ref>). When they differ a bit more, the model is usually overly biased towards shorter entity spans and misses out on informative information (Example <ref type="formula">(2)</ref>). When there are multiple entities in the retrieval context, the model sometimes selects the wrong entity, which suggests that a more powerful query generator might be desirable (Example <ref type="formula">(3)</ref>). Despite these issues, we find that these natural language queries make the reasoning process more interpretable, and easier for a human to verify or intervene as needed.</p><p>Limitations Although we have demonstrated that generating search queries with span selection works in most cases, it also limits the kinds of queries we can generate, and in some cases leads to undesired behavior. One common issue is that the entity of interest has a name shared by too many Wikipedia pages (e.g., "House Rules" the 2003 TV series). This sometimes results in the inclusion of extra terms in our oracle query to expand it (e.g., Example (4) specifies that "The Harmon" is a highrise building). In other cases, our span oracle makes use of too much information from the gold entities (Example <ref type="formula">(2)</ref>, where a human would likely query for "Eiffel 65 song released 15 January 1999" because "Blue" is not the only song mentioned in d 1 ). We argue, though, these are due to the simplifying choice of span selection for query generation and fixed number of query steps. We leave extensions to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we presented GOLDEN (Gold Entity) Retriever, an open-domain multi-hop question answering system for scalable multi-hop reasoning. Through iterative reasoning and retrieval, GOLDEN Retriever greatly improves the recall of gold supporting facts, thus providing the question answering model a much better set of context documents to produce an answer from, and demonstrates competitive performance to the state of the art. Generating natural languages queries for each step of reasoning, GOLDEN Retriever is also more interpretable to humans compared to previous neural retrieval approaches and affords better understanding and verification of model behavior.  multiply the document score by a heuristic constant between 1.05 and 1.5, depending on how well the document title matches the query, before reranking all search results. This results in a significant improvement in these cases. For the query "George W. Bush", the page for the former US president is ranked at the top after reranking. In <ref type="table" target="#tab_10">Table 7</ref>, we also provide results from the singlehop query to show the improvement from title score boosting introduced from the previous section and reranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Oracle Query Generation</head><p>We mainly employ three heuristics to find the semantic overlap between the retrieval context and the desired documents: longest common subsequence (LCS), longest common substring (LC-SubStr), and overlap merging which generalizes the two. Specifically, the overlap merging heuristic looks for contiguous spans in the retrieval context that have high rates of overlapping tokens with the desired document, determined by the total number of overlapping tokens divided by the total number of tokens considered in the span. In all heuristics, we ignore stop words and lowercase the rest in computing the spans to capture more meaningful overlaps, and finally take the span in the retrieval context that all the overlapping words are contained in. For instance, if the retrieval context contains "the GOLDEN Retriever model on HOTPOTQA" and the desired document contains "GOLDEN Retriever on the HOTPOTQA dataset", we will identify the overlapping terms as "GOLDEN", "Retriever", and "HOTPOTQA", and return the span "GOLDEN Retriever model on HOTPOTQA" as the resulting candidate query.</p><p>To generate candidates for the oracle query, we apply the heuristics between combinations of {cleaned question, cleaned question without punc-tuation} ? {cleaned document title, cleaned para-graph}, where cleaning means stop word removal and lowercasing. Once oracle queries are gener-  ated, we launch these queries against Elasticsearch to determine the rank of the desired paragraph. If multiple candidate queries are able to place the desired paragraph in the top 5 results, we further rank the candidate queries by other metrics (e.g., length of the query) to arrive at the final oracle query to train the query generators. We refer the reader to our released code for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Training Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Query Generators</head><p>Once the oracle queries are generated, we train our query generators to emulate them on the training set, and choose the best model with F 1 in span selection on the dev set. We experiment with hyperparameters such as learning rate, training epochs, batch size, number of word embeddings to finetune, among others, and report the final hyperparameters for both query generators (G 1 and G 2 ) in <ref type="table" target="#tab_12">Table 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Question Answering Model</head><p>Our final question answering component is trained with the paragraphs produced by the oracle queries (5 from each hop, 10 in total), with d 1 and d 2 inserted to replace the lowest ranking paragraph in each hop if they are not in the set already. We develop our model based on the baseline model of <ref type="bibr" target="#b8">Yang et al. (2018)</ref>, and reuse the same default hyperparameters whenever possible. The main differences in the hyperparameters are: we optimize our QA model with Adam (with default hyperparameters) (Kingma and Ba, 2015) instead of stochastic gradient descent with a larger batch size of 64; we anneal the learning rate by 0.5 with a patience of 3 instead of 1, that is, we multiply the learning rate by 0.5 after three consecutive failures to improve dev F 1 ; we clip the gradient down to a maximum 2 norm of 5; we apply a 10% dropout to the model, for which we have increased the hidden size to 128; and use 10 as the coefficient by which we multiply the supporting facts loss, before mixing it with the span prediction loss. We configure the model to read 10 context paragraphs, and limit each paragraph to at most 400 tokens including the title.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of an open-domain multi-hop question from the HOTPOTQA dev set, where "Ernest Cline" is the missing entity. Note from the search results that it cannot be easily retrieved based on merely the question. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model overview of GOLDEN Retriever. Given an open-domain multi-hop question, the model iteratively retrieves more context documents, and concatenates all retrieved context for a QA model to answer from.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Question answering component in GOLDEN Retriever. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>What government position was held by the woman who portrayed Corliss Archer in the film Kiss and Tell?Corliss Archer in the film Kiss and TellShirley TempleScott Parkin has been a vocal critic of Exxonmobil and another corporation that has operations in how many countries? Example oracle queries on the HOTPOTQA dev set. document QA component detailed in Section 3.5 to obtain the final answer to the original question.</figDesc><table><row><cell>Question</cell><cell>Hop 1 Oracle</cell><cell>Hop 2 Oracle</cell></row><row><cell></cell><cell>Scott Parkin</cell><cell>Halliburton</cell></row><row><cell cols="2">Are Giuseppe Verdi and Ambroise Thomas both Opera composers? Giuseppe Verdi</cell><cell>Ambroise Thomas</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Feldman and El-Yaniv, 2019) 30.61 40.26 16.65 47.33 10.85 27.01</figDesc><table><row><cell>System</cell><cell>Answer EM F 1</cell><cell>Sup Fact EM F 1</cell><cell>Joint EM</cell><cell>F 1</cell></row><row><cell>Baseline (Yang et al., 2018)</cell><cell>25.23 34.40</cell><cell>5.07 40.69</cell><cell cols="2">2.63 17.85</cell></row><row><cell>GRN + BERT</cell><cell cols="2">29.87 39.14 13.16 49.67</cell><cell cols="2">8.26 25.84</cell></row><row><cell>MUPPET (CogQA (Ding et al., 2019)</cell><cell cols="4">37.12 48.87 22.82 57.69 12.42 34.92</cell></row><row><cell>PR-Bert</cell><cell cols="4">43.33 53.79 21.90 59.63 14.50 39.11</cell></row><row><cell>Entity-centric BERT Pipeline</cell><cell cols="4">41.82 53.09 26.26 57.29 17.01 39.18</cell></row><row><cell>BERT pip. (contemporaneous)</cell><cell cols="4">45.32 57.34 38.67 70.83 25.14 47.60</cell></row><row><cell>GOLDEN Retriever</cell><cell cols="4">37.92 48.58 30.69 64.24 18.04 39.13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Pipeline ablative analysis of GOLDEN Retriever end-to-end QA performance by replacing each query generator with a query oracle.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Examples of predicted queries from the query generators on the HOTPOTQA dev set. The oracle query is displayed in blue in parentheses if it differs from the predicted one.</figDesc><table><row><cell>Model</cell><cell>Span EM</cell><cell>F 1</cell><cell>R@5</cell></row><row><cell>G 1</cell><cell cols="3">51.40 78.75 85.86</cell></row><row><cell>G 2</cell><cell cols="3">52.29 63.07 64.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc></figDesc><table><row><cell>: Span prediction and IR performance of the</cell></row><row><cell>query generator models for Hop 1 (G 1 ) and Hop 2 (G 2 )</cell></row><row><cell>evaluated separately on the HOTPOTQA dev set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>IR performance (recall in percentages) of various Elasticsearch setups on the HOTPOTQA dev set using the original question.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>? 10 ?4 , 1 ? 10 ?3 Finetune embeddings 0, 200, 500, 1000 Epoch 25, 40 Batch size 32, 64, 128 Hidden size 64, 128, 256, 512, 768 Max sequence length 15, 20, 30, 50, 100 Dropout rate 0.3, 0.35, 0.4, 0.45</figDesc><table><row><cell>Hyperparameter</cell><cell>Values</cell></row><row><cell>Learning rate</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Hyperparameter settings for the query generators. The final hyperparameters for the Hop 1 query generator are shown in bold, and those for the Hop 2 query generator are shown in underlined itallic.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code and pretrained models available at https:// github.com/qipeng/golden-retriever 2 http://trec.nist.gov/data/qamain.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In this work, we only consider extractive, or spanbased, QA tasks, but the problem statement and the proposed method apply to generative QA tasks as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In HOTPOTQA, documents usually describe entities, thus we use "documents" and "entities" interchangeably.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In the query result, the title of each document is delimited with special tokens &lt;t&gt; and &lt;/t&gt; before concatenation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For the latter, we use the fullwiki test input file the authors released, which contains the top-10 IR output from that retrieval system with the question as the query.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="50">candidate documents for each query for consideration, and boost the query scores of documents whose title exactly matches the search query, or is a substring of the search query. Specifically, we 10 https://hotpotqa.github.io/ wiki-readme.html 11 https://www.elastic.co/guide/ en/elasticsearch/reference/6.7/ analysis-shingle-tokenfilter.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Robin Jia among other members of the Stanford NLP Group, as well as the anonymous reviewers for discussions and comments on earlier versions of this paper. Peng Qi would also like to thank Suprita Shankar, Jamil Dhanani, and Suma Kasa for early experiments on BiDAF++ variants for HOTPOTQA. This research is funded in part by Samsung Electronics Co., Ltd. and in part by the SAIL-JD Research Initiative.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Elasticsearch Setup</head><p>A.1 Setting Up the Index</p><p>We start from the Wikipedia dump file containing the introductory paragraphs used in HOTPOTQA that <ref type="bibr" target="#b8">Yang et al. (2018)</ref> provide, 10 and add the fields corresponding to Wikipedia page titles and the introductory paragraphs (text) into the index.</p><p>For the title, we use Elasticsearch's simple analyzer which performs basic tokenization and lowercasing of the content. For the text, we join all the sentences and use the standard analyzer which further allows for removal of punctuation and stop words. For both fields, we index an auxiliary field with bigrams using the shingle filter, 11 and perform basic asciifolding to map non ASCII characters to a similar ASCII character (e.g., "?"? "e").</p><p>At search time, we launch a multi match query against all fields with the same query, which performs a full-text query employing the BM25 ranking function <ref type="bibr" target="#b4">(Robertson et al., 1995)</ref> with all fields in the index, and returns the score of the best field for ranking by default. To promote documents whose title match the search query, we boost the search score of all title-related fields by 1.25 in this query.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Reranking Search Results</head><p>In Wikipedia, it is common that pages or entity names share rare words that are important to search engines, and a naive full-text search IR system will not be able to pick the one that matches the query the best. For instance, if one set up Elasticsearch according to the instructions above and searched for "George W. Bush", he/she would be surprised to see that the actual page is not even in the top-10 search results, which contains entities such as "George W. Bush Childhood Home" and "Bibliography of George W. Bush".</p><p>To this end, we propose to rerank these query results with a simple but effective heuristic that alleviates this issue. We would first retrieve at least</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ask the right questions: Active question reformulation with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Buck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jannis</forename><surname>Bulian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Gajewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Gesmundo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Reading Wikipedia to answer opendomain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding dataset design choices for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micheline</forename><forename type="middle">M</forename><surname>Hancock-Beaulieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Gatford</surname></persName>
		</author>
		<title level="m">Okapi at trec-3. Nist Special Publication Sp</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page">109</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</title>
		<meeting>the Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents. Transactions of the Association of Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Coarse-grain fine-grain coattention network for multi-evidence question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
