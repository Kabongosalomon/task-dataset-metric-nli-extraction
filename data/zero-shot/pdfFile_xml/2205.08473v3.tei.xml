<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COLONFORMER: AN EFFICIENT TRANSFORMER BASED METHOD FOR COLON POLYP SEGMENTATION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-06-08">June 8, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">Thanh</forename><surname>Duc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">Thi</forename><surname>Oanh</surname></persName>
							<email>oanhnt@soict.hust.edu.vn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><forename type="middle">Thi</forename><surname>Thuy</surname></persName>
							<email>ntthuy@vnua.edu.vn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Minh</forename><surname>Triet</surname></persName>
							<email>tmtriet@fit.hcmus.edu.vn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dinh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
							<email>sangdv@soict.hust.edu.vn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information and Communication Technology</orgName>
								<orgName type="institution">Hanoi University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Information and Communication Technology Hanoi University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Faculty of Infomation Technology</orgName>
								<orgName type="institution">National University of Agriculture</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country key="VN">Vietnam</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">National University</orgName>
								<address>
									<settlement>Ho Chi Minh City</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">School of Information and Communication Technology Hanoi University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COLONFORMER: AN EFFICIENT TRANSFORMER BASED METHOD FOR COLON POLYP SEGMENTATION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-06-08">June 8, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic segmentation ? Deep learning ? Encoder-decoder network ? Polyp segmentation ? Colonoscopy</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Identifying polyps is challenging for automatic analysis of endoscopic images in computer-aided clinical support systems. Models based on convolutional networks (CNN), transformers, and their combinations have been proposed to segment polyps with promising results. However, those approaches have limitations either in modeling the local appearance of the polyps only or lack of multi-level feature representation for spatial dependency in the decoding process. This paper proposes a novel network, namely ColonFormer, to address these limitations. ColonFormer is an encoder-decoder architecture capable of modeling long-range semantic information at both encoder and decoder branches. The encoder is a lightweight architecture based on transformers for modeling global semantic relations at multi scales. The decoder is a hierarchical network structure designed for learning multi-level features to enrich feature representation. Besides, a refinement module is added with a new skip connection technique to refine the boundary of polyp objects in the global map for accurate segmentation. Extensive experiments have been conducted on five popular benchmark datasets for polyp segmentation, including Kvasir, CVC-Clinic DB, CVC-ColonDB, CVC-T, and ETIS-Larib. Experimental results show that our ColonFormer outperforms other state-of-the-art methods on all benchmark datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Colorectal cancer (CRC) is among the most common types of cancer worldwide, causing over 694,000 fatalities each year <ref type="bibr" target="#b0">[1]</ref>. The most common cause of CRC is colon polyps, particularly adenomas with high-grade dysplasia. According to a longitudinal study <ref type="bibr" target="#b1">[2]</ref>, every 1% increase in adenoma detection rate is linked to a 3% reduction in the risk of colon cancer. As a result, detecting and removing polyps at an early stage is critical for cancer prevention and treatment. Therefore, colonoscopy is regarded as the standard tool for detecting colon adenomas and colorectal cancer. In practice, overburdened healthcare systems, particularly in low-resource settings, may result in shorter endoscopy durations and more missed polyps. According to a literature review, the proportion of colon polyps missing during endoscopies could range from 20 to 47 percent <ref type="bibr" target="#b2">[3]</ref>. This may lead to high associated risk factors in patients. Therefore, research in developing computer-aided tools to assist endoscopists in endoscopy procedures is an essential need.</p><p>Advancements in artificial intelligence and deep learning have changed the landscape of such systems. Attempts have been made to develop learning algorithms to deploy in computer-aided diagnostic (CAD) systems for the automatic detection and prediction of polyps, which could benefit clinicians in detecting lesions and lower the miss detection rate <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Deep neural networks have shown great potential in assisting colon polyp detection in several retrospective investigations and diagnoses. A CAD system can support endoscopists in improving lesion detection rates, optimizing strategies during endoscopy for high-risk lesions, and increasing clinics' capacity while preserving diagnostic quality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Despite progress in machine learning and computer vision research, automatic polyp segmentation remains a challenging problem. Polyps are caused by abnormal cell growth in the human colon, meaning their appearances have strong relationships with the surroundings. Images of polyps come in various shapes, sizes, textures, and colors. In addition, the boundary between polyps and their surrounding mucosa is not always apparent during colonoscopy, especially in different lighting modes and in cases of flat lesions or unclean bowel preparation. These cause a lot of uncertainty for the learning models for polyp segmentation.</p><p>In recent years, the most widely used methods for image segmentation in general and polyp segmentation, in particular, are based on Convolutional Neural Networks (CNNs). Most segmentation models use a UNet-based architecture containing an encoder and a decoder, which are often built up from convolutional layers. Despite being widely used for segmentation tasks with impressive performance, CNNs pose certain limitations: They can only capture local information while ignoring spatial context and global information due to the limited receptive field. Furthermore, it was shown that CNNs act like a series of high-pass filters and favor high-frequency information.</p><p>Transformer <ref type="bibr" target="#b8">[9]</ref> is a recently proposed deep neural network architecture that models the global interactions among input components using attention mechanisms. While initially designed to tackle natural language and speech processing problems, Transformers have significantly impacted computer vision in recent years. In contrast to CNNs, self-attention layers in Transformers work as low-pass filters, and they can effectively capture long-range dependency. Therefore, combining the strengths of convolutional and self-attention layers can increase the representation power of deep networks. Very recently, there has been fast-growing interest in using Transformers for semantic image segmentation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>. These methods use well-known encoder-decoder architectures wherein Transformers and CNNs are combined in various settings. The works in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> proposed Transformer-CNN architectures, in which a Transformer is used as an encoder, and a traditional CNN is used as a decoder. The hybrid architecture of Transformers and CNN has been proposed in <ref type="bibr" target="#b12">[13]</ref>, in which the decoder is a traditional CNN or a Transformer, while the encoder is a combination of CNN and Transformer layers.</p><p>Inspired by these approaches for modeling multi-scale and multi-level features, we propose a new Transformers-based network called ColonFormer. The main design of our ColonFormeralso contains a transformer encoder and a CNN decoder, but our approach is different from the models mentioned above in several ways. In ColonFormer, the encoder is a hierarchically structured lightweight Transformer for learning multi-scale features. The decoder is a hierarchical pyramid structure with the capability of learning from heterogeneous data containing feature maps extracted from encoder blocks at different scales and subregions. Besides, a refinement module is proposed for further improving the segmentation accuracy on hard regions and small polyps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Our main contributions include:</head><p>? A novel deep neural network, namely ColonFormer, that integrates a hierarchical Transformer and a hierarchical pyramid CNN in a unified architecture for efficient and accurate polyp segmentation;</p><p>? An improved refinement technique using a newly proposed residual axial attention module for feature fusion and smoothing aiming at improving the segmentation accuracy;</p><p>? A set of experiments on five standard benchmark datasets for polyp segmentation (Kvasir, CVC-Clinic DB, CVC-ColonDB, CVC-T, and ETIS-Larib) and comparisons of the effectiveness of ColonFormerto current state-of-the-art methods.</p><p>The rest of the paper is organized as follows. We provide a brief review of related works in Section 2. The Colon-Formerarchitecture is described in Section 3. Section 4 presents our experiments and results. Finally, we conclude the paper and highlight future works in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 1:</head><p>The overall architecture of our ColonFormercontains three components: an encoder, a decoder, and a refinement module. The encoder is based on the Mix Transformer backbone. The decoder starts with a pyramid pooling module (PPM), where its outputs are combined layer-wise with the output feature maps of the encoder at multi levels to produce a global map. The refinement module aims to gradually refine the boundary of the global map to yield the final accurate segmentation. Besides this predicted output, the global map and two intermediate maps are also passed into the training loss in a deep supervision manner. Before calculating the training loss, all refined maps are upsampled back to the original image input size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we briefly review common methods and techniques that have been developed for polyp segmentation. We review CNN architectures and their variants, especially UNet models, in medical image segmentation. Then we investigate the attention mechanism as a promising technique that boosts the capability of a deep neural network in learning feature representation. Finally, vision Transformer and its applications in polyp segmentation and medical image processing are investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Convolutional Neural Networks</head><p>CNNs are one of the most widely used deep neural network architectures, especially in computer vision. A deep CNN extracts features on multiple layers with increasing levels of abstraction. Low-level features with high resolutions represent spatial details, while high-level features with low resolutions represent rich semantic information. CNNs are especially powerful in image analysis as they can extract highly informative and valuable features.</p><p>UNet <ref type="bibr" target="#b13">[14]</ref> is a pioneering CNN architecture for medical image segmentation. UNet consists of an encoder and a decoder. The encoder includes convolutional, pooling layers for feature extraction, and the decoder uses upsampling (or deconvolutional) and convolutional layers for yielding the final segmentation prediction. Later works attempted to improve UNet by introducing skip connections, which alleviate information loss caused by stacking multiple convolutional layers. However, retaining information from low-level may introduce noisy signals that degrade the performance. UNet variants such as UNet++ <ref type="bibr" target="#b14">[15]</ref> and DoubleUnet <ref type="bibr" target="#b15">[16]</ref> have achieved stellar results on segmentation benchmarks. UNet++ is constructed as an ensemble of nested UNets of varying depths, which partially share an encoder and jointly learn using deep supervision. DoubleUNet stacks two UNet blocks and uses ASPP <ref type="bibr" target="#b16">[17]</ref>, and SE blocks <ref type="bibr" target="#b17">[18]</ref> to enhance the feature representation.</p><p>UNet encoders often use an existing pretrained architecture, also known as the backbone. Widely used backbones include VGG <ref type="bibr" target="#b18">[19]</ref>, MobileNet <ref type="bibr" target="#b19">[20]</ref>, ResNet <ref type="bibr" target="#b20">[21]</ref>, DenseNet <ref type="bibr" target="#b21">[22]</ref>, etc. PraNet <ref type="bibr" target="#b22">[23]</ref> uses Res2Net as the backbone, while AG-CuResNeSt <ref type="bibr" target="#b23">[24]</ref> uses ResNeSt. Meanwhile HarDNet-MSEG <ref type="bibr" target="#b24">[25]</ref>, NeoUNet <ref type="bibr" target="#b25">[26]</ref> and BlazeNeo <ref type="bibr" target="#b26">[27]</ref> use HarDNet, an improvement of DenseNet to extract features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Attention Mechanism</head><p>The attention mechanism is a widely used technique to help deep neural networks learn better feature representations, especially on highly variant inputs. Oktay et al. <ref type="bibr" target="#b27">[28]</ref> proposed an Attention Gate module for UNet, which helps the model focus on necessary information while preserving computational efficiency. AG-ResUNet++ <ref type="bibr" target="#b28">[29]</ref> integrates the attention gates with the ResNet backbone to improve UNet++ <ref type="bibr" target="#b14">[15]</ref> for polyp segmentation. PraNet <ref type="bibr" target="#b22">[23]</ref> uses the Reverse Attention module <ref type="bibr" target="#b29">[30]</ref>, which enforces focus on the boundary between a polyp and its surroundings. In general, most CNNs and neural networks can benefit by adding attention modules. However, even with these attention mechanisms, CNNs are limited by the locality of convolution operations. This limitation makes them difficult to model natural long-range spatial dependencies between input segments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>The architecture of three neural blocks used in our ColonFormer. The left block (a) is the Mix Transformer block <ref type="bibr" target="#b30">[31]</ref>. The middle block (b) is the channel-wise feature pyramid block. The pyramid pooling module is shown on the right (c). Here DR stands for dilation rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Vision Transformer</head><p>Transformer <ref type="bibr" target="#b8">[9]</ref> is a highly influential deep neural network architecture, originally proposed to solve natural language processing and similar problems. While the original Transformer architecture is not very well suited for image analysis, attempts have been proposed to leverage its advantages for computer vision through some modifications. Vision Transformer (ViT) <ref type="bibr" target="#b31">[32]</ref> was the first successful application of Transformers for computer vision. ViT splits an image into patches and processes them as sequential tokens. This method greatly reduces computational costs and allows Transformers to work with large images feasibly.</p><p>A major issue of ViT is that it requires extensive datasets for training to remain effective while being severely limited when trained on small datasets. Such property hinders its usage in problems such as medical image analysis, including polyp segmentation, where data is typically scarce. The Kvasir dataset, for example, contains just 1000 images and their corresponding ground truth, despite being the largest public image dataset of the gastrointestinal tract for polyp segmentation.</p><p>Recent works have attempted to further enhance ViT in several ways. DeiT <ref type="bibr" target="#b32">[33]</ref> introduces a data-efficient training strategy combined with a distillation approach, which helps improve the performance when training on small datasets. Swin Transformer <ref type="bibr" target="#b9">[10]</ref> redesigned the encoder for Transformers. The Swin Transformer encoder computes self-attention among a collection of adjacent patches within a sliding window. Patches are merged every few blocks, reducing the number of tokens and forming a multi-resolution token hierarchy similar to convolutional blocks. Seg-Former <ref type="bibr" target="#b30">[31]</ref> is another hierarchical Transformer design, where patches are merged with overlap and preserving local continuity around patches. The authors also introduced Efficient Self-Attention, a modified attention mechanism for reducing computational complexity, and Mix-FFN for better positional information.</p><p>Both TransUNet <ref type="bibr" target="#b33">[34]</ref> and TransFuse <ref type="bibr" target="#b34">[35]</ref> models have been developed based on Transformers for polyp segmentation and yielded promising results. TransUNet uses a Transformer-based network with a hybrid ViT encoder and upsampling CNN decoder. The Hybrid ViT stacks the CNN and Transformer together, leading to high computational costs. TransFuse addressed this problem by using a parallel architecture. Both models use the attention gate mechanism <ref type="bibr" target="#b27">[28]</ref> and a so-called BiFusion Module. These components make the network architecture large and highly complex.</p><p>While there have been promising results in using Transformers to develop networks for polyp segmentation, there is plenty of room for improvement in this direction. Most notably, reduced network size and latency can greatly benefit downstream applications. In addition, improved accuracy and robustness can also be achieved with more optimized architectures. This paper seeks to design a Transformer-based architecture that achieves these goals.</p><p>3 ColonFormer <ref type="figure">Fig. 1</ref> depicts the overall architecture of our proposed network, ColonFormer. The network consists of a hybrid encoder, a decoder, and a refinement module. We will describe each component in detail in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>A hierarchically structured model that can extract coarse-to-fine features at multi-scale and multi-level is desired for semantic segmentation. Our model uses Mix Transformer (MiT) proposed in <ref type="bibr" target="#b30">[31]</ref> as the encoder backbone. MiT is a hierarchical Transformer encoder that can represent both high-resolution coarse and low-resolution fine features. Assume X ? R H?W ?C denotes the input image. MiT generates the CNN-like multi-level features F i . The hierarchical feature map F i has the resolution of</p><formula xml:id="formula_0">H 2 i+1 ? W 2 i+1 ? C i , where i ? {1, 2, 3, 4}</formula><p>and C i is ascending. The hierarchical feature representation is brought by the overlapped patch merging. After several Transformer blocks ( <ref type="figure">Fig. 2a</ref>), a kernel with a stride smaller than kernel size is used to divide the feature map into overlapping patches. Such an overlapping patch merging process ensures the local continuity around those patches.</p><p>Like other Transformer blocks, MiT blocks contain three main parts: Multi-head Self-Attention (MHSA) layers, Feed Forward Network (FFN), and Layer Norm. The MHSA is improved into Efficient Self-Attention, where the number of keys is decreased by a factor of R to reduce the computational complexity of self-attention layers. Another reason that we decide to choose MiT is the Mix-FFN. Instead of using the positional encoding (PE) as ViT, a 3 ? 3 convolution kernel is integrated into FFN. Since the resolution of PE is fixed, it can not utilize the positional information of the pretrained dataset like ImageNet when test resolution differs from the training one. In such cases, ViT <ref type="bibr" target="#b31">[32]</ref> suggests interpolating the PE, which can lead to a drop in accuracy. In contrast, arguing that convolutional layers are adequate for providing location information for Transformer, MiT directly uses a 3x3 convolutional layer for positional encoding. MiT has a series of variants, from MiT-B1 to MiT-B5, with the same architecture but different sizes. We name the variants of our model as ColonFormer-XS, ColonFormer-S, ColonFormer-L, ColonFormer-XL, ColonFormer-XXL, corresponding to different MiT backbone scales from MiT-B1 to MiT-B5, respectively. According to ablation study described later in Section 4.4, we found that ColonFormer-S and ColonFormer-L achieve the best results. Therefore, we mostly use ColonFormer-S and ColonFormer-L for comparison with other state-of-thearts in all experiments except where it is specified otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Decoder</head><p>In order to further capture global context information, the feature maps extracted from the final block of the encoder are first processed by a Pyramid Pooling Module (PPM) <ref type="bibr" target="#b35">[36]</ref> before being passed through the decoder blocks. The PPM simultaneously produces multi-scale outputs of the input feature map via a pyramid of pooling layers. The resulting feature maps, which form a hierarchy of features containing information at different scales and sub-regions, are then concatenated to produce an efficient prior global representation. <ref type="figure">Fig. 2c</ref> depicts the Pyramid Pooling Module in detail.</p><p>ColonFormeruses a decoder architecture inspired by UPerNet <ref type="bibr" target="#b36">[37]</ref>, which we denote as UPer Decoder. The decoder gradually fuses the prior global map produced by the PPM with multi-scale feature maps yielded by the MiT backbone. We suppose that applying convolutional layers to the feature maps of the MiT backbone is necessary since such layers can condense the information by emphasizing the coherence between neighboring elements and thus enhancing the resulting semantic map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Refinement Module</head><p>The decoder's outputs are further processed by a refinement module to achieve more precise and complete prediction maps. The refinement module consists of Channel-wise Feature Pyramid (CFP) module <ref type="bibr" target="#b37">[38]</ref>  <ref type="figure">(Fig. 2b)</ref> and our novel Reverse Attention module enhanced by a new residual axial attention block for incremental correction of polyp boundary <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>In the parallel reverse attention network architecture <ref type="bibr" target="#b22">[23]</ref>, the global map is derived from the deepest CNN layer, so it does not have many structural details and hence can present only rough locations of the polyp tissues. The proposed strategy to recover precise location and label is to exploit complementary regions and details in a sequential manner by removing previously estimated polyp regions from high-level side-output features, where the current estimation is up-sampled from the deeper layer. By using Reverse Attention, a coarse saliency map is guided to sequentially discover complement object regions and details by erasing the current predicted salient regions from side-output features. The current prediction is upsampled from its deeper layer. This erasing approach can refine the imprecise and coarse estimation into an accurate and complete prediction map. It was shown that self-attention layers in the MiT backbone work like low-pass filters. Therefore, we argue that using convolutional layers is important for the refinement module since such layers favor high-frequency components and can provide richer edge information for the boundary correction.</p><p>Inspired by CaraNet <ref type="bibr" target="#b39">[40]</ref>, we use Channel-wise Feature Pyramid (CFP) to extract features from the encoder in multiscale views. As depicted in <ref type="figure">Fig. 2b</ref>, the CFP module has K = 4 branches with different dilation rates that allow them to capture information at multiple scales. However, a direct concatenation of all branches could lead to some unwanted checkerboard or gridding artifacts that significantly impact the quality of the following boundary correction. In order to avoid this issue, the CFP module combines these branches step by step to build a final accurate feature map to correct the polyp boundaries.</p><p>CaraNet <ref type="bibr" target="#b39">[40]</ref> also enhanced the Reverse Attention module by an axial attention block, which is a straightforward generalization of self-attention that naturally aligns with the multiple dimensions of the tensors. This module is supposed to filter the necessary information for the refinement process. However, axial attention may not always be good for the network since it can accidentally eliminate important edge information. Therefore, we propose to relax this mechanism using an additional residual connection, which allows the network to omit the axial attention layers when required and thus facilitates the learning process. The novel refinement module is called Residual Axial Reverse Attention (RA-RA). We experimentally found that utilizing the RA-RA module up to the finest feature map does not help refine the polyp boundary better. Hence, we propose to use just three RA-RA blocks, as shown in <ref type="figure">Fig. 1</ref>. The effectiveness of the RA-RA module is investigated in detail in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>ColonFormeruses a compound loss combining the weighted focal loss and weighted IoU loss to train the model. The weighted focal loss is a distribution-based loss that treats every pixel individually. In contrast, the weighted IoU loss is a region-based loss that considers the relationships between neighboring pixels.</p><p>Image pixels can be easy to be correctly recognized. However, some pixels, such as those on the edge regions, may be harder to learn. Thus, the model should pay more attention to more challenging samples. In other words, some image pixels may be more important than others in contributing to the learning process. We represent the importance of pixel (i, j) by a weight ? ij . As suggested in <ref type="bibr" target="#b40">[41]</ref>, the weight ? ij for pixel (i, j) is defined as the difference between the center pixel and its neighbors:</p><formula xml:id="formula_1">? ij = m,n?Nij g mn |N ij | ? g ij<label>(1)</label></formula><p>where N ij represents the area of 31 ? 31 size surrounding the pixel (i, j), and g ij ? {0, 1} is the true label of pixel (i, j). A large value of ? ij indicates a pixel with considerable distinction from its vicinity, i.e., pixels at polyp edges. Such a weighting scheme enforces the model to focus more on the boundary regions.</p><p>Assume that p ij is the prediction probability of the pixel (i, j) belonging the polyp class. Let us define q ij as:</p><formula xml:id="formula_2">q ij = p ij , if g ij = 1 1 ? p ij , otherwise<label>(2)</label></formula><p>As polyp segmentation is a problem with highly imbalanced data, focal loss is employed to deal with class imbalance during training. It integrates a modulating term in order to focus learning on hard pixels. The weighted focal loss is </p><formula xml:id="formula_3">L wf ocal = ? H i=1 W j=1 (1 + ??ij)?(1 ? q ij ) ? log(q ij ) H i=1 W j=1 (1 + ?? ij )<label>(3)</label></formula><p>where ?, ? are tunable hyperparameters.</p><p>The weighted IoU loss is defined as follows:</p><formula xml:id="formula_4">L wiou = 1 ? H i=1 W j=1 (g ij * p ij ) * (1 + ?? ij ) H i=1 W j=1 (g ij + p ij ? g ij * p ij ) * (1 + ?? ij )<label>(4)</label></formula><p>where ? is a hyperparameter to adjust the impact of importance weights ? ij .</p><p>The total loss of our ColonFormeris calculated as:</p><formula xml:id="formula_5">L total = L wf ocal + L wiou 2<label>(5)</label></formula><p>The total loss in Eq. (5) is applied to train our model for multi-scale outputs as shown in <ref type="figure">Fig. 1</ref>. The final loss is the sum of all total losses computed at different output levels. Note that each output is upsampled back to the original size of the image's ground truth before the losses are evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We perform experiments on the five popular datasets for polyp segmentation: Kvasir <ref type="bibr" target="#b46">[47]</ref>, CVC-Clinic DB <ref type="bibr" target="#b47">[48]</ref>, CVC-Colon DB <ref type="bibr" target="#b48">[49]</ref>, CVC-T <ref type="bibr" target="#b49">[50]</ref>, and ETIS-Larib Polyp DB <ref type="bibr" target="#b50">[51]</ref>. Details of these datasets are described as follows:</p><p>? Kvasir dataset is collected using endoscopic equipment at Vestre Viken Health Trust (VV), Norway. Images are carefully annotated and verified by experienced gastroenterologists from VV and the Cancer Registry of Norway. The dataset consists of 1000 images with different resolutions from 720 ? 576 to 1920 ? 1072 pixels.</p><p>? CVC-ClinicDB dataset is a database of frames extracted from colonoscopy videos. The dataset consists of 612 images with a resolution of 384?288 pixels from 31 colonoscopy sequences. The dataset was used in the training stages of the MICCAI 2015 Sub-Challenge on Automatic Polyp Detection Challenge in Colonoscopy Videos.</p><p>? CVC-ColonDB dataset is provided by the Machine Vision Group (MVG). The dataset consists of 380 images with a resolution of 574 ? 500 pixels from 15 short colonoscopy videos.  <ref type="bibr" target="#b43">[44]</ref> 0.815 ? 0.018 0.736 ? 0.017 0.832 ? 0.018 0.830 ? 0.020 DoubleUNet <ref type="bibr" target="#b15">[16]</ref> 0.920 ? 0.018 0.866 ? 0.025 0.922 ? 0.027 0.928 ? 0.017 DDANet <ref type="bibr" target="#b44">[45]</ref> 0.860 ? 0.014 0.786 ? 0.017 0.858 ? 0.023 0.892 ? 0.014 ColonSegNet <ref type="bibr" target="#b45">[46]</ref> 0.817 ? 0.020 0.873 ? 0.024 0.926 ? 0.025 0.933 ? 0.014 HarDNet-MSEG <ref type="bibr" target="#b24">[25]</ref> 0.923 ? 0.020 0.873 ? 0.024 0.926 ? 0.025 0.933 ? 0.014 PraNet <ref type="bibr" target="#b22">[23]</ref> 0.933 ? 0.012 0.884 ? 0.015 0.940 ? 0.005 0.937 ? 0.016 ColonFormer-S (Ours) 0.948 ? 0.002 0.904 ? 0.004 0.958 ? 0.003 0.941 ? 0.004 ColonFormer-L (Ours) 0.947 ? 0.002 0.903 ? 0.003 0.956 ? 0.002 0.942 ? 0.005</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kvasir</head><p>UNet <ref type="bibr" target="#b13">[14]</ref> 0.708 ? 0.017 0.602 ? 0.010 0.805 ? 0.014 0.716 ? 0.020 ResUNet++ <ref type="bibr" target="#b43">[44]</ref> 0.780 ? 0.010 0.681 ? 0.008 0.834 ? 0.010 0.799 ? 0.010 DoubleUNet <ref type="bibr" target="#b15">[16]</ref> 0.879 ? 0.018 0.816 ? 0.026 0.902 ? 0.027 0.894 ? 0.039 DDANet <ref type="bibr" target="#b44">[45]</ref> 0.860 ? 0.005 0.791 ? 0.004 0.876 ? 0.015 0.892 ? 0.018 ColonSegNet <ref type="bibr" target="#b45">[46]</ref> 0.676 ? 0.037 0.557 ? 0.040 0.731 ? 0.088 0.730 ? 0.080 HarDNet-MSEG <ref type="bibr" target="#b24">[25]</ref> 0.889 ? 0.011 0.831 ? 0.011 0.892 ? 0.015 0.926 ? 0.014 PraNet <ref type="bibr" target="#b22">[23]</ref> 0.883 ? 0.020 0.822 ? 0.020 0.897 ? 0.020 0.906 ? 0.010 ColonFormer-S (Ours) 0.924 ? 0.008 0.875 ? 0.010 0.941 ? 0.010 0.927 ? 0.008 ColonFormer-L (Ours) 0.917 ? 0.006 0.865 ? 0.007 0.932 ? 0.007 0.926 ? 0.008  ? CVC-T dataset is the test set of a more extensive dataset called Endoscene. CVC-T consists of 60 images obtained from 44 video sequences acquired from 36 patients.</p><p>? ETIS-Larib dataset contains 196 high resolution (1226 ? 996) colonoscopy images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment settings</head><p>We implement ColonFormerusing the PyTorch framework. For a fair setting and comparison, we use the same parameters as <ref type="bibr" target="#b30">[31]</ref> for the MiT backbone: kernel size K = 7, stride S = 4, padding size P = 3, and K = 3, S = 2, P = 1  We use six experiment setups to evaluate our method; each setup is described in detail below:</p><p>? Experiment 1: We use the same split as suggested in <ref type="bibr" target="#b22">[23]</ref>, where 90% of the Kvasir and ClinicDB datasets are used for training. The remaining images in the Kvasir and CVC-ClinicDB datasets and all images from CVC-ColonDB, CVC-T, and ETIS-Larib are used for testing.</p><p>? Experiment 2: 5-fold cross-validation on the CVC-ClinicDB and Kvasir datasets. Each dataset is divided into five equal folds. Each run uses one fold for testing and four remaining folds for training.</p><p>? Experiment 3: Cross-dataset evaluation with 3 training-testing configurations: The first experiment compares our ColonFormermodel with state-of-the-art CNN-based and Transformer-based networks using the same widely-used dataset configuration as suggested in <ref type="bibr" target="#b22">[23]</ref>. The second experiment compares Colon-Former's learning ability to several recent polyp segmentation methods. Finally, the last experiment provides deeper insights into the generalization capability of ColonFormerand other benchmark models.</p><p>We use the Adam optimizer and cosine annealing scheduler with a learning rate of 1e-4. ColonFormeris trained in 20 epochs with a batch size of 8. The checkpoint of the last epoch is used for evaluation. Except for the second experiment with 5-fold cross-validation, we train ColonFormerfive times, and the ColonFormer's results are averaged over five runs.</p><p>In addition, we perform a series of ablation studies to evaluate the effectiveness of each component in the proposed ColonFormer. All ablation studies are performed on the dataset configuration for Experiment 1. <ref type="table" target="#tab_0">Table 1</ref> describes the comparison results for Experiment 1. ColonFormergenerally outperforms the benchmark models on most datasets. Notably, both ColonFormer-S and ColonFormer-L outperform the second-best TransFuse-L* by 3% in mDice and 2.7% in mIOU on the ColonDB dataset. Compared to the second-best CaraNet on the ETIS-Larib dataset, ColonFormer-S achieves an improvement of 5.2% in mDice, and 4.8% in mIOU, while ColonFormer-L achieves an improvement of 6.4% in mDice and 5.9% in mIOU. The high capacity of ColonFormer-L seems more suitable for the high resolution of images in the ETIS-Larib dataset. However, both ColonFormer-S and ColonFormer-L achieve roughly 1% lower metrics against TransFuse-L* on the CVC-ClinicDB dataset, whose images obtain very low resolution. <ref type="table" target="#tab_1">Table 2</ref> describes the comparison results for Experiment 2. We report both the average value and the standard deviation for each metric, which reflects the models' stability. One can see that both ColonFormer-S and ColonFormer-L outperform all other state-of-the-art models in mDice, mIOU, precision, and recall on both datasets. Notably, our ColonFormeris the most stable model on both datasets, achieving the lowest standard deviation for each evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with benchmark models</head><p>Qualitative results for Experiment 2 are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. ColonFormer-S demonstrates much fewer wrongly predicted pixels in segmentation results than other models. ColonFormer-S also produces better ROC and PR curves than the benchmark models, as depicted in <ref type="figure" target="#fig_1">Fig. 3</ref>.  <ref type="figure" target="#fig_3">Fig. 5</ref>. Similar to <ref type="figure" target="#fig_2">Fig. 4</ref>, one can see that ColonFormeryields better segmentation results compared to other state-of-the-arts. <ref type="table" target="#tab_5">Table 4</ref> compares ColonFormerwith other benchmark models in terms of size and computational complexity. One can see that our ColonFormer-S obtains competitive size and computational complexity compared to the most lightweight CNN-based models such as PraNet <ref type="bibr" target="#b22">[23]</ref>, and HarDNet-MSEG <ref type="bibr" target="#b24">[25]</ref>. Our ColonFormer-L is larger than most CNNbased neural networks but still more efficient than other Transformer-based methods in terms of GFlops.  <ref type="table" target="#tab_5">Table 4</ref> that UPer Decoder is also significantly less costly, requiring only 20.99 GFLOPs as opposed to MLP Decoder (33.68 GFLOPs). These results compel us to choose the UPer decoder for ColonFormer, which alleviates the large computation costs incurred with the Transformer backbone.</p><p>Effectiveness of the Refinement Module. We evaluate the performance of SegFormer-B3-Uper-ARA with the A-RA Refinement Module as in <ref type="bibr" target="#b39">[40]</ref>, and our ColonFormer-L with the adjusted Refinement Module as described in Section 3.3. Results are shown in <ref type="table" target="#tab_6">Table 5</ref>. Overall, incorporating the Refinement Module yields improvement across all datasets. Our ColonFormer-L also yields superior performance than SegFormer-B3-Uper-ARA on the Kvasir, CVC-ClinicDB, and most significantly, the ETIS-Larib datasets, while slightly underperforming on the CVC-ColonDB and CVC-T datasets.</p><p>Effectiveness of the MiT Backbone. The Mix Transformer (MiT) <ref type="bibr" target="#b30">[31]</ref> backbone has several variations ranging from MiT-B0 to MiT-B5. Accordingly, our ColonFormeralso have different variations, including ColonFormer-XS, ColonFormer-S, ColonFormer-L, ColonFormer-XL, ColonFormer-XXL, respectively. <ref type="table" target="#tab_7">Table 6</ref> shows our comparison between all variations of ColonFormer. Overall, ColonFormer-S and ColonFormer-L yield the best average results across our test datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes a novel deep neural network architecture called ColonFormerfor colon polyp segmentation. Our model leverages both the advantages of Transformers and CNNs architectures to learn a powerful multi-scale hierarchical feature representation. We also enhance the reverse attention with axial attention by relaxing it with a residual connection. The refinement module allows the network to incrementally correct the polyp boundary from a coarse global map produced by the decoder. Our extensive experiments show that ColonFormersignificantly outperforms existing state-of-the-art models on popular benchmark datasets.</p><p>In future works, we will investigate lightweight or sparse self-attention layers to reduce the computational complexity. In addition, other types of architectures for combining Transformers and CNNs can also be exploited.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Acknowledgments</head><p>This work was funded by Vingroup Innovation Foundation (VINIF) under project code VINIF.2020.DA17.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>ROC curves and PR curves on the 5-fold cross-validation on the Kvasir-SEG dataset. All the curves are averaged over 5 folds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative result comparison of different models trained on the first fold of the 5-fold cross-validation on the Kvasir dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative result comparison using CVC-Colon for training and CVC-Clinic for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of different methods on the Kvasir, ClinicDB, ColonDB, CVC-T and ETIS-Larib test sets. All results of ColonFormerare averaged over five runs.</figDesc><table><row><cell>Method</cell><cell cols="2">Kvasir</cell><cell cols="4">CVC-ClinicDB CVC-ColonDB</cell><cell cols="2">CVC-T</cell><cell cols="2">ETIS-Larib</cell></row><row><cell></cell><cell cols="10">mDice mIOU mDice mIOU mDice mIOU mDice mIOU mDice mIOU</cell></row><row><cell>UNet [14]</cell><cell>0.818</cell><cell>0.746</cell><cell>0.823</cell><cell>0.750</cell><cell>0.512</cell><cell>0.444</cell><cell>0.710</cell><cell>0.627</cell><cell>0.398</cell><cell>0.335</cell></row><row><cell>UNet++ [15]</cell><cell>0.821</cell><cell>0.743</cell><cell>0.794</cell><cell>0.729</cell><cell>0.483</cell><cell>0.410</cell><cell>0.707</cell><cell>0.624</cell><cell>0.401</cell><cell>0.344</cell></row><row><cell>SFA [42]</cell><cell>0.723</cell><cell>0.611</cell><cell>0.700</cell><cell>0.607</cell><cell>0.469</cell><cell>0.347</cell><cell>0.297</cell><cell>0.217</cell><cell>0.467</cell><cell>0.329</cell></row><row><cell>PraNet [23]</cell><cell>0.898</cell><cell>0.840</cell><cell>0.899</cell><cell>0.849</cell><cell>0.709</cell><cell>0.640</cell><cell>0.871</cell><cell>0.797</cell><cell>0.628</cell><cell>0.567</cell></row><row><cell>HarDNet-MSEG [25]</cell><cell>0.912</cell><cell>0.857</cell><cell>0.932</cell><cell>0.882</cell><cell>0.731</cell><cell>0.660</cell><cell>0.887</cell><cell>0.821</cell><cell>0.677</cell><cell>0.613</cell></row><row><cell>CaraNet [40]</cell><cell>0.918</cell><cell>0.865</cell><cell>0.936</cell><cell>0.887</cell><cell>0.773</cell><cell>0.689</cell><cell>0.903</cell><cell>0.838</cell><cell>0.747</cell><cell>0.672</cell></row><row><cell>TransUNet [34]</cell><cell>0.913</cell><cell>0.857</cell><cell>0.935</cell><cell>0.887</cell><cell>0.781</cell><cell>0.699</cell><cell>0.893</cell><cell>0.824</cell><cell>0.731</cell><cell>0.660</cell></row><row><cell>TransFuse-L* [35]</cell><cell>0.920</cell><cell>0.870</cell><cell>0.942</cell><cell>0.897</cell><cell>0.781</cell><cell>0.706</cell><cell>0.894</cell><cell>0.826</cell><cell>0.737</cell><cell>0.663</cell></row><row><cell cols="2">ColonFormer-S (Ours) 0.927</cell><cell>0.877</cell><cell>0.932</cell><cell>0.883</cell><cell>0.811</cell><cell>0.730</cell><cell>0.894</cell><cell>0.826</cell><cell>0.789</cell><cell>0.711</cell></row><row><cell cols="2">ColonFormer-L (Ours) 0.924</cell><cell>0.876</cell><cell>0.932</cell><cell>0.884</cell><cell>0.811</cell><cell>0.733</cell><cell>0.906</cell><cell>0.842</cell><cell>0.801</cell><cell>0.722</cell></row><row><cell>then defined as follows:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison of different methods on 5-fold cross-validation of the CVC-ClinicDB and Kvasir datasets. All results are averaged over 5 folds.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell>mDice</cell><cell>mIOU</cell><cell>Recall</cell><cell>Precision</cell></row><row><cell></cell><cell>UNet [14]</cell><cell>-</cell><cell>0.792</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>MultiResUNet [43]</cell><cell>-</cell><cell>0.849</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ResUNet++</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ClinicDB</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison of different methods on cross-dataset configurations. All results are averaged over five runs. In order to increase the model's robustness w.r.t image sizes, the training images are consequently scaled with a factor of {0.75, 1, 1.25}, respectively, and fed to the model for learning. None of the data augmentation techniques is used in the training phase.</figDesc><table><row><cell cols="3">Train Test</cell><cell>Method</cell><cell cols="4">mDice mIOU Recall Precision</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResUNet++ [44]</cell><cell>0.406</cell><cell>0.302</cell><cell>0.481</cell><cell>0.496</cell></row><row><cell>CVC-ColonDB</cell><cell>+ ETIS-Larib</cell><cell>CVC-ClinicDB</cell><cell>ColonSegNet [46] DDANet [45] DoubleUNet [16] HarDNet-MSEG [25] PraNet [23]</cell><cell>0.427 0.624 0.738 0.765 0.779</cell><cell>0.321 0.515 0.651 0.681 0.689</cell><cell>0.529 0.697 0.758 0.774 0.832</cell><cell>0.552 0.692 0.824 0.863 0.812</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ColonFormer-S (Ours)</cell><cell>0.851</cell><cell>0.771</cell><cell>0.853</cell><cell>0.896</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ColonFormer-L (Ours)</cell><cell>0.847</cell><cell>0.770</cell><cell>0.844</cell><cell>0.902</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResUNet++ [44]</cell><cell>0.339</cell><cell>0.247</cell><cell>0.380</cell><cell>0.484</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DoubleUNet [16]</cell><cell>0.441</cell><cell>0.375</cell><cell>0.423</cell><cell>0.639</cell></row><row><cell>CVC-ColonDB</cell><cell></cell><cell>CVC-ClinicDB</cell><cell cols="2">DDANet [45] ResNet101-Mask-RCNN [52] 0.641 0.476 ColonSegNet [46] 0.582 HarDNet-MSEG [25] 0.721 PraNet [23] 0.738</cell><cell>0.370 0.565 0.268 0.633 0.647</cell><cell>0.501 0.646 0.511 0.744 0.751</cell><cell>0.644 0.725 0.460 0.818 0.832</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ColonFormer-S (Ours)</cell><cell>0.816</cell><cell>0.731</cell><cell>0.809</cell><cell>0.881</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ColonFormer-L (Ours)</cell><cell>0.804</cell><cell>0.723</cell><cell>0.794</cell><cell>0.877</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ResUNet++ [44]</cell><cell>0.211</cell><cell>0.155</cell><cell>0.309</cell><cell>0.203</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ColonSegNet [46]</cell><cell>0.217</cell><cell>0.110</cell><cell>0.654</cell><cell>0.144</cell></row><row><cell>CVC-ClinicDB</cell><cell></cell><cell>ETIS-Larib</cell><cell cols="2">DDANet [45] ResNet101-Mask-RCNN [52] 0.565 0.400 DoubleUNet [16] 0.588 PraNet [23] 0.631 HarDNet-MSEG [25] 0.659 ColonFormer-S (Ours) 0.723</cell><cell cols="2">0.313 0.469 0.500 0.555 0.583 0.635 0.797 0.507 0.565 0.689 0.762 0.676</cell><cell>0.464 0.639 0.599 0.597 0.705 0.731</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ColonFormer-L (Ours)</cell><cell>0.760</cell><cell>0.673</cell><cell>0.859</cell><cell>0.734</cell></row></table><note>to produce features with the same size as the non-overlapping process. Based on experiments in [41], [53], we use ? = 5, ? = 0.25 and ? = 2 for the losses in Eq. (3) and Eq. (4). Training is performed using Google Colab on virtual machines with 16GB RAM and an NVIDIA Tesla P100 GPU. Input images are resized to 352 ? 352 for testing.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc>describes the comparison results for Experiment 3. Overall, both ColonFormer-S and ColonFormer-L significantly outperform benchmark models on cross-dataset metrics. For the first configuration, ColonFormer-S outperforms the second-best PraNet by 7 ? 8% on all metrics. In the second configuration, ColonFormer-S continues to achieve a 5.8% improvement in precision and 7.8% improvement in mDice over PraNet. For the third configuration, ColonFormer-L again shows its suitability to the ETIS-Larib dataset achieving a 10.1% improvement in mDice and 18.3% in recall over PraNet. These are highly significant improvements, showing that our ColonFormercan generalize very well to new unseen data. Some result samples for this experiment are shown in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Number of parameters and GFLOPs of different methodsEffectiveness of the UPer Decoder. We firstly compare the original SegFormer-B3<ref type="bibr" target="#b30">[31]</ref> with MLP Decoder and another model called SegFormer-B3-Uper that replaces the original MLP decoder with the UPer Decoder. Both models use the MiT-B3 backbone in terms of encoder. Results are shown in the first two rows ofTable 5. Both network versions show similar metrics across the test datasets, with slight variations of roughly 1%. However, one can see from</figDesc><table><row><cell>Method</cell><cell cols="2">Parameters (M) GFLOPs</cell></row><row><cell>PraNet [23]</cell><cell>32.55</cell><cell>13.11</cell></row><row><cell>HarDNet-MSEG [25]</cell><cell>33.34</cell><cell>11.38</cell></row><row><cell>CaraNet [40]</cell><cell>46.64</cell><cell>21.69</cell></row><row><cell>TransUNet [34]</cell><cell>105.5</cell><cell>60.75</cell></row><row><cell>TransFuse-L* [35]</cell><cell>-</cell><cell>-</cell></row><row><cell>SegFormer-B3 [31]</cell><cell>47.22</cell><cell>33.68</cell></row><row><cell>SegFormer-B3-Uper</cell><cell>46.61</cell><cell>20.99</cell></row><row><cell>ColonFormer-S (Ours)</cell><cell>33.04</cell><cell>16.03</cell></row><row><cell>ColonFormer-L (Ours)</cell><cell>52.94</cell><cell>22.94</cell></row><row><cell>4.4 Ablation studies</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the effectiveness of different components. All results are averaged over five runs.</figDesc><table><row><cell>Method</cell><cell cols="3">Uper A-RA RA-RA</cell><cell cols="2">Kvasir</cell><cell cols="4">CVC-ClinicDB CVC-ColonDB</cell><cell cols="2">CVC-T</cell><cell cols="2">ETIS-Larib</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="10">mDice mIOU mDice mIOU mDice mIOU mDice mIOU mDice mIOU</cell></row><row><cell>SegFormer-B3 [31]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.920</cell><cell>0.866</cell><cell>0.925</cell><cell>0.876</cell><cell>0.806</cell><cell>0.726</cell><cell>0.905</cell><cell>0.840</cell><cell>0.786</cell><cell>0.707</cell></row><row><cell>SegFormer-B3-Uper</cell><cell></cell><cell>-</cell><cell>-</cell><cell>0.916</cell><cell>0.864</cell><cell>0.924</cell><cell>0.876</cell><cell>0.811</cell><cell>0.731</cell><cell>0.900</cell><cell>0.832</cell><cell>0.784</cell><cell>0.707</cell></row><row><cell>SegFormer-B3-Uper-ARA</cell><cell></cell><cell></cell><cell>-</cell><cell>0.922</cell><cell>0.872</cell><cell>0.922</cell><cell>0.875</cell><cell>0.812</cell><cell>0.734</cell><cell>0.903</cell><cell>0.837</cell><cell>0.787</cell><cell>0.704</cell></row><row><cell>ColonFormer-L (Ours)</cell><cell></cell><cell>-</cell><cell></cell><cell>0.924</cell><cell>0.876</cell><cell>0.932</cell><cell>0.884</cell><cell>0.811</cell><cell>0.733</cell><cell>0.906</cell><cell>0.842</cell><cell>0.801</cell><cell>0.722</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Evaluation metrics for different variations of the MiT backbone. All results are averaged over five runs.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Kvasir</cell><cell cols="4">CVC-ClinicDB CVC-ColonDB</cell><cell cols="2">CVC-T</cell><cell cols="2">ETIS-Larib</cell></row><row><cell></cell><cell></cell><cell cols="10">mDice mIOU mDice mIOU mDice mIOU mDice mIOU mDice mIOU</cell></row><row><cell>ColonFormer-XS</cell><cell>MiT-B1</cell><cell>0.913</cell><cell>0.859</cell><cell>0.926</cell><cell>0.876</cell><cell>0.784</cell><cell>0.700</cell><cell>0.879</cell><cell>0.808</cell><cell>0.758</cell><cell>0.679</cell></row><row><cell>ColonFormer-S</cell><cell>MiT-B2</cell><cell>0.927</cell><cell>0.877</cell><cell>0.932</cell><cell>0.883</cell><cell>0.811</cell><cell>0.730</cell><cell>0.894</cell><cell>0.826</cell><cell>0.789</cell><cell>0.711</cell></row><row><cell>ColonFormer-L</cell><cell>MiT-B3</cell><cell>0.924</cell><cell>0.876</cell><cell>0.932</cell><cell>0.884</cell><cell>0.811</cell><cell>0.733</cell><cell>0.906</cell><cell>0.842</cell><cell>0.801</cell><cell>0.722</cell></row><row><cell>ColonFormer-XL</cell><cell>MiT-B4</cell><cell>0.920</cell><cell>0.870</cell><cell>0.923</cell><cell>0.875</cell><cell>0.814</cell><cell>0.735</cell><cell>0.905</cell><cell>0.840</cell><cell>0.795</cell><cell>0.715</cell></row><row><cell>ColonFormer-XXL</cell><cell>MiT-B5</cell><cell>0.920</cell><cell>0.872</cell><cell>0.924</cell><cell>0.876</cell><cell>0.802</cell><cell>0.724</cell><cell>0.899</cell><cell>0.831</cell><cell>0.776</cell><cell>0.700</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Comparative validation of polyp detection methods in video colonoscopy: Results from the miccai 2015 endoscopic vision challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajkbaksh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Angermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rustad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Balasingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pogorelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Debard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>C?rdova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>S?nchez-Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Histace</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1231" to="1249" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adenoma detection rate and risk of colorectal cancer and death</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><forename type="middle">R</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><forename type="middle">G</forename><surname>Doubeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jolanda</forename><surname>Zauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De Boer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">E</forename><surname>Fireman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schottinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New england journal of medicine</title>
		<imprint>
			<biblScope unit="volume">370</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1298" to="1306" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Factors influencing the miss rate of polyps in a back-to-back colonoscopy study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Am Leufkens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Oijen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Vleggaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siersema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">05</biblScope>
			<biblScope unit="page" from="470" to="475" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Computer-aided classification of gastrointestinal lesions in regular colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Mesejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Abergel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Rouquette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Beorchia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Poincloux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Bartoli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2051" to="2063" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy R Glissen</forename><surname>Berzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangping</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pan</surname></persName>
		</author>
		<title level="m">951e-a real-time automatic deep learning polyp detection system increases polyp and adenoma detection during colonoscopy: a prospective double-blind randomized study. Gastroenterology</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">156</biblScope>
			<biblScope unit="page">1511</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial intelligence and colonoscopy: Current status and future perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Shin-Ei Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenichi</forename><surname>Misawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toyoki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hayato</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Itoh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensaku</forename><surname>Oda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digestive Endoscopy</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="363" to="371" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate classification of diminutive colorectal polyps using computer-aided analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Jen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Chiung</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Ju</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Chun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry Horng-Shing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent S</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gastroenterology</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="568" to="575" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Advanced imaging for detection and differentiation of colorectal neoplasia: European society of gastrointestinal endoscopy (esge) guideline-update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raf</forename><surname>Bisschops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">E</forename><surname>East</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yark</forename><surname>Hazewinkel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Micha?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Kami?ski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Pellis?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Antonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Bustamante Balen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Endoscopy</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1155" to="1179" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="568" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12179" to="12188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unet++: A nested u-net architecture for medical image segmentation. In Deep learning in medical image analysis and multimodal learning for clinical decision support</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Md</forename><surname>Mahfuzur Rahman Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Doubleu-net: A deep convolutional neural network for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 33rd International symposium on computer-based medical systems (CBMS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="558" to="564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pranet: Parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge-Peng</forename><surname>Deng-Ping Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ag-curesnest: A novel method for colon polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Viet Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Quang</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngoc</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viet Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen Thi</forename><surname>Dao Van Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thuy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.00402</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hardnet-mseg: A simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn-Long</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.07172</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neounet: Towards accurate colon polyp segmentation and neoplasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Phan Ngoc Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sy An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tran</forename><forename type="middle">Quang</forename><surname>Dao Van Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh Viet</forename><surname>Thuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Blazeneo: Blazing fast polyp segmentation and neoplasm detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen S An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Trung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh V</forename><surname>Thuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Access</publisher>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention gated networks: Learning to leverage salient regions in medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michiel</forename><surname>Schaap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattias</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="197" to="207" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ag-resunet++: An improved encoder-decoder based method for polyp segmentation in colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen Ba Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thai</forename><surname>Duc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh Viet</forename><surname>Van Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 RIVF International Conference on Computing and Communication Technologies (RIVF)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reverse attention for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuli</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuelong</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="250" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Transunet: Transformers make strong encoders for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieneng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qihang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyin</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04306</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transfuse: Fusing transformers and cnns for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yundong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiye</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cfpnet: channel-wise feature pyramid for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ange</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Loew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1894" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Caranet: Context axial reverse attention network for segmentation of small medical objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ange</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyue</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Loew</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07368</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">F 3 net: Fusion, feedback and focus for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12321" to="12328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Selective feature aggregation network with areaboundary constraints for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Yu</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nabil</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Resunet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">De</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Symposium on Multimedia (ISM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Ddanet: Dual decoder attention network for automatic polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Nikhil Kumar Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharib</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>H?vard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halvorsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR International Workshop and Challenges</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Real-time polyp detection, localization and segmentation in colonoscopy using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharib</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Kumar Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>H?vard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halvorsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Ieee Access</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="40496" to="40510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Kvasir-seg: A segmented polyp dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smedsrud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P?l</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halvorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dag</forename><surname>Thomas De Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H?vard D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debora</forename><surname>Gil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Vilari?o</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computerized Medical Imaging and Graphics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automated polyp detection in colonoscopy videos using shape and context information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suryakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianming</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="630" to="644" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A benchmark for endoluminal scene segmentation of colonoscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>V?zquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Bernal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gloria</forename><surname>Fern?ndez-Esparrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><forename type="middle">M</forename><surname>L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of healthcare engineering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aymeric</forename><surname>Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Romain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Dray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><surname>Granado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Lars Aabakken, and Ilangko Balasingham. Polyp detection and segmentation using mask r-cnn: Does a deeper feature extractor cnn always perform better</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Hemin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghak</forename><surname>Qadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Solhusvik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bergsland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 13th International Symposium on Medical Information and Communication Technology (IS-MICT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
