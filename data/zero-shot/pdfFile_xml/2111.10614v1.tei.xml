<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GMSRF-Net: An improved generalizability with global multi-scale residual fusion network for polyp segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Srivastava</surname></persName>
							<email>abhisheksrivastava2397@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukalpa</forename><surname>Chanda</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">?stfold University College</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debesh</forename><surname>Jha</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">SimulaMet</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Arctic University of Norway</orgName>
								<address>
									<country key="NO">Norway</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umapada</forename><surname>Pal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharib</forename><surname>Ali</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Indian Statistical Institute</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GMSRF-Net: An improved generalizability with global multi-scale residual fusion network for polyp segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep learning</term>
					<term>polyp segmentation</term>
					<term>generaliza- tion</term>
					<term>multi-scale feature fusion</term>
					<term>colonoscopy</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Colonoscopy is a gold standard procedure but is highly operator-dependent. Efforts have been made to automate the detection and segmentation of polyps, a precancerous precursor, to effectively minimize missed rate. Widely used computeraided polyp segmentation systems actuated by encoder-decoder have achieved high performance in terms of accuracy. However, polyp segmentation datasets collected from varied centers can follow different imaging protocols leading to difference in data distribution. As a result, most methods suffer from performance drop and require re-training for each specific dataset. We address this generalizability issue by proposing a global multiscale residual fusion network (GMSRF-Net). Our proposed network maintains high-resolution representations while performing multi-scale fusion operations for all resolution scales. To further leverage scale information, we design cross multi-scale attention (CMSA) and multi-scale feature selection (MSFS) modules within the GMSRF-Net. The repeated fusion operations gated by CMSA and MSFS demonstrate improved generalizability of the network. Experiments conducted on two different polyp segmentation datasets show that our proposed GMSRF-Net outperforms the previous top-performing state-of-the-art method by 8.34% and 10.31% on unseen CVC-ClinicDB and unseen Kvasir-SEG, in terms of dice coefficient.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Colorectal cancer (CRC) has been consistently ranked third in terms of prevalence <ref type="bibr" target="#b0">[1]</ref>. The leading cause of CRC is colorectal adenomatous polyps, and thus identification and resection of polyps can reduce the occurrence of CRC. Colonoscopy serves as a gold standard technique for surveillance and treatment. Studies have shown that timely colonoscopy can reduce the chances of CRC by 30% <ref type="bibr" target="#b1">[2]</ref>. However, the success of careful identification of malicious polyps and their subsequent resection depends on the ability and experience of clinicians which makes it prone to human error. Such factors eventually lead to a high polyp missed rate <ref type="bibr" target="#b2">[3]</ref>. Artificial intelligence (AI) driven methods can be effective and provide precise detection and segmentation of polyps.</p><p>With the advent of convolution neural networks (CNNs) research for the polyp segmentation task has been widely conducted to reduce operator-dependent problems in colonoscopy. However, the variations in structures and size of polyps and fluctuation of contrast between polyps and their immediate surrounding make it a challenging task. Whilst methods such as U-Net <ref type="bibr" target="#b3">[4]</ref>, U-Net++ <ref type="bibr" target="#b4">[5]</ref>, PraNet <ref type="bibr" target="#b5">[6]</ref>, UACA-Net <ref type="bibr" target="#b6">[7]</ref>, MSRF-Net <ref type="bibr" target="#b7">[8]</ref> have demonstrated higher metric performances, when the intervention of imaging protocols varies, performance of these methods fall considerably. The imaging protocols used to acquire colonoscopy images at most times vary over different medical institutions, the performance drop in these methods when tested on unseen data need to be minimized.</p><p>We can observe various reincarnations of the U-Net developed for polyp segmentation task in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Similarly, PraNet <ref type="bibr" target="#b5">[6]</ref> aggregated deep features in their parallel partial decoder to form initial guidance area maps. ColonSegNet <ref type="bibr" target="#b10">[11]</ref> used only two encoder and two decoder layers that made their network parameters relatively smaller enabling a faster inference time. UACA-Net <ref type="bibr" target="#b6">[7]</ref> used a saliency map for each level in the decoder to calculate foreground, background, and uncertain area maps. However, a major drawback with encoder-decoder architectures like U-Net is that shallow features from the encoder and deep features from the decoder suffer from semantic gap <ref type="bibr" target="#b11">[12]</ref>. Deeplabv3+ <ref type="bibr" target="#b12">[13]</ref> introduced atrous spatial pyramid pooling with skip connections to aggregate global multi-scale context. Wang et al. <ref type="bibr" target="#b13">[14]</ref> designed a network where spatial precision is not compromised by maintaining highresolution representations throughout the process. Here, multiscale fusion is performed by repeated cross-scale fusion of features for all resolution scales. Inspired by deep fusion <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, MSRF-Net <ref type="bibr" target="#b7">[8]</ref> increased the number of fusion operations by introducing dual-scale dense fusion blocks, which allowed the preservation of both high-and low-level features for all resolution scales. The authors <ref type="bibr" target="#b7">[8]</ref> demonstrated the superior generalizability of MSRF-Net and HRNet on polyp segmentation tasks. Building upon these concepts we aim to increase generalizability of polyp segmentation task under different clinical settings by introducing a global multi-scale residual fusion network "GMSRF-Net".</p><p>Our GMSRF-Net uses a densely connected multi-scale fusion mechanism that fuses features from all resolution scales at once. The fusion of multi-scale features occurs at each convolutional layer of the densely connected structure which further increases the frequency of fusion operation while maintaining global multi-scale context throughout the process. Additionally, we design a novel cross multi-scale attention (CMSA) mechanism. These attention maps formed by the aggregation of multi-scale context boost the feature map representations in all resolution scales. Our multi-scale feature selection (MSFS) module, applies channel-wise attention on the features fused from all scales to further amplify the relevant features. Experiments demonstrate the improved generalizability of the proposed approach compared to former state-of-the-art (SOTA) methods. Thus, our GMSRF-Net opens new avenues to enhance the generalization capacity of CNNbased supervised learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. MATERIALS AND METHOD A. Materials</head><p>We have chosen two standard publicly available polyp segmentation datasets: Kvasir-SEG <ref type="bibr" target="#b16">[17]</ref> and CVC-ClinicDB <ref type="bibr" target="#b17">[18]</ref>. Kvasir-SEG was acquired in Vestre Viken Health Trust in Norway while CVC-ClinicDB was obtained in Hospital Clinic in Barcelona, Spain. To demonstrate the effectiveness of our technique we perform four experiments with different setups. Two experiments were carried out when the training and testing datasets are the same. Additionally, to establish the generalization capacity of our network, we trained and tested our model on different datasets, i.e., trained on Kvasir-SEG and tested on CVC-ClinicDB and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Method</head><p>In this section, we present the architecture of our GMSRF-Net (see <ref type="figure" target="#fig_0">Fig. 1</ref>). GMSRF-Net uses global multi-scale feature fusion mechanism which incorporates cross multi-scale attention and subsequent multi-scale feature selection module for accurate and generalizable segmentation of polyps. The encoder, two GMSRF modules, and decoder are detailed in the following subsections.</p><p>1) Encoder block: The colonoscopy images are first processed by ResNet50 <ref type="bibr" target="#b18">[19]</ref> backbone pre-trained on ImageNet. The number of feature maps for all scales is reduced by Receptive Field Blocks (RFBs) <ref type="bibr" target="#b19">[20]</ref> to reduce the computational cost incurred by the following global multi-scale residual fusion (GMSRF) and the decoder network (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). Here, the features generated by the RFB module be denoted as X i where i ? {1, 2, 3, 4} denote scales.</p><p>2) Global Multi-Scale Residual Fusion block: Let [X 1 , X 2 , X 3 , X 4 ] denote features of distinct spatial resolutions (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). In the initial layer l = 1, where l represents the layer number in GMSRF module, each set of feature maps undergoes a convolution operation with output number of feature maps set as k, k being the growth factor <ref type="bibr" target="#b20">[21]</ref>.</p><p>Cross multi-scale attention maps (CMSA) are calculated for each scale concurrently. Eq. 1 represents how the l th CMSA is calculated for the i th scale. {X w , X y , X z } = X i are first transformed to the spatial resolution size of the i th scale by suitable convolution or transposed convolution operations (see <ref type="figure">Fig. 2</ref>). They are concatenated and then processed by a 3?3 convolution operation, to effectively fuse the features of selected scales.</p><p>X att,?,l = Conv 1?1 (Conv 3?3 (X w,l?1 ? X y,l?1 ?X z,?1 )), {w, y, z} = i</p><p>Here, ? represents concatenation operation. Attention maps are then generated to identify spatial locations based on the fused multi-scale features of parallel resolution streams. The information conveyed from low-resolution streams helps to boost the feature maps in the high-resolution stream and vice versa. The subsequent combination with the CMSA module allows the selection of features that are relevant towards identifying the region-of-interest. Global multi-scale residual fusion (GMSRF) is performed as described in Eq. (2). The l th convolutional layer in the i th resolution stream receives concatenated feature maps from l ? 1 th convolutional layer from all resolution scales and previous convolutional layers for the same resolution stream (see <ref type="figure" target="#fig_0">Fig. 1(b)</ref>). This global multi-scale fusion with densely connected blocks increases the number of paths through which feature maps can propagate and undergoes varying operations before contributing to the final segmentation map prediction.</p><formula xml:id="formula_1">X i,l = Conv 3?3 (X i,0 ? ? ? X i,l?1 ? X w,l?1 ?X y,l?1 ? X z,l?1 ), {w, y, z} = i<label>(2)</label></formula><p>The feature maps can capture the global multi-scale context at each layer of the densely connected mechanism. Eq. <ref type="formula" target="#formula_2">(3)</ref> describes how CMSA maps are used to identify and propagate relevant features of the i th scale stream forward.</p><formula xml:id="formula_2">X i,l = X i,l ? X att,?,l<label>(3)</label></formula><p>Multi-scale feature selection (MSFS) module, is the next step where channel-wise attention is applied on the fused features using squeeze and excitation (S&amp;E) block <ref type="bibr" target="#b21">[22]</ref> (refer to <ref type="figure">Fig. 2</ref>). This enables the amplification of salient channels transmitted by various scale streams. The suppression of irrelevant channels by this module is also conducive to a higher level of accuracy. Residual connection from the input of the GMSRF module is added to improve gradient flow. For simplification purposes, we use the i th scale while describing this mechanism.</p><p>3) Decoder: To fully establish the contribution of our GMSRF-Net, we choose to use a vanilla decoder (see <ref type="figure" target="#fig_0">Fig. 1(a)</ref>). X i is the output of the GMSRF module for the i th scale. Each decoder block upscales the output from the previous decoder block and concatenates the resultant feature maps from the same scale output of the GMSRF module (see <ref type="bibr">Equation 4</ref>).</p><formula xml:id="formula_3">D i = Conv 3?3 (T ransConv(D i?1 ) ? X i )<label>(4)</label></formula><p>Here, TransConv is strided transposed convolutional layer and initially D 4 = X 4 . The output of all decoder blocks, i.e D 4 , D 3 , D 2 , are upscaled to the size of ground truth maps for improved gradient flow and regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Loss Function:</head><p>We use a dual loss function L DUAL = L BCE + L IoU , where L DUAL is a combination of weighted intersection over union loss (L IoU ) and binary cross entropy (L BCE ). For all supervise segmentation maps generated by all decoder levels, the total loss function is given by:</p><formula xml:id="formula_4">L GMSRF = i=4 i=1 L DUAL (D i ),</formula><p>where i is the number of decoder layers.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental setup</head><p>We evaluate our proposed GMSRF-Net on Kvasir-SEG and CVC-ClinicDB. All images are resized to 256 ? 256 as a preprocessing step. We reserve 80% data for training, 10% for validation, and 10% for testing. The training set is augmented using techniques like random flipping, cropping, color jittering etc. All experiments are conducted using the same train-valtest configuration. We train the network for 50 epochs using Adam optimizer with initial learning rate of 1e ? 4 and batch size of 8. All experiments were performed on an NVIDIA DGX-2 machine using NVIDIA V100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results and discussion</head><p>From <ref type="table" target="#tab_0">Table I</ref>, it can be observed that our GMSRF-Net is competitive to MSRF-Net on the same source data (Kvasir-SEG), while outperforming on unseen data (CVC-ClinicDB). An increase of 8.34%, 15.93%, 1.05%, 15.88% in dice coefficient (DSC), mean intersection over union (mIoU), recall and precision, respectively, can be seen when compared with the best performing SOTA method (MSRF-Net). A similar  <ref type="table" target="#tab_0">Table II</ref> where our proposed method outperformed SOTA method on unseen Kvasir-SEG by large margins on all metrics: improvement of 10.31%, 15.40%, 14.44% and 6.42% on DSC, mIoU, recall and precision, respectively. Moreover, we can see that GMSRF-Net achieves a DSC of 0.8606 when trained on CVC-ClinicDB and tested on Kvasir-SEG (see <ref type="table" target="#tab_0">Table II</ref>). However, some networks such as U-Net++, ColonSegNet, and HRNetV2-W18-Smallv2 reports relatively lower performance even when they are trained and tested on Kvasir-SEG (see <ref type="table" target="#tab_0">Table II</ref>). <ref type="figure">Fig. 3</ref> exhibits that our GMSRF-Net produces optimal segmentation mask predictions in both scenarios when training and test sources are either the same or different.</p><p>Our experiments demonstrate that the multi-scale fusion technique that combines features from all resolution scales such as HR-Net and MSRF-Net yields better generalization performances (see <ref type="table" target="#tab_0">Table I</ref>-II). Our GMSRF-Net using global multi-scale residual fusion increases the number of fusion operations together with attention modules (CMSA and MSFS) achieving an improved generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>In this paper, we propose a global multi-scale feature fusion technique that incorporates CMSA and MSFS mechanisms for aggregating reliable and robust global features at each stage. Our proposed network maintains high resolution representa-tions and enriches high-resolution features by fusion with lowresolution feature streams and vice versa. The proposed technique achieves significant performance gain on segmentation tasks where the training and testing datasets are from different distributions. The generalization performance of our GMSRF-Net is an important step towards improving the generalizability of supervised learning methods. In future, we will extend our work towards quantifying the generalizability of the proposed model on other biomedical imaging datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The proposed GMSRF-Net architecture, a) The GMSRF-Net architecture (left), b) The GMSRF module (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Computation of CMSA and MSFS for scale 1 and scale 4 (computed for scale 2 and scale 3 as well) Qualitative results for GMSRF-Net trend can be noted in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table I :</head><label>I</label><figDesc>Result comparison when source dataset is Kvasir-SEG</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell cols="2">Source data mIoU Recall</cell><cell cols="5">Unseen dataset "CVC-ClinicDB" Precision DSC mIoU Recall Precision</cell></row><row><cell>U-Net [4]</cell><cell>0.8629</cell><cell>0.8176</cell><cell>0.9094</cell><cell>0.8901</cell><cell>0.7172</cell><cell>0.6133</cell><cell>0.7255</cell><cell>0.7986</cell></row><row><cell>U-Net++ [5]</cell><cell>0.7475</cell><cell>0.6313</cell><cell>0.6865</cell><cell>0.8871</cell><cell>0.4265</cell><cell>0.3345</cell><cell>0.3939</cell><cell>0.6894</cell></row><row><cell>Deeplabv3+( Xception) [13]</cell><cell>0.8965</cell><cell>0.8575</cell><cell>0.8984</cell><cell>0.9496</cell><cell>0.6509</cell><cell>0.5385</cell><cell>0.6251</cell><cell>0.7947</cell></row><row><cell cols="2">Deeplabv3+ (Mobilenet) [13] 0.8656</cell><cell>0.8186</cell><cell>0.8808</cell><cell>0.9205</cell><cell>0.6303</cell><cell>0.4825</cell><cell>0.5957</cell><cell>0.7173</cell></row><row><cell>HRNetV2-W18-Smallv2 [23]</cell><cell>0.8179</cell><cell>0.7470</cell><cell>0.8016</cell><cell>0.8696</cell><cell>0.6428</cell><cell>0.5513</cell><cell>0.6811</cell><cell>0.7253</cell></row><row><cell>HRNetV2-W48 [23]</cell><cell>0.8896</cell><cell>0.8262</cell><cell>0.8973</cell><cell>0.9056</cell><cell>0.7901</cell><cell>0.6953</cell><cell>0.8796</cell><cell>0.7694</cell></row><row><cell>ColonSegNet [11]</cell><cell>0.8203</cell><cell>0.7435</cell><cell>0.8124</cell><cell>0.8832</cell><cell>0.6895</cell><cell>0.5813</cell><cell>0.7862</cell><cell>0.7177</cell></row><row><cell>PraNet [6]</cell><cell>0.9078</cell><cell>0.8561</cell><cell>0.9034</cell><cell>0.9352</cell><cell>0.7225</cell><cell>0.6328</cell><cell>0.7531</cell><cell>0.7888</cell></row><row><cell>UACANet-S [7]</cell><cell>0.8800</cell><cell>0.8250</cell><cell>0.8701</cell><cell>0.9229</cell><cell>0.5683</cell><cell>0.4907</cell><cell>0.5792</cell><cell>0.7095</cell></row><row><cell>UACANet-L [7]</cell><cell>0.9014</cell><cell>0.8555</cell><cell>0.8897</cell><cell>0.9381</cell><cell>0.5589</cell><cell>0.4849</cell><cell>0.5800</cell><cell>0.6775</cell></row><row><cell>MSRF-Net [8]</cell><cell>0.9217</cell><cell>0.8914</cell><cell>0.9198</cell><cell>0.9666</cell><cell>0.7921</cell><cell>0.6498</cell><cell>0.9001</cell><cell>0.7000</cell></row><row><cell>GMSRF-Net</cell><cell>0.9263</cell><cell>0.8843</cell><cell>0.9402</cell><cell>0.9310</cell><cell>0.8755</cell><cell>0.8091</cell><cell>0.9106</cell><cell>0.8588</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table II :</head><label>II</label><figDesc>Result comparison when source dataset is CVC-ClinicDB</figDesc><table><row><cell>Method</cell><cell>DSC</cell><cell cols="2">Source data mIoU Recall</cell><cell cols="5">Unseen dataset "Kvasir-SEG" Precision DSC mIoU Recall Precision</cell></row><row><cell>U-Net [4]</cell><cell>0.9145</cell><cell>0.8654</cell><cell>0.9178</cell><cell>0.9381</cell><cell>0.6222</cell><cell>0.4588</cell><cell>0.5129</cell><cell>0.8133</cell></row><row><cell>U-Net++ [5]</cell><cell>0.8453</cell><cell>0.7559</cell><cell>0.8917</cell><cell>0.8323</cell><cell>0.5926</cell><cell>0.4564</cell><cell>0.7352</cell><cell>0.5462</cell></row><row><cell>Deeplabv3+ (Xception) [13]</cell><cell>0.8897</cell><cell>0.8706</cell><cell>0.9251</cell><cell>0.9366</cell><cell>0.6746</cell><cell>0.5327</cell><cell>0.7757</cell><cell>0.6296</cell></row><row><cell cols="2">Deeplabv3+ (Mobilenet) [13] 0.8985</cell><cell>0.8588</cell><cell>0.9160</cell><cell>0.9287</cell><cell>0.6474</cell><cell>0.5098</cell><cell>0.6632</cell><cell>0.6878</cell></row><row><cell>HRNetV2-W18-Smallv2 [23]</cell><cell>0.9073</cell><cell>0.8457</cell><cell>0.9137</cell><cell>0.9191</cell><cell>0.7012</cell><cell>0.6009</cell><cell>0.7184</cell><cell>0.7666</cell></row><row><cell>HRNetV2-W48 [23]</cell><cell>0.9244</cell><cell>0.8747</cell><cell>0.9234</cell><cell>0.9296</cell><cell>0.7404</cell><cell>0.6233</cell><cell>0.7293</cell><cell>0.8511</cell></row><row><cell>ColonSegNet [11]</cell><cell>0.9132</cell><cell>0.8600</cell><cell>0.9072</cell><cell>0.9292</cell><cell>0.6324</cell><cell>0.5183</cell><cell>0.6112</cell><cell>0.7897</cell></row><row><cell>PraNet [6]</cell><cell>0.9072</cell><cell>0.8575</cell><cell>0.9227</cell><cell>0.9134</cell><cell>0.7293</cell><cell>0.6262</cell><cell>0.8007</cell><cell>0.7623</cell></row><row><cell>UACANet-S [7]</cell><cell>0.9190</cell><cell>0.8700</cell><cell>0.9285</cell><cell>0.9201</cell><cell>0.6945</cell><cell>0.5894</cell><cell>0.7692</cell><cell>0.7377</cell></row><row><cell>UACANet-L [7]</cell><cell>0.9098</cell><cell>0.8649</cell><cell>0.9174</cell><cell>0.9114</cell><cell>0.7312</cell><cell>0.6383</cell><cell>0.7417</cell><cell>0.8314</cell></row><row><cell>MSRF-Net [8]</cell><cell>0.9420</cell><cell>0.9043</cell><cell>0.9567</cell><cell>0.9427</cell><cell>0.7575</cell><cell>0.6337</cell><cell>0.7197</cell><cell>0.8414</cell></row><row><cell>GMSRF-Net</cell><cell>0.9326</cell><cell>0.8882</cell><cell>0.9376</cell><cell>0.9307</cell><cell>0.8606</cell><cell>0.7877</cell><cell>0.8641</cell><cell>0.9056</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This research study was conducted retrospectively using human subject data made available in open access by Kvasir-SEG and CVC-ClinicDB. Ethical approval was not required.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krapcho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tatalovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mariotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lewis</surname></persName>
		</author>
		<title level="m">Seer cancer statistics review</title>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
	<note>national cancer institute. bethesda, md</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Colorectal cancer epidemiology: incidence, mortality, survival, and risk factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Haggar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Boushey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinics in colon and rectal surgery</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="191" to="197" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Endoscopic polyp segmentation using a hybrid 2d/3d cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Puyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brandao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mountney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="295" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">UNet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Med. Imag</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">PraNet: parallel reverse attention network for polyp segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-P</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat</title>
		<meeting>of Internat</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="263" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Uacanet: Uncertainty augmented context attention for polyp semgnetaion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.02368</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Msrf-net: A multi-scale residual fusion network for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Johansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Halvorsen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.07451</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ResUNet++: An advanced architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat. Sympos. Multime</title>
		<meeting>of Internat. Sympos. Multime</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="225" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">UNet++: A nested U-Net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
	<note>in Deep learn. med. ima. anal. multimo. learn. clini. deci. sup.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-Time Polyp Detection, Localisation and Segmentation in Colonoscopy Using Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Acc</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multiresunet: Rethinking the u-net architecture for multimodal biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ibtehaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Rahman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neur. Networ</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="74" to="87" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Europ. conf. comput. vis</title>
		<meeting>of the Europ. conf. comput. vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE trans. patt. analy. mach</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Igcv3: Interleaved low-rank group convolutions for efficient deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00178</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Interleaved structured sparse convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8847" to="8856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kvasir-SEG: A Segmented Polyp Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Internat. Conf. Multimed. Model</title>
		<meeting>of Internat. Conf. Multimed. Model</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="451" to="462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer. Medi. Imag. Graph</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Receptive field block net for accurate and fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="385" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Comput. Vis. and Patt</title>
		<meeting>of Comput. Vis. and Patt</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Other</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Patt. Analy. Mach. Intelli</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
