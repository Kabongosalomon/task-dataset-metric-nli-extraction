<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene Completion</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxia</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RGBD Based Dimensional Decomposition Residual Network for 3D Semantic Scene Completion</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>RGB images differentiate from depth as they carry more details about the color and texture information, which can be utilized as a vital complement to depth for boosting the performance of 3D semantic scene completion (SSC). SSC is composed of 3D shape completion (SC) and semantic scene labeling while most of the existing approaches use depth as the sole input which causes the performance bottleneck. Moreover, the state-of-the-art methods employ 3D CNNs which have cumbersome networks and tremendous parameters. We introduce a light-weight Dimensional Decomposition Residual network (DDR) for 3D dense prediction tasks. The novel factorized convolution layer is effective for reducing the network parameters, and the proposed multi-scale fusion mechanism for depth and color image can improve the completion and segmentation accuracy simultaneously. Our method demonstrates excellent performance on two public datasets. Compared with the latest method SSCNet, we achieve 5.9% gains in SC-IoU and 5.7% gains in SSC-IOU, albeit with only 21% network parameters and 16.6% FLOPs employed compared with that of SSCNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We live in a 3D world where everything occupies part of the physical space under the view of the human perception system. Similarly, 3D scene understanding is of importance since it is a reflection about the real-world scenario. As one of the most vital fields in 3D scene understanding, Semantic Scene Completion (SSC) has verity of applications, including robotic navigation <ref type="bibr" target="#b12">[13]</ref>, scene reconstruction <ref type="bibr" target="#b13">[14]</ref>, autodriving <ref type="bibr" target="#b21">[22]</ref> etc. However, due to the dimensional curse brought by 3D representation <ref type="bibr" target="#b39">[40]</ref> and the limited annotation datasets, the research field of SSC still step slowly in the past decades. * First two authors contributed equally.</p><p>With the renaissance of deep learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref> and a few large-scale datasets being made available <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36]</ref> in recent years. The research activities of 3D shape processing thrive again in the computer vision community, injecting new possibilities and objectives for SSC as well introducing some unprecedented challenges.</p><p>Conventional methods usually utilize the hand-crafted features, such as voxel <ref type="bibr" target="#b18">[19]</ref> and TSDF <ref type="bibr" target="#b17">[18]</ref> to represent the 3D object shape, and make use of the graph model to infer the scene occupations and semantic labeling individually <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>. The current state-of-the-art technique SSC-Net <ref type="bibr" target="#b35">[36]</ref>, instead uses an end-to-end 3D network to conduct the scene completion and category labeling simultaneously. Through combining the semantic and geometrical information implicitly via the network learning process, the two individual tasks can benefit from each other.</p><p>Though remarkable gains in terms of scene completion and labeling accuracy have been achieved, the massive amount of parameters brought by the 3D representation make it computing-intensive. Moreover, another problem suffered in the existing SSC is the low-resolution representation <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b10">11]</ref>. In particular, due to the limitation of computation resources, both of the conventional and deep learning based methods sacrifice high-resolution to compromise an acceptable speed.</p><p>On the other hand, most of existing methods solely use depth as input, which is struggled to differentiate objects from various categories. For example, a paper and a tablecloth on the same table can be easily distinguished by color or texture information. To sum up, depth and color image are different modalities captured by the sensor, they all provide us with what the scene looks like. The former gives us more sense about the object shape and distance, while the later transfers more information about the object texture and saliency. It is proved that both of the two modalities are helpful to boost the performance of SSC task <ref type="bibr" target="#b6">[7]</ref>, although how to fuse them is still an unsolved problem.</p><p>To overcome the problems as mentioned above, we propose a light-weight semantic scene completion network, which utilizes both of the depth and RGB information. It formulates the 3D scene completion and labeling as a joint task and learns in an end-to-end way. The main contributions of this paper are three-fold:</p><p>? Firstly, we propose the dimensional decomposition residual (DDR) blocks for 3D convolution, which dramatically reduces the model parameters without performance degradation. ? Secondly, 3D feature maps of RGB and depth are fused in multi-scale seamlessly, which enhances the network representation ability and boost the performance of SC and SSC tasks. ? Thirdly, the proposed end-to-end training network achieves state-of-the-art performance on NYU <ref type="bibr" target="#b34">[35]</ref> and NYUCAD <ref type="bibr" target="#b5">[6]</ref> datasets. The rest of this paper is organized as follows. Section 2 briefly summaries the related works, Section 3 introduces the methodology. Section 4 presents the experimental results.Section 5 analyses different parts of the proposed method. Section 6 summarizes our findings and concludes with future research interests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Scene Completion and Semantic Labeling</head><p>As an important branch in 3D scene understanding, semantic scene completion (SSC) has many real-world applications and has received increasing attention recently with the support of deep learning <ref type="bibr" target="#b20">[21]</ref> and the large-scale annotated dataset <ref type="bibr" target="#b35">[36]</ref>.</p><p>SSCNet <ref type="bibr" target="#b35">[36]</ref> is the first one which formulates the shape completion and semantic labeling as a joint task and learns the task in an end-to-end way. TS3D <ref type="bibr" target="#b6">[7]</ref> is based on SSC-Net, and utilizes an additional network to incorporate the color information into the learning loop. Both of SSC-Net and TS3D adopt truncated signed distance function (TSDF <ref type="bibr" target="#b17">[18]</ref>) to encode the 3D volume, where every voxel stores the distance value d to its closest surface, and the sign of the value indicates whether the voxel is in free space or occluded. However, TSDF is computationally intensive since it requires the calculation of the distance between the points on the surface and each point belong to the objects. Although with remarkable performance achieved, the 3D convolution representation results in a network that is computationally expensive with highly redundant parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Computation-efficient Networks</head><p>As a milestone in deep learning architectures, ResNet <ref type="bibr" target="#b15">[16]</ref> uses a residual block to prevent the performance degradation that occurs when network layers become deep. The extreme deep network leads to the state-of-the-art performance in many tasks including image classification <ref type="bibr" target="#b20">[21]</ref>, object detection <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b25">26]</ref> and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. However, this is very expensive concerning computation resource and heavy-burden <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>. To cater to the appeal for real-world applications, there is a trend to tailor the heavy-burden networks to the light-weight network in recent years.</p><p>Feature Representation Considering the redundant information contained in the 3D scene completion, the first spectrum of work try to model the scene with sparse feature representation. Specifically, OctNet <ref type="bibr" target="#b30">[31]</ref> and O-CNN <ref type="bibr" target="#b38">[39]</ref> utilize the Octree-based CNN to represent the 3D object shape. PointNet <ref type="bibr" target="#b1">[2]</ref> and Kd-Networks <ref type="bibr" target="#b19">[20]</ref> employ point clouds to indicate the occupation of the scene. Although saving the memory and computation, the neighbor pixels are usually mapped to the same voxel, which inevitably causes the detail missing for semantic labeling and scene understanding.</p><p>Group Convolution In recent two years, there are several popular light-weight networks have been proposed, include MobileNet <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref> and ShuffleNet <ref type="bibr" target="#b42">[43]</ref>. In Mo-bileNet, depth-wise convolutions and point-wise convolutions are utilized to separate the channels as well as reduce the parameters and the calculations. In ShuffleNet, besides the group point-wise convolution and depth-wise convolution are adopted, shuffle layer is developed for information exchange between different shuffle units. However, the above models heavily rely on depth-wise convolution and group convolution, and mainly target at 2D networks thus can not directly be applied for 3D tasks.</p><p>Spatial Group Convolution To improve the computing efficiency of the 3D network. EsscNet <ref type="bibr" target="#b41">[42]</ref> is introduced, rather than to conduct the group convolution on feature channel dimension, which adopts the group convolution on the spatial aspect. The drawback of spatial group convolution is that it splits the features manually into separate parts, which cause the performance drops. Meanwhile, the splitting process involves hash table maintaining and coordinate with other blocks, and is cumbersome for transplantation. On the contrary, the proposed DDR block is much flexible, and it can be planted to any network which contains the 3D modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Modality Fusion in SSC</head><p>There are many works focused on RGBD fusion in 2D applications <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b11">12]</ref>. RGBD sensor can capture the depth and color images simultaneously, although depth can be used to infer the geometry of the scene, which is too sparse to reconstruct the occluded parts of the scene. Compared with depth, color image carries more cues about texture, color, and reflections, which can be viewed as an essential complement to the depth for SSC task. Following the design philosophy of SSCNet, TS3D <ref type="bibr" target="#b6">[7]</ref> adds the color image into the work-flow. However, the scene labeling needs to be estimated twice, and the depth flow and color flow are still apart from each other from the essential.</p><p>In <ref type="bibr" target="#b9">[10]</ref>, two fusion strategies were proposed, one is early-stage fusion which concatenates the feature at the first layer, and another is mid-level fusion which concatenates the features before the output layer. Although follow the overall design and reuse the features of SSCNet, the performance of adopting both fusion strategies are unexpectedly worse than that of SSCNet.</p><p>The most related work for feature fusion is RDFNet <ref type="bibr" target="#b26">[27]</ref>, which utilizes multi-scale fused features from color images, and aims to build a 2D segmentation framework. However, fusing the features in the 3D network is much more challenging as mentioned before. In this paper, we propose a novel fusion strategy which effectively fuses the 3D depth and color features on multi-scales without bringing in extra parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>This section presents the proposed light-weight network for SSC. The computation-efficient Dimensional Decomposition Residual (DDR) block, as well as a novel modality fusion module, are emphasized. On the one hand, through dimensional splitting on 3D convolutions and dense connection, using DDR blocks can dramatically reduce the network parameters. On the other hand, through fusing the 3D features of depth and color image seamlessly, the proposed network can efficiently make use of the information captured by the RGBD sensors, and various modulates of inputs complement with each other thus boost the perfor-mance of shape completion and scene labeling simultaneously. The framework of the proposed network is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The network has two feature extractors, which take a full resolution depth and the corresponding color image as inputs, respectively. The network first uses 2D DDR blocks to learn the local textures and the geometry representation. Then, the 2D feature maps are projected to 3D space by a projection layer. A multi-level fusion strategy is then applied to fuse the texture and geometry information. After that, the network responses are then concatenated and fed into the subsequent light-weight Atrous Spatial Pyramid Pooling (ASPP) module to aggregate information in multiple scales. In the end, another three pointwise convolutional layers are used to predict the final voxel labels. The following parts will explain the design details of each module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dimensional Decomposition Residual Blocks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Basic DDR</head><p>Residual layers <ref type="bibr" target="#b15">[16]</ref> have the property of allowing convolutional layers to approximate residual functions,</p><formula xml:id="formula_0">x t = F d (x t?1 , {W i }) + x t?1<label>(1)</label></formula><p>where x t?1 and x t are the input and output.. The function F d (x t?1 , {W i }) represents the residual mapping to be learned and d is the dilation rate within the block. This residual formulation facilitates learning and alleviates the degradation problem present in architectures that stack a large number of layers <ref type="bibr" target="#b32">[33]</ref>. Directly applying the original (2D) ResNet block into the 3D dense prediction task, the two corresponding 3D residual layers will be: the non-bottleneck design with two-layer 3 ? 3 ? 3 convolutions as described in <ref type="figure" target="#fig_1">Figure 2</ref>(a), and the three-layer bottleneck version as depicted in <ref type="figure" target="#fig_1">Figure 2</ref>(c).</p><formula xml:id="formula_1">(a) ResNet 3D (c) Deeper ResNet 3D 1?1?1 , w/4 3?3?3 , w/4 1?1?1 , w relu relu relu ?1 channel=w 3?3?3 , w 3?3?3 , w relu relu ?1 channel=w ? ? + ?1 1?1?3, w 1?3?1, w 3?1?1, w relu relu relu ?1 channel=w relu relu relu relu 1?1?1, w/4 1?1?3, w/4 1?3?1, w/4 relu ?1 channel=w 3?1?1, w/4 1?1?1, w (b) Basic DDR (d) Deeper DDR</formula><p>However, both of the two structures will suffer the problem of high computational costs as the network parameters grow in cubic. We propose to redesign the residual through decomposing the 3D convolution into three consecutive layers along each dimension. The proposed basic DDR block is shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b) and its deeper bottleneck version is shown in <ref type="figure" target="#fig_1">Figure 2</ref>(d). In this way, the network can reduce parameters and capable of capturing 3D geometric information according to the theory in <ref type="bibr" target="#b36">[37]</ref>.</p><p>Here we provide an episode to illustrate the effectiveness of DDR block for reducing network parameters: Considering a 3D CNN with input channels c in , output channels c out , and kernel size of k x ? k y ? k z . Without losing the generality, we can assume k x = k y = k z = k. The original block in 3D CNN is then be decomposed into three consecutive layers with filter size 1?1?k, 1?k ?1 and k ?1?1, accordingly. The computational costs of the original block and DDR block are proportional to c in ?c out ?k?k?k and c in ?c out ?(k+k+k), respectively. The advantage of DDR for reducing network parameters will be enlarged when k become large, since 3k k 3 . As an example, the parameters of a typical 3D convolutional layer with a 3 ? 3 ? 3 kernel will drops to 1/3 after adopting DDR block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Deeper DDR</head><p>Inspired by the bottleneck design <ref type="bibr" target="#b15">[16]</ref>, we further deliver a deeper DDR block. In specific, for each residual function, a 1?1?1 layer is added at both the top and bottom of the Dimensional Decomposition convolutions. The 1?1?1 layers are responsible for reducing and restoring the dimensions, which make the three Dimensional Decomposition convolutional layers form a bottleneck with smaller input/output dimensions.</p><p>Moreover, parameter-free identity shortcuts are added within each dimensional decomposition convolution. The dense identity connections are not only helpful for robust feature representation but also have the merits of alleviating the vanishing-gradient problem and strengthen the feature propagation <ref type="bibr" target="#b15">[16]</ref>. In the remainder of the paper, DDR refers to the deeper DDR block unless specifically noted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Two Modality Multi-level Feature Fusion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Feature Extractor Module</head><p>In our network, there are two parallel branches for feature extraction corresponding to the depth and color image. As shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, the feature extractor module is composed of three components: a 2D feature extractor, a 3D feature extractor and a projection layer which mapping the 2D feature to the 3D feature. The network first utilizes 2D feature extractor to learn local color and texture representation. After feature mapping to the 3D space by a projection layer, 3D feature extractor is employed to acquire the geometry and context information.</p><p>2D Feature Extractor To extract features from a 2D depth and color image, a 2D point-wise convolution is firstly used to increase the channels of feature maps. Then two 2D DDR blocks are stacked for residual learning. Through the process, the resolution of the output feature map keeps the same as the input image. Please note, in our network, the number of parameters for 2D DDR blocks is 192, which is insignificant when compared with the 195k parameters for 3D DDR blocks. Therefore, we mainly focus on the light-weight operations of 3D DDR blocks.</p><p>Projection Layer Since each pixel in the depth map corresponding to a tensor in the 2D feature map, every feature tensor can be projected into the 3D volume at the location with the same depth value. This step yields an incomplete 3D volume that assigns to every surface voxel its corresponding feature tensor. For the voxels that are not occupied by any depth values, their feature vectors are set to zeros. The mapping index T u,v at (u, v) can be computed using the depth value I u,v and camera pose C which are provided along with each image. Because the feature volume resolution is lower than the feature map resolution, several neighboring features will be projected into the same voxel, and we use max-pooling to simulate this step. With the feature projection layer, the 2D feature maps extracted by the 2D CNN are converted to a view-independent 3D feature volume. During training, the mapping indexes T between feature map tensors and voxels are recorded in a table for gradient back-propagation.</p><p>3D Feature Extractor After the feature projection layer, a view-independent 3D feature volume is acquired. In this step, we further extract features using two 3D DDR blocks. A down-sample block is added in front of each DDR blocks to reduce the size of the feature maps and increase the dimension of its channel. <ref type="figure" target="#fig_0">Figure 1(c)</ref> shows the structure of the down-sample block. A pooling layer and a pointwise convolution layer are concatenated to increase the channels of the output feature map of the down-sample block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Multi-level Feature Fusion</head><p>One primary challenge of 3D RGBD based semantic segmentation is how to effectively extract the color features along with depth features and to utilize those features for the labeling. To fully use the multi-modal features, we propose a novel feature fusion strategy which is inspired by <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b26">27]</ref>. We employ multi-modal CNN feature fusion while preserving the lower computational cost. In specific, different levels of features are extracted through multiple DDR modules, and then these features are merged together by element-wise add. The reason for using element-wise add rather than other operations is because it can fuse the features neatly with insignificant computation costs.</p><p>Through the cascaded DDR blocks, both low-level features and high-level features are captured, which enhance the representation ability of the network and is beneficial for the performance of semantic scene completion task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Light-weight ASPP Module</head><p>Different object categories have various physical 3D sizes in indoor scenes. This requires the network to capture information at different scales in order to recognize the objects reliably. Atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> exploits multi-scale features by employing multiple parallel filters with different dilatation rates and has been proved to be powerful to improve the CNN's ability to handle objects with various sizes. However, directly applying ASPP in 3D semantic scene completion would bring in tremendous parameters as well as large computations.</p><p>Based on this consideration, we introduce a light-weight ASPP (LW-ASPP) which is capable of handling scale variability while requiring fewer computations. In specific, LW-ASPP uses multiple parallel DDR blocks with different sampling (dilation) rates. The dilated DDR is implemented by setting a dilation rate in the three-dimensional decomposition convolutions within the DDR block. The dilated DDR explicitly adjusts the field-of-view of filters as well as controls the resolution of the feature responses. The features extracted from different sampling rates are further concatenated and fused to generate the final result with the output layer, which is constructed by the three 3D point-wise convolution layers as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training and Loss</head><p>Training Given the training dataset (i.e. the RGBD images and ground truth volumetric object labels of 3D scenes), our method can be trained end-to-end. SSCNet <ref type="bibr" target="#b35">[36]</ref> sets a small value (0.05) as the weight of the voxels in free space for data balancing in the training process. We adopt the same strategy in our early training process. With each additional 50 training epochs, the weight of empty voxels is gradually doubled until it is set to be the same as the other occupied voxels. All the experiments are conducted using the py-Torch framework on GPU. Our model is trained using the SGD optimizer with a momentum of 0.9, weight decay of 10 ?4 and batch size is 2, the initial learning rate is 0.01 and divided by a factor of 10 when the training loss changes less than 1e-4 within 5 consecutive epochs.</p><p>Loss For training the network, we employ the softmax cross entropy loss on the unnormalized network outputs y:</p><formula xml:id="formula_2">L = ? N c=1 w c?i,c log e yic N c e y ic<label>(2)</label></formula><p>where? i,c are the binary ground truth vectors, i.e.? i,c = 1 if voxel i is labeled by class c, N is the number of classes, and w c is the loss weight. To compute the loss function, we remove all voxels outside the field of view and the room and include all non-empty voxels plus occluded voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate and compare the proposed method with the state-of-the-art approaches on two public datasets, i.e. NYU <ref type="bibr" target="#b34">[35]</ref> and NYUCAD <ref type="bibr" target="#b5">[6]</ref>. Both the quantitative and qualitative results demonstrate the superiority of our algorithm on SSC task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Metrics</head><p>Dataset We evaluate the proposed method on the NYUv2 dataset <ref type="bibr" target="#b34">[35]</ref>, which is in the following denoted as NYU. NYU consists of 1449 indoor scenes that are captured via a Kinect sensor. Following SSCNet <ref type="bibr" target="#b35">[36]</ref>, we use the 3D annotated labels provided by <ref type="bibr" target="#b31">[32]</ref> for semantic scene completion task. NYUCAD <ref type="bibr" target="#b5">[6]</ref> uses the depth maps generated from the projections of the 3D annotations to reduce the misalignment of depths and the annotations. We compare our method with the state-of-the-art methods on both NYU and NYUCAD datasets. Metrics As the evaluation metric, the voxel-level intersection over union (IoU) between the predicted voxel label and ground truth label is used. For the task of semantic scene completion, we evaluate the IoU of each object classes on both the observed and occluded voxels. For the task of scene completion, we treat all non-empty object class as one category and evaluate IoU of the binary predictions on the occluded voxels. <ref type="table" target="#tab_0">Table 1</ref> shows the results on NYU dataset acquired by our method and the state-of-the-art methods. We achieve state-of-the-art performance regarding different metrics. Specifically, we achieve the best performance for both the tasks of scene completion and semantic scene completion and also rank the second best of recall and precision for scene completion. We outperform the previous SSCNet by a significant margin in overall performance, that are 5.7% gains in semantic scene completion and 5.9% gains in scene completion. The proposed network demonstrate the superior performance in some categories such as ceil., table, tvs, furn. etc. We inspect this improvement due to the novel architecture, that makes use of the robust features from multilevel and multi-modalities, and data fusion, which effectively complement the details from the color image to these textureless objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with the State-of-the-art Methods</head><p>To validate the robustness and generalization of the proposed network, we also conduct experiments on NYUCAD dataset as shown in <ref type="table">Table 2</ref>. The comparison results with the state-of-the-art methods present the same trend. Among all of the methods, we achieve the best performance for semantic scene completion and scene completion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Params/k FLOPs/G SC-IoU SSC-IoU SSCNet <ref type="bibr" target="#b35">[36]</ref> 930.0 163. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Quantitative Analysis</head><p>Since we target at a light-weight 3D network for semantic scene completion, in this section, we list the params and FLOPs of the proposed method as well as the baseline method. As shown in <ref type="table">Table 4</ref>. In specific, compared with the state-of-the-art method SSCNet, the parameters in our method is 21.0% of that in SSCNet, and the FLOPs is 16.6% of that SSCNet. However, the performance of both scene completion and semantic scene completion is around 6% higher than that of SSCNet. Compared with the Essc-Net <ref type="bibr" target="#b41">[42]</ref>, depth solely is used as the input for a fair comparison, our method is computationally cheaper than EsscNet with 6% reduction in FLOPS and increased performance. For SC and SSC tasks, EsscNet reaches the accuracies of 56.2% (SC) and 26.7% (SSC), and we achieve 59.0% (SC) and 28.9% (SSC). <ref type="figure">Figure 3</ref>.4 shows visualized results (in different scenarios) of the scene segmentation generated by the proposed method (c) and SSCNet (d), ground truth (b) are also provided as a reference. All the results are acquired on the NYUCAD validation set. As can be seen, compared with SSCNet, the scene completion results of our method is much more abundant in detail and less error-prone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative Analysis</head><p>It can be easily seen that our method performs better for objects such as furn, wall, and floor. For example, in the second and third rows, SSC will cause some missing in the details of the wall, which is rarely happening in our algorithm. Part of the reason that our method is better for handling the texture-less and small objects we attribute which come from the incorporated color features. In row(1), our  method effectively captures the detail information about the leg of the chair. In addition, compared with SSCNet, the proposed method maintains the segmentation consistency for objects with big sizes, such as the wall and floor in the row <ref type="bibr" target="#b1">(2)</ref> and ceiling in the row <ref type="bibr" target="#b3">(4)</ref>. And row <ref type="formula">(3)</ref> shows a much challenging instance, i.e. the window, both SSCNet and our method cannot acquire satisfied results. However, our method can recognize part of the information. Row <ref type="bibr" target="#b4">(5)</ref> and row <ref type="formula">(6)</ref> show the failure cases in our methods, specifically, in the row <ref type="formula">(5)</ref>, the fresco on the wall has the similar texture with the stuff on the bookshelf, it thus wrongly classified into the category of the bookshelf. In row <ref type="formula">(6)</ref>, the ground-truth furniture circled by the red dashed rectangle, SSCNet wrongly predicts it into the object category, and our network wrongly classifies it as a chair, which may due to the quite similar shape and color information between the furniture and the chair category. In supplementary materials, more visualized results are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation Study</head><p>RGB and Depth Fusion Both RGB and Depth information are important for 3D scene understanding. To verify the effectiveness of the proposed multi-level fusion strategy, we evaluate the performance of our method with only depth or RGB image as the input. As can be seen in <ref type="table">Table 3</ref>, and the performance on SSC of our method for only using depth or color image as input are 28.9% and 27.2%, respectively. Since RGB images carry more details such as color and texture, which is beneficial for the semantic information, this can be seen from the results of category tvs and category sofa. However, the advantage of using depth lies on it carries more geometry information, for the objects which are difficult to differentiate through color information, it is much easier to tell the difference according to their shapes. Such as table and floor. Moreover, depth is less sensitive regarding illumination changes and the dramatic color variation within the same category, which may explain for the indoor scene, the result of using depth is a bit better than that of using a color image as input. Meanwhile, merging depth and color features in our method significantly improve the SSC performance, which proves the two-modality information can be an excellent complement to each other. And benefit from the lightweight DDR block applied in the network, the overall computations and parameters remain small. Light-weight ASPP The effectiveness of ASPP has been verified in 2D semantic segmentation <ref type="bibr" target="#b2">[3]</ref> task. However, the direct expansion of ASPP from 2D to 3D would bring  in a massive amount of parameters as well as make the network cumbersome. Lightweight ASPP (LW-ASPP) using DDR block as the primary core, which not only effectively reduces the network parameters but also inherits the merits of ASPP for capturing multi-scale information, thus is beneficial to the 3D task.</p><p>In order to verify the validity of LW-ASPP, we design a group of experiments in which LW-ASPP was removed from the network or replaced with 3D ASSP directly extended from ASPP. As can be seen in <ref type="table" target="#tab_3">Table 5</ref>, when compared with the network without ASPP module, adding LW-ASPP boosts the SC-IoU 3.2% and SSC-IoU 3.6%. When replacing LW-ASPP with 3D-ASPP, the performance can be further improved by a small margin but with the sacrifice of over two times params and around three times FLOPs. Change of speed/memory and performance As shown in <ref type="table" target="#tab_5">Table 6</ref>, DDRNet with quite a few parameters and FLOPs compared to SSCNet. DDRNet has a deeper structure, thus stronger non-linear representation ability than the 3D-ResNet version, albeit less memory cost required. Moreover, DDRNet achieves much faster speed with an insignificant performance loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes a novel structure for handling the semantic scene completion problem. Specifically, an endto-end light-weight Dimensional Decomposition Residual (DDR) network is delivered for scene completion and semantic scene labeling. The two contributions are the proposed factorized convolution layer and a novel twomodality fusion mechanism. The former is effective to reduce the parameters within the network, and the later can fuse the depth and color image seamlessly in multi-level, the state-of-the-art results are achieved for both SSC and SC task on two public datasets. In the future, considering to differentiate instances of the indoor scene as well as to incorporate the shuffle layer into the proposed light-weight network will be our research interests.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Network architecture for semantic scene completion. Taking RGBD image as input, the network predicts occupancies and object labels simultaneously. (b) Detailed structure of the feature extractor. (c) Structure of the down-sample block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Residual blocks and the proposed DDR blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. Lin et al. Results on the NYU dataset. Bold numbers represent the best scores.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">scene completion</cell><cell></cell><cell cols="3">semantic scene completion</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="3">prec. recall [23] 58.5 49.9 36.4 0.0</cell><cell>11.7 13.3 14.1</cell><cell>9.4</cell><cell>29.0 24.0</cell><cell>6.0</cell><cell>7.0</cell><cell>16.2</cell><cell>1.1</cell><cell>12.0</cell></row><row><cell cols="3">Geiger et al. [8] 65.7</cell><cell cols="2">58.0 44.4 10.2 62.5 19.1 5.8</cell><cell>8.5</cell><cell>40.6 27.7</cell><cell>7.0</cell><cell>6.0</cell><cell>22.6</cell><cell>5.9</cell><cell>19.6</cell></row><row><cell cols="2">SSCNet [36]</cell><cell>57.0</cell><cell cols="2">94.5 55.1 15.1 94.7 24.4 0.0</cell><cell cols="3">12.6 32.1 35.0 13.0</cell><cell>7.8</cell><cell cols="2">27.1 10.1 24.7</cell></row><row><cell cols="2">EsscNet [42]</cell><cell>71.9</cell><cell cols="2">71.9 56.2 17.5 75.4 25.8 6.7</cell><cell cols="3">15.3 53.8 42.4 11.2</cell><cell>0</cell><cell cols="2">33.4 11.8 26.7</cell></row><row><cell>ours</cell><cell></cell><cell>71.5</cell><cell cols="2">80.8 61.0 21.1 92.2 33.5 6.8</cell><cell cols="6">14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .Table 3 .</head><label>23</label><figDesc>scene completion semantic scene completion Methods prec. recall IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. 74.2 33.8 92.9 46.8 27.0 27.9 61.6 51.6 27.6 26.9 44.5 22.0 42.Results on the NYUCAD dataset. Bold numbers represent the best scores. FLOPs/G ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. Ablation experiments of RGB and Depth fusion.</figDesc><table><row><cell cols="2">Zheng et al. [44] 60.1</cell><cell>46.7 34.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Firman et al. [6]</cell><cell>66.5</cell><cell>69.7 50.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSCNet [36]</cell><cell>75.4</cell><cell cols="5">96.3 73.2 32.5 92.6 40.2 8.9</cell><cell cols="4">33.9 57.0 59.5 28.3</cell><cell>8.1</cell><cell cols="3">44.8 25.1 40.0</cell></row><row><cell>TS3D [7]</cell><cell>80.2</cell><cell cols="13">91.0 1</cell></row><row><cell>ours</cell><cell>88.7</cell><cell cols="9">88.5 79.4 54.1 91.5 56.4 14.9 37.0 55.7 51.0 28.8</cell><cell>9.2</cell><cell cols="3">44.1 27.8 42.8</cell></row><row><cell cols="2">Methods Params/k Ours-Depth 155.0</cell><cell>20.6</cell><cell cols="4">30.6 93.0 28.6 6.7</cell><cell cols="4">13.6 60.3 20.0 12.3</cell><cell>0.</cell><cell cols="3">30.9 12.0 28.9</cell></row><row><cell>Ours-RGB</cell><cell>155.0</cell><cell>20.6</cell><cell cols="4">19.3 91.8 30.5 3.7</cell><cell cols="4">13.1 44.4 37.1 10.6</cell><cell>5.5</cell><cell cols="3">31.0 11.9 27.2</cell></row><row><cell>Ours-RGBD</cell><cell>195.0</cell><cell>27.2</cell><cell cols="4">21.1 92.2 33.5 6.8</cell><cell cols="8">14.8 48.3 42.3 13.2 13.9 35.3 13.2 30.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Qualitative results on NYUCAD. From left to right: Input RGB-D image, ground truth, results obtained by our approach, and results obtained by SSCNet<ref type="bibr" target="#b35">[36]</ref>. Overall, our completed semantic 3D scenes are less cluttered and show a higher voxel class accuracy compared to SSCNet. Refer to Section 4.4 for the detailed analysis. Params, FLOPs and Performance with/without ASPP.</figDesc><table><row><cell>Ceil</cell><cell>floor</cell><cell>wall</cell><cell>window</cell><cell>Chair</cell><cell>bed</cell><cell>sofa</cell><cell>table</cell><cell>tvs</cell><cell>furn</cell><cell>objects</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The inference speed and GPU memory usage of our DDR-Net and the 3D-ResNet based networks. All results are acquired on a GTX1080ti GPU and evaluated on the NYU<ref type="bibr" target="#b32">[33]</ref> test set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is supported by the National Natural Science Foundation of China under grants 61603184 and 61773210. We also gratefully acknowledge the support of the Australian Research Council through grants CE140100016 and FL130100102.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06158</idno>
		<title level="m">Matter-port3d: Learning from rgb-d data in indoor environments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Q</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaichun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.02611</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Julier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Two stream 3d semantic scene completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03550</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint 3d object and layout inference from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From motion blur to motion flow: a deep learning solution for removing heterogeneous motion blur</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2319" to="2328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B S</forename><surname>Guedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">View-volume network for semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.05361</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Indoor scene understanding with rgb-d images: Bottom-up segmentation, object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="133" to="149" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="564" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scene completion using millions of photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kinectfusion: real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d scene understanding by voxel-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1425" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kdnetworks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic analysis of dynamic scenes and collision risks assessment to improve driving safety</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Paromtchik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrollaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Yoder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mekhnacha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>N?gre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ITS Magazine</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4" to="19" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1925" to="1934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning pairwise relationship for multi-object detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.03796</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rdfnet: Rgb-d multi-level residual feature fusion for indoor semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3d graph neural networks for rgbd semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5199" to="5208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Completing 3d object shape from one depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thorsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2484" to="2493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-ITS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="272" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="190" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning common and specific features for rgb-d semantic segmentation with deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="664" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y.</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning 3d shape from a single facial image via non-linear manifold embedding and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="414" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Two-stream convolutional networks for blind image quality assessment. IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2200" to="2211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Efficient semantic scene completion network with spatial group convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yaoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liaoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="733" to="749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond point clouds: Scene understanding by reasoning geometry and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3127" to="3134" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
