<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DPIT: Dual-Pipeline Integrated Transformer for Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaitao</forename><surname>Zhao</surname></persName>
							<email>zhaoshuaitao@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Institute of Advanced Communication and Data Science</orgName>
								<orgName type="laboratory">Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">JD Logistics</orgName>
								<address>
									<postCode>100176</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Huang</surname></persName>
							<email>huangyuhang@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Institute of Advanced Communication and Data Science</orgName>
								<orgName type="laboratory">Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Bao</surname></persName>
							<email>baoqian@jd.com</email>
							<affiliation key="aff2">
								<orgName type="department">JD Explore Academy</orgName>
								<address>
									<postCode>100176</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Zeng</surname></persName>
							<email>dzeng@shu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Shanghai Institute of Advanced Communication and Data Science</orgName>
								<orgName type="laboratory">Joint International Research Laboratory of Specialty Fiber Optics and Advanced Communication</orgName>
								<orgName type="institution">Shanghai University</orgName>
								<address>
									<postCode>200444</postCode>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
							<email>liuwu1@jd.com</email>
							<affiliation key="aff2">
								<orgName type="department">JD Explore Academy</orgName>
								<address>
									<postCode>100176</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DPIT: Dual-Pipeline Integrated Transformer for Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Human Pose Estimation ? Dual-Pipeline Integration ? Transformer ? Information Interaction</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Human pose estimation aims to figure out the keypoints of all people in different scenes. Current approaches still face some challenges despite promising results. Existing top-down methods deal with a single person individually, without the interaction between different people and the scene they are situated in. Consequently, the performance of human detection degrades when serious occlusion happens. On the other hand, existing bottom-up methods consider all people at the same time and capture the global knowledge of the entire image. However, they are less accurate than the top-down methods due to the scale variation. To address these problems, we propose a novel Dual-Pipeline Integrated Transformer (DPIT) by integrating top-down and bottom-up pipelines to explore the visual clues of different receptive fields and achieve their complementarity. Specifically, DPIT consists of two branches, the bottom-up branch deals with the whole image to capture the global visual information, while the top-down branch extracts the feature representation of local vision from the single-human bounding box. Then, the extracted feature representations from bottom-up and top-down branches are fed into the transformer encoder to fuse the global and local knowledge interactively. Moreover, we define the keypoint queries to explore both full-scene and single-human posture visual clues to realize the mutual complementarity of the two pipelines. To the best of our knowledge, this is one of the first works to integrate the bottom-up and top-down pipelines with transformers for human pose estimation. Extensive experiments on COCO and MPII datasets demonstrate that our DPIT achieves comparable performance to the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human Pose Estimation (HPE) has been widely investigated as a fundamental task in computer vision, which aims to localize keypoints of the human, including eyes, nose, shoulders, wrists, etc., from a single RGB image. Accurate human pose estimation can provide geometric and motion information about the human, which can be widely applied in action recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, human-computer interaction, motion analysis, augmented reality (AR), etc.</p><p>Early human pose estimation methods do not depend on deep learning and mainly focus on the keypoints localization of a single person, which can be roughly divided into two categories. The first category treats the pose estimation task as a classification or regression problem through a global feature <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b31">32]</ref>. However, this kind of method does not exhibit high precision and is only suitable for clean scenes. The other category is the methods adopt graphic model to extract the feature representation for a single keypoint <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. The location of a single part can be obtained using DPM (Deformable Part-based Model) <ref type="bibr" target="#b9">[10]</ref>, and the pair-wise relationships are required to optimize the association between keypoints at the same time.</p><p>Recently, with the rapid development of deep learning, Convolutional Neural Networks (CNNs) have shown strong dominance in human pose estimation. We can roughly classify these superior networks into top-down and bottom-up methods. The top-down methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> first obtain a set of the bounding box of people from the input image through an off-the-shelf human detector , then apply a single-person pose estimator to each person. This type of method mainly focuses on the investigation and improvement of the latter pose estimation network. Different from the top-down pipeline, the bottom-up methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref> directly predict all the joints in an image and then group them using a certain assignment strategy to achieve multi-person pose estimation.</p><p>The top-down methods rely on the result of human detection and achieve promising performance for singleperson pose estimation. However, because they deal with each person individually, there is no awareness of the interaction with the other persons and the environment, which is more prevalent in real-life scenarios. When <ref type="figure">Fig. 1</ref>: Illustration of capturing information from different perspectives when predicting the left shoulder. The dotted lines point to the interest areas of the keypoint, and the thicker line indicates the more interest to the area. After the integration of different visual clues, the keypoint location is predicted by the heatmap. there is serious occlusion among different people, the performance of human detection becomes unreliable. Furthermore, when the target persons are very close to each other, the pose estimator may be misled by nearby persons, e.g., the predicted keypoints may come from adjacent persons. As a result, the top-down pipeline exhibits an inherent limitation in how to explore the interaction clues among different persons. Differently, the bottom-up methods do not rely on any detection process and take all people of the image into account simultaneously. They first detect the keypoints of all people, then align them into each person by a certain grouping strategy. This pipeline leverages full-scene information to realize locating keypoints, which can observe the interactions of different people from a global perspective. However, it suffers from scale variation, i.e., different people in the image are at different scales and very unevenly distributed, which is unfriendly to the training of the network, leading to relatively poor performance. In summary, both kinds of approaches show different advantages. Consequently, integrating the advantages of the two pipelines is potential for human pose estimation.</p><p>To achieve the complementarity of two pipelines, we propose an effective network called DPIT to further promote the visual exploration of the image for human pose estimation. The proposed network integrates the advantages of the top-down and bottom-up pipelines to learn long-range visual clues with different receptive fields, which capture the full-scene and posture information of a single person. Firstly, we design two branches to extract global scene features and local features of a single person, respectively. Secondly, to fuse these features, we employ the transformer to capture different visual cues. The final output of our DPIT is predicted in the heatmap fashion.</p><p>Since the two pipelines of features contained different information are both essential for keypoints localization, our core idea is to incorporate the two pipelines into one network and perform effective information interaction. An example of how to predict the location of the left shoulder is shown in <ref type="figure">Fig. 1</ref>. Information from different perspectives can assist the network in better understanding the image scenes, and extend the interest region, which is of positive effects for human pose estimation in complex scenes. More specifically, we first employ a two-branch structure, in which the bottom-up branch captures full-scene information of the entire image, the top-down branch extracts the single-person feature of the detected human bounding box. Motivated by the great success of the recent work on Vision Transformers, the encoder of the transformer is employed as the clue interaction structure to fuse the two-branch features. In detail, we split the features into patches and take linear patch embeddings to form input visual tokens. Meanwhile, we define a set of randomly initialized embeddings as the keypoint queries, which can capture the single-person pose, full-scene visual clues, and their distributional relations from the different tokens. Finally, we only apply the heatmap generator to the keypoint embeddings which have aggregated local and global information, and reshape them into heatmaps. To the best of our knowledge, this is one of the first works to integrate the two pipelines with transformers.</p><p>In summary, the contributions of this paper are mainly summarized as below:</p><p>-We propose a novel architecture named DPIT for human pose estimation, which is one of the first works to integrate the bottom-up and top-down pipelines in an end-to-end training manner. -We design a Transformer-based module to capture both full-scene visual clues and posture information of a single person simultaneously, which can allocate different attention levels to different visual areas.</p><p>-We demonstrate an improvement over baselines on the widely used COCO and MPII datasets, and surpass the state-of-the-art methods.</p><p>The rest of this paper is organized as follows. In Sec. 2, we review the related works. In Sec. 3, we introduce the proposed DPIT for human pose estimation. Extensive experiments are conducted in Sec. 4 to compare the proposed DPIT with state-of-the-art methods on two benchmark datasets. The conclusion is given in Sec. 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our proposed method is related to the previous research on top-down human pose estimation, bottom-up human pose estimation, and applications of transformer in vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Top-down Human Pose Estimation</head><p>The Top-down pipeline consists of two main components: the human detector and the pose estimation network. Most of the work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref> focused on the design and improvement of the latter pose estimation network. CPN <ref type="bibr" target="#b5">[6]</ref> implemented the coarse-to-fine process through a two-stage network, where the GlobalNet learns a welldefined feature representation based on a feature pyramid network to provide sufficient semantic information to locate simple keypoints. Further, the RefineNet is employed to handle the "difficult" keypoints by fusing the multi-level features of the GlobalNet. MSPN <ref type="bibr" target="#b13">[14]</ref> performed stacking of multiple stages based on CPN's globalNet to achieve better information communication. Xiao et al. <ref type="bibr" target="#b36">[37]</ref> employed ResNet as the backbone and added some de-convolution layers behind it, which built a simple but effective structure to produce a highresolution representation of the keypoint heatmap. HRNet <ref type="bibr" target="#b29">[30]</ref> started to give attention to the importance of spatial resolution. A novel high-resolution network is proposed to learn the reliable high-resolution features by connecting multi-resolution sub-networks in parallel, as well as performing repetitive multi-scale fusion. Cai et al. <ref type="bibr" target="#b2">[3]</ref> proposed a multi-stage network where the Residual Step Network (RSN) explores delicate local features through an effective inter-level feature fusion strategy. In addition, a new attention mechanism (PRM) was also proposed to learn different contributions for local and global features, achieving more accurate keypoint localization. Wang et al. <ref type="bibr" target="#b33">[34]</ref> proposed a graph-based, model-independent two-stage network, Graph-PCNN. This framework added a localization sub-network and a graph structure pose optimization module to the original heatmap-based regression method. The heatmap regression network was employed as the first stage to provide rough localization of each keypoint. The localization sub-network was designed as the second stage to extract visual features from the candidate keypoints. Although these top-down methods can achieve satisfactory performance for the single-person bounding box, they are unreliable in obscured scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Bottom-up Human Pose Estimation</head><p>The bottom-up pipeline consists of two main stages, including the joints detection and grouping of all human keypoints in the image <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. OpenPose <ref type="bibr" target="#b3">[4]</ref> predicted the heatmap of keypoints to locate the position of each keypoint in the image. The Part Affinity Field (PAF) was proposed to achieve the connection of keypoints, which speeds up the bottom-up human pose estimation to a great extent. Associative Embedding <ref type="bibr" target="#b20">[21]</ref> not only predicted the heatmaps but also output an embedding for each keypoint, aiming to make the embeddings of the same person as similar as possible. RMPE <ref type="bibr" target="#b8">[9]</ref> proposed a two-step framework, which mainly solved the positioning error and the redundancy of the bounding box. HigherHRNet <ref type="bibr" target="#b6">[7]</ref> provided a simple extension to HRNet <ref type="bibr" target="#b29">[30]</ref> by deconvoluting the high-resolution heatmap to obtain the higher resolution representation. SIMPLE <ref type="bibr" target="#b40">[41]</ref> employed knowledge distillation by treating the top-down network as a teacher network to train the bottom-up network. Both human detection and keypoint estimation were considered as unified point learning issues that complement each other in a single framework. DEKR <ref type="bibr" target="#b10">[11]</ref> proposed a simple but effective method that employs adaptive convolution through a pixel-wise spatial transformer to activate pixels in the keypoint regions. Separate regression of different keypoints was also performed using a multi-branch structure. The separated representations can notice the keypoint regions separately so that the keypoint regression is more spatially accurate. However, the variation of the person scale in the image significantly affects the performance of these bottom-up methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transformers in Vision</head><p>The amazing achievements of the transformer in natural language have attracted the vision community to explore its application to computer vision tasks. Recently, the transformer has been widely applied in different  vision tasks including image classification <ref type="bibr" target="#b7">[8]</ref>, object detection <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b41">42]</ref>, segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref>, and generation <ref type="bibr" target="#b34">[35]</ref>, etc.</p><p>ViT <ref type="bibr" target="#b7">[8]</ref> completely abandoned CNN and applied transformer to image classification, which splits the image into fix-sized patches, each of which is expanded into sequential form and fed to the encoder of transformer by linear projection. DeiT <ref type="bibr" target="#b30">[31]</ref> incorporated distillation into the training of ViT. It introduced a teacher-student training strategy, in which the convolution network was employed as the teacher network. DETR <ref type="bibr" target="#b4">[5]</ref> applied transformer to the object detection. The image is processed by CNN for feature extraction, then fed into the transformer in the manner of feature sequences. The transformer architecture directly outputs an unordered set. Each element of the set contains object categories and coordinates. SegFormer <ref type="bibr" target="#b37">[38]</ref> proposed a simple and efficient structure for semantic segmentation, consisting of a positional-encoding-free, hierarchical transformer encoder and an MLP decoder, which achieves high efficiency and accuracy.</p><p>In human pose estimation, the transformer has also received extensive attention and application <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. POET <ref type="bibr" target="#b27">[28]</ref> proposed an encoder-decoder structure combining CNN and transformer, which can directly regress the pose of all individuals using a bipartite matching scheme. Based on the regression manner, TFPose <ref type="bibr" target="#b19">[20]</ref> implemented direct human pose estimation, overcoming the feature-mismatch problem of previous regression-based methods. TransPose <ref type="bibr" target="#b39">[40]</ref> introduced a transformer-based structure to predict the location of human keypoints based on heatmaps, which can effectively capture the spatial relationships of images. Following ViT <ref type="bibr" target="#b7">[8]</ref>, TokenPose <ref type="bibr" target="#b14">[15]</ref> divided the image into several patches to form the visual tokens, which incorporated the visual cue and constraint cue into a unified network. Swin-Pose <ref type="bibr" target="#b38">[39]</ref> proposed a transformer-based structure to capture the long-range dependencies between pixels, using the pre-trained Swin Transformer <ref type="bibr" target="#b18">[19]</ref> as the backbone to extract image features. In addition, the feature pyramid architecture was adopted to fuse features from different stages for feature enhancement. Different from these transformer-based methods, our method employ the transformer to integrate the top-down and bottom-up pipelines. In this way, our network can capture the global clues and local clues simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>The presented framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, where all modules are trained in an end-to-end manner. Given an image as input, a two-branch architecture is employed for feature extraction, where the bottom-up branch and top-down branch extract full-scene information and posture of a single person, respectively. Then, a transformerbased integration network is designed to jointly establish keypoint-person and keypoint-scene interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Two-branch Architecture</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, to integrate the top-down and bottom-up pipelines into a unified network, we employ a parallel two-branch CNN for feature extraction in the first stage of our network. The two CNN backbones are pre-trained on the ImageNet classification task <ref type="bibr" target="#b26">[27]</ref>. Specifically, an image x 1 ? R H1?W1?3 is first detected by an existing human detector, from which we obtain the human bounding box x 2 ? R H2?W2?3 . The bottom-up encoder E BU extracts the full-scene feature representation F BU ? R H 1 ?W 1 ?C from the input image x 1 . Taking x 2 as input, the top-down encoder E T D outputs the posture representation F T D ? R H 2 ?W 2 ?C of single-person. The process of feature extraction of the two branches can be represented as:</p><formula xml:id="formula_0">F BU = E BU (x 1 ), F T D = E T D (x 2 ),<label>(1)</label></formula><p>where E BU and E T D denote bottom-up and top-down CNN encoders, respectively.</p><p>The featuremap extracted by the bottom-up encoder comes from the whole image, where we can observe the full-scene information, including the pose of all people, the interaction of different people, and the scene information of the image. The top-down branch focuses on the spatial posture information of a single person with uniform resolution. In real scenes, the environment is usually undefinable, there are various interactions among different people. As a result, capturing visual clues from different view fields can assist in locating the keypoints more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transformer-based Integration</head><p>Inspired by the wide applications of transformer in vision tasks, we employ the encoder of the transformer to capture and integrate the visual clues of different view fields. Together with the patch embeddings from the bottom-up and top-down branches, the keypoint embeddings are defined as input queries. In this way, our network can capture the knowledge of global fields and local fields that comes from the whole image and single-human bounding box, respectively.</p><p>Input Queries Following the process of ViT <ref type="bibr" target="#b7">[8]</ref>, The featuremap F BU and F T D are split into two patch sets:</p><formula xml:id="formula_1">p 1 = [F BU 1 , ..., F BU N1 ] ? R N1?P h 1 ?P w 1 ?C , p 2 = [F T D 1 , ..., F T D N2 ] ? R N2?P h 2 ?P w 2 ?C , where (P h * , P w * ) ( * = 1, 2)</formula><p>is the patch size, N * = (H * ? W * )/(P h * ? P w * ) is the number of patches, C is the number of channels. The standard Transformer <ref type="bibr" target="#b32">[33]</ref> receives a 1D sequence of the token embeddings as input. Every patch is flattened into a 1D vector with the size of P h * ? P w * ? C . Then, the linear projection is performed to each 1D vector, we can get the visual queries of two branches: q 1 = [E 1 , ..., E N1 ], q 2 = [E 1 , ..., E N2 ],where E ? R (P h * ?P w * ?C )?D and D is the embedding dimension. It is worth mentioning that human pose estimation is a location sensitive vision task, so the 2D position embeddings are added to the sequence of patches to capture positional information.</p><p>Moreover, we introduce a set of learnable keypoint embeddings: kpt = [k 1 , ..., k K ] ? R K?D , where K is the number of keypoints. Each keypoint embedding is initialized randomly and assigned to a single keypoint (eyes, wrists, ...), which is employed to generate the final heatmap. Benefit from the self-attention mechanism of transformer, each keypoint embedding can capture the corresponding interest visual regions from both global and local features. In addition, it can give attention to the clues of other keypoints, which simplifies the difficulty of locating keypoints, especially in complex scenarios. The keypoint embeddings kpt, bottom-up visual query q 1 , and top-down visual query q 2 are put together as the input queries, which are processed jointly by the transformer encoder, as depicted in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Transformer Encoder The transformer encoder consists of multi-encoder layers, which mainly depend on the self-attention mechanism. Each encoder layer contains a multi-head self-attention (M SA) block followed by a feed-forward network (F F N ). Layer Normalization (LN ) is also employed before every module and residual connections after every module. Specifically, for the input sequence X, linear projections are performed to obtain Query (Q), Key (K), and Value (V ):</p><formula xml:id="formula_2">Q = X * W Q , K = X * W K , V = X * W V ,<label>(2)</label></formula><p>where W Q , W K , W V are the corresponding weight matrices. The M SA process can formulated:</p><formula xml:id="formula_3">M SA(Q, K, V ) = sof tmax( Q ? K T ? d k ) ? V,<label>(3)</label></formula><p>where d k is the dimension of keys. Each query is calculated with all the keys, each (Q, K) pair is divided by d k . Then, the SoftMax function is employed to obtain the attention scores, each score determines the attention level to the token for current query.</p><p>Heatmap Generator The transformer encoder outputs a D-dimensional sequence. After the transformer's self-attention mechanism, the corresponding keypoint embeddings have captured the visual clues of the fullscene and single-person. We only take the keypoint queries for prediction, which are linearly mapped into H ? W dimensions. Then the mapped 1D representations are reshaped into 2D heatmaps with the shape of H ? W . Finally, on output heatmaps, we find the maximum response position by channel to locate the corresponding human keypoint. In addition, between the output heatmap and the ground-truth heatmap, we adopt MSE as the loss function to train the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In this paper, we conducted extensive experiments on two human pose datasets, COCO <ref type="bibr" target="#b15">[16]</ref> and MPII <ref type="bibr" target="#b0">[1]</ref>, to train and validate our network. For a fair comparison, we follow the same dataset split ratio as the comparison methods <ref type="bibr" target="#b29">[30]</ref>. The datasets are introduced below.</p><p>COCO Dataset COCO is a large-scale dataset in human pose estimation task, containing over 200K images and 250K person instances, annotated with 17 keypoints. The dataset is divided into a train set (118k images), a validation set (5K images), and a test-dev set (20K images). We take the train set to train our network. The validation set and test-dev set are employed to measure the performance of our network.</p><p>MPII Dataset MPII is a well-known benchmark for the evaluation of human pose estimation, which contains around 25K images and over 40K people with annotated 16 joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>For the COCO dataset, the Object Keypoint Similarity (OKS) is calculated for the reported metrics, which measures the similarity between the ground truth and predicted keypoints. The OKS is defined as follows:</p><formula xml:id="formula_4">OKS = i exp( ?d 2 i 2s 2 k 2 i )?(v i &gt; 0) i ?(v i &gt; 0) ,<label>(4)</label></formula><p>where d i is the Euclidean distances between each corresponding ground truth and detected keypoint. v i is the visibility flag of the ground truth. s denotes the square root of the person's proportion to the image area, ? i is the normalized parameter of the ith keypoint. Based on OKS, the standard average precision and average recall are reported, including AP (the mean value of AP at OKS = 0.5, 0.55, ..., 0.9, 0.95), AP 50 (AP at OKS = 0.5), AP 75 (AP at OKS = 0.75), AP M (AP of medium-scale objects), AP L (AP of large-scale objects), and AR (mean value of AR at OKS = 0.5, 0.55, ..., 0.9, 0.95). For MPII, the head-normalized Percentage of Correct Keypoints (PCKh@0.5) <ref type="bibr" target="#b0">[1]</ref> is employed as the metric, which calculates the percentage of the normalized distance between the ground truths and detected keypoints that are lower than the setting threshold (0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>Regarding the training scheme, all modules of our network are trained with adaptive moment estimation optimizer (ADAM) <ref type="bibr" target="#b12">[13]</ref>, whose parameters are: ? = 0.001, ? 1 = 0.9, ? 2 = 0.999. The initial learning rate is 10 ?3 for our network, which is trained for a total of 240 epochs. The learning rate is reduced to 10% of the previous number at the 190th and 220th epochs, respectively. For the backbones of the bottom-up and topdown branches, we adopt the models trained on the ImageNet classification task as our pre-trained models. To improve the varieties of training data, following <ref type="bibr" target="#b29">[30]</ref>, the data augmentations are conducted, including random While training, for the bottom-up branch, we resize the image to a fixed size: 512 ? 512, then fed it into the encoder of this branch. As introduced before, we split the featuremap output from the encoder into patches with the size of 8 ? 8. On the other hand, the input of the top-down branch is also rescaled into fixed resolution. There are different settings for COCO and MPII datasets. On COCO, the patch size is set to 4 ? 3 with input size of 256 ? 192. On MPII, with a uniform input shape of 256 ? 256, the feature is split into 4 ? 4 patches. In addition, our network set up two configuration versions, DPIT-B and DPIT-L. The detailed settings of them are shown in <ref type="table" target="#tab_0">Table 1</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Quantitative Results</head><p>To validate the effectiveness and superiority of our method, we conducted quantitative experiments on COCO and MPII datasets. The quantitative results show that our method achieves better performance on human pose estimation, the specific results are analyzed as follows:</p><p>Results on COCO Dataset The quantitative results using the ground-truth bounding box on the COCO validation set are shown in <ref type="table" target="#tab_1">Table 2</ref>. For the different methods, we perform quantitative comparisons based on different backbones and input resolutions. Following TokenPose <ref type="bibr" target="#b14">[15]</ref>, we do not employ the whole HRNet as our backbone, but its first three stages. In this case, the network parameters are only 25% of the original version, as indicated by HRNet-W32-s and HRNet-W48-s in <ref type="table" target="#tab_0">Table 1</ref>. Compared to SimpleBaseline <ref type="bibr" target="#b36">[37]</ref> and HRNet <ref type="bibr" target="#b29">[30]</ref>, it can be observed that our method achieves better performance while being more lightweight. Quantitatively, our DPIT-L achieves improvements of 0.7 AP and 0.4 AR compared to the HRNet-W48, which demonstrates the superiority of our method. In addition, as shown in <ref type="table" target="#tab_2">Table 3</ref>, we compare our method with the state-of-the-art methods including G-RMI <ref type="bibr" target="#b22">[23]</ref>, CPN <ref type="bibr" target="#b5">[6]</ref>, RMPE <ref type="bibr" target="#b8">[9]</ref>, SimpleBaseline <ref type="bibr" target="#b36">[37]</ref> and HRNet <ref type="bibr" target="#b29">[30]</ref> on COCO test-dev set. Compared with other methods, DPIT exhibits the best performance on AP and AR, achieves comparable results on other metrics. Results on MPII Dataset The PCKh@0.5 results on the MPII validation set are reported in <ref type="table" target="#tab_3">Table 4</ref> with a uniform input size of 256?256. DPIT-L/D6 represents the configured DPIT-L with 6 transformer encoder layers. Specifically, compared with SimpleBaseline <ref type="bibr" target="#b36">[37]</ref> and HRNet <ref type="bibr" target="#b29">[30]</ref>, our DPIT-L/D6 achieves the best performance on the metrics reported by Elb, W ri, Ank, and M ean. On most other metrics, it also achieves the second-best level. In summary, the quantitative results indicate the comparable performance of our DPIT on the MPII dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative Results</head><p>Our method incorporates both top-down and bottom-up pipelines, where the full-scene information of different visual fields can help capture human pose information, especially in complex scenes with multi-person interac- tions. Given an image, our network can accurately localize the location of keypoints for persons in the image. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the pose estimation results of DPIT in different scenes are demonstrated. From the first row, we can observe that for complex pose scenes of a single person, our network can achieve accurate localization of keypoints. Benefitting from the fusion of visual information from different receptive fields and interactions of visual clues, the network still performs well while serious occlusion among different people. In addition, our method is not affected by the scale variation of the persons in the image. It is observable that accurate human pose estimation can still be reported for scenes with large-scale differences. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, we further illustrate the heatmaps of keypoints, where the maximum response location represents the corresponding location of keypoints. As we can see, our network can predict precise heatmaps for different keypoints and different persons in the image. The heatmap manner effectively preserves the spatial location information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Ablation Studies</head><p>In this section, to verify the contributions of different components and the influence of the structure parameter settings, we perform ablation experiments on the COCO dataset.</p><p>Is it useful to employ the bottom-up branch to capture the full-scene information? The bottom-up branch extracts the full-scene information from the whole image with the encoder E BU . To verify the usefulness of the bottom-up branch in locating human keypoints, we delete this branch in our DPIT, and obtain a network that does not utilize the feature of the entire image, called w/o BU . As shown in <ref type="table" target="#tab_4">Table 5</ref>, we report quantitative results in the row with w/o BU . Without the help of the bottom-up branch, we observe degradation in the performance on different metrics. In addition, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>, the qualitative results indicate that the absence of full-scene information leads to inaccurate human pose estimation in the scenes with multi-person interaction and occlusion. As a result, both quantitative and qualitative results demonstrate the usefulness of the bottom-up branch.</p><p>Why using transformer encoder? To evaluate the contribution of the transformer encoder to our DPIT, we perform another ablation experiment, i.e., removing the transformer encoder from the network. Alternatively, the featuremap of the bottom-up branch is integrated with the top-down branch by summation operation after some convolution layers, then the heatmap of the keypoints is predicted by the integrated features. It is worth mentioning that the simplified backbone no longer can capture enough information about the image in this case, which aggravates the network. In comparison, the transformer can capture long-term dependencies of the same visual field, while also integrating visual clues and posture clues from different perspectives with the help of self-attention mechanism. As shown in the row with w/o T ransf ormer of <ref type="table" target="#tab_4">Table 5</ref>, the poorer performance of the network proves the effectiveness of the transformer.   What's the impact of depth of transformer encoder? For networks equipped with transformer, the depth of the encoder is a significant setting for the performance. As shown in <ref type="table" target="#tab_5">Table 6</ref>, to explore the impact of the different number of encoder layers, we conduct quantitative experiments on the COCO validation set. Specifically, we only change the number of encoder layers of the transformer, keeping the other configurations fixed. Three different encoder layers are validated. It can be observed that different settings have different performances on the quantitative metrics. When the depth is shallow, the network performance improves as the depth increases. The metrics, however, also exhibit a decrease with too many encoder layers. Based on the experimental validation, our final configuration of transformer encoder depth is 12 on COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a novel Dual-Pipeline Integrated Transformer called DPIT for human pose estimation. To the best of our knowledge, this is one of the first works to integrate the bottom-up and top-down pipelines in one network with transformers. The proposed DPIT consists of two parts, a two-branch structure, and a feature interaction module. In our framework, the bottom-up branch and top-down branch capture full-scene information and posture visual clues with different receptive fields and perspectives, respectively. To achieve the effective integration of the two branches, the encoder of the transformer is applied to explore the longterm local-visual clues, global-visual clues, and their interactions. In addition, the defined keypoint embedding not only focuses on the different interest regions for a particular keypoint but also be allowed to concern the structural information between different keypoints. The reported quantitative and qualitative results on two public datasets demonstrate the effectiveness of our DPIT for human pose estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overall training architecture of our network. The image is input to the bottom-up branch to get the full-scene feature. The single-person bounding box output from the human detector is fed into the topdown branch to extract the single-human pose feature. Then the defined random embeddings are treated as the keypoint queries, which is sent into the transformer encoder together with the visual tokens in sequential fashion. The outputs of our network are the heatmaps of keypoints with the shape of H ? W ? N um k, where N um k is the number of keypoints. All components of the network are trained together in an end-to-end manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>rotation ([?45 ? , 45 ? ]), random scale ([0.65, 1.35]) and flipping.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Illustration of human pose estimation results of DPIT in different scenes on COCO validation set. The first row shows the effect of pose estimation in single-person situations with different postures. The scenes in the second row contain interactions of different people, including self-shadowing and inter-shadowing between two persons. Finally, more complex scenes with multiple people are further visualized in the third row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of the heatmaps of all persons in the image predicted by the network. Each heatmap of the same row denotes the location response of different keypoints of one person. It can be observed that in the presence of different human distractions, our method can still accurately estimate the location of key points in the human body.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Some qualitative results are illustrated to show the contributions of the bottom-up branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The network configurations. HRNet-W32-s and HRNet-W48-s denote the first three stages of HRNet-W32 and HRNet-W48, respectively.</figDesc><table><row><cell>Model</cell><cell cols="3">Backbone Depth Heads Hidden Dim</cell></row><row><cell cols="2">DPIT-B HRNet-W32-s 12</cell><cell>8</cell><cell>192</cell></row><row><cell cols="2">DPIT-L HRNet-W48-s 12</cell><cell>8</cell><cell>192</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on COCO validation set across various state-of-the-art methods with the groundtruth bounding boxes. R and H denote the ResNet and HRNet, respectively. #Params indicates the size of each model, excluding the cost of the human detection network. In each column, the best result is in bold, the second best is underlined. Backbone Input Size #Params AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell></row><row><cell></cell><cell>R-50</cell><cell>256 ? 192 34.0M 72.4 91.5 80.4 69.7 76.5 75.6</cell></row><row><cell></cell><cell>R-50</cell><cell>384 ? 288 34.0M 74.1 92.6 80.5 70.5 79.6 76.9</cell></row><row><cell>Simple Baseline [37]</cell><cell cols="2">R-101 256 ? 192 53.0M 73.4 92.6 81.4 70.7 77.7 76.5 R-101 384 ? 288 53.0M 75.5 92.5 82.6 72.4 80.8 78.4</cell></row><row><cell></cell><cell cols="2">R-152 256 ? 192 68.6M 74.3 92.6 82.5 71.6 78.7 77.4</cell></row><row><cell></cell><cell cols="2">R-152 384 ? 288 68.6M 76.6 92.6 83.6 73.7 81.3 79.3</cell></row><row><cell>HRNet [30]</cell><cell cols="2">H-W32 256 ? 192 28.5M 76.5 93.5 83.7 73.9 80.8 79.3 H-W48 256 ? 192 63.6M 77.1 93.6 84.7 74.1 81.9 79.9</cell></row><row><cell>DPIT-B</cell><cell>-</cell><cell>256 ? 192 20.8M 76.9 93.5 83.7 73.7 81.5 79.6</cell></row><row><cell>DPIT-L</cell><cell>-</cell><cell>256 ? 192 38.0M 77.8 93.6 84.8 74.8 82.2 80.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison with various state-of-the-art methods with detected bounding boxes from the same human detector on COCO test-dev set. Input Size #Params AP AP 50 AP 75 AP M AP L AR</figDesc><table><row><cell>Method</cell><cell></cell></row><row><cell>G-RMI [23]</cell><cell>353 ? 257 42.6M 64.9 85.5 71.3 62.3 70.0 69.7</cell></row><row><cell>CPN [6]</cell><cell>384 ? 288 45.0M 72.1 91.4 80.0 68.7 77.2 78.5</cell></row><row><cell>RMPE [9]</cell><cell>320 ? 256 28.1M 72.3 89.2 79.1 68.0 80.8 78.6</cell></row><row><cell cols="2">SimpleBaseline-R152 [37] 384 ? 288 68.6M 73.7 91.9 81.1 70.3 80.0 79.0</cell></row><row><cell>HRNet-W48 [30]</cell><cell>256 ? 192 63.6M 74.2 92.4 82.4 70.9 79.7 79.5</cell></row><row><cell>DPIT-B</cell><cell>256 ? 192 20.8M 73.6 91.4 81.2 70.4 79.5 78.9</cell></row><row><cell>DPIT-L</cell><cell>256 ? 192 38.0M 74.6 91.9 82.1 71.3 80.6 79.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative Results on MPII validation set. Experiments for all architectures are performed at the uniform input size: 256 ? 256.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone #Params Hea Sho Elb Wri Hip Kne Ank Mean</cell></row><row><cell></cell><cell>R-50</cell><cell>34.0M 96.4 95.3 89.0 83.2 88.4 84.0 79.6 88.5</cell></row><row><cell>SimpleBaseline [37]</cell><cell>R-101</cell><cell>53.0M 96.9 95.9 89.5 84.4 88.4 84.5 80.7 89.1</cell></row><row><cell></cell><cell>R-152</cell><cell>68.6M 97.0 95.9 90.0 85.0 89.2 85.3 81.3 89.6</cell></row><row><cell>HRNet [30]</cell><cell>H-W32</cell><cell>28.5M 96.9 96.0 90.6 85.8 88.7 86.6 82.6 90.1</cell></row><row><cell>DPIT-B</cell><cell>-</cell><cell>21.6M 97.1 95.7 90.0 84.6 89.4 85.9 80.7 89.6</cell></row><row><cell>DPIT-L/D6</cell><cell>-</cell><cell>31.8M 96.7 95.9 90.8 85.9 89.2 86.0 82.6 90.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Results on COCO validation set with the input size of 256 ? 192. w/o BU : without bottom-up branch. w/o T ransf ormer: without transformer.</figDesc><table><row><cell>Model</cell><cell>AP AP 50 AR</cell></row><row><cell>w/o BU</cell><cell>76.6 92.5 79.4</cell></row><row><cell cols="2">w/o T ransf ormer 76.5 93.6 79.3</cell></row><row><cell>DPIT-B</cell><cell>76.9 93.5 79.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Ablation studies with different transformer encoder layers are performed on COCO validation dataset.</figDesc><table><row><cell>Model</cell><cell cols="2">Depth AP AP 50 AR</cell></row><row><cell>DPIT-B-D6</cell><cell>6</cell><cell>76.3 92.9 79.1</cell></row><row><cell cols="3">DPIT-B-D12 12 76.9 93.5 79.6</cell></row><row><cell cols="3">DPIT-B-D16 16 76.5 92.6 79.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning delicate local representations for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="455" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bottom-up human pose estimation via disentangled keypoint regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14676" to="14686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<title level="m">Rethinking on multi-stage networks for human pose estimation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tokenpose: Learning keypoint tokens for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11313" to="11322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">T-c3d: Temporal convolutional 3d network for real-time action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A real-time action representation with temporal encoding and deep compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="647" to="660" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15320</idno>
		<title level="m">Tfpose: Direct human pose estimation with transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards accurate multiperson pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on Computer Vision</title>
		<meeting>the IEEE international conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3487" to="3494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Randomized trees for human pose detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rihan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Orrite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">End-to-end trainable multi-instance pose estimation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Stoffl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12115</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7262" to="7272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sparse probabilistic regression for activity-independent human pose inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graph-pcnn: Two stage human pose estimation with graph pose refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="492" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sceneformer: Indoor scene generation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yeshwanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="106" to="115" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.07384</idno>
		<title level="m">Swin-pose: Swin transformer based human pose estimation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transpose: Keypoint localization via transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11802" to="11812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Simple: Single-network with mimicking and point learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02486</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
