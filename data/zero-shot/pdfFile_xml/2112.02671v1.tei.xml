<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><forename type="middle">P</forename><surname>Panousis</surname></persName>
							<email>k.panousis@cut.ac.cy</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sotirios</forename><surname>Chatzis</surname></persName>
							<email>sotirios.chatzis@cut.ac.cy</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergios</forename><surname>Theodoridis</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cyprus University of Technology Limassol</orgName>
								<address>
									<country key="CY">Cyprus</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cyprus University of Technology Limassol</orgName>
								<address>
									<country key="CY">Cyprus</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Kapodistrian University of Athens</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Aalborg University</orgName>
								<address>
									<settlement>Aalborg</settlement>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Local Winner-Takes-All Networks Enable Profound Adversarial Robustness</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work explores the potency of stochastic competition-based activations, namely Stochastic Local Winner-Takes-All (LWTA), against powerful (gradient-based) white-box and black-box adversarial attacks; we especially focus on Adversarial Training settings. In our work, we replace the conventional ReLU-based nonlinearities with blocks comprising locally and stochastically competing linear units. The output of each network layer now yields a sparse output, depending on the outcome of winner sampling in each block. We rely on the Variational Bayesian framework for training and inference; we incorporate conventional PGD-based adversarial training arguments to increase the overall adversarial robustness. As we experimentally show, the arising networks yield state-of-the-art robustness against powerful adversarial attacks while retaining very high classification rate in the benign case.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In this paper, we revisit the novel stochastic formulation of deep networks with LWTA activations <ref type="bibr" target="#b11">(Panousis et al., 2019</ref>, and delve deeper into its potency against adversarial attacks in the context of PGD-based Adversarial Training (AT) <ref type="bibr" target="#b8">(Madry et al., 2017)</ref>. We evaluate the robustness of the emerging networks against powerful gradient-based white-box as well as black-box adversarial attacks using the well-known AutoAttack (AA) framework <ref type="bibr" target="#b2">(Croce and Hein, 2020)</ref>. We provide the related source code at: https://github.com/konpanousis/Adversarial-LWTA-AutoAttack. We experimentally show that Stochastic LWTA-based networks not only yield state-of-the-art accuracy against all the considered attacks, but do so while retaining very high classification accuracy in the benign case. , and transformed into probabilities via the softmax operation. Then, a Discrete sample ? b = [? b,1 , . . . , ? b,U ] is drawn; this constitutes an one-hot vector with a single non-zero entry at position u , denoting the winner unit in the block. This winner unit, u , passes its linear response to the next layer; the rest pass zero values.</p><p>In contrast, in a fully connected LWTA-based network layer, singular nonlinear units are replaced by U linear competing units aggregated together in a (LWTA) block; in the following, we denote with B the number of LWTA blocks in an LWTA-based layer. The associated weights are now arranged as a three dimensional matrix W ? R J?B?U signifying that the input x is presented to each block and each unit therein. Specifically, in the LWTA-based framework, the u th competing unit within the b th block computes its activation h b,u via the standard inner product computation h b,u = w T b,u x = J j=1 w j,b,u ? x j ? R; then, competition takes place among the units in the block. The underlying principle is that out of the U units in an LWTA block, only one can be the winner; this unit gets to pass its linear activation to the next layer, while the rest output zero values. Thus, the output of an LWTA layer y ? R B?U is composed of B subvectors y b ? R U , one for each LWTA block and each with a single non-zero entry. It is apparent that this competition process results in a sparse representation, since all units, except one in each block, produce a zero output. In related literature, the competition procedure is deterministic, i.e., the winner unit is the one with the highest activation. However, stochastic competition principles have been recently proposed in <ref type="bibr" target="#b11">Panousis et al. (2019</ref><ref type="bibr" target="#b10">Panousis et al. ( , 2021</ref>; <ref type="bibr" target="#b14">Voskou et al. (2021)</ref>.</p><p>In this context, to encode the winner unit in each of the B LWTA blocks that constitute a stochastic LWTA layer, we introduce an appropriate set of discrete latent indicator vectors ? ? one_hot(U ) B . This vector comprises B component subvectors, where each component entails exactly one non-zero value at the index position that corresponds to the winner unit in each respective LWTA block.</p><p>Thus, the output y of a stochastic LWTA layer's (b, u) th component y b,u is defined as:</p><formula xml:id="formula_0">y b,u = ? b,u J j=1 w j,b,u ? x j ? R<label>(1)</label></formula><p>where ? b,u denotes the u th component of ? b , and ? b ? one_hot(U ) holds the b th subvector of ?.</p><p>We postulate that the latent winner indicator variables ? b , ?b in Eq.(1) are drawn from a data-driven Categorical distribution with probabilities proportional to the intermediate linear computations that each unit performs. Therefore, the higher the linear response of a particular unit in a particular block, the higher its probability of it being the winner in said block; this yields:</p><formula xml:id="formula_1">q(? b ) = Categorical ? b softmax J j=1 [w j,b,u ] U u=1 ? x j (2) where [w j,b,u ] U u=1 denotes the vector concatenation of the set {w j,b,u } U u=1 .</formula><p>A graphical illustration of the proposed stochastic LWTA block is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>. Each stochastic LWTA layer comprises multiple such LWTA blocks, as illustrated in <ref type="figure" target="#fig_1">Fig. 2a</ref>. Note that stochastically selecting the winner of each LWTA block in each layer introduces stochasticity to the network activations. Presented with the same input, different subnetworks may be activated and a different subpath is followed to the output, as a result of winner sampling.</p><p>Further, to also accommodate in our study architectures based on the convolutional operation, we adopt the corresponding variant of the Stochastic LWTA activation in <ref type="bibr" target="#b11">Panousis et al. (2019)</ref>. We assume an input tensor X ? R H?L?C and define a set of kernels, each with weights W b ? R h?l?C?U , where h, l, C, U are the kernel height, length, channels and competing feature maps, and b = 1, . . . , B. Analogously to the grouping of linear units in dense layers, in this case, local competition is performed among feature maps on a position-wise basis. Each kernel is treated as an LWTA block with competing feature maps; each layer comprises B kernels. Specifically, each feature map u = 1, . . . , U in the b th LWTA block of a convolutional LWTA layer computes:</p><formula xml:id="formula_2">H b,u = W b,u X ? R H?L<label>(3)</label></formula><p>Then, competition takes place among competing feature maps on a position-wise basis. The competitive random sampling procedure reads:</p><formula xml:id="formula_3">q(? b,h ,l ) = Categorical ? b,h ,l softmax ([H b,1,h ,l , . . . , H b,U,h ,l ]) , ?h , l<label>(4)</label></formula><p>In each kernel, b = 1, . . . , B, for each position, only the winner feature map contains a non-zero entry; all the rest feature maps contain zero values at this position. This yields sparse feature maps with mutually exclusive active positions.</p><p>Thus, at a given layer of the proposed convolutional variant, the output Y ? R H?L?B?U is obtained via concatenation of the subtensors Y b,u that read:</p><formula xml:id="formula_4">Y b,u = ? b,u W b,u X , ?b, u<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">? b,u = [? b,u,h ,l ] H,L h ,l =1 .</formula><p>The corresponding illustration of the proposed stochastic convolutional LWTA block is depicted in <ref type="figure" target="#fig_2">Fig. 3</ref>. Convolutional Stochastic LWTA-based layers comprise multiple such blocks as shown in <ref type="figure" target="#fig_1">Fig. 2b</ref>. Only the winner feature map contains a non-zero entry in a specific position. This leads to sparse feature maps, each comprising uniquely position-wise activated pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training</head><p>Since a network composed of such stochastic LWTA layers entails latent variables ?, we resort to a Bayesian treatment to perform effective parameter estimation. To this end, we turn to a stochastic gradient variational Bayes treatment <ref type="bibr" target="#b6">(Kingma and Welling, 2014)</ref> for scalability. The resulting objective takes the form of an evidence lower-bound (ELBO) as described next.</p><formula xml:id="formula_6">Considering data D = {X i , Y i } N i=1</formula><p>, we define the categorical cross-entropy between the data labels Y i and the class probabilities f (X i ;?), generated by the penultimate Softmax layer of a Stochastic LWTA-based network, as CE(Y i , f (X i ;?)). Here,? denotes sample instances of all the latent variables, ?, in all layers. We stress that the output class probabilities depend on the winner selection process in each layer, which is stochastic.</p><p>This way, the Evidence Lower Bound (ELBO) reads:</p><formula xml:id="formula_7">L = ? Xi,Yi?D CE(Y i , f (X i ;?)) ? KL[q(?)||p(?)])<label>(6)</label></formula><p>For simplicity, and without loss of generality, we consider a symmetric Categorical distribution for the latent variable indicators ?; hence, p(? b ) = Categorical(1/U ) ?b for dense layers, and p(? b,h ,l ) = Categorical(1/U ) ?b, h , l for convolutional ones. In our work, we perform Monte-Carlo sampling using a single reparameterized sample for each of the corresponding latent variables. These are obtained via the reparameterization trick of the continuous relaxation of the Categorical distribution <ref type="bibr" target="#b7">(Maddison et al., 2017;</ref><ref type="bibr" target="#b5">Jang et al., 2017)</ref> as described next. We focus on the reparameterization trick for the dense case; the convolutional case is analogous.</p><p>Let? denote the probabilities of q(?) (Eqs. <ref type="formula">(2)</ref> and <ref type="formula" target="#formula_3">(4)</ref>). Then, the samples? are expressed as: <ref type="figure" target="#fig_0">? Uniform(0, 1)</ref>, and ? ? (0, ?) is a temperature factor, controlling how "closely" the continuous relaxation approximates the Categorical distribution.</p><formula xml:id="formula_8">? b,u = Softmax((log? b,u + g b,u )/? ), ?b = 1, . . . , B, u = 1, . . . , U (7) where g b,u = ? log(? log V b,u ), V b,u</formula><p>On this basis, we can write the KL divergence term present in Eq. (6) as:</p><formula xml:id="formula_9">KL[q(? b )||p(? b )] = E q(? b ) [log q(? b ) ? log p(? b )] ? log q(? b ) ? log p(? b ), ?b<label>(8)</label></formula><p>Hence, the final ELBO expression yields:</p><formula xml:id="formula_10">L = ? Xi,Yi?D CE(Y i , f (X i ;?)) ? b log q(? b ) ? log p(? b )<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prediction</head><p>At prediction time, we directly draw L samples from the trained posteriors q(?) in order to determine the winning units in each block of the network. As mentioned before, each time we sample for the same input, a different subpath is followed. This is a key aspect that stochastically alters the information flow in the network and obstructs an adversary from attacking the model.</p><p>The sampling process results in a set of L output logits of the network, which we can average to obtain the final prediction:</p><formula xml:id="formula_11">f (X i ;?) = 1 L L l=1 f (X i ;? l )<label>(10)</label></formula><p>where? l denotes a sample drawn from q(?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Results</head><p>We investigate the potency of LWTA-based networks against adversarial attacks under an Adversarial Training regime; we employ a PGD adversary <ref type="bibr" target="#b8">(Madry et al., 2017)</ref>. To this end, we use the wellknown WideResNet-34 <ref type="bibr" target="#b17">(Zagoruyko and Komodakis, 2016)</ref> architecture, considering three different widen factors: 1, 5, and 10; we focus on the CIFAR-10 dataset and adopt experimental settings similar to <ref type="bibr" target="#b16">Wu et al. (2021)</ref>. We use a batch-size of 128 and an initial learning rate of 0.1; we halve the learning rate at every epoch after the 75 th epoch. We use a single sample L = 1 for prediction. All experiments were performed using a single NVIDIA Quadro P6000.</p><p>For evaluating the robustness of the proposed structure, we initially consider the conventional PGD attack with 20 steps, step size 0.007 and = 8/255. In <ref type="table" target="#tab_0">Table 1</ref>, we compare the robustness of LWTA-based WideResNet networks against the baseline results of <ref type="bibr" target="#b16">Wu et al. (2021)</ref>. As we observe, our Stochastic LWTA-based networks yield significant improvements in robustness under a traditional PGD attack; they retain extremely high natural accuracy (up to ? 13% better), while exhibiting a staggering, up to ? 32.6%, difference in robust accuracy compared to the exact same architectures employing the conventional ReLU-based nonlinearities and trained in the exact same fashion.</p><p>Further, and to ensure that our approach does not cause the well-known obfuscated gradient problem <ref type="bibr" target="#b0">(Athalye et al., 2018)</ref>, we resort to stronger parameter-free attacks using the newly introduced AutoAttack (AA) framework <ref type="bibr" target="#b2">(Croce and Hein, 2020)</ref>. AA comprises an ensemble of four powerful white-box and black-box attacks, e.g., the commonly employed A-PGD attack; this is a step-free variant of the standard PGD attack <ref type="bibr" target="#b8">(Madry et al., 2017)</ref>, which avoids the complexity and ambiguity of step-size selection. In addition, for the entailed L ? attack, we use the common = 8/255 value. Thus, in <ref type="table" target="#tab_1">Table 2</ref>, we compare the LWTA-based networks to several recent state-of-the-art approaches evaluated on AA 1 . The reported accuracies correspond to the final reported robust accuracy of the methods after sequentially performing all the considered AA attacks. Once again, we observe that the proposed networks yield state-of-the-art robustness against all SOTA methods, with an improvement of ? 16.72%, even when compared with methods that employ substantial data augmentation to increase robustness, e.g. <ref type="bibr" target="#b3">Gowal et al. (2021)</ref>. These results vouch for the potency of Stochastic LWTA networks in adversarial settings.</p><p>Finally, since our considered networks consist of stochastic components, i.e. the competitive random sampling procedure to determine the winner in each LWTA block, the output of the classifier might change at each iteration; this obstructs the attacker from successfully altering the final decision.</p><p>To counter such randomness in the involved computations, Croce and Hein (2020) combine the  <ref type="bibr" target="#b18">(Zhang et al., 2019)</ref> 53.08 Early-Stop <ref type="bibr" target="#b12">(Rice et al., 2020)</ref> 53.42 FAT <ref type="bibr" target="#b19">(Zhang et al., 2020)</ref> 53.51 HE <ref type="bibr" target="#b9">(Pang et al., 2020)</ref> 53.74 WAR <ref type="bibr" target="#b16">(Wu et al., 2021)</ref> 54.73 Pre-training <ref type="bibr" target="#b4">(Hendrycks et al., 2019)</ref>   <ref type="bibr" target="#b0">(Athalye et al., 2018)</ref>. Thus, we use AA combined with EoT for further performance evaluation of the proposed LWTA-based networks.</p><p>The corresponding results are presented in <ref type="table" target="#tab_3">Table 3</ref>. As we observe, all of the considered networks retain state-of-the-art robustness against the powerful AA &amp; EoT attacks. This conspicuously supports the usefulness of Stochastic LWTA activations towards adversarial robustness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we explored the potency of Stochastic LWTA-based networks against powerful whitebox and black-box attacks. The experimental results vouch for the efficacy of the arising networks, yielding state-of-the-art robustness in all the experimental settings. We obtained an immense improvement in robustness compared to the second best performing alternative, which notably relies on substantial data augmentation. A potentially key principle towards adversarial robustness may be the stochastic alteration of the information flow in Stochastic LWTA-based networks; this, arises from the considered data-driven winner selection mechanism in each LWTA block. Different subpaths, stochastically emerging even for the same input, essentially obstruct the adversary from successfully attacking the model. Further evaluation against stochasticity countermeasures, i.e., Expectation over Transformation (EoT), further validate our findings, as the induced decrease in the final robustness was negligible.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A detailed bisection of the b th Stochastic LWTA block in an LWTA layer. Presented with an input x ? R J , each unit u = 1, . . . , U computes its activation h b,u via different weights w b,u ? R J , i.e., h b,u = w T b,u x. The linear responses of the units are concatenated, such that h b = [h b,1 , . . . , h b,U ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) A graphical representation of our competition-based modeling approach. Rectangles denote LWTA blocks, and circles the competing units therein. The winner units are denoted with bold contours (? = 1). (b) The convolutional LWTA variant. Competition takes place position-wise among the feature maps comprising a kernel. For each position, only the winner feature map contains a non-zero entry; for the rest feature maps in the kernel, the value at said position is zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A detailed bisection of the b th convolutional stochastic LWTA block. Presented with an input X ? R H?L?C , competition now takes place among feature maps on a position-specific basis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Natural and Robust accuracy under a conventional PGD attack with 20 steps and 0.007 step-size using WideResNet-34 models with different widen factors. We use the same PGD-based Adversarial Training scheme for all models<ref type="bibr" target="#b8">(Madry et al., 2017)</ref>.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Adversarial Training-PGD</cell><cell></cell></row><row><cell></cell><cell cols="2">Natural Accuracy (%)</cell><cell cols="2">Robust Accuracy (%)</cell></row><row><cell cols="5">Widen Factor Baseline Stochastic LWTA Baseline Stochastic LWTA</cell></row><row><cell>1</cell><cell>74.04</cell><cell>87.0</cell><cell>49.24</cell><cell>81.87</cell></row><row><cell>5</cell><cell>83.95</cell><cell>91.88</cell><cell>54.36</cell><cell>83.4</cell></row><row><cell>10</cell><cell>85.41</cell><cell>92.26</cell><cell>55.78</cell><cell>84.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Robust Accuracy (%) comparison under the AutoAttack framework. ? denotes models that are trained with additional unlabeled data. The AutoAttack performance corresponds to the final robust accuracy after employing all the attacks in AA. Results directly from the AA leaderboard.</figDesc><table><row><cell>Method</cell><cell>AutoAttack</cell></row><row><cell>TRADES</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Robustness against AA combined with 20 iterations of EoT. APGD-DLR corresponds to the APGD attack, using a different loss, i.e., the Difference of Logits Ratio<ref type="bibr" target="#b2">(Croce and Hein, 2020)</ref>.</figDesc><table><row><cell cols="4">Widen Factor Nat. Acc. APGD APGD-DLR</cell></row><row><cell>1</cell><cell>87.00</cell><cell>79.67</cell><cell>76.15</cell></row><row><cell>5</cell><cell>91.88</cell><cell>81.67</cell><cell>77.65</cell></row><row><cell>10</cell><cell>92.26</cell><cell>82.55</cell><cell>79.00</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/fra31/auto-attack</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work has received funding from the European Union's Horizon 2020 research and innovation program under grant agreement No 872139, project aiD.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unlabeled data improves adversarial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Croce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Uncovering the limits of adversarial training against norm-bounded adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03593</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using pre-training can improve model robustness and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Categorical reparametrization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vladu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06083</idno>
		<title level="m">Towards deep learning models resistant to adversarial attacks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Boosting adversarial training with hypersphere embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local competition and stochasticity for adversarial robustness in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonparametric Bayesian deep networks with local competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Panousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatzis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Theodoridis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11569</idno>
		<title level="m">Overfitting in adversarially robust deep learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Hydra: Pruning adversarially robust neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sehwag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic transformer networks with linear competing units: Application to end-to-end sl translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Voskou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Panousis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kosmopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chatzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving adversarial robustness requires revisiting misclassified examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Do wider neural networks really help adversarial robustness?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">In Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Hsieh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04684</idno>
		<title level="m">The limitations of adversarial training and the blind-spot attack</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attacks which do not kill training make adversarial learning stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
