<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Intra-Inter Camera Similarity for Unsupervised Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Xuan</surname></persName>
							<email>shiyuxuan@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">School of EECS</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<postCode>100871</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Intra-Inter Camera Similarity for Unsupervised Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most of unsupervised person Re-Identification (Re-ID) works produce pseudo-labels by measuring the feature similarity without considering the distribution discrepancy among cameras, leading to degraded accuracy in label computation across cameras. This paper targets to address this challenge by studying a novel intra-inter camera similarity for pseudo-label generation. We decompose the sample similarity computation into two stage, i.e., the intra-camera and inter-camera computations, respectively. The intra-camera computation directly leverages the CNN features for similarity computation within each camera. Pseudo-labels generated on different cameras train the reid model in a multi-branch network. The second stage considers the classification scores of each sample on different cameras as a new feature vector. This new feature effectively alleviates the distribution discrepancy among cameras and generates more reliable pseudo-labels. We hence train our re-id model in two stages with intra-camera and inter-camera pseudo-labels, respectively. This simple intrainter camera similarity produces surprisingly good performance on multiple datasets, e.g., achieves rank-1 accuracy of 89.5% on the Market1501 dataset, outperforming the recent unsupervised works by 9+%, and is comparable with the latest transfer learning works that leverage extra annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-Identification (ReID) aims to match a given query person in an image gallery collected from nonoverlapping camera networks <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b22">23]</ref>. Thanks to the powerful deep Convolutional Neural Network (CNN), great progresses have been made in fully-supervised person ReID <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref>. To relieve the requirement of expensive person ID annotation, increasing efforts are being made on The code is available at https://github.com/SY-Xuan/ IICS. unsupervised person ReID <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6]</ref>, i.e., training with labeled source data and unlabeled target data, or fully relying on unlabeled target data for training.</p><p>Existing unsupervised person ReID works can be grouped into three categories: a) using domain adaptation to align distributions of features between source and target domains <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>, b) applying Generative Adversarial Network (GAN) to perform image style transfer, meanwhile maintaining the identity annotations on source domains <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b2">3]</ref>, and c) generating pseudolabels on target domains for training via assigning similar images with similar labels via clustering, KNN search, etc. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b27">28]</ref>. The first two categories define unsupervised person ReID as a transfer learning task, which leverages the labeled data on source domains. Generating pseudo-labels makes it possible to train ReID models with fully unsupervised setting, thus shows better flexibility.</p><p>Most of pseudo-labels prediction algorithms share a similar intuition, i.e., first computing sample similarities, then assigning similar samples identified by clustering or KNN with similar labels. During this procedure, the computed sample similarity largely decides the ReID accuracy. To generate high quality pseudo-labels, samples of the same identity are expected share larger similarities than with those from different identities. However, the setting of unsupervised person ReID makes it difficult to learn reliable sample similarities, especially for samples from different cameras. For example, each identity can be recorded by multi-cameras with varied parameters and environments. Those factors may significantly change the appearance of the identity. In other words, the domain gap among cameras makes it difficult to identify samples of the same identity, as well as to optimize of intra-class feature similarity. We illustrated the feature distribution of different cameras in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>.</p><p>This paper addresses the above challenge by studying a more reasonable similarity computation for pseudo-labels generation. Identifying samples of the same identify within the same camera is easier than performing the same task among different cameras. Meanwhile, domain gaps can be alleviated by learning generalizable classifiers. We hence decompose the sample similarity computation into two stages to progressively seek reliable pseudo-labels. The first stage computes sample similarity within each camera with CNN features. This "intra-camera" distance guides pseudolabel generation within each camera by clustering samples and assigning samples within the same cluster with the same label. Independent pseudo-labels in C cameras hence train the ReID model with a C-branch network, where the shared backbone is optimized by multiple tasks, and each branch is optimized by a specific classification task within the same camera. This stage simplifies pseudo-label generation, thus ensures high quality pseudo-labels and efficient backbone optimization.</p><p>The second stage proceeds to compute sample similarities across cameras. Sample similarity computed with CNN features can be affected by domain gap, e.g., large domain gap decreases the similarity among samples of the same identity as illustrated in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>. As discussed in previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, the classification probability is more robust the domain gap than raw features. We alleviate the domain gap by enhancing the generalization ability of trained classifiers in the first stage. Specifically, we classify each sample with C classifiers, and use their classification scores as a new feature vector. To ensure the classification scores robust to the domain gap, each classifier trained on one camera should generalize well on other cameras. This is achieved with the proposed Adaptive Instance and Batch Normalization (AIBN), which enhances the generalization ability of classifier without reducing their discriminative ability. Classification scores produced by C classifiers are hence adopted to calculate the "inter-camera" similarity to seek pseudo-labels across cameras. The ReID model is finally optimized by pseudo-labels generated with both stages. Distribution of features learned by our method is illustrated in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, where the domain gaps between cameras are effectively eliminated.</p><p>We test our approach in extensive experiments on multiple ReID datasets including Market1501 <ref type="bibr" target="#b40">[41]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b22">[23]</ref> and MSMT17 <ref type="bibr" target="#b30">[31]</ref>, respectively. Experiments show that each component in our approach is valid in boosting the ReID performance. A complete approach consisting of intra-inter camera similarities exhibits the best performance. For instance, without leveraging any annotations, our approach achieves rank-1 accuracy of 89.5% on the Market1501 dataset, outperforming the recent unsupervised works by 9+%. Our method also performs better than many recent transfer learning works that leverage extra annotations. For instance, the recent MMT <ref type="bibr" target="#b6">[7]</ref> and NRMT <ref type="bibr" target="#b39">[40]</ref> achieves lower rank-1 accuracies of 87.7% and 87.8% respectively, even they leverage extra annotations on DukeMTMC-ReID <ref type="bibr" target="#b22">[23]</ref> for training.</p><p>The promising performance demonstrates the validity of our method, which decomposes the similarity computation into two stages to progressively seek better pseudo-labels for training. This strategy is more reasonable than directly predicting pseudo-labels across cameras in that, it effectively alleviates the domain gap between cameras. Besides that, those two stages corresponds to different difficulty in predicting pseudo-labels, thus are complementary to each other in optimizing the ReID model. To the best of our knowledge, this is an original work studying better similarity computation strategies in unsupervised person ReID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>This work is closely related to unsupervised person ReID and, domain adaptation and generalization. Recent works on those two topics will be reviewed briefly in following paragraphs.</p><p>Unsupervised person ReID has been studied with three types of methods, i.e., by distribution alignment, training GANs, and generating pseudo-labels, respectively. Distribution alignment based methods follow the traditional domain adaptation methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref> to align the feature distribution of source and target domains. Wu et al. <ref type="bibr" target="#b31">[32]</ref> proposed a Camera-Aware Similarity Consistency Loss to align the pairwise distribution of intra-camera matching and cross-camera matching. Lin et al. <ref type="bibr" target="#b14">[15]</ref> utilized Maximum Mean Discrepancy (MMD) distance <ref type="bibr" target="#b7">[8]</ref> to align the distribution of mid-level features from source and the target domains. Some other methods use GANs <ref type="bibr" target="#b45">[46]</ref> to perform image-to-image style translation to transfer source images into target style. Zou et al. <ref type="bibr" target="#b48">[49]</ref> disentangled idrelated/unrelated features to enforce the adaptation to work on the id-related feature space. Wei et al. <ref type="bibr" target="#b30">[31]</ref> proposed person transfer GAN, which can transfer person images with the style of target dataset and keep the identity label of the person.</p><p>Pseudo-labels based methods first generate pseudolabels by formulating certain rules based on sample similarity, then train the ReID model with those pseudo-labels. The quality of computed pseudo-labels determines the perfor-mance of these methods. Unsupervised clustering method is one of the most commonly used methods to generate pseudo-labels <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b16">17]</ref>. Fan et al. <ref type="bibr" target="#b4">[5]</ref> used standard k-means clustering method to generate pseudo-labels and fine-tuned model with these labels. Lin et al. <ref type="bibr" target="#b15">[16]</ref> proposed a bottom-up clustering approach to generate pseudolabels. To avoid re-initializing the classifier at each epoch, an extra memory bank was added into the network. Wang et al. <ref type="bibr" target="#b27">[28]</ref> formulated unsupervised person ReID as multilabel classification task and used memory bank to train the network. NRMT <ref type="bibr" target="#b39">[40]</ref>, MMT <ref type="bibr" target="#b6">[7]</ref> and MEB-Net <ref type="bibr" target="#b35">[36]</ref> used mutual-training <ref type="bibr" target="#b38">[39]</ref> to reduce the influence of low-quality pseudo-labels.</p><p>Domain adaptation and generalization are commonly considered to improve the generalization ability of CNN models. Recently, some works have found that Batch Normalization (BN) <ref type="bibr" target="#b10">[11]</ref> and Instance Normalization (IN) <ref type="bibr" target="#b26">[27]</ref> could improve the network's generalization ability on multiple domains <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1]</ref>. IBN-Net <ref type="bibr" target="#b20">[21]</ref> integrated IN and BN to enhance the generalization capacity of CNNs to unseen domain without fine-tuning. Chang et al. <ref type="bibr" target="#b0">[1]</ref> improved the performance of unsupervised domain adaptation using domain-specific BN. Zhuang et al. <ref type="bibr" target="#b47">[48]</ref> designed a camerabased BN to alleviate the distribution gap between a camera pair in person ReID. Their method improved the generalization ability of the model across unseen cameras.</p><p>Most pseudo-labels based methods try to mitigate the impact of low-quality pseudo-labels or find high-quality part from generated pseudo-labels. The work most similar to us is <ref type="bibr" target="#b46">[47]</ref> which utilizes extra ID labels within each camera as supervision and simply uses classification results to find matching candidates across cameras during intercamera training. Different from those works, our work is motivated to seek a reliable similarity by progressively eliminating negative influences of pose variances, illumination, occlusions through intra-camera training, and domain gap through inter-camera training. This leads to the proposed AIBN and inter-camera similarities. As shown in our experiments, our method produces surprisingly good performance on multiple datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>Given an unlabeled person image dataset with camera information X = {X c }, where X c is a collection of person images and the superscript c = 1 : C denotes the index of cameras, respectively. Our goal is to train a person ReID model on X . For any query person image q, the ReID model is expected to produce a feature vector to retrieve image I g containing the same person from a gallery set G. The trained ReID model should guarantee q share more similar <ref type="figure" target="#fig_1">Figure 2</ref>. Illustrations of the proposed method for unsupervised person ReID. The Intra-camera training is conducted within each camera separately. It generates pseudo-labels by clustering using intra-camera similarity computed with the CNN feature f . The inter-camera training generates pseudo-labels by clustering all samples using the inter-camera similarity, which is computed with classification scores. These two stages are executed alternately during the whole training process to optimize the ReID feature f with complementary intra and inter camera losses.</p><p>feature with I g than with other images in G, i.e.,</p><formula xml:id="formula_0">g * = arg max g?G sim(f g , f q ),</formula><p>(</p><p>where f ? R d is a d-dimensional feature vector extracted by the person ReID model. sim(?) computes the feature similarity. Suppose a person p is captured by cameras in X , the collection of images of p and X can be denoted as X p and X = {X p } p=1:P , respectively, where P is the total number of persons in X . An estimation towards {X p } p=1:P would make the optimization to Eq. (1) possible, e.g., through minimizing feature distance within each {X p }, meanwhile enlarging distance between {X i } and {X j } with i = j. A commonly used strategy is performing clustering on X to generate pseudo-labels. The training objective in label prediction could be conceptually denoted as,</p><formula xml:id="formula_2">T * = arg min T D(T , {X p } p=1:P ),<label>(2)</label></formula><p>where T denotes the clustering result and D(?) computes its differences with {X p } p=1:P . The optimization towards Eq. (2) requires to identify images of the same person across cameras. This could be challenging because the appearance of an image can be affected by complicated factors. Using I c n ? X c to denote an image of person p captured by camera c, we conceptually describe the appearance of I c n as,</p><formula xml:id="formula_3">I c n . = A p + S c + E n ,<label>(3)</label></formula><p>where A p denotes the appearance of the person p. S c represents the setting of cameras c including its parameters, viewpoint, environment, etc., that affect the appearance of its captured images. We use E n to represent other stochastic factors affecting the appearance of I c n including pose, illumination, occlusions, etc.</p><p>According to Eq. (3), the challenge of Eq. (2) lies in learning feature f to alleviate the effects of S c and E n , and finding image clusters across cameras according to A p . To conquer this challenge, we propose to perform pseudo-label prediction with two stages to progressively enhance the robustness of f to E n and S c , respectively.</p><p>The robustness to E n can be enhanced by performing Eq. (2) within each camera using existing pseudo-label generation methods, then training f according to the clustering result. Suppose the clustering result for the c-th camera is T c , the training loss on c-th cameras can be represented as,</p><formula xml:id="formula_4">L c intra = In?X c ,In?T c m loss c (f n , m),<label>(4)</label></formula><p>where m denotes the cluster ID, which is used as the pseudo-label of I n for loss computation. To ensure the robustness of f towards complicated E n under different cameras, Eq. (4) can be computed on different cameras by sharing the same f . This leads to a multi-branch CNN, where each branch corresponds to a classifier, and their shared backbone learns the feature f . The robustness to S c is enhanced in the second stage by clustering images of the same person across cameras. Directly using the learned f to measure similarity for clustering suffers from S c . We propose to compute a more robust inter-camera similarity. The intuition is to train classifiers with domain adaption strategies to gain enhanced generalization ability, e.g., the classifier on camera c is expected to be discriminative on other cameras. We thus could identify images of the same person from different cameras based on their classification scores, and enlarge their similarity with the inter-camera similarity, i.e., <ref type="bibr" target="#b4">(5)</ref> where s n denotes the classification score of image I n . ?(s m , s n ) is the probability that I m and I n are from the same identity. Eq. (5) enlarges the similarity of two images from different cameras, if they are identified as the same person. It effectively alleviates S c during similarity computation and image clustering. We hence further optimize f with the inter-camera loss based on the clustering result T , i.e., L inter = In?Tm loss(f n , m).</p><formula xml:id="formula_5">SIM inter (I m , I n ) = sim(f m , f n ) + ??(s m , s n ),</formula><p>Our method is progressively optimized by Eq. (4) and Eq. <ref type="formula" target="#formula_6">(6)</ref>, respectively to gain f with the robustness to S c , E n . Their detailed computations, as well the implementation of ?(?) and generalization ability enhancement will be presented in the following parts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Intra-camera Training</head><formula xml:id="formula_7">loss c (f n , m) = (F (w c , f n ) , m) ,<label>(7)</label></formula><p>where F(w c , ?) denotes a classier with learnable parameters w c . (?) computes the softmax cross entropy loss on classifier outputs and the groundtruth label m.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, the intra-camera training treats each cameras as a training task and trains the f with multiple tasks. The overall training loss can be denoted as</p><formula xml:id="formula_8">L intra = C c=1 L c intra ,<label>(8)</label></formula><p>where C is the total number of cameras. As discussed in Sec. 3.1, Eq. (8) effectively boosts the discriminative power of f within each camera. Besides that, optimizing f on multi-tasks boosts its discriminative power on different domains, which in-turn enhances the generalization ability of learned classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Inter-camera Training</head><p>To estimate the probability that two samples from different cameras belong to the same identity, a domainindependent feature is needed. As discussed in related works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26]</ref>, samples belonging to the same identity should have similar distribution of classification probability produced by each classifier. We use the jaccard similarity of classification probability to compute the ?(s m , s n ), which reflects the probability that I m and I n are from the same identity</p><formula xml:id="formula_9">?(s m , s n ) = s m ? s n s m ? s n ,<label>(9)</label></formula><p>where ? is the element-wise min of two vectors and ? is the element-wise max of two vectors. The classification score s m is acquired by concatenating the classification scores from C classifiers,</p><formula xml:id="formula_10">s m =[s 1 m , ? ? ? , s c m ], s c m =[p(1|f m , w c ), ? ? ? , p(k|f m , w c )],<label>(10)</label></formula><p>where p(k|f m , w c ) is the classification probability of at class k computed by the classifier F (w c ; ?) and s c m is the classification score of image I m on camera c.</p><p>To make the ?(s m , s n ) work as expected, classifier trained on each camera needs to generalize well on other cameras. The f trained by multi-task learning in the intracamera stage provides basic guarantee for generalization ability of the network. In order to further improve generalization of different classifiers, we propose AIBN which will be described in detail at Sec. 3.4.</p><p>With ?(s m , s n ), clustering can be performed based on inter-camera similarity to generate pseudo-labels on X . Then Eq. (6) can be computed as:</p><formula xml:id="formula_11">L inter = 1 |B| In?B (F (w, f n ) , m) + ?L triplet ,<label>(11)</label></formula><p>where B is a training mini-batch, is the softmax cross entropy loss, m is its pseudo-label assigned by clustering result, ? is loss weight and L triplet is the hard-batch triplet loss <ref type="bibr" target="#b9">[10]</ref>. We randomly select P clusters and K samples from each cluster to construct the training mini-batch B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adaptive Instance and Batch Normalization</head><p>As discussed above, we propose AIBN to boost the generalization ability of learned classifiers. Instance Normalization (IN) <ref type="bibr" target="#b26">[27]</ref> can make the network invariant to appearance changes. However, IN reduces the inter-class variance, making the network less discriminative. Different from IN, Batch Normalization (BN) <ref type="bibr" target="#b10">[11]</ref> retains variations across different classes and reduces the internal covariate shift during network training. In other words, IN and BN are complementary to each other.</p><p>In order to gain the advantages of both IN and BN, we propose the AIBN. It is computed by linearly fusing the statistics (mean and var) obtained by IN and BN, i.e.,</p><formula xml:id="formula_12">x[i, j, n] = ? x[i, j, n] ? (?? bn + (1 ? ?)? in ) ?? 2 bn + (1 ? ?)? 2 in + + ?,<label>(12)</label></formula><p>where x[i, j, n] ? R H?W ?N is the feature map of each channel, ? bn and ? bn are the mean and variance calculated by BN, ? in and ?in are the mean and variance calculated by IN, ? and ? are affine parameters and ? is a learnable weighting parameter. The optimization of ? can be conducted with back-propagation during CNN training. We add no constraints to ? during training back-propagation. During network forward inference using Eq. (12), we clamp ? into [0, 1] to avoid negative values. Discussion: To show the effects of two training stages, we visualize the distribution of similarities between samples in <ref type="figure" target="#fig_2">Fig. 3</ref>. We use Red color to indicate the distribution of similarity between samples from different cameras. It is clear in <ref type="figure" target="#fig_2">Fig. 3 (a)</ref> that, the Red color is mixed with the Yellow and Green color, which indicate the distribution of similarity between samples of different identities. Therefore, clustering using similarity in <ref type="figure" target="#fig_2">Fig. 3</ref> (a) would lead to poor performance. It also can be observed that, the intracamera training and inter-camera training progressively improves the discriminative power of feature similarity. The inter-camera training produces the most reliable similarity. More evaluations will be presented in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Evaluation Metrics</head><p>We evaluate our methods on three commonly used person ReID datasets, e.g., DukeMTMC-ReID <ref type="bibr" target="#b22">[23]</ref>, Mar-ket1501 <ref type="bibr" target="#b40">[41]</ref>, and MSMT17 <ref type="bibr" target="#b30">[31]</ref>, respectively.</p><p>DukeMTMC-ReID is collected from 8 non-overlapping camera views, containing 16,522 images of 702 identities for training, 2,228 images of the other 702 identities for query, and 17,661 gallery images.</p><p>Market1501 is a large-scale dataset captured from 6 cameras, containing 32,668 images with 1,501 identities. It is divided into 12,936 images of 751 identities for training and 19,732 images of 750 identities for testing.</p><p>MSMT17 is a newly published person ReID dataset. It contains 126,441 images of 4,101 identities captured from 15 cameras. It is divided into 32,621 images of 1,041 identities for training and 93,820 images of 3,060 identities for testing.</p><p>During training, we only use images and camera labels from the training set of each dataset and do not utilize any other annotation information. Performance is evaluated by the Cumulative Matching Characteristic (CMC) and mean Average Precision (mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use ResNet-50 <ref type="bibr" target="#b8">[9]</ref> pre-trained on ImageNet <ref type="bibr" target="#b1">[2]</ref> as backbone to extract the feature. The layers after pooling-it. During testing and clustering, we extract the pooling-5 feature to calculate the similarity. All models are trained with PyTorch.</p><p>During training, the input image is resized to 256 ? 128. Image augmentation strategies such as random flipping and random erasing are performed. At each round we perform intra-camera stage and inter-camera stage in order. The number of training round is set as 40.</p><p>At intra-camera training stage, the batch size is 8 for each camera. The SGD is used to optimize the model. The learning rate for ResNet-50 base layers is 0.0005, and the one for other layers is 0.005.</p><p>At inter-camera training stage, a mini-batch of 64 is sampled with P = 16 randomly selected clusters and K = 4 randomly sampled images per cluster. The SGD is also used to optimize the model. The learning rate for ResNet-50 base layers is 0.001, and the one for other layers is 0.01. The loss weight ? in Eq. (11) is fixed to 1. Margin in triplet loss is fixed to 0.3. The training progressively uniforms the distribution of features from different cameras. Therefore, the initial ? in Eq. (5) is set as 0.02, and follows the poly policy for decay.</p><p>For Market1501 and DukeMTMC-ReID, we train the model for 2 epochs at both stages. For MSMT17 we train the model for 12 epochs at intra-camera stage and 2 epochs at inter-camera stage. We use the standard Agglomerative Hierarchical method <ref type="bibr" target="#b21">[22]</ref> for clustering. For Market1501 and DukeMTMC-ReID, the number of clusters is 600 for each camera at intra-camera stage and 800 at inter-camera stage. For MSMT17, the number of clusters is 600 for each camera at intra-camera stage and 1200 at inter-camera stage.</p><p>Although additional clustering within each camera is performed, this is more efficient than clustering on the entire set. Therefore, the computational complexity of our method is acceptable. It takes about 4-5 hours to finish the training with a GPU on Market1501.</p><p>For the AIBN, the mixture weight ? is initialized to 0.5. We replace all BNs in layer3 and layer4 of ResNet-50 with AIBN. Mixture weights are shared at each BottleNeck module. The detailed analysis of this component is performed in Section. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>The impact of individual components. In this section we evaluate the effectiveness of each component in our method, the experimental results of each setting are summarized in <ref type="table">Table 1</ref>. As shown in the table, when only intercamera stage is used for training, performance is not satisfactory. This shows that, the similarity between samples from different cameras is unreliable. Clustering directly using this similarity can lead to a poor performance. The rank-1 accuracy on Market1501 and DukeMTMC-ReID can achieve 71.6% and 62.9%, respectively, with only intracamera training stage. This indicates that the similarity between samples from the same camera is reliable. Without considering the distribution gap between cameras, the addition of the inter-camera training stage leads to a decrease in performance on DukeMTMC-ReID. It is clear that although the feature produced by the model has been improved after the intra-camera stage, the similarity between samples from different cameras is still unreliable. Our method achieves the best performance when the inter camera similarity in Eq. (5) is used in inter-camera training stage. It demonstrates that the inter-camera similarity is more effective than CNN features similarity and is crucial for the our performance enhancement. Since the jaccard similarity can be used to calculate the probability that samples belong to the same identity, Eq. (9) can also be used as similarity for inter-camera clustering. This setting can also achieve good performance, which means the jaccard similarity is also more robust to domain gap between cameras. Experimental results show that each component in our method is important for performance boost, and their combination achieves the best performance.</p><p>The impact of AIBN. To test the validity of AIBN, we test it with different training settings. The results are summarized in <ref type="table">Table 2</ref>. Replacing BNs in backbone with IN can improve the performance on DukeMTMC-ReID but decrease performance on Market1501, which shows that only applying IN can not bring stable performance enhancement. AIBN can improve the performance on both dataset even though the mixture weight ? of AIBN is fixed at 0.5 during training, which indicates that the combination of IN and BN brings more stable performance gains. Optimizing mixture weight ? can further improve the performance on Mar-ket1501 and DukeMTMC-ReID. It is clear that AIBN can improve the generalization ability of trained network on different domains and cameras. IBN <ref type="bibr" target="#b20">[21]</ref> is another method of combining BN and IN to improve network generalization ability. The result shows that our AIBN substantially outperforms IBN.  <ref type="table">Table 3</ref>. Evaluation on the generalization ability of backbone with/without AIBN. To further test the generalization of the network with AIBN, we train the network with labels on each dataset and test it directly on another dataset without fine-tuning. The results are shown in <ref type="table">Table 3</ref>. On Market1501 and DukeMTMC-ReID, the AIBN improves the rank-1 accuracy by 5.6% and 11.3% for the direct transfer task, respectively. It is clear that AIBN can improve the generalization of the network.</p><p>Hyper-parameter Analysis. We investigate some important hyper-parameters in this section. <ref type="figure" target="#fig_3">Fig. 4</ref> shows effects of parameter ? in Eq. <ref type="bibr" target="#b4">(5)</ref>. We can see that, as ? increases from 0 to 0.02, rank-1 accuracy on Market1501 and DukeMTMC-ReID increases from 78.6% and 55.1% to 88.8% and 72.1%, respectively. This shows that larger u brings considerable performance gains. It is also clear that ? is easy to tune, i.e., ? &gt; 0.01 leads to similar performance on different datasets.</p><p>We also conduct several experiments to verify the impacts of replacing BN with AIBN in different layers of the network and different weight sharing methods of ? in Eq. <ref type="bibr" target="#b11">(12)</ref>. The results are shown in <ref type="table">Table 4</ref>.</p><p>Replacing BNs in the deep layer of the network brings more substantial performance gains than replacing BNs in the shallow layer of the network. It is clear that, replacing BNs with AIBN in Layer4 brings more significant performance gains than the replacements in Layer1 and Layer2.</p><p>Since replacing BNs of Layer3 and Layer4 gives slightly better results, this setting is used in our experiments. We evaluate three settings of weight sharing methods of ?: (a) Each AIBN has its own ?; (b) AIBNs in the same Bot-tleNeck module share the same ?; (c) AIBNs in the same Layer of ResNet-50 share the same ?. The results show that different weight sharing methods of ? has limited impacts on the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>We compare our method with recent unsupervised and transfer learning methods on Market1501 <ref type="bibr" target="#b40">[41]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b22">[23]</ref> and MSMT17 <ref type="bibr" target="#b30">[31]</ref>. <ref type="table" target="#tab_3">Table 5</ref> and <ref type="table">Table 6</ref> summarize the comparison.</p><p>We first compare our method with methods trained with only unlabeled data. Compared methods include hand-craft features based methods, and deep learning based methods. It can be seen from <ref type="table" target="#tab_3">Table 5</ref> that compared with other deep learning based methods, our method surpasses these methods by a large margin. This significant improvement is mainly thanks to the more reliable similarity between samples used in clustering.</p><p>We also compare with the unsupervised domain adaptation methods, including GAN based methods (PTGAN <ref type="bibr" target="#b30">[31]</ref>, etc.), Distribution alignment based methods (TJ-AIDL <ref type="bibr" target="#b28">[29]</ref>, etc.), and Pseudo-labels based methods (MAR <ref type="bibr" target="#b32">[33]</ref>, etc.). Pseudo-labels based methods perform better than other types of methods in most cases. Many transfer learning methods use extra labeled source domain data for training. Our method still outperforms them using only unlabeled data for training. The performance of our method can be further improved by using the re-ranking similarity <ref type="bibr" target="#b41">[42]</ref> instead of the cosine similarity. Note that, re-ranking similarity is a commonly used similarity in unsupervised ReID <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref> and is only used during training. Therefore, it only increases the training time and has no effect on the network inference time and online ReID time.  <ref type="table">Table 6</ref>. Performance comparison with recent methods on MSMT17 <ref type="bibr" target="#b30">[31]</ref>. IICS denotes our method. ?denotes using the cosine similarity to compute the CNN feature similarity. ?denotes using the re-ranking similarity <ref type="bibr" target="#b41">[42]</ref> to replace the cosine similarity.</p><p>To further verify the effectiveness of our algorithm, we conduct experiments on a larger and more challenging dataset MSMT17. Our method outperforms existing methods under both unsupervised and unsupervised transfer settings by a large margin. We achieve the rank-1 accuracy of 56.4%, about 11% higher than the recent NRMT <ref type="bibr" target="#b39">[40]</ref>, which adopts extra DukeMTMC-ReID for training. Those above experiments clearly demonstrate the superior performance of the proposed method.</p><p>Discussion Our method uses pre-defined clustering numbers in both stages, thus the clustering number is a critical parameter for pseudo label generation. The clustering number can also be adaptively determined by setting a similarity threshold. Generalizable strategies for determining the clustering number for different datasets will be studied in our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a intra-inter camera similarity method for unsupervised person ReID which iteratively optimizes Intra-Inter Camera similarity through generating intra-and inter-camera pseudo-labels. The intra-camera training stage is proposed to train a multi-branch CNN using generated intra-camera pseudo-labels. Based on the classification score produced by each classifier trained at intracamera training stage, a more robust inter-camera similarity can be calculated. Then the network can be trained with the pseudo-label generated by performing clustering across cameras with this inter-camera similarity. Moreover, AIBN is introduced to boost the generalization ability of the network. Extensive experimental results demonstrate the effectiveness of the proposed method in unsupervised person ReID.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>t-SNE visualization [20] of features from a subset of DukeMTMC-ReID. Different colors indicate samples from different cameras. Baseline features in (a) suffer from feature distribution discrepancies among cameras. Features learned by our method are visualized in (b), where features from different cameras have similar distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>illustrates our framework, where the person ReID feature f is optimized by two stages. The intra-camera training stage divides the training set X into subsets {X c } according to the camera index of each image. Then, it performs clustering on each subset according to the similarity computed with feature f . Assigning images within each cluster with identical label turns each X c into a labeled dataset, allowing the function loss c (?) in L c intra can be computed as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The distribution of similarity on DukeMTMC-ReID. Blue and Red color indicates the distribution of similarity between samples of the same identity from the same camera and different cameras, respectively. Yellow and Green color indicates the distribution of similarity between samples of different identities from the same camera and different cameras, respectively. To show an intuitive visualization of real data, similarities are normalized into [0, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Evaluation of parameter ? in Eq. (5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison with recent methods on Market1501 and DukeMTMC-ReID. IICS denotes our method. ?denotes using the cosine similarity to compute the CNN features similarity. ?denotes using the re-ranking similarity<ref type="bibr" target="#b41">[42]</ref> to replace the cosine similarly. * denotes the same backbone ResNet-50 is used in MEB-Net.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">Reference</cell><cell>Source</cell><cell cols="4">Market1501 mAP Rank-1 Rank-5 Rank-10</cell><cell>Source</cell><cell cols="3">DukeMTMC-ReID mAP Rank-1 Rank-5 Rank-10</cell></row><row><cell cols="2">PTGAN [31]</cell><cell cols="2">CVPR18</cell><cell>Duke</cell><cell>-</cell><cell>38.6</cell><cell>-</cell><cell>66.1</cell><cell>Market</cell><cell>-</cell><cell>27.4</cell><cell>-</cell><cell>50.7</cell></row><row><cell>HHL [43]</cell><cell></cell><cell cols="2">ECCV18</cell><cell>Duke</cell><cell>31.4</cell><cell>62.2</cell><cell>78.8</cell><cell>84.0</cell><cell>Market</cell><cell>27.2</cell><cell>46.9</cell><cell>61.0</cell><cell>66.7</cell></row><row><cell cols="2">DG-Net++ [49]</cell><cell cols="2">ECCV20</cell><cell>Duke</cell><cell>61.7</cell><cell>82.1</cell><cell>90.2</cell><cell>92.7</cell><cell>Market</cell><cell>63.8</cell><cell>78.9</cell><cell>87.8</cell><cell>90.4</cell></row><row><cell cols="2">TJ-AIDL [29]</cell><cell cols="2">CVPR18</cell><cell>Duke</cell><cell>26.5</cell><cell>58.2</cell><cell>74.8</cell><cell>81.8</cell><cell>Market</cell><cell>23.0</cell><cell>44.3</cell><cell>59.6</cell><cell>65.0</cell></row><row><cell>MMFA [15]</cell><cell></cell><cell cols="2">BMVC18</cell><cell>Duke</cell><cell>27.4</cell><cell>56.7</cell><cell>75.0</cell><cell>81.8</cell><cell>Market</cell><cell>24.7</cell><cell>45.3</cell><cell>59.8</cell><cell>66.3</cell></row><row><cell>CSCL [32]</cell><cell></cell><cell cols="2">ICCV19</cell><cell>Duke</cell><cell>35.6</cell><cell>64.7</cell><cell>80.2</cell><cell>85.6</cell><cell>Market</cell><cell>30.5</cell><cell>51.5</cell><cell>66.7</cell><cell>71.7</cell></row><row><cell>MAR [33]</cell><cell></cell><cell cols="2">CVPR19</cell><cell cols="2">MSMT17 40.0</cell><cell>67.7</cell><cell>81.9</cell><cell>-</cell><cell cols="2">MSMT17 48.0</cell><cell>67.1</cell><cell>79.8</cell><cell>-</cell></row><row><cell cols="2">AD-Cluster [35]</cell><cell cols="2">CVPR20</cell><cell>Duke</cell><cell>68.3</cell><cell>86.7</cell><cell>94.4</cell><cell>96.5</cell><cell>Market</cell><cell>54.1</cell><cell>72.6</cell><cell>82.5</cell><cell>85.5</cell></row><row><cell>NRMT [40]</cell><cell></cell><cell cols="2">ECCV20</cell><cell>Duke</cell><cell>71.7</cell><cell>87.8</cell><cell>94.6</cell><cell>96.5</cell><cell>Market</cell><cell>62.2</cell><cell>77.8</cell><cell>86.9</cell><cell>89.5</cell></row><row><cell cols="2">MMT-500 [7]</cell><cell cols="2">ICLR20</cell><cell>Duke</cell><cell>71.2</cell><cell>87.7</cell><cell>94.9</cell><cell>96.9</cell><cell>Market</cell><cell>63.1</cell><cell>76.8</cell><cell>88.0</cell><cell>92.2</cell></row><row><cell cols="2">MEB-Net  *  [36]</cell><cell cols="2">ECCV20</cell><cell>Duke</cell><cell>71.9</cell><cell>87.5</cell><cell>95.2</cell><cell>96.8</cell><cell>Market</cell><cell>63.5</cell><cell>77.2</cell><cell>87.9</cell><cell>91.3</cell></row><row><cell>LOMO [14]</cell><cell></cell><cell cols="2">CVPR15</cell><cell>None</cell><cell>8.0</cell><cell>27.2</cell><cell>41.6</cell><cell>49.1</cell><cell>None</cell><cell>4.8</cell><cell>12.3</cell><cell>21.3</cell><cell>26.6</cell></row><row><cell>BOW [41]</cell><cell></cell><cell cols="2">ICCV15</cell><cell>None</cell><cell>14.8</cell><cell>35.8</cell><cell>52.4</cell><cell>60.3</cell><cell>None</cell><cell>8.3</cell><cell>17.1</cell><cell>28.8</cell><cell>34.9</cell></row><row><cell>BUC [16]</cell><cell></cell><cell cols="2">AAAI19</cell><cell>None</cell><cell>29.6</cell><cell>61.9</cell><cell>73.5</cell><cell>78.2</cell><cell>None</cell><cell>22.1</cell><cell>40.4</cell><cell>52.5</cell><cell>58.2</cell></row><row><cell>HCT [34]</cell><cell></cell><cell cols="2">CVPR20</cell><cell>None</cell><cell>56.4</cell><cell>80.0</cell><cell>91.6</cell><cell>95.2</cell><cell>None</cell><cell>50.7</cell><cell>69.6</cell><cell>83.4</cell><cell>87.4</cell></row><row><cell>MMCL [28]</cell><cell></cell><cell cols="2">CVPR20</cell><cell>None</cell><cell>45.5</cell><cell>80.3</cell><cell>89.4</cell><cell>92.3</cell><cell>None</cell><cell>40.2</cell><cell>65.2</cell><cell>75.9</cell><cell>80.0</cell></row><row><cell>JVTC+ [13]</cell><cell></cell><cell cols="2">ECCV20</cell><cell>None</cell><cell>47.5</cell><cell>79.5</cell><cell>89.2</cell><cell>91.9</cell><cell>None</cell><cell>50.7</cell><cell>74.6</cell><cell>82.9</cell><cell>85.3</cell></row><row><cell>IICS  ?</cell><cell></cell><cell cols="2">This paper</cell><cell>None</cell><cell>72.1</cell><cell>88.8</cell><cell>95.3</cell><cell>96.9</cell><cell>None</cell><cell>59.1</cell><cell>76.9</cell><cell>86.1</cell><cell>89.8</cell></row><row><cell>IICS  ?</cell><cell></cell><cell cols="2">This paper</cell><cell>None</cell><cell>72.9</cell><cell>89.5</cell><cell>95.2</cell><cell>97.0</cell><cell>None</cell><cell>64.4</cell><cell>80.0</cell><cell>89.0</cell><cell>91.6</cell></row><row><cell>Methods</cell><cell cols="2">Source</cell><cell cols="4">MSMT17 mAP Rank-1 Rank-5 Rank-10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTGAN [31]</cell><cell cols="2">Market</cell><cell>2.9</cell><cell>10.2</cell><cell>-</cell><cell>24.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [44]</cell><cell cols="2">Market</cell><cell>8.5</cell><cell>25.3</cell><cell>36.3</cell><cell>42.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSG [6]</cell><cell cols="3">Market 13.2</cell><cell>31.6</cell><cell>-</cell><cell>49.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NRMT [40]</cell><cell cols="3">Market 19.8</cell><cell>43.7</cell><cell>56.5</cell><cell>62.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">DG-Net++ [49] Market 22.1</cell><cell>48.4</cell><cell>60.9</cell><cell>66.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">MMT-1500 [7] Market 22.9</cell><cell>49.2</cell><cell>63.1</cell><cell>68.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTGAN [31]</cell><cell cols="2">Duke</cell><cell>3.3</cell><cell>11.8</cell><cell>-</cell><cell>27.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [44]</cell><cell cols="2">Duke</cell><cell>10.2</cell><cell>30.2</cell><cell>41.5</cell><cell>46.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSG [6]</cell><cell cols="2">Duke</cell><cell>13.3</cell><cell>32.2</cell><cell>-</cell><cell>51.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NRMT [40]</cell><cell cols="2">Duke</cell><cell>20.6</cell><cell>45.2</cell><cell>57.8</cell><cell>63.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DG-Net++ [49]</cell><cell cols="2">Duke</cell><cell>22.1</cell><cell>48.8</cell><cell>60.9</cell><cell>65.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMT-1500 [7]</cell><cell cols="2">Duke</cell><cell>23.3</cell><cell>50.1</cell><cell>63.9</cell><cell>69.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMCL [28]</cell><cell cols="2">None</cell><cell>11.2</cell><cell>35.4</cell><cell>44.8</cell><cell>49.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>JVTC+ [13]</cell><cell cols="2">None</cell><cell>17.3</cell><cell>43.1</cell><cell>53.8</cell><cell>59.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IICS  ?</cell><cell cols="2">None</cell><cell>18.6</cell><cell>45.7</cell><cell>57.7</cell><cell>62.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>IICS  ?</cell><cell cols="2">None</cell><cell>26.9</cell><cell>56.4</cell><cell>68.8</cell><cell>73.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Domain-specific batch normalization for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woong-Gi</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tackgeun</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suha</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Coelho De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A fast, consistent kernel twosample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaid</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sriperumbudur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Global distance-distributions separation for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Joint visual and temporal consistency for unsupervised domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by local maximal occurrence representation and metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-task mid-level feature alignment network for unsupervised cross-dataset person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Tsun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Chichung</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain adaptive person re-identification via coupling optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="547" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-guided hash coding for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MIPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="246" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ergys</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved texture networks: Maximizing quality and diversity in feed-forward stylization and texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transferable joint attribute-identity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vp-reid: Vehicle and person re-identification system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMR, ICMR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="501" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by camera-aware similarity consistency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ad-cluster: Augmented discriminative clustering for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiple expert brainstorming for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-training with progressive augmentation for unsupervised cross-domain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Alignedreid: Surpassing human-level performance in person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilai</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiqi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with noise resistible mutual-training for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengcai</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Sen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Camera style adaptation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Pietro Morerio, Vittorio Murino, and Shaogang Gong. Intracamera supervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangping</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05046</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking the distribution gap of person re-identification with camera-based batch normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijie</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longhui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haizhou</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Joint disentangling and adaptation for crossdomain person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
