<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SFace: Sigmoid-constrained Hypersphere Loss for Robust Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyao</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyue</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongchao</forename><surname>Wen</surname></persName>
						</author>
						<title level="a" type="main">SFace: Sigmoid-constrained Hypersphere Loss for Robust Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep face recognition has achieved great success due to large-scale training databases and rapidly developing loss functions. The existing algorithms devote to realizing an ideal idea: minimizing the intra-class distance and maximizing the inter-class distance. However, they may neglect that there are also low quality training images which should not be optimized in this strict way. Considering the imperfection of training databases, we propose that intra-class and inter-class objectives can be optimized in a moderate way to mitigate overfitting problem, and further propose a novel loss function, named sigmoid-constrained hypersphere loss (SFace). Specifically, SFace imposes intra-class and inter-class constraints on a hypersphere manifold, which are controlled by two sigmoid gradient re-scale functions respectively. The sigmoid curves precisely re-scale the intra-class and inter-class gradients so that training samples can be optimized to some degree. Therefore, SFace can make a better balance between decreasing the intra-class distances for clean examples and preventing overfitting to the label noise, and contributes more robust deep face recognition models. Extensive experiments of models trained on CASIA-WebFace, VGGFace2, and MS-Celeb-1M databases, and evaluated on several face recognition benchmarks, such as LFW, MegaFace and IJB-C databases, have demonstrated the superiority of SFace.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>D EEP face recognition has obtained surprising improvement recent years <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. The pipeline for deep face recognition has been widely used for its practical usage <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>. That is, deep face recognition models are trained on web-collected databases <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, and work as deep feature extractors to evaluate on other testing databases <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>.</p><p>The large-scale training databases <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> are fundamental for the success of deep face recognition. For training databases of deep face recognition, we can never expect to obtain a "perfect" training database which should include, but not limited to, sufficient numbers of identities, and adequate images of each identity. Considering the copyright and privacy protection, the number of identities in the webcollected training databases is limited compared with the global population, and celebrities of web-collected databases may be far from the testing settings in daily life <ref type="bibr" target="#b9">[10]</ref>. In addition, we can hardly collect images with full intra-class <ref type="bibr">Yaoyao</ref>  variation to model the large pose, face expressions and illumination variance of each identity <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b21">[22]</ref>, therefore there are a significant portion of under-represented identities <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Considering the open-set protocal and the limitations of training databases, current research focus is trying to make best use of the training databases, and improve the ability of loss functions to obtain a more discriminative feature extractor. One of the most effective loss functions is the large margin loss function <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. They incorporate large margins to softmax loss to encourage the intra-class compactness and the inter-class orthogonality, which has alleviated the aforementioned quantity limitation and imbalance problem of identities to some degree.</p><p>Existing mainstream methods devote to minimizing the intra-class distance and maximizing the inter-class distance. Despite the success, they may neglect that, in addition to the high quality training images, there are also low quality training images such as misaligned images, low-resolution images, and label noise, which cannot provide effective information for distinguishing the labeled identity. Even human annotations are not reliable as we thought, because humans often struggle to distinguish between hard examples and low quality training images, and they have already been surpassed by deep face recognition models a few years ago <ref type="bibr" target="#b27">[28]</ref>. For this reason, although training databases have been elaborated by semiautomatic data cleaning algorithms <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b6">[7]</ref>, there still exists noise inevitably. Due to the imperfection of training databases, strictly minimizing the intra-class distance and maximizing the inter-class distance would lead to overfitting. Therefore, our aim is to design a new loss function, which can increase the possibility of finding the best compromise between underfitting and overfitting to a specific training database, in order to obtaining better generalization ability.</p><p>Considering the imperfection of the training databases, formally, we abandon the softmax-based loss while start from the primary and fundamental idea: optimize intra-class and inter-class distances to some extent, to improve the generalization ability of models. Furthermore, we propose a novel loss function, named sigmoid-constrained hypersphere loss (SFace), to implement this idea. SFace imposes intra-class and inter-class constraints on a hypersphere manifold. The intraclass and inter-class constraints are controlled by two sigmoid curves. The sigmoid curves precisely re-scale intra-class and inter-class gradients so that intra-class and inter-class distances are optimized to some extent. As illustrated in <ref type="figure">Figure 1</ref>, for the deep feature of a training sample, the optimizing direction is always along the tangent of the hypersphere while the moving speed is controlled by the designed gradients precisely. <ref type="figure">Fig. 1</ref>. Schematic illustration of the sigmoid-constrained hypersphere loss, which imposes intra-class and inter-class constraints on a hypersphere manifold. The optimizing directions of samples and target embedding are always along the tangent of the hypersphere while the moving speed is controlled by two sigmoid curves respectively. Specifically, the moving speed of the deep feature and its target center decreases gradually as they approaching to each other, while the moving speed of and other target centers increases rapidly as they start approaching to each other.</p><p>Specifically, the moving speed of the deep feature and its target center decreases gradually as they approaching to each other, while the moving speed of and other target centers increases rapidly as they start approaching to each other.</p><p>Compared with optimizing training samples strictly, the advantage of SFace is that it provides a relatively better balance between overfitting and underfitting, for the reason that SFace adopts sigmoid functions of intra-class and inter-class gradient re-scale terms to achieve excellent control respectively. We give a simple and easy example in <ref type="figure" target="#fig_0">Figure 2</ref> for understanding. Under the label noise setting, the model would overfit to the label noise by strictly dragging the noisy samples to the wrong labeled identities. In contrast, SFace can mitigate this problem in some degree because it optimizes noisy samples in a moderate way. With the precisely control, the clean training samples are optimized earlier and more easily, while the label noise can be left behind.</p><p>Our major contributions can be summarized as follows:</p><p>? Considering the imperfection of face training databases, we introduce a new idea: optimizing intra-class and interclass objectives in a moderate way to mitigate overfitting problem to face training databases. ? Under the guidance of this idea, we propose a new loss function, named sigmoid-constrained hypersphere loss (SFace), which can increase the possibility of finding the best compromise between underfitting and overfitting, in order to obtaining better generalization ability. ? Our method is evaluated on three training databases including CASIA-WebFace <ref type="bibr" target="#b10">[11]</ref>, VGGFace2 <ref type="bibr" target="#b13">[14]</ref> and MS-Celeb-1M <ref type="bibr" target="#b11">[12]</ref>, and consistently outperforms the state-of-the-art methods on several benchmarks including LFW <ref type="bibr" target="#b15">[16]</ref>, YTF <ref type="bibr" target="#b16">[17]</ref>, CALFW <ref type="bibr" target="#b17">[18]</ref>, CPLFW <ref type="bibr" target="#b18">[19]</ref>, MegaFace <ref type="bibr" target="#b19">[20]</ref>, IJB-A <ref type="bibr" target="#b20">[21]</ref> and IJB-C <ref type="bibr" target="#b21">[22]</ref> databases. The remainder of the paper is organized as follows. Section II briefly reviews the related deep face recognition works. In Section III, we first give a general introduction to the proposed sigmoid-constrained hypersphere loss (SFace). Then, we detail the gradient re-scale function of SFace. Finally, we discuss the relationship between SFace and softmax based loss functions. Experimental settings and results are presented in Section IV. Section V summarizes the conclusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we discuss and compare the loss functions in deep face recognition, which are almost entirely around the idea of minimizing the intra-class distance and maximizing the inter-class distance. There are mainly two types.</p><p>The first type applies metric learning method in deep learning <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, which maps face images to a deep feature space and directly optimizes distances, so that the inter-class distance is larger than the intra-class distance. The contrastive loss <ref type="bibr" target="#b0">[1]</ref>, triplet loss <ref type="bibr" target="#b1">[2]</ref> and N-pair loss <ref type="bibr" target="#b28">[29]</ref> are early methods to enhance the discrimination ability of deep features, which optimize intra-class and inter-class variance by using face pairs. Combined with softmax loss, centerloss <ref type="bibr" target="#b2">[3]</ref> obtains promising performance by simultaneously learns a center for deep features of each class and minimizes the distances between training samples and their corresponding class centers. Then, range loss <ref type="bibr" target="#b23">[24]</ref> minimizes overall intrapersonal differences and maximizes inter-personal differences in one mini-batch. Marginal loss <ref type="bibr" target="#b29">[30]</ref> is further proposed to maximize the inter-class distance and minimize the intra-class distance simultaneously by focusing on the marginal samples.</p><p>The second type makes modification on cross-entropy loss (usually referred to as "softmax loss") to learn more discriminative features <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Some early works incorporate weights or features normalization <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b31">[32]</ref>. L2softmax <ref type="bibr" target="#b30">[31]</ref> is proposed to add an L2-constraint to the deep features and restrict them to lie on a hypersphere of a fixed radius. NSoftmax <ref type="bibr" target="#b3">[4]</ref> is proposed to normalize both features and weights of the last inner-product layer. Ring loss <ref type="bibr" target="#b31">[32]</ref> applies soft normalization by gradually learning to constrain the norm to the scaled unit circle while preserving convexity. Then, based on previous works <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b3">[4]</ref>, the large margin <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> is introduced to obtain better discriminative power by further enforcing the extra intra-class compactness and inter-class discrepancy simultaneously. L-Softmax <ref type="bibr" target="#b32">[33]</ref> first incorporates a large margin to softmax loss to learn discriminative face features by strictly separating the hard samples. Instead of the multiplicative margin, CosFace <ref type="bibr" target="#b5">[6]</ref> and ArcFace <ref type="bibr" target="#b6">[7]</ref> introduce the additive margin to guarantee the convergence, which is easy for implementation. However, AdaCos <ref type="bibr" target="#b7">[8]</ref> and P2SGrad <ref type="bibr" target="#b8">[9]</ref> point that the inflexible form of softmax based loss functions lacks the ability to precisely supervise the cosine distances, and they improve the large margin angular loss functions by setting the direct mapping relation between classification probability and cosine distances, which can further decrease the intra-class angles of training databases. MV-Softmax <ref type="bibr" target="#b33">[34]</ref> is proposed to improve softmax based loss functions by mining the mis-classified samples and emphasizing them to guide the discriminative feature learning. CurricularFace <ref type="bibr" target="#b34">[35]</ref> further develops MV-Softmax by incorporating curriculum learning, which automatically emphasizes easy samples first and hard samples later. Recent works <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b35">[36]</ref> also point that inter-class and intra-class objectives of softmax based loss functions would interact and lead to relaxation on each other. Although recent works have pointed out some shortcomings of softmax based loss functions, overall, weight/feature normalization softmaxbased loss functions and large margin softmax based loss functions have significantly boosted the performance of deep face recognition. Our method can be categorized as the first type method in the form of metric learning, which directly optimizes the intraclass and inter-class distances. However, it also has a close connection to the second type based on softmax loss, which we will discuss in details in Section III-C. In addition, there are also some works <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> aiming to solve the noise-robust training in deep face recognition, which usually use training databases with high-level label noise to obtain comparable performance with the model trained with clean databases. While our work is devoted to improving performance of models trained on clean databases which have been refined by semi-automatic data cleaning algorithms <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODOLOGY A. Sigmoid-constrained Hypersphere Loss</head><p>In this section, we introduce the proposed loss function. First, we give some denotations and descriptions. The deep face recognition models embeds an image into a -dimensional Euclidean space.</p><p>? R denotes the embedding feature of the -th training image, and is the label of . = { 1 , 2 , . . . , } ? R ? denotes the weight of the last fully connected layer, where denotes the number of identities in the training database.</p><p>? R is seen as the target center feature of identity .</p><p>Recent works <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> have empirically demonstrated the superiority of constraining deep face features to be discriminative on a hypersphere manifold, where gradients are restricted in the tangent of the hypersphere. We also map deep face features to the hypersphere manifold and optimize cosine similarity to restrict directions of gradients. To help understanding, we illustrate it in <ref type="figure">Figure 1</ref>. With the restricted directions of gradients, the moving directions of samples and target centers are always along the tangent of the hypersphere.</p><p>The aim is to decrease the intra-class distance and increase the inter-class distance in a moderate way. Therefore, the sigmoid-constrained hypersphere loss (SFace) of can be formulated as = + , where is the angular distance between / and , and ( ? ) is the angular distance between / and . Specifically, and are formulated as follows:</p><formula xml:id="formula_0">= ?[ ] cos , = ?? =1, ? [ ] cos .<label>(1)</label></formula><p>In the above equations, cos = , and cos = , ? . Since the goal is to obtain precisely control of the optimization degree, we design functions and to re-scale intra-class and inter-class objectives respectively to further restrict the optimizing speed.</p><p>[?] is the block gradient operator, which prevents the contribution of its inputs to be taken into account for computing gradients. In the forward propagation process of SFace,</p><formula xml:id="formula_1">= ?[ ] cos + ?? =1, ? [ ] cos . (2)</formula><p>While in the backward propagation process,</p><formula xml:id="formula_2">= ?[ ] cos + ?? =1, ? [ ] cos , = ?[ ] cos , = [ ] cos ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">cos = 1 ( ? cos ), cos = 1 ( ? cos ), cos = 1 ( ? cos ), cos = 1 ( ? cos ).</formula><p>(4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Gradient Re-scale Function</head><p>The optimization gradients are always along the tangent direction, because</p><formula xml:id="formula_4">cos( ) , = 0, cos( ) , = 0, cos( ) ,</formula><p>= 0, and cos( ) , = 0 (refer to the illustration in <ref type="figure" target="#fig_1">Figure 3</ref>). In addition, , and almost remain unchanged in the training process, for the reason that there are no components of gradients in the radial direction. Function and are designed to re-scale intra-class and inter-class objectives respectively. These two terms actually re-scale the gradient, i.e. control the moving speed of samples and target centers in <ref type="figure">Figure 1</ref>. Therefore we name and as the gradient re-scale functions. Since the original gradient scales of intraclass and inter-class objectives are proportional to and (refer to Function (4) and <ref type="figure" target="#fig_1">Figure 3</ref>), the final gradient scales are proportional to = and = .</p><p>(a) (b) At the beginning of training, the initial angular distances and are all about 2 . The intra-class loss function decreases gradually while the inter-class loss function prevents from being decreased. Therefore, the ideal functions of and should satisfy at least three properties as follows: <ref type="formula" target="#formula_0">(1)</ref> The function should be non-negative and monotonically increasing on the interval [0, 2 ], so that the moving speed of and decreases gradually as they approaching to each other. <ref type="bibr">(</ref>2) The function should be non-negative on the interval [0, 2 ], so that the moving speed of and increases rapidly as they start approaching to each other. (3) Considering the imperfection of training databases, there should be two flexible intervals to suppress the moving speed, one is around ? 0 of and the other is around ? 2 of , so that both intra-class and inter-class objectives can be optimized with a moderate target rather than be minimized or maximized strictly.</p><formula xml:id="formula_5">(c) (d)</formula><p>Eventually, we choose sigmoid functions as the gradient rescale functions. The specific forms are,</p><formula xml:id="formula_6">= 1 + ? * ( ? ) , = 1 + * ( ? ) .<label>(5)</label></formula><p>is the upper asymptote of two sigmoid curves as the initial scale of gradient, and is the control the slope of sigmoid curves. Hyperparameters and decide the horizontal intercept of two sigmoid curves and actually control the flexible interval to suppress the moving speed. Therefore and are vital parameters should be selected according to characteristics of a specific training database, which we will discuss later. The sigmoid curve functions of and are illustrated in (a) of   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relation to Softmax Based Loss</head><p>We have mentioned in Section II that SFace in form can be categorized as the metric learning method, but it has a close connection to the softmax based loss functions. In this section, we discuss this relation in details.</p><p>We start from the original softmax loss function. For each embedding feature , the softmax loss can be formulated as:</p><formula xml:id="formula_7">= ? log = ? log + =1 + .<label>(6)</label></formula><p>? R denotes the embedding feature of the -th training image, and is the label of .</p><p>is the predicted probability of assigning to class . is the number of identities, ? R is the -th column of the weight of the last fully connected layer, ? R is the bias. Softmax based loss functions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> remove the bias term and transform = cos . To further improve the performance, large margin is adopted in the cos term <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Therefore, softmax based loss functions can be formulated as:</p><formula xml:id="formula_8">= ? log = ? log ( ) ( ) + =1, ? cos ,<label>(7)</label></formula><p>where ( ) = cos in NSoftmax <ref type="bibr" target="#b3">[4]</ref>, ( ) = cos ? in CosFace <ref type="bibr" target="#b5">[6]</ref>, and ( ) = cos( + ) in ArcFace <ref type="bibr" target="#b6">[7]</ref>. With the influence of the loss function, is decreased and is increased in theory. In the backward propagation process,</p><formula xml:id="formula_9">cos = ( ? 1) ( ) cos = ? =1, ? cos ( ) + =1, ? cos ( ) cos , cos = = cos ( ) + =1, ? cos ,<label>(8)</label></formula><p>where ( ) cos = 1 in NSoftmax <ref type="bibr" target="#b3">[4]</ref> and CosFace <ref type="bibr" target="#b5">[6]</ref>, and</p><formula xml:id="formula_10">( ) cos = sin( + ) sin</formula><p>in ArcFace <ref type="bibr" target="#b6">[7]</ref>.</p><p>(a) NSoftmax <ref type="bibr" target="#b3">[4]</ref> (b) CosFace <ref type="bibr" target="#b5">[6]</ref> (c) ArcFace <ref type="bibr" target="#b6">[7]</ref>  , and inter-class gradient , of (a) NSoftmax <ref type="bibr" target="#b3">[4]</ref>, (b) CosFace <ref type="bibr" target="#b5">[6]</ref>, and (c) ArcFace <ref type="bibr" target="#b6">[7]</ref>. Softmax based loss functions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> can be understand as a kind of special metric learning method with specific speed constraints decided by the intra-class distance and the inter-class distances ( ? ).</p><p>Further, the softmax based functions are equivalent to the following loss functions for training face models</p><formula xml:id="formula_11">= ?[ , ] cos + ?? =1, ? [ , ] cos ,<label>(9)</label></formula><p>where gradient re-scale functions are,</p><formula xml:id="formula_12">, = =1, ? cos ( ) + =1, ? cos ( ) cos ,<label>(10)</label></formula><p>and</p><formula xml:id="formula_13">, = cos ( ) + =1, ? cos .<label>(11)</label></formula><p>Since only backward propagation have influence on the network parameters of deep face recognition models, and the backward propagation function <ref type="formula" target="#formula_9">(8)</ref> of softmax based loss functions and function <ref type="formula" target="#formula_11">(9)</ref> are the same. Therefore loss function <ref type="bibr" target="#b6">(7)</ref> are equivalent to loss function <ref type="bibr" target="#b8">(9)</ref> in the training process. Now from equations (8)(9)(10)(11), we can see that softmax based loss functions can be understood as a kind of special metric learning method with the speed constraints on a hypersphere. However, both the gradient re-scale functions (speed constraints) of intra-class and inter-class are decided by the intra-class distance and the inter-class distances ( ? ). To better understanding of the optimization of softmax based loss functions, we hypothesize that all the inter-class distances ( ? ) are the same ideally, and plot scale curves of the intra-class gradient , = , and the inter-class gradient , = , of (a) NSoftmax <ref type="bibr" target="#b3">[4]</ref>, (b) CosFace <ref type="bibr" target="#b5">[6]</ref>, and (c) ArcFace <ref type="bibr" target="#b6">[7]</ref> in <ref type="figure" target="#fig_4">Figure 5</ref>. At the beginning of training, the intra-class distance and inter-class distances is about 90 degrees ( 2 ). We can see that, from the intra-class sub-figure (left) of <ref type="figure" target="#fig_4">Figure 5</ref>, with the high intra-class gradient , the intra-class distance will decrease gradually. While at the same time, as the intraclass distance decreases, from the inter-class sub-figure (right) of <ref type="figure" target="#fig_4">Figure 5</ref>, the inter-class gradient will decrease, which will relax the inter-class constraints and decrease the inter-class distance . Then, we come back to the intra-class sub-figure (left) of <ref type="figure" target="#fig_4">Figure 5</ref>, as the inter-class distances decrease, the change curve of intra-class gradient will also changed. In the optimization of softmax based loss, the intra-class and inter-class distance will always have influence on each other. Therefore, in conclusion, softmax based loss functions actually lack the ability to control intra-class and inter-class optimizations precisely. However, compared with softmax based loss functions, both intra-class and inter-class distance of SFace <ref type="figure" target="#fig_2">(Figure 4)</ref> can be constrained to a designed degree therefore can be optimized in a moderate way, which is exactly the advantage of SFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental settings</head><p>We separately train models on training databases including CASIA-WebFace <ref type="bibr" target="#b10">[11]</ref>, VGGFace2 <ref type="bibr" target="#b13">[14]</ref>, MS1MV2 <ref type="bibr" target="#b11">[12]</ref> databases, which have been elaborated by semi-automatic data cleaning algorithms, to evaluate our methods and conduct fair comparison with state-of-the-art loss functions. The compared loss functions include softmax, NSoftmax <ref type="bibr" target="#b3">[4]</ref>, SphereFace <ref type="bibr" target="#b4">[5]</ref>, CosFace <ref type="bibr" target="#b5">[6]</ref>, ArcFace <ref type="bibr" target="#b6">[7]</ref>, Combined loss <ref type="bibr" target="#b6">[7]</ref>, D-softmax <ref type="bibr" target="#b35">[36]</ref> and so on.</p><p>Evaluation Databases. We evaluate on LFW <ref type="bibr" target="#b15">[16]</ref>, YTF <ref type="bibr" target="#b16">[17]</ref>, CFP-FP <ref type="bibr" target="#b38">[39]</ref>, AgeDB-30 <ref type="bibr" target="#b40">[40]</ref>, CALFW <ref type="bibr" target="#b17">[18]</ref>, CPLFW <ref type="bibr" target="#b18">[19]</ref>, MegaFace <ref type="bibr" target="#b19">[20]</ref>, IJB-A <ref type="bibr" target="#b20">[21]</ref> and IJB-C <ref type="bibr" target="#b21">[22]</ref> databases.</p><p>LFW <ref type="bibr" target="#b15">[16]</ref> database contains 13,233 face images from 5,749 different identities. YTF <ref type="bibr" target="#b16">[17]</ref> is a database of face video collected from YouTube, which consists of 3,425 videos of 1,595 different people. CFP-FP database <ref type="bibr" target="#b38">[39]</ref> is built for facilitating large pose variation in unconstrained settings. AgeDB-30 database <ref type="bibr" target="#b40">[40]</ref> is a manually collected cross-age database in unconstrained settings. Cross-Age LFW (CALFW) <ref type="bibr" target="#b17">[18]</ref> and Cross-Pose LFW (CPLFW) <ref type="bibr" target="#b18">[19]</ref> databases are constructed based on LFW database, to emphasize cross-age challenge and cross-pose challenge in face recognition.</p><p>MegaFace <ref type="bibr" target="#b19">[20]</ref> is a large public available testing benchmark, which evaluates the performance of face models at the million scale distractors. We use FaceScrub database <ref type="bibr" target="#b41">[41]</ref> as the probe set, which contains 106,863 images from 530 celebrities. The gallery set is a subset of Flickr photos and it consists of more than one million images. Recently, research <ref type="bibr" target="#b6">[7]</ref> points out that there are many wrong labels in the MegaFace database and the noise significantly affects the performance. Therefore, for comparison, in this paper we report experimental results on both the original MegaFace database and the refined version <ref type="bibr" target="#b6">[7]</ref>. IJB-A <ref type="bibr" target="#b20">[21]</ref> and IJB-C <ref type="bibr" target="#b21">[22]</ref> databases address the unconstrained face recognition, which contain both still images and video frames. IJB-A database contains 500 subjects with 5,396 still images and 20,395 video frames. IJB-C database further increases emphasis on occlusion and diversity of subject occupation and geographic origin population, containing 3,531 subjects with 31.3K still images and 117.5K frames from 11,779 videos. We evaluate the models on the standard verification setting (matching between the Mixed Media probes and two galleries) and identification protocol (1:N Mixed Media probes across two galleries).</p><p>Training and Testing. We use MxNet <ref type="bibr" target="#b42">[42]</ref> to implement all the experiments. For the fair comparison, the CNN architecture used in our work is the same ResNet <ref type="bibr" target="#b43">[43]</ref> networks as <ref type="bibr" target="#b6">[7]</ref>, which applies the "BN [44]-Dropout [45]-FC-BN" structure to get the final 512-embedding feature. The data preprocessing follows settings of insightface <ref type="bibr" target="#b6">[7]</ref>. That is, horizontally flip with a probability of 50% is used for training data augmentation. In addition, all the images are normalized by subtracting 127.5 and dividing by 128. All the models are trained with stochastic gradient descent (SGD) algorithm from scratch. Models trained on CASIA-WebFace database are trained on 2 GPUs and the total batch size is 256. The learning rate is started from 0.1 and divided by 10 at the 100k, 140k, 160k iterations. Models trained on MS1MV2 database are trained on 4 GPUs and the total batch size is 512. The learning rate is started from 0.1 and divided by 10 at the 100k, 160k, 220k iterations. Models trained on VGGFace2 database are trained on 4 GPUs and the total batch size is 512. The learning rate is started from 0.1 and divided by 10 at the 80k, 100k, 160k iterations. The parameter for SFace is set to 64, is set to 80. The intra-class and inter-class parameters and control the optimization and should be decided according to specific training databases, which will be introduced later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment on the CASIA-WebFace Database</head><p>CASIA-WebFace database <ref type="bibr" target="#b10">[11]</ref> contains 0.49M images from 10,575 celebrities, which is the first widely used large training database in deep face recognition. While recently it has been seen as a relatively small-scale database compared with other Million-scale ones <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>. According to the research <ref type="bibr" target="#b14">[15]</ref>, there are 9.3%-13.0% label noise in CASIA-WebFace database. That is, the original CASIA-WebFace database is exactly the database using semi-automatic annotation with low level noise. We use the arcface version <ref type="bibr" target="#b6">[7]</ref> with 0.49M images from 10,572 identities. We first implement our method on it and compare with the state-of-art loss functions. Then, we experiment on the noise-controlled Web-Face database to further evaluate our method under training databases with different noise levels, and study the choice of hyper-parameters.</p><p>1) Experiment on the CASIA-WebFace Database: We train face models on CASIA-WebFace database supervised by softmax, NSoftmax <ref type="bibr" target="#b3">[4]</ref>, SphereFace <ref type="bibr" target="#b4">[5]</ref>, CosFace <ref type="bibr" target="#b5">[6]</ref>, ArcFace <ref type="bibr" target="#b6">[7]</ref>, Combined loss <ref type="bibr" target="#b6">[7]</ref> with combined margin cos ( 1 + 2 )? 3 , D-softmax <ref type="bibr" target="#b35">[36]</ref>, and SFace respectively. The source codes of most compared methods can be downloaded from the github. In addition, we implement D-softmax <ref type="bibr" target="#b35">[36]</ref> by ourselves. Since the performance of all the above loss functions is sensitive to the choice of hyper-parameters, we list them in the <ref type="table" target="#tab_2">Table I</ref>, which are determined according to the suggestion. All the models are trained on the ResNet50 which we have mentioned above. For SFace, we choose intra-class and inter-class hyperparameters and by taking reference to the experience of the final models of large margin loss functions, and then tuning them. In the experiment, both intra-class and interclass parameters have crucial influence. <ref type="table" target="#tab_2">Table I</ref> lists the experimental results, our method is compared with the recent advanced loss functions. As shown, under the same training and test settings, our method significantly improves the results on several evaluation benchmarks, especially TAR at very low FAR on the well-known challenging IJB-C database, which demonstrates the superiority of our method on a semiautomatic annotated face training database with low level noise.</p><p>From <ref type="table" target="#tab_2">Table I</ref>, we select three models trained supervised by SFace and two classic methods, NSoftmax and ArcFace, respectively, and analyze these models. We extract the deep features of images in the training database, and calculate intraclass and inter-class angles (distances) statistics. Specifically, using the manual refined image list <ref type="bibr" target="#b46">[46]</ref> released by <ref type="bibr" target="#b3">[4]</ref>, we can split the training database (0.49M images) into clean images (0.45M) and label noises (0.04M). Therefore, the mean angles (distances) between embedding feature and the embedding feature of clean images and label noise can be calculated respectively. In addition, we calculate the mean angles between different . The results are listed in <ref type="table" target="#tab_2">Table II</ref>. We can see that, compared with NSoftmax and SFace, ArcFace optimizes training samples in a more strict way. That is, the intra-class angles (distances) of ArcFace are smaller. The decrease of intra-class angles (distances) of clean images is a good trend. However, the intra-class angles (distances) of label noise are also decreased, which is not a good phenomenon. While SFace keep a better balance between decreasing the intra-class angles (distances) and preventing overfitting to label noise. The reason may be that with the precisely control to a cutoff point, the clean training samples are optimized earlier and more easily, while the label noise can be left behind to prevent them close to the wrong labeled targets. At the same time, the inter-class class optimization guarantees that different identities still remain to be orthogonal to each other.</p><p>To evaluate the proposed gradient re-scale function of SFace, we compare face models trained on loss function <ref type="formula" target="#formula_0">(1)</ref> with three different gradient re-scale functions: constant value (no gradient re-scale), the piecewise functions, and the sigmoid functions (SFace). Specifically, the piecewise function can be seen as the "steep version" of the sigmoid functions, formulated as follows,</p><formula xml:id="formula_14">= * max ? , 0 , = * max ? , 0 ,<label>(12)</label></formula><p>where ( * ) is the sign function to extract the sign of a real number. For the piecewise and sigmoid functions, the hyper-parameters , are set as the same, 0.9 and 1.3. The experimental results are listed in <ref type="table" target="#tab_2">Table III</ref>. We can see that, the proposed sigmoid gradient re-scale function has better performance than the constant value and the piecewise version. <ref type="figure">Fig. 6</ref>. Images of an identity in WebFace-Clean, WebFace-ArcFace and WebFace-Noisy databases. The WebFace-Clean database is a manually cleaned version <ref type="bibr" target="#b46">[46]</ref>, <ref type="bibr" target="#b6">[7]</ref>. The noise in WebFace-ArcFace <ref type="bibr" target="#b6">[7]</ref> database is from the label noise that derive from the collection process of the CASIA-WebFace database <ref type="bibr" target="#b10">[11]</ref>. Based on WebFace-ArcFace database, we add images from MS-Celeb-1M database <ref type="bibr" target="#b11">[12]</ref> evenly across each identity of WebFace-ArcFace database, which means that we incorporate outliers in WebFace-Noisy database.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Experiment on the Noise-Controlled WebFace Database:</head><p>To further evaluate our method on the training databases with low level noise, we train deep face models under noisecontrolled settings. Specifically, we use three databases with different noise level. (1) Since we have the manual refined image list released by <ref type="bibr" target="#b46">[46]</ref>, we first clean the ArcFace version <ref type="bibr" target="#b6">[7]</ref>  of CASIA-WebFace database. Finally, we obtain a manually cleaned version of CASIA-WebFace database (0.45M images from 10,572 identities). This database is named as WebFace-Clean.</p><p>(2) Then, ArcFace version <ref type="bibr" target="#b6">[7]</ref> of CASIA-WebFace database (0.49M images from 10,572 identities) is used as first noise level database. We name this database as WebFace-ArcFace. (3) Finally, we augment the ArcFace version <ref type="bibr" target="#b6">[7]</ref> of CASIA-WebFace database with synthesis images. We add images from MS-Celeb-1M database <ref type="bibr" target="#b11">[12]</ref> evenly across each identity of WebFace-ArcFace database. That is to say, we incorporate outliers in this training database. The database is referred to as WebFace-Noisy. We use this setting because in practice, outliers noise is a more common type of label noise than label flip noise. The noise level of WebFace-Clean, WebFace-ArcFace and WebFace-Noisy is approximately 0%, 10% and 20%, respectively. Some identities of WebFace- <ref type="figure">Fig. 8</ref>. Comparison of ArcFace and SFace models on the IJB-A database <ref type="bibr" target="#b20">[21]</ref>. Left: ROC (higher is better). Middle: DET (lower is better). Right: CMC (higher is better). Our method is represented using red color. <ref type="figure">Fig. 9</ref>. Comparison of ArcFace and SFace models on the IJB-C database <ref type="bibr" target="#b21">[22]</ref>. Left: ROC (higher is better). Middle: DET (lower is better). Right: CMC (higher is better). Our method is represented using red color. ArcFace and WebFace-Noisy databases are shown in <ref type="figure">Figure 6</ref>. Note that the 10% noise in WebFace-ArcFace database is from the label noise that derive from the collection process of the CASIA-WebFace database. While the 20% label noise in WebFace-Noisy contains 10% noise in WebFace-ArcFace and other 10% synthetic outliers. We train ResNet34 models on WebFace-Clean, WebFace-ArcFace and WebFace-Noisy databases supervised by softmax, NSoftmax, CosFace, ArcFace and SFace. The experimental results are shown in <ref type="figure">Figure 7</ref>, which demonstrates the ro- bustness of SFace to low level label noise. We also list the choice of hyper-parameters of SFace in <ref type="table" target="#tab_2">Table IV</ref>. We can conclude that parameter should be larger, i.e. curves should move to the right, as the noise level increases, which indicates that the speed of intra-class is decreased more early to prevent overfitting. Although inter-class parameters are also important for training, we find the optimal groups of them are the same for the three training databases, the reason may be that noisy data is relatively balanced across all identities. Another interesting phenomenon is that the model have similar performance on WebFace-Clean and WebFace-ArcFace. This result indicates that the manual cleaned data by human annotations actually has limited influence on these face models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Results on Several Benchmarks</head><p>We first evaluate our method on LFW <ref type="bibr" target="#b15">[16]</ref> and YTF <ref type="bibr" target="#b16">[17]</ref>. For fair comparison, we train models using ResNet100 on MS1MV2 database <ref type="bibr" target="#b11">[12]</ref>, strictly following the settings in <ref type="bibr" target="#b6">[7]</ref>. MS1MV2 database is a refined version of MS-Celeb-1M database <ref type="bibr" target="#b11">[12]</ref>, cleaned by insightface <ref type="bibr" target="#b6">[7]</ref>. MS1MV2 database contains 5.8M images of 85,742 celebrities. We use this semi-artificial cleaned face database as a large-scale training database to further evaluate our method. For SFace trained on MS1MV2 database, the hyper-parameters , are set as 0.90 and 1.20. The experimental results on LFW and YTF are shown in <ref type="table" target="#tab_4">Table V</ref>. SFace model trained on MS1MV2 database with ResNet100 obtains comparable results as the baseline method such as CosFace <ref type="bibr" target="#b5">[6]</ref> and ArcFace <ref type="bibr" target="#b6">[7]</ref>. We report the performance on CALFW <ref type="bibr" target="#b17">[18]</ref> and CPLFW <ref type="bibr" target="#b18">[19]</ref> databases in <ref type="table" target="#tab_2">Table VI</ref>. As shown in <ref type="table" target="#tab_2">Table VI</ref>, SFace outperforms both human performance and the advanced deep face models on CALFW and CPLFW databases by a significant margin.</p><p>Then, we evaluate our method on MegaFace database <ref type="bibr" target="#b19">[20]</ref> including both the original MegaFace database and the refined version <ref type="bibr" target="#b6">[7]</ref>. We report the rank-1 face identification accuracy with 1M distractors, and the face verification TAR@FAR=1e-6, shown in Table VII. In the second and third cell, methods are compared in the same setting with ResNet100 models trained on MS1MV2 database. As reported in <ref type="table" target="#tab_2">Table VII</ref>, our method shows superiority over CosFace and ArcFace on both identification and verification settings on MegaFace challenge.</p><p>Finally, we evaluate our method on IJB-A <ref type="bibr" target="#b20">[21]</ref> and IJB-C <ref type="bibr" target="#b21">[22]</ref> databases on both identification and verification settings. Our method is compared with ArcFace using the same databases and models, other results are cited from the original papers. For fair comparison, we also train ResNet50 models on VGGFace2 database <ref type="bibr" target="#b13">[14]</ref> following <ref type="bibr" target="#b6">[7]</ref>. VGGFace2 training database has 3.13 million images of 8,631 identities, and has large variations in pose, age, illumination, ethnicity and profession. For SFace model trained on VGGFace2 database, the hyper-parameters , are set as 0.88 and 1.25. The results on IJB-A database are exhibited in <ref type="table" target="#tab_2">Table VIII</ref> and <ref type="figure">Figure 8</ref>. The results on IJB-C database are shown in <ref type="table" target="#tab_2">Table IX</ref> and <ref type="figure">Figure 9</ref>. For verification, we report TAR@FAR (ROC curves, higher is better). For identification, the performance is reported using TPIR@FPIR (DET curve, lower is better) and Rank-N accuracy (CMC curve, higher is better). Compared with ArcFace models trained on both VGGFace2 and MS1MV2 databases, our method performs better in both identification and verification settings, especially the TAR at very low FAR, which demonstrates the effectiveness and superiority of SFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, different from previous works which minimize the intra-class distances and maximize the inter-class distance, we introduce a new idea which aims to optimize intra-class and inter-class distance to some extent for the purpose of mitigating overfitting problems to the imperfect training databases. To carry out this idea, we propose a new loss function SFace to improve the performance of models in the robust unconstrained face recognition. SFace imposes intra-class and interclass constraints on a hypersphere manifold with precisely controlled intra-class and inter-class gradients so that intraclass and inter-class distances are optimized to some extent. To promote further understanding of SFace, we explain the relationship to softmax based loss functions, and show that, compared with softmax based loss, the advantage of SFace is the precisely control ability of both intra-class and interclass optimization. The proposed SFace makes a better balance between underfitting and overfitting, and further improves the generalization ability of deep face models. Experiments on several benchmarks including LFW, YTF, CALFW, CPLFW, MegaFace, IJB-A and IJB-C databases, have demonstrated the effectiveness and superiority of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head><p>This work was supported by Canon Information Technology (Beijing) Co., Ltd. under Grant No. OLA19023, and supported by BUPT Excellent Ph.D. Students Foundation CX2020201.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>(a) The model would overfit to the label noise by strictly dragging the noisy samples to the wrong labeled identities. (b) In contrast, SFace can mitigate this problem in some degree because it optimizes samples in a moderate way.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of Function<ref type="bibr" target="#b3">(4)</ref>, which means that optimization gradients are along the tangent direction. (a)(b)(c)(d) interprets the four orthogonality relationships of Function (4) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>With the gradient re-scale functions, scales of intra-class gradient and inter-class gradient in theory are proportional to = and = , shown in (b) ofFigure 4. The entire training process of SFace is summarized in Algorithm 1, which is easy for implementation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>8 Fig. 4 .</head><label>84</label><figDesc>Update parameters and by = ? ( ) , = ? ( ) ; 9 end Output: , . (a) The sigmoid curves of intra-class gradient re-scale function and inter-class gradient re-scale function of SFace. (b) The final scale curves of intra-class gradient and inter-class gradient of SFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Under some ideal assumptions, the scale of intra-class gradient</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Zhong, Weihong Deng, and Jiani Hu are with the Pattern Recognition and Intelligent System Laboratory, School of Artificial Intelligence, Beijing University of Posts and Telecommunications, Beijing 100876, China (email: zhongyaoyao@bupt.edu.cn; whdeng@bupt.edu.cn; jnhu@bupt.edu.cn). Weihong Deng is the corresponding author.</figDesc><table /><note>Dongyue Zhao, Xian Li, and Dongchao Wen are with Canon Informa- tion Technology (Beijing) Co., Ltd. (e-mail: zhaodongyue@canon-ib.com.cn; lixian@canon-ib.com.cn; wendongchao@canon-ib.com.cn).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF DIFFERENT LOSS FUNCTIONS WITH SFACE. MODELS ARE TRAINED ON CASIA-WEBFACE [11] USING RESNET50. THE COMBINED LOSS [7] ADOPTS THE COMBINED MARGIN cos ( 1 + 2 ) ? 3 . THE EVALUATION BENCHMARK CONTAINS IJB-C [22] (TAR@FAR=1E-5,1E-4,1E-3), YTF [17] (%) DATABASES, AND AVERAGE PERFORMANCE (%) ON LFW<ref type="bibr" target="#b15">[16]</ref>, CFP-FP<ref type="bibr" target="#b38">[39]</ref>, AGEDB-30<ref type="bibr" target="#b40">[40]</ref>, CALFW<ref type="bibr" target="#b17">[18]</ref> AND CPLFW<ref type="bibr" target="#b18">[19]</ref> </figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DATABASES.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell>FAR=1e-5</cell><cell>IJB-C FAR=1e-4</cell><cell>FAR=1e-3</cell><cell>YTF</cell><cell>Avg.</cell><cell>LFW</cell><cell>CFP-FP</cell><cell>AgeDB-30</cell><cell>CPFLW</cell><cell>CALFW</cell></row><row><cell></cell><cell cols="2">softmax</cell><cell></cell><cell></cell><cell>64.57</cell><cell>77.57</cell><cell>88.03</cell><cell>95.60</cell><cell>93.82</cell><cell>99.25</cell><cell>95.10</cell><cell>93.28</cell><cell>88.97</cell><cell>92.48</cell></row><row><cell cols="4">NSoftmax [4] (s=20.0)</cell><cell></cell><cell>67.82</cell><cell>79.88</cell><cell>89.33</cell><cell>95.54</cell><cell>93.72</cell><cell>99.23</cell><cell>95.00</cell><cell>93.17</cell><cell>88.82</cell><cell>92.40</cell></row><row><cell cols="5">SphereFace [5] (m=1.35)</cell><cell>46.73</cell><cell>61.54</cell><cell>76.10</cell><cell>93.18</cell><cell>92.99</cell><cell>99.17</cell><cell>94.76</cell><cell>92.60</cell><cell>86.50</cell><cell>91.93</cell></row><row><cell cols="4">CosFace [6] (m=0.35)</cell><cell></cell><cell>75.58</cell><cell>85.03</cell><cell>92.00</cell><cell>95.76</cell><cell>94.91</cell><cell>99.53</cell><cell>95.50</cell><cell>95.23</cell><cell>90.32</cell><cell>93.97</cell></row><row><cell cols="4">ArcFace [7] (m=0.3)</cell><cell></cell><cell>73.55</cell><cell>84.60</cell><cell>91.90</cell><cell>95.80</cell><cell>94.65</cell><cell>99.57</cell><cell>95.26</cell><cell>94.40</cell><cell>90.10</cell><cell>93.93</cell></row><row><cell cols="4">ArcFace [7] (m=0.4)</cell><cell></cell><cell>72.49</cell><cell>83.76</cell><cell>91.21</cell><cell>96.06</cell><cell>94.91</cell><cell>99.52</cell><cell>95.76</cell><cell>95.00</cell><cell>90.43</cell><cell>93.87</cell></row><row><cell cols="4">ArcFace [7] (m=0.5)</cell><cell></cell><cell>70.15</cell><cell>81.48</cell><cell>90.26</cell><cell>95.66</cell><cell>94.83</cell><cell>99.52</cell><cell>95.60</cell><cell>95.30</cell><cell>89.97</cell><cell>93.77</cell></row><row><cell cols="5">Combined [7] (m = 0.9,0.4,0.15)</cell><cell>73.99</cell><cell>83.91</cell><cell>91.63</cell><cell>95.86</cell><cell>94.90</cell><cell>99.48</cell><cell>95.56</cell><cell>94.97</cell><cell>90.68</cell><cell>93.82</cell></row><row><cell cols="4">D-softmax [36] (d=0.9)</cell><cell></cell><cell>71.48</cell><cell>83.56</cell><cell>91.23</cell><cell>95.42</cell><cell>94.29</cell><cell>99.50</cell><cell>95.44</cell><cell>93.95</cell><cell>89.60</cell><cell>92.95</cell></row><row><cell cols="4">SFace (a=0.87, b=1.20)</cell><cell></cell><cell>77.13</cell><cell>86.38</cell><cell>92.52</cell><cell>95.82</cell><cell>94.93</cell><cell>99.50</cell><cell>95.81</cell><cell>95.10</cell><cell>90.18</cell><cell>94.07</cell></row><row><cell cols="4">SFace (a=0.90, b=1.20)</cell><cell></cell><cell>76.77</cell><cell>85.95</cell><cell>92.37</cell><cell>95.86</cell><cell>94.88</cell><cell>99.57</cell><cell>95.67</cell><cell>95.00</cell><cell>90.22</cell><cell>93.95</cell></row><row><cell cols="4">SFace (a=0.93, b=1.20)</cell><cell></cell><cell>77.77</cell><cell>86.38</cell><cell>92.52</cell><cell>96.08</cell><cell>94.88</cell><cell>99.48</cell><cell>95.81</cell><cell>94.87</cell><cell>90.28</cell><cell>93.97</cell></row><row><cell cols="4">SFace (a=0.90, b=1.30)</cell><cell></cell><cell>76.92</cell><cell>87.27</cell><cell>93.11</cell><cell>96.00</cell><cell>94.80</cell><cell>99.57</cell><cell>95.26</cell><cell>94.82</cell><cell>90.68</cell><cell>93.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">THE ANGLES (DISTANCES) STATISTICS UNDER DIFFERENT LOSS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">FUNCTIONS (NSOFTMAX [4], ARCFACE [7] AND SFACE MODELS</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">TRAINED ON WEBFACE DATABASE (0.49M IMAGES)). EACH COLUMN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">DENOTES ONE LOSS FUNCTION. "CLEAN-INTRA" AND "NOISE-INTRA"</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">REFERS TO CALCULATE THE MEAN ANGLES (DISTANCES) BETWEEN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">EMBEDDING FEATURE</cell><cell cols="4">AND THE EMBEDDING FEATURE</cell><cell>OF CLEAN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">IMAGES AND LABEL NOISE, RESPECTIVELY. WE USE THE MANUAL</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">REFINED IMAGE LIST RELEASED BY [46] TO SPLIT THE TRAINING</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">DATABASE (0.49M IMAGES) INTO CLEAN IMAGES (0.45M) AND LABEL</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">NOISES (0.04M). "DELTA-INTRA" IS THE DIFFERENCE BETWEEN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">"NOISE-INTRA" AND "CLEAN-INTRA". "INTER" REFERS TO THE MEAN</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">ANGLES BETWEEN DIFFERENT</cell><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">NSoftmax [4]</cell><cell>ArcFace [7]</cell><cell>SFace</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Clean-Intra</cell><cell></cell><cell cols="2">44.42</cell><cell>35.31</cell><cell>39.68</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Noise-Intra</cell><cell></cell><cell cols="2">50.85</cell><cell>40.09</cell><cell>47.30</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Delta-Intra</cell><cell></cell><cell cols="2">6.43</cell><cell>4.78</cell><cell>7.62</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Inter</cell><cell></cell><cell cols="2">89.75?5.55</cell><cell>89.99?4.73</cell><cell cols="2">89.96?4.67</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">COMPARISON OF THREE DIFFERENT GRADIENT RE-SCALE FUNCTIONS:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">CONSTANT VALUE (NO GRADIENT RE-SCALE), THE PIECEWISE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">FUNCTIONS, AND THE SIGMOID FUNCTIONS (SFACE). MODELS ARE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">TRAINED ON CASIA-WEBFACE [11] USING RESNET50. THE AVERAGE</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">PERFORMANCE (%) ON LFW [16], CFP-FP [39], AGEDB-30 [40],</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">CALFW [18] AND CPLFW [19] DATABASES IS USED FOR EVALUATION.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Avg.</cell><cell cols="2">LFW</cell><cell>CFP-FP</cell><cell>AgeDB-30</cell><cell>CPLFW</cell><cell>CALFW</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Constant</cell><cell>90.05</cell><cell cols="2">98.30</cell><cell>90.46</cell><cell>89.55</cell><cell>83.15</cell><cell>88.78</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Piecewise</cell><cell>94.64</cell><cell cols="2">99.45</cell><cell>94.90</cell><cell>94.73</cell><cell>90.08</cell><cell>94.03</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Sigmoid</cell><cell>94.80</cell><cell cols="2">99.57</cell><cell>95.26</cell><cell>94.82</cell><cell>90.68</cell><cell>93.70</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV STUDY</head><label>IV</label><figDesc>ON THE CHOICE OF HYPE-PARAMETERS AND OF SFACE (RESNET34). AS THE NOISE LEVEL INCREASES, PARAMETER SHOULD BE LARGER, i.e.</figDesc><table><row><cell></cell><cell cols="4">CURVES SHOULD MOVE TO THE RIGHT,</cell></row><row><cell cols="5">WHICH INDICATES THAT THE SPEED OF INTRA-CLASS IS DECREASED</cell></row><row><cell cols="4">MORE EARLY TO PREVENT OVERFITTING.</cell><cell></cell></row><row><cell>Noise</cell><cell cols="2">Parameters a b</cell><cell cols="2">IJB-C FAR=1e-4 FAR=1e-3</cell></row><row><cell></cell><cell>0.81</cell><cell>1.28</cell><cell>84.70</cell><cell>91.71</cell></row><row><cell>WebFace-Clean</cell><cell>0.80</cell><cell>1.28</cell><cell>85.72</cell><cell>92.52</cell></row><row><cell>(Noise Level ? 0%)</cell><cell>0.80</cell><cell>1.25</cell><cell>83.99</cell><cell>91.17</cell></row><row><cell></cell><cell>0.80</cell><cell>1.30</cell><cell>85.45</cell><cell>92.20</cell></row><row><cell>WebFace-ArcFace (Noise Level ? 10%)</cell><cell>0.80 0.82 0.82</cell><cell>1.28 1.28 1.25</cell><cell>85.39 86.30 84.17</cell><cell>92.09 92.43 91.32</cell></row><row><cell>WebFace-Noisy (Noise Level ? 20%)</cell><cell>0.82 0.84 0.84</cell><cell>1.28 1.28 1.25</cell><cell>83.97 84.80 84.09</cell><cell>91.36 91.84 91.43</cell></row><row><cell cols="5">Fig. 7. Comparison of verification TAR@FAR=1e-4 and TAR@FAR=1e-3</cell></row><row><cell cols="5">results on the IJB-C database [22] of softmax, NSoftmax, CosFace, ArcFace</cell></row><row><cell cols="5">and SFace models (ResNet34) which are trained with databases of different</cell></row><row><cell cols="5">noise level (WebFace-Clean (?0%), WebFace-ArcFace (?10%), and WebFace-</cell></row><row><cell>Noisy (?20%)).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>USING FACESCRUB [41] AS THE PROBE SET. "ACC." REFERS TO THE RANK-1 FACE IDENTIFICATION ACCURACY WITH 1M DISTRACTORS, AND "VER." REFERS TO THE FACE VERIFICATION TAR@FAR=1E-6. "R" REFERS TO DATA REFINEMENT ON BOTH PROBE SET AND 1M DISTRACTORS FOLLOWING [7]. IN THE SECOND AND THIRD CELL, METHODS ARE COMPARED IN THE SAME SETTING WITH RESNET100 MODELS TRAINED ON MS1MV2 DATABASE [12].</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell></row><row><cell cols="4">VERIFICATION PERFORMANCE ON LFW [16] AND YTF [17] DATABASES.</cell><cell cols="4">FACE IDENTIFICATION AND VERIFICATION EVALUATION ON MEGAFACE</cell></row><row><cell cols="4">THE STATE-OF-ART MODELS IN FACE RECOGNITION COMMUNITY ARE LISTED FOR COMPARISON. Method #Images LFW YTF DeepID [1] 0.2M 99.47 93.20 DeepFace [47] 4.4M 97.35 91.4 VGG Face [48] 2.6M 98.95 97.30 FaceNet [2] 200M 99.63 95.10 Baidu [49] 1.3M 99.13 -</cell><cell>CHALLENGE 1 [20] Method</cell><cell>Protocol</cell><cell>Acc.</cell><cell>Ver.</cell></row><row><cell>Center Loss [3]</cell><cell>0.7M</cell><cell>99.28</cell><cell>94.9</cell><cell>FaceNet [2]</cell><cell>Large</cell><cell>70.49</cell><cell>86.47</cell></row><row><cell>Range Loss [24]</cell><cell>5M</cell><cell>99.52</cell><cell>93.70</cell><cell>CosFace [6]</cell><cell>Large</cell><cell>82.72</cell><cell>96.65</cell></row><row><cell>Marginal Loss [30]</cell><cell>3.8M</cell><cell>99.48</cell><cell>95.98</cell><cell>AdaptiveFace [26], R</cell><cell>Large</cell><cell>95.023</cell><cell>95.608</cell></row><row><cell>SphereFace [5]</cell><cell>0.5M</cell><cell>99.42</cell><cell>95.0</cell><cell>P2SGrad [9], R</cell><cell>Larget</cell><cell>97.25</cell><cell>-</cell></row><row><cell>SphereFace+ [50]</cell><cell>0.5M</cell><cell>99.47</cell><cell>-</cell><cell>AdaCos [8], R</cell><cell>Large</cell><cell>97.41</cell><cell>-</cell></row><row><cell>CosFace [6]</cell><cell>5M</cell><cell>99.73</cell><cell>97.6</cell><cell>MS1MV2, R100, CosFace [6]</cell><cell>Large</cell><cell>80.56</cell><cell>96.56</cell></row><row><cell>MS1MV2, R100, ArcFace [7]</cell><cell>5.8M</cell><cell>99.83</cell><cell>98.02</cell><cell>MS1MV2, R100, ArcFace [7]</cell><cell>Large</cell><cell>81.03</cell><cell>96.98</cell></row><row><cell>MS1MV2, R100, SFace</cell><cell>5.8M</cell><cell>99.82</cell><cell>98.06</cell><cell>MS1MV2, R100, SFace</cell><cell>Large</cell><cell>81.15</cell><cell>97.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MS1MV2, R100, CosFace [6], R</cell><cell>Large</cell><cell>97.91</cell><cell>97.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MS1MV2, R100, ArcFace [7], R</cell><cell>Large</cell><cell>98.35</cell><cell>98.48</cell></row><row><cell cols="2">TABLE VI</cell><cell></cell><cell></cell><cell>MS1MV2, R100, SFace, R</cell><cell>Large</cell><cell>98.50</cell><cell>98.61</cell></row><row><cell cols="4">VERIFICATION PERFORMANCE ON ON LFW [16], CALFW [18] AND</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">CPLFW [19] DATABASES. THE SECOND CELL LISTS RESULTS OF THE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">OPEN-SOURCED FACE RECOGNITION MODELS OF STATE-OF-ART</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">METHODS. IN THE THIRD CELL, OUR METHOD IS EVALUATED STRICTLY</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FOLLOWING ARCFACE [7].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>LFW</cell><cell>CALFW</cell><cell>CPLFW</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HUMAN-Indivadual</cell><cell>97.27</cell><cell>82.32</cell><cell>81.21</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HUMAN-Fusion</cell><cell>99.85</cell><cell>86.50</cell><cell>85.24</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Center Loss [3]</cell><cell>98.75</cell><cell>85.48</cell><cell>77.48</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SphereFace [5]</cell><cell>99.27</cell><cell>90.30</cell><cell>81.40</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VGGFace2 [14]</cell><cell>99.43</cell><cell>90.57</cell><cell>84.00</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS1MV2, R100, ArcFace [7]</cell><cell>99.82</cell><cell>95.45</cell><cell>92.08</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MS1MV2, R100, SFace</cell><cell>99.82</cell><cell>96.07</cell><cell>93.28</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VIII FACE</head><label>VIII</label><figDesc>IDENTIFICATION AND VERIFICATION EVALUATION OF DIFFERENT METHODS ON THE IJB-A [21] DATABASE. IN THE FIRST CELL, EXPERIMENTAL RESULTS ARE READ FROM ORIGINAL PAPERS. FOR COMPARISON, WE IMPLEMENT EXPERIMENTAL RESULTS IN THE SECOND CELL USING ARCFACE AND SFACE TRAINED ON VGGFACE2 AND MS-CELEB-1M DATABASES, RESPECTIVELY.TABLE IX FACE IDENTIFICATION AND VERIFICATION EVALUATION OF DIFFERENT METHODS ON THE IJB-C DATABASE [22]. EXPERIMENTAL RESULTS IN THE FIRST CELL ARE READ FROM ORIGINAL PAPERS, AND ALL THE MODELS ARE TRAINED ON VGGFACE2 DATABASE [14] OR MS-CELEB-1M DATABASE [12]. FOR COMPARISON, WE IMPLEMENT EXPERIMENTAL RESULTS IN THE SECOND CELL USING ARCFACE AND SFACE TRAINED ON VGGFACE2 AND MS-CELEB-1M DATABASES, RESPECTIVELY.</figDesc><table><row><cell>Method</cell><cell cols="2">FAR=1e-3</cell><cell cols="2">1:1 FAR=1e-2</cell><cell cols="2">FAR=1e-1</cell><cell cols="2">FPIR=0.01</cell><cell cols="2">FPIR=0.1</cell><cell>1:N Rank-1</cell><cell>Rank-5</cell><cell>Rank-10</cell></row><row><cell>VGGFace [48]</cell><cell></cell><cell>62.00</cell><cell></cell><cell>83.40</cell><cell></cell><cell>95.40</cell><cell></cell><cell>45.40</cell><cell></cell><cell>74.80</cell><cell>92.50</cell><cell>97.20</cell><cell>98.30</cell></row><row><cell cols="2">Template Adaption [51]</cell><cell>83.60</cell><cell></cell><cell>93.90</cell><cell></cell><cell>97.90</cell><cell></cell><cell>77.40</cell><cell></cell><cell>88.20</cell><cell>92.80</cell><cell>97.70</cell><cell>98.60</cell></row><row><cell>NAN [52]</cell><cell></cell><cell>88.10</cell><cell></cell><cell>94.10</cell><cell></cell><cell>97.80</cell><cell></cell><cell>81.70</cell><cell></cell><cell>91.70</cell><cell>95.80</cell><cell>98.00</cell><cell>98.60</cell></row><row><cell>VGGFace2 [14]</cell><cell></cell><cell>92.10</cell><cell></cell><cell>96.80</cell><cell></cell><cell>99.00</cell><cell></cell><cell>88.30</cell><cell></cell><cell>94.60</cell><cell>98.20</cell><cell>99.30</cell><cell>99.40</cell></row><row><cell>FTL [25]</cell><cell></cell><cell>91.20</cell><cell></cell><cell>95.30</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>96.00</cell><cell>98.30</cell><cell>98.70</cell></row><row><cell>UniformFace [53]</cell><cell></cell><cell>92.30</cell><cell></cell><cell>96.90</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell>97.90</cell><cell>98.80</cell><cell>-</cell></row><row><cell>L2-Face [31]</cell><cell></cell><cell>94.30</cell><cell></cell><cell>97.00</cell><cell></cell><cell>98.40</cell><cell></cell><cell>91.50</cell><cell></cell><cell>95.60</cell><cell>97.30</cell><cell>-</cell><cell>98.80</cell></row><row><cell>Crystal Loss [54]</cell><cell></cell><cell>94.90</cell><cell></cell><cell>96.90</cell><cell></cell><cell>98.40</cell><cell></cell><cell>91.80</cell><cell></cell><cell>95.90</cell><cell>97.20</cell><cell>-</cell><cell>98.80</cell></row><row><cell cols="2">VGGFace2, R50, ArcFace [7]</cell><cell>96.24</cell><cell></cell><cell>98.64</cell><cell></cell><cell>99.51</cell><cell></cell><cell>92.07</cell><cell></cell><cell>97.80</cell><cell>99.19</cell><cell>99.67</cell><cell>99.73</cell></row><row><cell cols="2">VGGFace2, R50, SFace</cell><cell>96.85</cell><cell></cell><cell>98.74</cell><cell></cell><cell>99.67</cell><cell></cell><cell>92.51</cell><cell></cell><cell>98.19</cell><cell>99.19</cell><cell>99.68</cell><cell>99.80</cell></row><row><cell cols="2">MS1MV2, R100, ArcFace [7]</cell><cell>97.60</cell><cell></cell><cell>98.75</cell><cell></cell><cell>99.53</cell><cell></cell><cell>93.47</cell><cell></cell><cell>98.11</cell><cell>98.83</cell><cell>99.33</cell><cell>99.51</cell></row><row><cell cols="2">MS1MV2, R100, SFace</cell><cell>98.02</cell><cell></cell><cell>98.93</cell><cell></cell><cell>99.51</cell><cell></cell><cell>94.84</cell><cell></cell><cell>98.50</cell><cell>98.93</cell><cell>99.44</cell><cell>99.55</cell></row><row><cell>Method</cell><cell>FAR=1e-5</cell><cell cols="2">FAR=1e-4</cell><cell cols="2">1:1 FAR=1e-3</cell><cell cols="2">FAR=1e-2</cell><cell cols="2">FAR=1e-1</cell><cell cols="2">FPIR=0.01</cell><cell>FPIR=0.1</cell><cell>1:N Rank-1</cell><cell>Rank-5</cell><cell>Rank-10</cell></row><row><cell>VGGFace2, ResNet50 [14]</cell><cell>73.40</cell><cell>82.50</cell><cell></cell><cell>90.00</cell><cell></cell><cell>95.00</cell><cell></cell><cell>98.00</cell><cell></cell><cell>73.50</cell><cell>83.00</cell><cell>89.80</cell><cell>93.90</cell><cell>95.30</cell></row><row><cell>VGGFace2, SENet50 [14]</cell><cell>74.70</cell><cell>84.00</cell><cell></cell><cell>91.00</cell><cell></cell><cell>96.00</cell><cell></cell><cell>98.70</cell><cell></cell><cell>74.60</cell><cell>84.20</cell><cell>91.20</cell><cell>94.90</cell><cell>96.20</cell></row><row><cell>VGGFace2, MN-v [55]</cell><cell>75.50</cell><cell>85.20</cell><cell></cell><cell>92.00</cell><cell></cell><cell>96.50</cell><cell></cell><cell>98.80</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGFace2, MN-vc [55]</cell><cell>77.10</cell><cell>86.20</cell><cell></cell><cell>92.70</cell><cell></cell><cell>96.80</cell><cell></cell><cell>98.90</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGFace2, ResNet50+DCN(Kpts) [56]</cell><cell>-</cell><cell>86.70</cell><cell></cell><cell>94.00</cell><cell></cell><cell>97.90</cell><cell></cell><cell>99.70</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGFace2, ResNet50+DCN(Divs) [56]</cell><cell>-</cell><cell>88.00</cell><cell></cell><cell>94.40</cell><cell></cell><cell>98.10</cell><cell></cell><cell>99.80</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGFace2, SENet50+DCN(Kpts) [56]</cell><cell>-</cell><cell>87.40</cell><cell></cell><cell>94.40</cell><cell></cell><cell>98.10</cell><cell></cell><cell>99.80</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGFace2, SENet50+DCN(Divs) [56]</cell><cell>-</cell><cell>88.50</cell><cell></cell><cell>94.70</cell><cell></cell><cell>98.30</cell><cell></cell><cell>99.80</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS1M, Inception-ResNet, P2SGrad [9]</cell><cell>87.84</cell><cell>92.25</cell><cell></cell><cell>95.58</cell><cell></cell><cell>97.79</cell><cell></cell><cell>99.03</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MS1M, Inception-ResNet, AdaCos [8]</cell><cell>88.03</cell><cell>92.40</cell><cell></cell><cell>95.65</cell><cell></cell><cell>97.72</cell><cell></cell><cell>99.06</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>VGGFace2, R50, ArcFace [7]</cell><cell>86.03</cell><cell>92.12</cell><cell></cell><cell>95.93</cell><cell></cell><cell>98.23</cell><cell></cell><cell>99.34</cell><cell></cell><cell>79.50</cell><cell>89.53</cell><cell>94.75</cell><cell>96.94</cell><cell>97.64</cell></row><row><cell>VGGFace2, R50, SFace</cell><cell>87.08</cell><cell>93.12</cell><cell></cell><cell>96.50</cell><cell></cell><cell>98.34</cell><cell></cell><cell>99.25</cell><cell></cell><cell>82.84</cell><cell>90.69</cell><cell>95.01</cell><cell>96.97</cell><cell>97.55</cell></row><row><cell>MS1MV2, R100, ArcFace [7]</cell><cell>93.15</cell><cell>95.65</cell><cell></cell><cell>97.20</cell><cell></cell><cell>98.18</cell><cell></cell><cell>99.01</cell><cell></cell><cell>90.32</cell><cell>94.52</cell><cell>95.72</cell><cell>97.10</cell><cell>97.47</cell></row><row><cell>MS1MV2, R100, SFace</cell><cell>94.21</cell><cell>96.11</cell><cell></cell><cell>97.50</cell><cell></cell><cell>98.33</cell><cell></cell><cell>99.00</cell><cell></cell><cell>92.41</cell><cell>95.17</cell><cell>96.21</cell><cell>97.41</cell><cell>97.76</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep learning face representation by joint identification-verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1988" to="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="499" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Normface: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM international conference on Multimedia</title>
		<meeting>the 25th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="212" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cosface: Large margin cosine loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5265" to="5274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Arcface: Additive angular margin loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4690" to="4699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Adacos: Adaptively scaling cosine logits for effectively learning deep face representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">P2sgrad: Refined gradients for optimizing deep face models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9906" to="9914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Naive-deep face recognition: Touching the limit of lfw benchmark or not</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.04690</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ms-celeb-1m: A dataset and benchmark for large-scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Level playing field for million scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7044" to="7053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Vggface2: A dataset for recognising faces across pose and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 13th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The devil of face recognition is in the noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="765" to="780" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Labeled faces in the wild: A database for studying face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007-10" />
			<biblScope unit="page" from="7" to="49" />
			<pubPlace>Amherst</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Cross-age LFW: A database for studying cross-age face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.08197</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cross-pose lfw: A database for studying crosspose face recognition in unconstrained environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<idno>18-01</idno>
		<imprint>
			<date type="published" when="2018-02" />
		</imprint>
		<respStmt>
			<orgName>Beijing University of Posts and Telecommunications</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The megaface benchmark: 1 million faces for recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4873" to="4882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1931" to="1939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Iarpa janus benchmark-c: Face dataset and protocol</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Maze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Niggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on Biometrics (ICB)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="158" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-grained face verification: Fglfw database, baselines, and human-dcmn partnership</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="63" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Range loss for deep face recognition with long-tailed training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5409" to="5418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature transfer learning for face recognition with under-represented data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5704" to="5713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adaptiveface: Adaptive margin and sampling for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unequal-training for deep face recognition with long-tailed noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7812" to="7821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Surpassing human-level face verification performance on lfw with gaussian face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3811" to="3819" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Marginal loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="60" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">L2-constrained softmax loss for discriminative face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ring loss: Convex feature normalization for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5089" to="5097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Misclassified vector guided softmax loss for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00833</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Curricularface: adaptive curriculum learning loss for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5901" to="5910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Softmax dissection: Towards understanding intra-and inter-clas objective for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01281</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Noise-tolerant paradigm for training face recognition cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Co-mining: Deep face recognition with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9358" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Frontal to profile face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Agedb: the first manually collected, in-the-wild age database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moschoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papaioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kotsia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="51" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE international conference on image processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Mxnet: A flexible and efficient machine learning library for heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.01274</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Website of CASIA-WebFace Cleaned List</title>
		<ptr target="https://github.com/happynear/FaceVerification" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Targeting ultimate accuracy: Face recognition via deep embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.07310</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Learning towards minimum hyperspherical energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6222" to="6233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="35" to="48" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Neural aggregation network for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4362" to="4371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Uniformface: Learning deep equidistributed representation for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3415" to="3424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Crystal loss and quality pooling for unconstrained face verification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01159</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Multicolumn networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Comparator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="782" to="797" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
