<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-similarity Based Hyperrelation Network for Few-Shot Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangwen</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaobing</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lian</forename><surname>He</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianghong</forename><surname>Tang</surname></persName>
						</author>
						<title level="a" type="main">Multi-similarity Based Hyperrelation Network for Few-Shot Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Few-shot Learning</term>
					<term>Semantic Segmentation</term>
					<term>Few-shot Segmentation</term>
					<term>Image Feature Similarity</term>
					<term>Multi- Similarity 1 Introduction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot semantic segmentation aims at recognizing the object regions of unseen categories with only a few annotated examples as supervision. The key to few-shot segmentation is to establish a robust semantic relationship between the support and query images and to prevent overfitting. In this paper, we propose an effective Multi-Similarity Hyperrelation Network (MSHNet) to tackle the few-shot semantic segmentation problem. In MSHNet, we propose a new Generative Prototype Similarity (GPS), which, together with cosine similarity, establishes a strong semantic relationship between supported images and query images. In addition, we propose a Symmetric Merging Block (SMB) in MSHNet to efficiently merge multi-layer, multi-shot, multi-similarity features to generate hyperrelation features for semantic segmentation. On two benchmark semantic segmentation datasets i Pascal-5 and i COCO-20 , MSHNet achieves new state-of-the-art performances on 1-shot and 5-shot semantic segmentation tasks. Our code is available at https://github.com/Alex-ShiLei/MSHNet .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The image semantic segmentation aims to classify each pixel of an image into a specific class. It is important fundamental processing in medical analysis and industry defect detection. With the great success of deep learning, image semantic segmentation has made great progress, such as FCN <ref type="bibr" target="#b3">[4]</ref>, U-Net <ref type="bibr" target="#b6">[7]</ref>, DeepLab <ref type="bibr" target="#b8">[9]</ref>, and so on. But training these segmentation models requires a lot of pixel-wise annotated images, which is tedious and costly. Moreover, a trained model can only make predictions within a set of pre-defined classes.</p><p>To perform semantic segmentation for new categories with only a few labeled samples, many few-shot semantic segmentation methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27]</ref> have been proposed. Most of the few-shot segmentation methods are based on two-branch architecture, as shown in <ref type="figure" target="#fig_1">Fig 1,</ref> with a support branch and a query branch. Support images are labeled with corresponding categories, and query images are segmented according to the labels of the support image. The two-branch structure uses a pre-trained convolutional neural network (CNN) to extract the high-level features of support and query images, and then uses a relational transformation block to establish the semantic relationship between support and query images to perform semantic segmentation on query images. The key to fewshot semantic segmentation is to find the relationship between support images and query images in high-dimensional features.</p><p>There are two ways to establish the semantic relationship between support and query images. The first method <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25]</ref> is that concatenate support features with query feature-map in deep layers to generate the relationship. Another way <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref> is to seek the cosine similarity between the deep query feature and support feature as a prior probability, and then concatenate it with the query feature map to segment the query image. However, objects are diverse, and it is difficult to describe the relationship between the query and support images robustly using only just one relational function. The HSNet <ref type="bibr" target="#b30">[30]</ref> proposed by Min J et al. tells us that using more relations between feature layers is helpful to establish a robust relationship between support and query images.</p><p>Motivated by these, we propose a Multi-similarity Hyperrelation Network (MSHNet) to address few-shot semantic segmentation. First, we use generative prototype similarity and cosine similarity to construct a multi-layer and multi-scale high-dimensional spatial relationship between the query and support images. Compared with feature-based methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b38">38]</ref>, our prototype-similarity is universal and can generalize models to new categories better. We then use Symmetric Merging Block (SMB) to merge these multiple relationships and establish robust hyperrelation for the support and query images. The locally generated prototype similarity based on global feature is logically complementary to the global cosine similarity based on local feature, and the relationship between the query image and the supported image can be expressed more comprehensively by using the two similarities simultaneously. In summary, MSHNet is based on the similarity relationship, rather than the features of the query image, which reduces the impact of the bias between train and test classes. In addition, the network efficiently integrates the multi-layer and multi-scale relationship features, which can better solve the problem of the diversity of object shapes and sizes. We experiment with the proposed method on i Pascal-5 dataset and i COCO-20 dataset, and we achieve a new the-state-of-art on both datasets. Through experiments, we find that even if we only use one of the two similarities, we can get results close to the existing optimal methods. The contributions of this paper can be summarized as follows.</p><p>? We propose a new prototype similarity method, which can effectively describe the similarity between query features and support vectors.</p><p>? We use multiple similarities to establish a more robust relationship between support and query images.</p><p>? We proved through experiments that the proposed prototype similarity and cosine similarity are complementary.</p><p>? The proposed Symmetric Merging Block (SMB) module is not only simple but also efficiently merges the similarity feature of multi-layer, multi-shot, and multi-similarity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this chapter, we briefly review some work related to this paper, such as few-shot learning, image semantic segmentation, few-shot semantic segmentation and so on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Semantic Segmentation</head><p>Image semantic segmentation is an essential component in many visual understanding systems, such as autonomous vehicles, and medical image diagnostics. The goal of image semantic segmentation is to classify each pixel of an image into a set of predefined semantic classes. Long et al. <ref type="bibr" target="#b3">[4]</ref> proposed fully convolutional networks which trained end-to-end, and pixels-to-pixels on semantic segmentation exceed the previous best results. Encoder-Decoder <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> is another popular semantic segmentation architecture. The encoder uses convolution and down-sampling to obtain deep features of different scales of the input image, while the decoder merges and upsamples these features to obtain the final semantic segmentation result. The DeepLab family <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref> uses atrous convolution and atrous spatial pyramid pooling (ASPP) to merge the multi-scale information, which can address the decreasing resolution in the encoder and robustly segment objects at multiple scales. Although these methods can achieve image semantic segmentation well, they need a large number of pixel-level annotated images to supervise the network, and the trained model cannot be used for new categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot Learning for Image Classification</head><p>Few-shot image classification <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref> aims to classify new images with a few examples. Meta-learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref> is a common method to solve the few-shot problem. It iteratively trains few-shot tasks on the train set to learn meta-knowledge and adapt the model to the situation with only a few samples. Many metalearning methods have been proposed based on metric learning and optimization algorithms. Metric-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b17">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19]</ref> aim to learn a general metric function to infer the distances between the query image and samples. The optimization-based <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b21">21,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b23">23]</ref> method aims to design effective learning strategies for fewshot tasks. For example, Chelsea et al. <ref type="bibr" target="#b20">[20]</ref> proposed a good parameter initialization learning method, which can make the model adapt to new tasks quickly. Most of the few-shot learning methods are developed for other image tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Few-shot Semantic Segmentation</head><p>Few-shot segmentation <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b30">30]</ref> is the extension of meta-learning methods, which can perform semantic segmentation within a few labeled samples. This research problem was introduced by Shaban et al. <ref type="bibr" target="#b0">[1]</ref>, who proposed a classical two-branch network. The most challenge for few-shot segmentation is how to design a transformation block and establish a class-agnostic relationship between the support and query images. The relationship is mainly based on generative similarity and cosine similarity. The generative-based methods <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b38">38]</ref> directly concatenate the support and query features into the merge block to estimate the prediction. For example, Zhang et al. <ref type="bibr" target="#b24">[24]</ref> upsamples the prototype feature vector from the support set and concatenates it with the query feature for dense predictions. However, the training category is different from the test category. Using a large number of parameters to express the relationship between the support and query features will result in overfitting. In the cosine-based methods, <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b27">27]</ref> compute the cosine similarity between the prototypes vector of support feature and pixels of query feature, <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b41">41]</ref> introduce a graph attention mechanism to establish pixel-to-pixel similarity between support and query features. However, these models only use a few feature layers, so it is difficult to establish a robust relationship between the support and query features. Recently, <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b46">46]</ref> used 4D convolution to establish the hyperrelation between multi-layer features, but 4D convolution has high spatial complexity and time complexity. In this paper, we use multi-similarity to build a more robust semantic relationship between support and query images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Description ?</head><p>The few-shot segmentation aims to learn a model to perform segmentation on novel classes with a few pixellevel annotated samples. In the K-shot segmentation task, each class has K pixel-level labeled samples. Suppose we have two non-overlapping data sets Dtrain and Dtest. The train set Dtrain is constructed by classes Ctrain and the test set Dtest is constructed by Ctest, where Ctrain and Ctest are non-overlapping ( train test CC =?). The model only uses the Dtrain to train the network, and the categories in the Ctest are not involved in the training process.</p><p>Following the previous few-shot methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">30]</ref>, we train and test the model in episodes. In each episode, we randomly sample a support set is the corresponding binary segmentation masks. The support set S consists of N classes and each class contains K samples <ref type="bibr" target="#b10">11</ref> 1</p><formula xml:id="formula_0">{( , )...( , )} K K N c c c c c S x m x m = = .</formula><p>In this paper, we train and tested on 1shot and 5-shot tasks. We are supposed to learn a mapping function </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our Method Overview</head><p>We propose a novel few-shot semantic segmentation architecture, Multi-similarity Hyperrelation Network (MSHNet), which is based on generative prototype similarity and cosine similarity. The overall architecture is illustrated in <ref type="figure" target="#fig_5">Fig 2,</ref> which is built upon a two-branch architecture. The support branch and query branch share a CNN block (pretrained on ImageNet <ref type="bibr" target="#b41">[41]</ref>) which is used to obtain the high-dimensional semantic features of the input images. We then used these high-level features to calculate the prototype similarity and cosine similarity between the support and query images. Each symmetrical merging block (SMB) takes the prototype similarity and cosine similarity as input to generate hyperrelation features for semantic segmentation. To reduce the risk of overfitting, we train the model in episode <ref type="bibr" target="#b34">[34]</ref>. The overview of the whole process is provided in Algorithm 1.</p><p>In each episode, we first input the random sampled <ref type="bibr" target="#b2">3</ref>  use ResNet-50 <ref type="bibr" target="#b31">[31]</ref> and ResNet-101 <ref type="bibr" target="#b31">[31]</ref> as the CNN backbone to extract the feature of support images and query images.</p><p>We select the standard cross-entropy loss  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Object Feature Extractor and Prototype</head><p>The ResNet <ref type="bibr" target="#b31">[31]</ref> is composed of many layers, and each convolutional layer has a specific convolution kernel to generate feature maps. In general, the more pairs of intermediate feature maps</p><p>,, 01</p><formula xml:id="formula_1">{(F ,F ) } b l b l L B s q l b</formula><p>== have been used, the relationship between support and query image is more robust. To balance computational complexity and relational complexity, we only select the features marked in the support image to calculate similarity, as</p><formula xml:id="formula_2">? ? ( , ) , ,</formula><p>, ,</p><formula xml:id="formula_3">( 1, 1) ( , ) x h y w b l o b l b l s s s xy F F M x y c == == ?? == ??<label>(2)</label></formula><p>where (x, y) is the spatial position of the feature map; [] is a selector function, if true, the index value is selected,</p><formula xml:id="formula_4">otherwise discarded; ,, l nd b l o s FR ? ?</formula><p>. The prototype vector of the specific class object is computed by: </p><formula xml:id="formula_5">,,<label>, , , ,, 0 ,</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Prototype Similarity and Cosine Similarity</head><p>Because the class of the test set is different from that of the train set, the feature-based methods <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b47">47]</ref> tend to be associated with specific category features. We reduce this bias by class-independent similarity, and use only a few parameters in prototype similarity to further reduce the influence of overfitting. We upsample the prototype vector ,, </p><p>where p</p><p>x ,, b l c s P is the class prototype vector of the annotated object, q</p><p>x is a vector in query feature map, and <ref type="bibr">12 d</ref> WR ? ? is a full connection layer weight. The output of GPS is a one-dimensional feature map, and the value represents the probability that the position is the target class. Objects in nature come in a variety of shapes and sizes that are difficult to represent with just a global average prototype. We use the cosine similarity to establish a more detailed relation between query and support in pixel by pixel. The cosine similarity between each point in query feature map and support features is defined as below:</p><p>,,</p><formula xml:id="formula_7">( , ) b l o s F T q s i i qp qs b l o s xx LU xx CS x x F = ? = ? (5)<label>, 1 ,, Re ( )</label></formula><p>where q x is a vector of a point in , . As done in <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b46">46]</ref>, we use the ReLU() function to make the network focus only on how the query point is similar to the specific class object in the supporting image, rather than how they are different. The CS() describes the similarity between each point in query feature map and all feature points in the support set.</p><p>After the prototypical similarity and cosine similarity of each layer were obtained, we used a symmetrical merge block to refine the two similarities between the same scale layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Symmetric Merging Block</head><p>We have obtained prototype similarity and cosine similarity for the multi-layers and multi-shots of the support images and query image. To merge the two similarities more consistently and stably, we propose a twobranch symmetric merging block as shown in <ref type="figure" target="#fig_9">Figure 3</ref>. The input prototype similarity and cosine similarity are one-dimensional feature map, which were calculated from GPS and CS functions respectively. In Block-Conv, a 1x1 convolution layer was used to integrate similarity features from multiple layers, and then a 3x3 convolution kernel was used to optimize similarity features within the local range. In prototype Shot-Conv, we use multi-scale atrous convolution to obtain the similarity features of different scales in a larger range. Since cosine similarity is stable, we only use a 3x3 convolution layer to merge features in cosine Shot-Conv. In Merge-Conv we use the channel-attention mechanism <ref type="bibr" target="#b29">[29]</ref> to highlight the layers of similarity we need to focus on. The symmetric merging block (SMB) merges two similarities and outputs hyperrelational features for semantic segmentation.  hyper sim ] to get segmentation result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.Experiment</head><p>In this section, we experiment our proposed model on two benchmark datasets and compare the results with current excellent methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pascal-5 i</head><p>i Pascal-5 using images and annotations in Pascal-VOC 2012 <ref type="bibr" target="#b44">[44]</ref> and extended annotations from SDS <ref type="bibr" target="#b45">[45]</ref>. Following <ref type="bibr" target="#b30">[30]</ref>, the 20 object categories in Pascal-VOC 2012 dataset is sub-divided into 4 folds i {0,?,3}, each fold having 5 classes Lfold {4x+i}, where x?{0,?,4}. Follw <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b36">36]</ref>, we use cross-validation to evaluate the proposed model, taking the classes in one of folds as test set Ctest and the classes in the other fold as the train set Ctrain. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the sample number distribution of different categories in the dataset is very uneven, which will bias the prediction result to the category with more samples. To solve this problem, we randomly select 750 images of each train class to form the train set. Another potential problem is that if objects belonging to the test class appear in the image of the training set, the category will always be predicted as the background during training, and the network will also tend to predict such objects as the background during testing. Therefore, the pictures containing the class of the test set are not used in the training of the network in this paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">COCO-20 i</head><p>i COCO-20 is based on the MS COCO2014 dataset <ref type="bibr" target="#b32">[32]</ref>. We also use cross-validation to test our model.</p><p>Fellowing <ref type="bibr" target="#b30">[30]</ref>, the 80 classes are divided into 4 folds i {0,1,2,3}, and each contain 20 classes, Lfold {4x+i}, where x?{0,1,?,19}. There are ambiguity and overwriting problems when converting JSON-formatted annotation files into grayscale images. For example, if a child wears a dress with a cake pattern, the pattern will be marked as both a person and a cake, while the grayscale image can only choose one of two. In this paper, we directly use JSONformat file to train my network. To solve the problem of unbalanced data, we randomly selected the classes in each training epoch, and then randomly selected images according to the selected classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Training and Evaluation</head><p>Our MSHNet is implemented on PyTorch <ref type="bibr" target="#b33">[33]</ref>. We use ResNet-50 and ResNet-101 pre-trained on ImageNet as our CNN backbones. We adopt the same method as in <ref type="bibr" target="#b30">[30]</ref> to extract the features of each layer. We resize input spatial sizes of both support and query image to 473 x473, thus we having H2, W2 60, H3, W3 30 and H4, W4 15. The CNN backbone parameters are fixed and other parameters in MSHNet are initialized by the default setting of Pytorch. We use SGD as our optimizer, the weight decay is 0.0005 and the moment is 0.9. We train the network with an initial learning rate of 0.025 and use exponential decay as the learning rate decay strategy. The models are trained on a single Nvidia Tesla P100 GPU with batch size 10. In each fold test dataset, we random sample 1000 support-query pairs of test images from the selected test classes as test data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation Metric</head><p>Following previous works <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26]</ref>, we use the mean intersection-over-union(mIoU) and foreground- , where n is the number of classes. FB-IoU is the average of foreground and background IoUs, and ignore the specific object class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results and Comparision</head><p>In table2, we compare MSHNet with the previous state-of-the-art methods on i Pascal-5 dataset. In the previous methods, HSNet also used multi-layer and multi-scale CNN backbone features. Results in table1 show that the MSHNet outperforms the state-of-the-art methods in both 1-shot and 5-shot tasks. In the 1-shot segmentation tasks, ResNet-50 and ResNet-101 were used as the CNN backbone respectively, and our method achieve 1.3% and 0.5% improvement in mIoU. In the 5-shot segmentation task, using ResNet-50 as the backbone, our model outperforms the CyCTR by 4.3% on mIoU, and using ResNet-101 as the backbone, our method outperforms the CyCTR by 5.7% on mIoU. The result of 5-shot task of HSNet <ref type="bibr" target="#b30">[30]</ref> method is obtained by multiple 1-shot tasks. In our proposed method, 5-shot segmentation result is obtained directly through the model. Compared to HSNet, with RESNET-50 as the backbone, our network improved 0.4% mIoU and 1.9% mIoU in 1-Shot and 5-Shot tasks,repectively, and with ResNet-101 as the backbone, our method improved 0.4% mIoU and 1.9% mIoU on 1-shot and 5-Shot tasks. <ref type="table">Table 2</ref>. Performance of 1-shot and 5-shot segmentation on Pascal-5 i data set. ("-" means the original paper does not report its performance for this metric.) In <ref type="table" target="#tab_4">Table 3</ref>, we compare MSHNet with the state-of-the-art methods on the i COCO-20 dataset. For both 1-shot and 5-shot segmentation tasks our model set a new state-of-the-art. Under 1-shot settings, it outperforms the previous state-of-art method 3.8% and 4.9% mIoU with Resnet-50 and ResNet-101 respectively. Under 5-shot setting, our model outperforms the previous state-of-art method 7.9% and 6.5 % mIoU. In summary, the network model proposed by us is very effective in solving the task of few-shot semantic segmentation. The background of the i COCO-20 data set is complex, and a good segmentation result requires more local and global information. Prototype similarity and Cosine similarity complement each other to provide more information.  <ref type="figure" target="#fig_11">Fig.4</ref> shows some qualitative results of our models. We can see that our network can accurately segment query images with only a few labeled sample images. In the 1-shot task, our network can segment object categories and backgrounds well. In the 5-shot task, our network can segment object categories finely, even if it was a wire fence. The main reason for this is that prototype similarity provides a lot of details for semantic segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Ablation Study</head><p>In this section, we specifically analyze the role of two similarities in the model by analyzing the energy map and using only one similarity for the comparative experiment. Both comparative and analytical experiments were validated on i Pascal-5 data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Energy Map Analysis</head><p>The energy map can effectively display the focus of the current convolution layer. We use a pre-trained ResNet as the backbone network to extract high-dimensional features, and calculated the generative prototype similarity and cosine similarity of each feature layer. MSHNet uses all feature maps of the last three blocks of ResNet to calculate the similarity. We use the average of all similarities in one block to represent the energy map, and the strength of the energy value represents the concern of the current layer. From <ref type="figure">Figure 5</ref>, we can see that prototype similarity pays more attention to local content and edge content, and cosine similarity pays more attention to global content. In general, the two similarities are complementary. It can be seen from the energy map that the similarity of deep-level (Block4) features can well represent semantic correlation, but lacks object details. The similarity of low-level features preserves details, but the semantic correlation is not very strong. This indicates that we can use multiple levels of similarity to better segment query images. <ref type="figure">Fig.5</ref> The Block4, Block3, and Block2 energy map of MSHNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Test the segmentation effect of single similarity.</head><p>In this section, we test the segmentation by using a similarity function alone. We use ResNet-50 and ResNet-101 as the backbone to extract deep features and test the segmentation ability of prototype similarity and cosine similarity respectively. When verifying prototype similarity, we removed the parts only related to cosine similarity from MSHNet, the rest remained unchanged, and so did when verifying prototype similarity. We trained the network with a single similarity. As shown in <ref type="table" target="#tab_5">Table 4</ref>, even if only one similarity is used, our network can get a result close to the current best methods. The single-use of either similarity is not as good as the result of the use of both, which shows that the two similarities are complementary.</p><p>In <ref type="figure">Figure 6</ref>, we compared the segmentation results of different similarity. As shown in the figure, although not all segmentation results are better than using only one similarity, the combination of two similarities can reduce obvious missegmentation.  <ref type="figure">Fig.6</ref> Examples of segmentation with different similarity. We obtained the results with Resnet-101 as the backbone network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper, we propose a new few-shot semantic segmentation method called MSHNet, which uses two similarities to find the relation between the support image and query image. The significant improvement in i Pascal-5 and i COCO-20 data sets shows that it is effective to use multiple similarities to address few-shot segmentation problems. According to the energy map, we know that the global feature-based local generative prototype similarity and the local feature-based global cosine similarity are logically complementary, and the relationship between the query image and support image can be more comprehensively expressed by using these two similarities at the same time. The segmentation result of the two similarities is much better than that of the single similarity, which indicates that the symmetric merging block (SMB) proposed by us is very effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">discussion</head><p>In this paper, we use multi-similarity to solve the few-shot semantic segmentation, and get the best segmentation results. This shows that using multiple similarities can make the relationship between support and query images more robust. But the more similarity a network uses, the more complex it becomes and the more computing resources it requires. Which similarity to choose and how to synthesize these similarities is worth further exploration. References?</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?</head><label></label><figDesc>MSHNet achieves a new state-of-the-art performance on both i Pascal-5 [1] and i COCO-20 [32] datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1</head><label>1</label><figDesc>Two branches few-shot semantic segmentation structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where ? is the parameters of the model and M is the predicted mask. Once the mapping function is learned, the parameters in the model are fixed and we do not optimize the model on the Dtest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>as our loss function. As shown in Fig.2, Each symmetric merging block of the transformation block has an intermediate output and an intermediate loss i ( {1, 2,3}) i nner Li ? to help optimize the network. The final prediction of MSHNet generates another loss function out L . The total loss L is the sum of inner L and out L as: the number of symmetric merging blocks. The output of each symmetric merging block is segmentation at a specific scale, and the final output of the network merges the segmentation results at a variety of scales. The model parameters are only updated during training and fixed during testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2</head><label>2</label><figDesc>Overview of MSHNet framework. We extract image features by Resnet-50 or Resnet-101 in multi-scale and layers. In each symmetric merging block, we get the layer relationship of the same scale between the support image and query image. Finally, we merge the hyperrelational features of different scales with pyramid structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>F</head><label></label><figDesc>| means the number of pixel features in ,, is the i-th value in the support feature set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>P</head><label></label><figDesc>to the query feature map and concatenate it to , bl q F , and then we learn a projection function to get the generative prototype similarity (GPS) between prototype and query features, as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 3</head><label>3</label><figDesc>We proposed symmetric merging block. The upper part is used for prototype similarity, and the lower part is used for cosine similarity. Block-Merge-Conv synthesizes these similarity and outputs the hyperrelation features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>background intersection over union (FB-IoU) as our quantitative evaluation. IoU is defined as TP TP FP FN ++ , where the TP, FP and FN denote the number of true positive, false positive and false negative pixels of the predicted segmentation mask. The mIoU is the average of all classes IoU:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative results of our network. The left is the result of the 5-shot semantic segmentation and the right is the result of the 1-shot semantic segmentation. Experiments on -5 i Pascal data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Algorithm 1 :</head><label>1</label><figDesc>testing an episode for a K-shot semantic segmentation task.</figDesc><table><row><cell cols="4">Input: Support set S, Suport mask s M and query set Q</cell></row><row><cell cols="4">Extract query deep features , bl F from Q</cell></row><row><cell></cell><cell></cell><cell></cell><cell>q</cell></row><row><cell>for 2,.., bB =</cell><cell>do</cell><cell></cell></row><row><cell cols="4">Extract support deep features , , bl sk F from</cell><cell>S</cell><cell>k</cell></row><row><cell cols="2">for 1,..., kK =</cell><cell cols="2">do</cell></row><row><cell cols="3">for 1,..., lL =</cell><cell>do</cell></row><row><cell></cell><cell cols="3">Apply , sk M to obtain ,, sk , b l o F</cell><cell>by Eqn. (2)</cell></row><row><cell></cell><cell cols="3">Obtain ,, , b l c sk P from ,, sk , b l o F</cell><cell>by Eqn. (3)</cell></row><row><cell></cell><cell cols="3">Compute the generative prototype similarity</cell><cell>, bl k GPS from ( , bl q F , ,, sk , b l c P ) by Eqn. (4)</cell></row><row><cell></cell><cell cols="3">Compute the cosine similarity , bl k CS from ( , bl q F , ,, sk , b l o F</cell><cell>) by Eqn. (5)</cell></row><row><cell></cell><cell cols="2">end</cell></row><row><cell cols="4">Concatenate [</cell><cell>, bl k GPS ,?,</cell><cell>, bL k GPS ] and input it to Psimilarity Block Conv to get _ b k blc GPS</cell></row><row><cell cols="4">Concatenate [ , bl k CS ,?, , bL k CS ] and input it to Csimilarity Block Conv to get _ b k blc CS</cell></row><row><cell cols="2">end</cell><cell></cell></row><row><cell cols="4">Compute the average of [ _ b k blc GPS ,?, _ b K blc GPS ] and input it to Psimilarity Shot Conv to get</cell><cell>_ shot GPS</cell><cell>b</cell></row><row><cell cols="4">Compute the average of [ _ b k blc CS ,?, _ b K blc CS ] and input it to Csimilarity Shot Conv to get</cell><cell>_ b shot CS</cell></row><row><cell cols="4">Input ( _ b shot GPS , _ b shot CS ) to Block Merge Conv to get hyperrelation</cell><cell>_ hyper sim</cell><cell>b</cell></row><row><cell>end</cell><cell></cell><cell></cell></row><row><cell cols="4">Merge the multi-scale hyperrelational features [</cell><cell>_ hyper sim ,.., b</cell></row></table><note>_ B</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table . 1</head><label>.</label><figDesc>Pascle-5 i data set category and corresponding quantity distribution</figDesc><table><row><cell>Object name</cell><cell>aero-plane</cell><cell>bicycle</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell></row><row><cell>Fold0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number</cell><cell>528</cell><cell>428</cell><cell>583</cell><cell>371</cell><cell>376</cell></row><row><cell>Object name</cell><cell>Bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell></row><row><cell>Fold1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number</cell><cell>367</cell><cell>848</cell><cell>987</cell><cell>988</cell><cell>235</cell></row><row><cell>Object name</cell><cell>dining-tables</cell><cell>dog</cell><cell>horse</cell><cell>motorbike</cell><cell>person</cell></row><row><cell>Fold2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number</cell><cell>504</cell><cell>1101</cell><cell>370</cell><cell>440</cell><cell>3386</cell></row><row><cell>Object name</cell><cell>potted-plant</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv-monitor</cell></row><row><cell>Fold3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Number</cell><cell>359</cell><cell>276</cell><cell>487</cell><cell>488</cell><cell>476</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Performance of 1-shot and 5-shot segmentation on i COCO-20 data set.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1-shot</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>5-shot</cell><cell></cell><cell></cell></row><row><cell>Backbone</cell><cell>Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>mIoU</cell><cell>Fold-0</cell><cell>Fold-1</cell><cell>Fold-2</cell><cell>Fold-3</cell><cell>mIoU</cell></row><row><cell></cell><cell>PPNet [37]</cell><cell>28.1</cell><cell>30.8</cell><cell>29.5</cell><cell>27.7</cell><cell>29.0</cell><cell>39.0</cell><cell>40.8</cell><cell>37.1</cell><cell>37.3</cell><cell>38.5</cell></row><row><cell></cell><cell>RPMM [38]</cell><cell>29.5</cell><cell>36.8</cell><cell>29.0</cell><cell>27.0</cell><cell>30.6</cell><cell>33.8</cell><cell>42.0</cell><cell>33.0</cell><cell>33.3</cell><cell>35.5</cell></row><row><cell>ResNet-50</cell><cell>CyCTR [36]</cell><cell>38.9</cell><cell>43.0</cell><cell>39.6</cell><cell>39.8</cell><cell>40.3</cell><cell>41.1</cell><cell>48.9</cell><cell>45.2</cell><cell>47.0</cell><cell>45.6</cell></row><row><cell></cell><cell>HSNet [30]</cell><cell>36.3</cell><cell>43.1</cell><cell>38.7</cell><cell>38.7</cell><cell>39.2</cell><cell>43.3</cell><cell>51.3</cell><cell>48.2</cell><cell>45.0</cell><cell>46.9</cell></row><row><cell></cell><cell>MSHNet (ours)</cell><cell>39.6</cell><cell>48.3</cell><cell>45.5</cell><cell>42.8</cell><cell>44.1</cell><cell>49.2</cell><cell>62.5</cell><cell>55.6</cell><cell>52.0</cell><cell>54.8</cell></row><row><cell></cell><cell>FWB [39]</cell><cell>17.0</cell><cell>18.0</cell><cell>21.0</cell><cell>28.9</cell><cell>21.2</cell><cell>19.1</cell><cell>21.5</cell><cell>23.9</cell><cell>30.1</cell><cell>23.7</cell></row><row><cell></cell><cell>PFENet [35]</cell><cell>36.8</cell><cell>41.8</cell><cell>38.7</cell><cell>36.7</cell><cell>38.5</cell><cell>40.4</cell><cell>46.8</cell><cell>43.2</cell><cell>40.5</cell><cell>42.7</cell></row><row><cell>ResNet-101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>HSNet [30]</cell><cell>37.2</cell><cell>44.1</cell><cell>42.4</cell><cell>41.3</cell><cell>41.2</cell><cell>45.9</cell><cell>53.0</cell><cell>51.8</cell><cell>47.1</cell><cell>49.5</cell></row><row><cell></cell><cell>MSHNet (ours)</cell><cell>41.3</cell><cell>49.9</cell><cell>46.4</cell><cell>46.6</cell><cell>46.1</cell><cell>51.7</cell><cell>62.6</cell><cell>55.9</cell><cell>53.8</cell><cell>56.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>Results of single similarity on</figDesc><table><row><cell>i Pascal-5 data set</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03410</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Incremental product search via deep meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7549" to="7564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="234" to="241" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7062</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4367" to="4375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G S</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly Correlated Knowledge Integration for Few-shot Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Intelligence Research</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="24" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Instance credibility inference for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12836" to="12845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multi-scale kronecker-product relation networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Differentiable earth mover&apos;s distance for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06777</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Metric learning for large scale image classification: Generalizing to new classes at near-zero cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mensink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="488" to="501" />
			<pubPlace>Berlin; Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Empirical bayes transductive meta-learning with synthetic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P G</forename><surname>Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12696</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sygnowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Objectness-Aware Few-Shot Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02945</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sg-one: Similarity guidance network for one-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="3855" to="3865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Few-shot semantic segmentation with democratic attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="730" to="746" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J Y</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hypercorrelation squeeze for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="6941" to="6952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><forename type="middle">M</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Few-shot image semantic segmentation with prototype</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J H</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9197" to="9206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prior guided feature enrichment network for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot segmentation via cycle-consistent transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Part-aware prototype network for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="142" to="158" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Prototype mixture models for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="763" to="778" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning meta-class memory for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Feature weighting and boosting for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="622" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adaptive prototype learning and allocation for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="8334" to="8343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge 2012 (voc2012) development kit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis, Statistical Modelling and Computational Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="297" to="312" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Cost Aggregation Is All You Need for Few-Shot Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11685</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y P</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2869" to="2878" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
