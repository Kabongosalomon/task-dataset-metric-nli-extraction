<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bermuth</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Software &amp; Systems Engineering</orgName>
								<orgName type="institution">University of Augsburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Poeppel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Software &amp; Systems Engineering</orgName>
								<orgName type="institution">University of Augsburg</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Reif</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Software &amp; Systems Engineering</orgName>
								<orgName type="institution">University of Augsburg</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Finstreder: Simple and fast Spoken Language Understanding with Finite State Transducers using modern Speech-to-Text models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In Spoken Language Understanding (SLU) the task is to extract important information from audio commands, like the intent of what a user wants the system to do and special entities like locations or numbers. This paper presents a simple method for embedding intents and entities into Finite State Transducers, and, in combination with a pretrained general-purpose Speechto-Text model, allows building SLU-models without any additional training. Building those models is very fast and only takes a few seconds. It is also completely language independent. With a comparison on different benchmarks it is shown that this method can outperform multiple other, more resource demanding SLU approaches. Index Terms: spoken language understanding, speech to intent, offline voice assistant, finite state transducer decoding</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When building models for Spoken Language Understanding (SLU), there are two alternative approaches: Using two separate stages of transcribing the spoken command to text (Speech-to-Text, STT) and then extracting the useful information out of the transcribed sentence (Natural Language Understanding, NLU), or using a direct SLU approach which combines the two parts into one single model. The first has the benefit, that the two models can be trained independently and the STT-model can often be used across multiple different domains, while the NLU-model can be trained relatively quickly. The second approach on the other hand is often more accurate, because it does not have the problem that errors from the STT transcription are propagated into the NLU module.</p><p>Recent systems for NLU or SLU parsing usually build upon neural networks as feature extractors. While they generally achieve a high recognition accuracy, the downside is that training those networks can take a lot of time. In this work a completely different approach is investigated, which does not require any special training, and therefore allows very fast creation of SLU models. It uses Finite State Transducers (FSTs) instead of neural networks for SLU parsing. The speech recognition part still uses a neural network, but this only has to be trained once per language on general-purpose STT tasks. In this work a Quartznet [1] model from previous work in Scribosermo <ref type="bibr" target="#b1">[2]</ref>, as well as a Conformer [3] model from NeMo <ref type="bibr" target="#b3">[4]</ref>, converted to tensorflow-lite with Scribosermo, are used. Both models, after conversion to tflite and quantization, can run faster than real-time on a RaspberryPi4.</p><p>A comparison on multiple benchmarks shows that the performance of this approach is highly competitive with other solutions, despite the fact that the concept is very simple and the models are built in a few seconds.</p><p>The source-code of the presented method for training-free SLU parsing, named finstreder, as well as the models from Scribosermo, can be found at: https://gitlab.com/Jaco-Assistant Using FSTs in speech recognition tasks is not a new idea and was quite common some years ago <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>. FSTs are used by Kaldi <ref type="bibr" target="#b6">[7]</ref>, where a neural network outputs phonemes which are then decoded to sentences with a combination of multiple FSTs. Some works already explored the usage of FSTs for parsing NLU information by adding semantic tags into the FSTs, either from textual inputs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> or from speech transcription hypotheses of a hidden Markov model <ref type="bibr" target="#b10">[10]</ref>. The authors of <ref type="bibr" target="#b11">[11]</ref> explored how to use grammar fragments with embedded tags to improve word confusions. In [12] a dialog system is described which transforms textual user utterances into response sentences using weighted FSTs, with the goal to be able to run a full back-and-forth dialog with the users. It was extended by <ref type="bibr" target="#b13">[13]</ref> to accept n-best hypotheses from a triphone model acoustic model, which were combined with an additional 3-gram language model, as input. Eesen <ref type="bibr" target="#b14">[14]</ref> introduced FST-decoding to models outputting character-based Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b15">[15]</ref> labels, similar to the Quartznet model of Scribosermo. Alexa also uses FSTs for its skill kit, but keeps separate models for STT and NLU <ref type="bibr" target="#b16">[16]</ref>. This work follows a very similar decoding approach as Eesen, which allows using recent CTC-based STT models (in difference to <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b13">13]</ref>), but alters the Grammar-FST (explained in the next chapters) to embed NLU information into it, similar to the semantic tagging of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">10,</ref><ref type="bibr" target="#b11">11]</ref>, which allows combining the two distinct STT+NLU models into a single SLU decoder.</p><p>OpenFST <ref type="bibr" target="#b17">[17]</ref> is used as library for handling the FSTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Foundations</head><p>In general, the language models used for decoding speech features with FSTs can be split into different parts: (1) A Grammar-FST, conventionally denoted as G, which stores the information of complete sentences. <ref type="figure">Figure 1</ref> shows a very simple grammar that can accept exactly two different sentences. <ref type="figure">Figure 1</ref>: Simple Grammar-FST which accepts the sentences "ab ab" or "abba abba". The part in the transition on the left side of the colon denotes the input which is required to make the transition and the one on the right side the output which is received afterwards.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2206.14589v1 [cs.CL] 29 Jun 2022</head><p>(2) A Lexicon-FST, denoted as L, which builds words out of characters, as shown in <ref type="figure">Figure 2</ref>. For later optimization of the FST, a disambiguation symbol is required, which ensures that the path for "ab" doesn't also accept "abab". Instead of introducing a special symbol, each word has to end with a space. <ref type="figure">Figure 2</ref>: The Lexicon-FST for the words "ab" and "abba".</p><p>(3) In the case of CTC-Inputs, a Token-FST (T) which converts the frame-level CTC-labels to characters. CTC-labels are commonly used in STT-systems and contain a special &lt;blank&gt; label (here also written as "-") besides the default alphabet characters. To generate normal text from those labels, all repeated tokens are merged into one single character, except they are separated by a &lt;blank&gt; label, which is removed after the merging step. <ref type="figure" target="#fig_0">Figure 3</ref> shows a simple FST for the alphabet "&lt;space&gt;,a,b,&lt;blank&gt;". To allow the usage of sentencepiece-style labels <ref type="bibr" target="#b18">[18]</ref> from the Conformer model, this FST was extended in a way that the pieces are split into single characters. Instead of using three single FSTs while decoding, they can be composed together. Afterwards an optimization step can be applied, which restructures the graph to remove unnecessary or duplicated transitions. <ref type="figure" target="#fig_2">Figure 4</ref> shows the composition of the Lexicon-FST with Grammar-FST. The result can then be composed with the Token-FST, similar to the approach in Eesen <ref type="bibr" target="#b14">[14]</ref>, so that the composition can handle CTC-labels as input.</p><p>The input of the combined TLG model also has to be in form of a FST. Thus the step-by-step labels are converted into a stateby-state FST with one transition for each character between the two states of a timestep ( <ref type="figure" target="#fig_1">Figure 5</ref>). The label probabilities in the range [0-1], where higher is better, have to be converted so that a lower value is better. The normalization approach of <ref type="bibr" target="#b19">[19]</ref> is used for this. Additionally, an extra timestep is added as last timestep, which has the space character as its highest probability, to ensure that the last word ends with a space (the disambiguation symbol of the lexicon). The reason behind the probability conversion is that after composition, the resulting FST includes all possible (acceptable) combinations of words that can be created with the given input. Since only the best matching result is desired, the shortest path algorithm from OpenFST can be used to search the path with the lowest transition weights (which came from the CTClabels). The result is a new FST, which accepts the best matching input words ( <ref type="figure">Figure 6</ref>). The decoding algorithm can then traverse this FST state-by-state and extract the output of each transition, which can then be merged into a returnable sentence. <ref type="figure">Figure 6</ref>: The Output-FST which accepts the sentence "ab ab" as most probable (shortest) path, with the total path length in the last state.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Finstreder's approach</head><p>In Spoken Language Understanding, the goal is to extract important features from a spoken utterance. In the case of users speaking voice commands, which is a common area for such applications, the software developers are mainly interested in the intent, the main goal of the command, and in special entities/slots, like names, locations or numbers.</p><p>This chapter shows how intent and entity information can be included directly in the Grammar-FST, with only slight adjustments. The decoder is then able to transcribe the text and extract the required information in one single step, thus removing the time-consuming step of training a specialized NLU-model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Building the models</head><p>Building the new Grammar-FST is separated into four different steps. As input a json-file following the syntax structure of Jaco <ref type="bibr" target="#b20">[20]</ref> is required:</p><formula xml:id="formula_0">{ "intents": { "get-looks": [ "(is a|are) [---](animal) cute" ]</formula><p>}, "lookups": { "animal": [ "whitemargin stargazer", "atlantic stargazer", "aye aye", "(hairy frogfish)-&gt;striated frogfish" ] } }</p><p>In the first step, a new text file for each intent is created, including all example sentences. The files are then converted to FSTs, either using n-grams or as fixed grammar. All final  In the next step one FST for each slot is created. It also includes special symbols as slot markers and handles synonym replacements, by adding the synonym value after the actual value into the graph. See <ref type="figure" target="#fig_4">Figure 8</ref>. In the third step, the Slot-FSTs are inserted directly before their placeholders in the Intent-FSTs. The placeholder itself is kept, because it is later needed to determine the corresponding slot type. At last the merged Intent-FSTs are each composed with the Lexicon-FST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoding</head><p>Decoding works as follows: First an Input-FST is built using the CTC-labels. Then it is composed with the Token-FST. Afterwards the resulting IT-FST is composed with each LG-FST of the different intents. The shortest path in each resulting FST is calculated and the overall shortest path is returned as result.</p><p>To improve and speed up decoding, several optimizations were implemented. Instead of uniting the LG-FSTs for each intent and composing it with the Token-FST into a single large TLG-FST, they are kept separate. Using multiple FSTs for the intents allows automatic parallelization with the IT-FST if there are many intents. This also allows including or excluding specific intents for each request. To exclude less necessary input characters, one parameter allows removing labels where the CTC-probability is below the top-k best values of each timestep. Another one removes all labels where the probability is below the mean value of the k-most-probable character over all timesteps (mean-k). Both parameters can greatly speed up decoding time with a very small impact on accuracy. A well working combination is top-k = 5-12 and mean-k = 21. For reweighing the input labels, a parameter was introduced that executes exponential scaling of the probabilities in a timestep, which increases or decreases the relative difference between label probabilities. A second parameter can be used for linear scaling of the input weights versus the grammar weights from the ngram model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The performance of finstreder was evaluated in multiple benchmarks. The first three benchmarks were reused from previous work in the Jaco project <ref type="bibr" target="#b20">[20]</ref>. Since the Quartznet model was already used in Jaco, combined with NLU models trained with Rasa <ref type="bibr" target="#b22">[21]</ref>, the experiments with it allow a direct comparison between the two semantic parsing methods. The STT models (QuartzNet-15x5/ConformerCTC-L) reach a greedy Word Error Rate (WER) of 4.6/2.2 % on LibriSpeech <ref type="bibr" target="#b23">[22]</ref> in English and 17.5/8.0 % on CommonVoice <ref type="bibr" target="#b24">[23]</ref> in French. All of the benchmark code is released in the same repository as this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spoken Language Understanding</head><p>The Barrista benchmark was published by Picovoice <ref type="bibr" target="#b25">[24]</ref> and consists of 620 commands of different people ordering coffee in English. The audio is mixed with different volume levels of background noise from cafe and kitchen environments. An example of a command would be: "i'd like a [medium roast] [large] [mocha] with [lots of cream] and [a little bit of brown sugar]". A command is correctly detected if the intent, as well as all the slots, could be retrieved by the assistant. This metric will be used for the following benchmarks as well.</p><p>The results in <ref type="figure">Figure 9</ref> show that finstreder with the Quartznet model performs as well as Jaco and Watson, and, like Jaco, has some problems with noisy backgrounds, which is most likely related to poorer recognition in the acoustic model. In comparison to that, using the Conformer model improves the results, especially in noisy environments. <ref type="figure">Figure 9</ref>: Benchmark coffee orders with noisy backgrounds. The results of DialogFlow, Watson, Luis and Rhino have been taken from <ref type="bibr" target="#b25">[24]</ref>, the results of Alexa and Jaco from <ref type="bibr" target="#b20">[20]</ref>. Because the SLU model is built only from text files, it is very simple to add frequent transcription errors as synonyms into the dialog definition. After adding similar sounding synonyms as replacements (+SSR) for the three most common errors ((rose|roast), (ons|ounce), (ice moka|iced mocha)), and rebuilding the SLU model, the accuracy improved notably.</p><p>The SmartLights benchmark from Snips <ref type="bibr" target="#b26">[25]</ref> tests the capability of controlling lights in different rooms. It consists of 1660 requests which are split into five partitions for a 5fold evaluation. A sample command could be: "please change the [bedroom] lights to [red]" or "i'd like the [living room] lights to be at [twelve] percent". The benchmark results are presented in <ref type="table" target="#tab_0">Table 1</ref>. The performance of finstreder with the Quartznet model is on-par with the two-step approach of Jaco in this benchmark, as well as with the E2E-SLU approach of AT-AT <ref type="bibr" target="#b27">[26]</ref>, and outperforms them with the Conformer model. Snips <ref type="bibr" target="#b28">[27]</ref>, which was a voice assistant, but is not available anymore, uses Kaldi as STT module and their own NLU module. The models were trained on online servers and could then be downloaded, which enabled the assistant to decode the voice requests without an internet connection. Lugosch et al. <ref type="bibr" target="#b29">[28]</ref> use the features of a pretrained STT-network and add a SLUdecoder on top of it. Their model is then finetuned with a combination of real and synthetic speech data. AT-AT <ref type="bibr" target="#b27">[26]</ref> uses two single encoder networks for audio and text inputs, but combines them in a single decoder network, with the advantage that the decoder can be finetuned with text only data and the network can also apply the learned knowledge to audio inputs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy WER</head><p>Google <ref type="bibr" target="#b26">[25]</ref> 0.793 ? Snips <ref type="bibr" target="#b26">[25]</ref> 0.842 ? Alexa <ref type="bibr" target="#b20">[20]</ref> 0.792 ? Houndify <ref type="bibr" target="#b20">[20]</ref> 0.545 0.108 Jaco <ref type="bibr" target="#b20">[20]</ref> 0.854 0.108 Finstreder (Quartznet) 0.848 0.107 Finstreder (Conformer) 0.880 0.061 AT-AT <ref type="bibr" target="#b27">[26]</ref> 0.849 ? Lugosch et al. <ref type="bibr" target="#b29">[28]</ref> 0.714 ? The SmartSpeaker benchmark tests the performance of reacting to music player commands in English as well as in French. The benchmark is from Snips <ref type="bibr" target="#b26">[25]</ref>, too, and is the only one that could be found which includes a language other than English. It has the difficulty of containing many artist or music tracks with uncommon names in the commands, like "play music by [a boogie wit da hoodie]" or "I'd like to listen to [Kinokoteikoku]". As shown in <ref type="table" target="#tab_1">Table 2</ref>, using finstreder's SLU approach greatly improves accuracy compared to Jaco's STT+NLU concept. This could partially be explained by a different handling of the artist names in the ngram language models. The ngrams of Jaco directly include the names, which allows the parser to leave out some parts of the names or mix them up, whereas the ones of finstreder only have a placeholder which then has to be matched exactly.</p><p>In the TimersAndSuch benchmark <ref type="bibr" target="#b30">[29]</ref> common use-cases involving numbers are tested. It includes commands like "set an alarm for 9:24 a.m." or "compute 12.15 plus 26.9". The main difficulty is to recognize many different numbers, there are only very few command prefixes (like "set an alarm for", ...), there- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>English French</head><p>Snips <ref type="bibr" target="#b26">[25]</ref> 0.687 0.751 Google <ref type="bibr" target="#b26">[25]</ref> 0.478 0.423 Jaco <ref type="bibr" target="#b20">[20]</ref> 0.627 0.480 Alexa <ref type="bibr" target="#b20">[20]</ref> 0.455 0.889 Finstreder (Quartznet) 0.776 0.778 Finstreder (Conformer) 0.804 0.783 fore a fixed grammar model is used instead of a 2-gram model, which showed a better performance in the other benchmarks. The results in <ref type="table" target="#tab_2">Table 3</ref> show that finstreder performs generally well in this task too, and could outperform the benchmark's large baseline SLU model, which was trained specifically for this task, as well as the model from <ref type="bibr" target="#b31">[30]</ref>, which used the baseline's architecture but included additional unsupervised training for the model's encoder. A test with Alexa was skipped, because Alexa's built-in number entity did not understand numbers with decimals. FluentSpeechCommands <ref type="bibr" target="#b32">[31]</ref> tests simple voice assistant requests. It includes commands like "turn up the [bathroom] temperature", "switch the lights on" or "go get me my [shoes]". The benchmark is run with a fixed grammar model, too. The results can be found in <ref type="table" target="#tab_3">Table 4</ref>. Kim et al. <ref type="bibr" target="#b33">[32]</ref> are combining a textual BERT model with a vq-wav2vec-BERT model and a DeepSpeech2 acoustic model to a large SLU endto-end network. This followed the idea of knowledge distillation from the text model to the speech encoder during training.</p><p>Even though the focus of this work is on training-free SLU, it is of course possible to finetune the acoustic model with Scribosermo on the used dataset. After a short training (about 1:20 h on a single RTX2070) the model (+AMT) reaches state-of-theart performance.  <ref type="bibr" target="#b30">[29]</ref> 0.988 Cao et al. <ref type="bibr" target="#b34">[33]</ref> 0.990 FANS <ref type="bibr" target="#b35">[34]</ref> 0.990 Reptile <ref type="bibr" target="#b36">[35]</ref> 0.992 Finstreder (Quartznet) 0.992 Saxon et al. <ref type="bibr" target="#b37">[36]</ref> 0.994 AT-AT <ref type="bibr" target="#b27">[26]</ref> 0.995 Finstreder (Conformer) 0.995 Borgholt et al. <ref type="bibr" target="#b38">[37]</ref> 0.996 Seo et al. <ref type="bibr" target="#b39">[38]</ref> 0.997 Qian et al. <ref type="bibr" target="#b40">[39]</ref> 0.997 Kim et al. <ref type="bibr" target="#b33">[32]</ref> 0.997 Finstreder (Quartznet) + AMT 0.997</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Textual inputs</head><p>Instead of decoding CTC-labels which were predicted from an audio input, it is also possible to use the generated LG-FSTs for NLU extraction from textual inputs. A very simple approach, which was tested here, is to convert the textual input to CTClabels. <ref type="table" target="#tab_4">Table 5</ref> shows that NLU parsing with finstreder can outperform traditional approaches on simple datasets (in SmartSpeaker only the artist needs to be extracted), but falls behind if the datasets are more complex (see next chapter for explanation). Instead of assigning a probability of 1 to the actual character and 0 to the rest, a very high probability around 0.99 is used and the other characters get a very low probability around 0.001. The smaller value includes some random noise, which is important for the top-k optimization, to ensure that not always the same characters are chosen. Another important note is that between every character a timestep containing the blank symbol as highest probability is added. This has two reasons. First, it ensures that repeated characters are not merged into one, and second, it greatly improved the accuracy in some experiments. An explanation might be that it allows the model to slightly change words or invent new ones, if the input sentence doesn't match the training examples very well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Limitations</head><p>The FST-based approach of finstreder also has some limitations which should be mentioned. First, it does not work with open questions or commands and can only recognize predefined lookup values. The performance also decreases if there is a large difference between the textual training examples and the test sentences. These limitations can be seen in the Spoken Language Understanding Resource Package (SLURP) benchmark <ref type="bibr" target="#b41">[40]</ref>, which is currently the largest and most complicated SLU benchmark and includes multiple different domains and open questions like "give me the weather forecast for this week", "who won the presidential election this year" or "if you had to kill someone to save three people would you do it and if so why". The results in <ref type="table" target="#tab_5">Table 6</ref> show that finstreder only understands less than a half of the questions, which is much less than the baseline presented with the benchmark. The model from SLURP uses state-of-the-art STT and NLU models which were finetuned on this dataset. Testing Alexa was planned as well, but was not possible, because the total number of intents and slots included in the dataset was too high and raised an error message when trying to build the skill. A second limitation is that the decoding process slows down with growing datasets. Small benchmarks like Barrista run about 8/4? (Quartznet/Conformer) faster than real-time on a standard desktop CPU (AMD-3700X) using the tflite-runtime, but the large SLURP run is only 1.6/1.7? faster, and uses a lower top-k decoding parameter. The decoding speed is influenced by two characteristics of the STT model: The computation complexity of the model itself, which is much higher for the Conformer model compared to the Quartznet model, as well as the number of CTC-timesteps in the model outputs, where the Conformer has half the length of the Quartznet model, which has a growing impact on larger FST models. For future development, two options could be interesting to improve the decoding speed for large models: Testing decoders from the Kaldi project <ref type="bibr" target="#b6">[7]</ref>, which are optimized on large FSTs, and splitting up the skills into sub-grammars, similar to the approach of Alexa, which requires an activation phrase to start a skill after speaking the wake-word. This could be implemented with a small FST, which differentiates between the skill activation word, and enables to select only the Intent-FSTs required for this specific skill for the following decoding step. Such splitting should also have a positive effect on recognition accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusion</head><p>In this paper a simple method for direct Spoken Language Understanding without training is presented under the name of finstreder. As basis Finite State Transducers are used and optimized for the task of intent and entity extraction. In multiple benchmarks a performance greater than multiple direct SLU or two-step STT+NLU approaches could be achieved.</p><p>The main advantage of finstreder over other approaches is that no extra training of the SLU model is required. Only text files describing possible requests are needed, which are easy to create or adjust, and could be distributed, similar as in the voice assistants Jaco, Alexa or others, through shareable skills.</p><p>Building the proposed SLU-FST models for the investigated benchmarks took between 1 second (FluentSpeechCommands, Barrista, SmartLights) and about 30 seconds (SLURP) on a standard desktop computer (AMD-3700X). Compared to Jaco <ref type="bibr" target="#b20">[20]</ref> this is much faster, training the NLU models of Rasa on the same hardware took about 15 minutes for Barrista and about 20-25 minutes for SmartLights and SmartSpeaker, even though the hyperparameters were optimized for training speed.</p><p>The models can also be built and used on edge-devices like a RaspberryPi 4. Building the model for FluentSpeech-Commands, in which the request are most similar to a simple voice assistant, took 2.2 seconds (compared to 0.7 seconds on the desktop computer). The benchmark itself then runs about 1.1/1.5? (Quartznet/Conformer) faster than real time using the quantized tflite models. As mentioned in the last chapter, it can be seen here that on the RaspberryPi, the reduction of the number of CTC-steps outweighs the slower inference speed of the Conformer model.</p><p>Most other approaches did not mention training times or the used hardware, except for the SpeechBrain run on Timers-AndSuch <ref type="bibr" target="#b31">[30]</ref>, where published logs indicate that the training took about 2-3 hours on unknown hardware, as well as the approach of Kim et al. on FluentSpeechCommands <ref type="bibr" target="#b33">[32]</ref>, which required 8? Nvidia-V100 GPUs to train its network.</p><p>The presented method is especially interesting for use-cases that have frequent domain changes, relatively small datasets or restricted training possibilities. For example this could be the case in customizable smart home assistants running on edgedevices or on smartphones, where using a cloud service is undesirable, either to be independent of unstable internet connections or due to privacy concerns.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>The Token-FST which merges CTC-labels like "aaab ab-b" to the characters "ab abb".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>An Input-FST with the most probable path "ab", ending with an extra space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Optimized  LG-FST which has characters as input and the sentences from the simple grammar above as output.states are then forwarded using an epsilon transition to include the intent name directly into the FST, as shown inFigure 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Example Intent-FST with placeholder for entity values and the intent name at the end.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Example Slot-FST with special markers and synonym replacements (for the frogfish).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on SmartLights dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Accurracy on SmartSpeaker dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy on TaS dataset.</figDesc><table><row><cell>TaS-baseline [29]</cell><cell>0.816</cell></row><row><cell>SpeechBrain [30]</cell><cell>0.940</cell></row><row><cell>Finstreder (Quartznet)</cell><cell>0.900</cell></row><row><cell>Finstreder (Conformer)</cell><cell>0.954</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy on FSC dataset.</figDesc><table><row><cell>Alexa</cell><cell>0.987</cell></row><row><cell>FSC-baseline</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>NLU only test with textual inputs.</figDesc><table><row><cell></cell><cell cols="2">SmartSpeaker</cell><cell cols="2">SmartLights</cell></row><row><cell></cell><cell>Accuracy</cell><cell>WER</cell><cell>Accuracy</cell><cell>WER</cell></row><row><cell>Jaco (Rasa)</cell><cell>0.977</cell><cell>?</cell><cell>0.960</cell><cell>?</cell></row><row><cell>Finstreder</cell><cell>0.994</cell><cell>0.153</cell><cell>0.889</cell><cell>0.054</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Results on SLURP dataset.</figDesc><table><row><cell></cell><cell>ScenAct-F1</cell><cell>Entity-F1</cell><cell>SLU-F1</cell></row><row><cell>Multi-SLURP [40]</cell><cell>0.783</cell><cell>0.642</cell><cell>0.708</cell></row><row><cell>Finstreder (Quartznet)</cell><cell>0.432</cell><cell>0.313</cell><cell>0.380</cell></row><row><cell>Finstreder (Conformer)</cell><cell>0.531</cell><cell>0.395</cell><cell>0.452</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beliaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6124" to="6128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Scribosermo: Fast Speech-to-Text models for German and other Languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bermuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poeppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Reif</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.07982</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Nemo: a toolkit for building ai applications using neural modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hrinchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kriman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beliaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lavrukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cook</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09577</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weighted finite-state transducers in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="88" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Speech recognition algorithms using weighted finite-state transducers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Speech and Audio Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="162" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The Kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Glembek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hannemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Motlicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Signal Processing Society</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative and discriminative algorithms for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech 2007-8th Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximizing SLU performance with minimal training data using hybrid RNN plus rule-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Homma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Arantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T G</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Togami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th</title>
		<meeting>the 19th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Annual SIGdial Meeting on Discourse and Dialogue</title>
		<imprint>
			<biblScope unit="page" from="366" to="370" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the use of finite state transducers for semantic interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>B?chet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Damnati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving spoken language understanding using word confusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Gorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<pubPlace>Interspeech</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Recent advances in WFST-based dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohtake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kashioka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Expansion of wfst-based dialog management for handling multiple asr hypotheses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ohtake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kawai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Spoken Dialogue Systems Technology</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="61" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">EESEN: End-to-end speech recognition using deep RNN models and WFST-based decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Just ASK: building an architecture for extensible self-service spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hoffmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dreyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peshterliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gandhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filiminov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.00549</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OpenFst: A general and efficient weighted finite-state transducer library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allauzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Skut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Implementation and Application of Automata</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06226</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Sampling from Stochastic Finite Automata with Applications to CTC Decoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jansche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gutkin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.08760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Jaco: An Offline Running Privacy-aware Voice Assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bermuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Poeppel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Reif</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022</title>
		<meeting>the 2022</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<title level="m">ACM/IEEE International Conference on Human-Robot Interaction</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="618" to="622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Inc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on acoustics, speech and signal processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5206" to="5210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Common voice: A massively-multilingual speech corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henretty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kohler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Tyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Weber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.06670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Barrista Benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picovoice</surname></persName>
		</author>
		<ptr target="https://github.com/Picovoice/speech-to-intent-benchmark" />
		<imprint>
			<date type="published" when="2021-03" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12735</idno>
		<title level="m">Spoken language understanding on the edge</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploring Transfer Learning For End-to-End Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rongali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Arkoudas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08549</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Using speech synthesis to train end-to-end spoken language understanding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8499" to="8503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Papreja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Heba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01604</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Speechbrain</surname></persName>
		</author>
		<idno>v1.0</idno>
		<ptr target="https://github.com/speechbrain/speechbrain/tree/develop/recipes/timers-and-such" />
		<title level="m">SLU recipes for Timers and Such</title>
		<imprint>
			<date type="published" when="2021-03" />
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Two-stage Textual Knowledge Distillation to Speech Encoder for Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13105</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sequential End-to-End Intent and Slot Label Classification and Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Potdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1229" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">FANS: Fusing ASR and NLU for On-Device SLU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kunzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rastrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1224" to="1228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Improving End-to-End Speechto-Intent Classification with Reptile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Gorinski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.01994</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">End-to-End Spoken Language Understanding for Generalized Voice Assistants</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saxon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mouchtaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4738" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Do we still need automatic speech recognition for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Borgholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Havtorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abdou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maal?e</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>S?gaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14842</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Integration of Pre-trained Networks with Continuous Token Interface for End-to-End Spoken Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07253</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Speech-language pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bianv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7458" to="7462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">SLURP: A Spoken Language Understanding Resource Package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bastianelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vanzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swietojanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13205</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
