<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-column Deep Neural Networks for Image Classification Multi-column Deep Neural Networks for Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="201213-02">February 2012 13 Feb 2012 February 2012</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cire?an</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dalle Molle Institute for Artificial Intelligence Galleria 2</orgName>
								<orgName type="institution">IDSIA is a joint institute of both University of Lugano (USI)</orgName>
								<address>
									<postCode>6928</postCode>
									<settlement>Manno</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dalle Molle Institute for Artificial Intelligence Galleria 2</orgName>
								<orgName type="institution">IDSIA is a joint institute of both University of Lugano (USI)</orgName>
								<address>
									<postCode>6928</postCode>
									<settlement>Manno</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dalle Molle Institute for Artificial Intelligence Galleria 2</orgName>
								<orgName type="institution">IDSIA is a joint institute of both University of Lugano (USI)</orgName>
								<address>
									<postCode>6928</postCode>
									<settlement>Manno</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Cire?an</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences of Southern Switzerland (SUPSI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences of Southern Switzerland (SUPSI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Applied Sciences of Southern Switzerland (SUPSI)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-column Deep Neural Networks for Image Classification Multi-column Deep Neural Networks for Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="201213-02">February 2012 13 Feb 2012 February 2012</date>
						</imprint>
					</monogr>
					<note>and was founded in 1988 by the Dalle Molle Foundation which promoted quality of life. 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winnertake-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent publications suggest that unsupervised pre-training of deep, hierarchical neural networks improves supervised pattern classification <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. Here we train such nets by simple online back-propagation, setting new, greatly improved records on MNIST <ref type="bibr" target="#b19">[20]</ref>, Latin letters <ref type="bibr" target="#b12">[13]</ref>, Chinese characters <ref type="bibr" target="#b22">[23]</ref>, traffic signs <ref type="bibr" target="#b35">[36]</ref>, NORB (jittered, cluttered) <ref type="bibr" target="#b20">[21]</ref> and CIFAR10 <ref type="bibr" target="#b17">[18]</ref> benchmarks.</p><p>We focus on deep convolutional neural networks (DNN), introduced by <ref type="bibr" target="#b10">[11]</ref>, improved by <ref type="bibr" target="#b19">[20]</ref>, refined and simplified by <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7]</ref>. Lately, DNN proved their mettle on data sets ranging from handwritten digits (MNIST) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>, handwritten characters <ref type="bibr" target="#b5">[6]</ref> to 3D toys (NORB) and faces <ref type="bibr" target="#b36">[37]</ref>. DNNs fully unfold their potential when they are big and deep <ref type="bibr" target="#b6">[7]</ref>. But training them requires weeks, months, even years on CPUs. High data transfer latency prevents multi-threading and multi-CPU code from saving the situation. In recent years, however, fast parallel neural net code for graphics cards (GPUs) has overcome this problem. Carefully designed GPU code for image classification can be up to two orders of magnitude faster than its CPU counterpart <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>. Hence, to train huge DNN in hours or days, we implement them on GPU, building upon the work of <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7]</ref>. The training algorithm is fully online, i.e. weight updates occur after each error back-propagation step. We will show that properly trained big and deep DNNs can outperform all previous methods, and demonstrate that unsupervised initialization/pretraining is not necessary (although we don't deny that it might help sometimes, especially for small datasets). We also show how combining several DNN columns into a Multi-column DNN (MCDNN) further decreases the error rate by 30-40%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>The initially random weights of the DNN are iteratively trained to minimize the classification error on a set of labeled training images; generalization performance is then tested on a separate set of test images. Our architecture does this by combining several techniques in a novel way:</p><p>(1) Unlike the shallow NN used in many 1990s applications, ours are deep, inspired by the Neocognitron <ref type="bibr" target="#b10">[11]</ref>, with many (6-10) layers of non-linear neurons stacked on top of each other, comparable to the number of layers found between retina and visual cortex of macaque monkeys <ref type="bibr" target="#b2">[3]</ref>.</p><p>(2) It was shown <ref type="bibr" target="#b13">[14]</ref> that such multi-layered DNN are hard to train by standard gradient descent <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>, the method of choice from a mathematical/algorithmic point of view. Today's computers, however, are fast enough for this, more than 60000 times faster than those of the early 90s <ref type="bibr" target="#b0">1</ref> . Carefully designed code for massively parallel graphics processing units (GPUs normally used for video games) allows for gaining an additional speedup factor of 50-100 over serial code for standard computers. Given enough labeled data, our networks do not need additional heuristics such as unsupervised pre-training <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b9">10]</ref> or carefully prewired synapses <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>(3) The DNN of this paper ( <ref type="figure" target="#fig_0">Fig. 1a</ref>) have 2-dimensional layers of winner-take-all neurons <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b40">41]</ref> with overlapping receptive fields whose weights are shared <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7]</ref>. Given some input pattern, a simple max pooling technique <ref type="bibr" target="#b28">[29]</ref> determines winning neurons by partitioning layers into quadratic regions of local inhibition, selecting the most active neuron of each region. The winners of some layer represent a smaller, down-sampled layer with lower resolution, feeding the next layer in the hierarchy. The approach is inspired by Hubel and Wiesel's seminal work on the cat's primary visual cortex <ref type="bibr" target="#b39">[40]</ref>, which identified orientation-selective simple cells with overlapping local receptive fields and complex cells performing down-sampling-like operations <ref type="bibr" target="#b14">[15]</ref>.</p><p>(4) Note that at some point down-sampling automatically leads to the first 1-dimensional layer. From then on, only trivial 1-dimensional winner-take-all regions are possible, that is, the top part of the hierarchy becomes a standard multi-layer perceptron (MLP) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref>. Receptive fields and winner-take-all regions of our DNN often are (near-)minimal, e.g., only 2x2 or 3x3 neurons. This results in (near-)maximal depth of layers with non-trivial (2-dimensional) winner-take-all regions. In fact, insisting on minimal 2x2 fields automatically defines the entire deep architecture, apart from the number of different convolutional kernels per layer <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b6">7]</ref> and the depth of the plain MLP on top.</p><p>(5) Only winner neurons are trained, that is, other neurons cannot forget what they learnt so far, although they may be affected by weight changes in more peripheral layers. The resulting decrease of synaptic changes per time interval corresponds to biologically plausible reduction of energy consumption. Our training algorithm is fully online, i.e. weight updates occur after each gradient computation step.</p><p>(6) Inspired by microcolumns of neurons in the cerebral cortex, we combine several DNN columns to form a Multi-column DNN (MCDNN). Given some input pattern, the predictions of all columns are democratically averaged. Before training, the weights (synapses) of all columns are randomly initialized. Various columns can be trained on the same inputs, or on inputs preprocessed in different ways. The latter helps to reduce both error rate and number of columns required to reach a given accuracy. The MCDNN architecture and its training and testing procedures are illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In the following we give a detailed description of all the experiments we performed. We evaluate our architecture on various commonly used object recognition benchmarks and improve the state-of-the-art on all of them. The description of the DNN architecture used for the various experiments is given in the following way: 2x48x48-100C5-MP2-100C5-MP2-100C4-MP2-300N-100N-6N represents a net with 2 input images of size 48x48, a convolutional layer with 100 maps and 5x5 filters, a max-pooling layer over non overlapping regions of size 2x2, a convolutional layer with 100 maps and 4x4 filters, a max-pooling layer over non overlapping regions of size 2x2, a fully connected layer with 300 hidden units, a fully connected layer with 100 hidden units and a fully connected output layer with 6 neurons (one per class). We use a scaled hyperbolic tangent activation function for convolutional and fully connected layers, a linear activation function for max-pooling layers and a softmax activation function for the output layer. All DNN are trained using on-line gradient descent with an annealed learning rate. During training, images are continually translated, scaled and rotated (even elastically distorted in case of characters), whereas only the original images are used for validation. Training ends once the validation error is zero or when the learning rate reaches its predetermined minimum. Initial weights are drawn from a uniform random distribution in the range [?0.05, 0.05].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MNIST</head><p>The original MNIST digits <ref type="bibr" target="#b19">[20]</ref> are normalized such that the width or height of the bounding box equals 20 pixels. Aspect ratios for various digits vary strongly and we therefore create six additional datasets by normalizing digit width to <ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20</ref> pixels. This is like seeing the data from different angles. We train five DNN columns per normalization, resulting in a total of 35 columns for the entire MCDNN. All 1x29x29-20C4-MP2-40C5-MP3-150N-10N DNN are trained for around 800 epochs with an annealed learning rate (i.e. initialized with 0.001 multiplied by a factor of 0.993/epoch until it reaches 0.00003). Training a DNN takes almost 14 hours and after 500 training epochs little additional improvement is observed. During training the digits are randomly distorted before each epoch (see <ref type="figure" target="#fig_1">Fig. 2a</ref> for representative characters and their distorted versions <ref type="bibr" target="#b6">[7]</ref>). The internal state of a single DNN is depicted in <ref type="figure" target="#fig_1">Figure 2c</ref>, where a particular digit is forward propagated through a trained network and all activations together with the network weights are plotted.  <ref type="table" target="#tab_0">Table 1</ref>. MCDNN of 5 nets trained with the same preprocessor achieve better results than their constituent DNNs, except for original images (Tab. 1). The MCDNN has a very low 0.23% error rate, improving state of the art by at least 34% <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref> (Tab. 2). This is the first time an artificial method comes close to the ?0.2% error rate of humans on this task <ref type="bibr" target="#b21">[22]</ref>. Many of the wrongly classified digits either contain broken or strange strokes, or have wrong labels. The 23 errors ( <ref type="figure" target="#fig_1">Fig. 2b</ref>) are associated with 20 correct second guesses.</p><formula xml:id="formula_0">(a) (b) L0-Input 1 @ 29x29 20 @ 4x4</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results of all individual nets and various MCDNN are summarized in</head><p>We also trained a single DNN on all 7 datasets simultaneously which yielded worse result (0.52%) than both MCDNN and their individual DNN. This shows that the improvements come from the MCDNN and not from using more preprocessed data.  How are the MCDNN errors affected by the number of preprocessors? We train 5 DNNs on all 7 datasets. A MCDNN 'y out-of-7' (y from 1 to 7) averages 5y nets trained on y datasets. <ref type="table" target="#tab_2">Table 3</ref> shows that more preprocessing results in lower MCDNN error.</p><p>We also train 5 DNN for each odd normalization, i.e. W11, W13, W15, W17 and W19. The 60-net MCDNN performs (0.24%) similarly to the 35-net MCDNN, indicating that additional preprocessing does not further improve recognition.</p><p>We conclude that MCDNN outperform DNN trained on the same data, and that different preprocessors further decrease the error rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NIST SD 19</head><p>The 35-columns MCDNN architecture and preprocessing used for MNIST are also applied to Latin characters from NIST SD 19 <ref type="bibr" target="#b12">[13]</ref>. For all tasks our MCDNN achieves recognition rates 1.5-5 times better than any published result (Tab. 4). In total there are 82000 characters in the test set, but there are many more easy to classify digits (58000) than hard to classify letters (24000). This explains the lower overall error rate of the 62-class problem compared to the 52-class letters problem. From all errors of the 62-class problem 3% of the 58000 digits are misclassified and 33% of the 24000 letters are misclassified. Letters are in general more difficult to classify, but there is also a higher amount of confusion between similar lower-and upper-case letters such as i,I and o,O for example. Indeed, error rates for the case insensitive task drop from 21% to 7.37%. If the confused upper-and lower-case classes are merged, resulting in 37 different classes, the error rate is only slightly bigger (7.99%). Upper-case letters are far easier to classify (1.83% error rate) than lowercase letters (7.47%) due to the smaller writer dependent in-class variability.</p><p>For a detailed analysis of all the errors and confusions between different classes, the confusion matrix is most informative (Supplementary material <ref type="figure" target="#fig_0">Fig. S1</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Chinese characters</head><p>Compared to Latin character recognition, isolated Chinese character recognition is a much harder problem, mainly because of the much larger category set, but also because of wide variability of writing styles, and the confusion between similar characters. We use a dataset from the Institute of Automation of Chinese Academy of Sciences (CASIA <ref type="bibr" target="#b22">[23]</ref>), which contains 300 samples for each of 3755 characters (in GB1 set). This resulted in a data set with more than 1 Million characters (3GB of data) which posed a major computational challenge even to our system. Without our fast GPU implementation the nets on this task would train for more than one year. Only the forward propagation of the training set takes 27h on a normal CPU, and training a single epoch would consequently have lasted several days. On our fast GPU implementation on the other hand, training a single epoch takes 3.4h, which makes it feasible to train a net within a few days instead of many months. We train following DNN, 1x48x48-100C3-MP2-200C2-MP2-300C2-MP2-400C2-MP2-500N-3755N, on offline as well as on online characters. For the offline character recognition task, we resize all characters to 40x40 pixels and place them in the center of a 48x48 image. The contrast of each image is normalized independently. As suggested by the organizers, the first 240 writers from the database CASIA-HWDB1.1 are used for training and the remaining 60 writers are used for testing. The total numbers of training and test characters are 938679 and 234228, respectively.</p><p>For the online dataset, we draw each character from its list of coordinates, resize the resulting images to 40x40 pixels and place them in the center of a 48x48 image. Additionally, we smooth-out the resulting images with a Gaussian blur filter over a 3x3 pixel neighborhood and uniform standard deviation of 0.75. As suggested by the organizers, the characters of 240 writers from database CASIA-OLHWDB1.1 are used for training the classifier and the characters of the remaining 60 writers are used for testing. The resulting numbers of training and test characters are 939564 and 234800, respectively.</p><p>All methods previously applied to this dataset perform some feature extraction followed by a dimensionality reduction, whereas our method directly works on raw pixel intensities and learns the feature extraction and dimensionality reduction in a supervised way. On the offline task we obtain an error rate of 6.5% compared to 10.01% of the best method <ref type="bibr" target="#b22">[23]</ref>. Even though much information is lost when drawing a character from it's coordinate sequence, we obtain a recognition rate of 5.61% on the online task compared to 7.61% of the best method <ref type="bibr" target="#b22">[23]</ref>.</p><p>We conclude that on this very hard classification problem, with many classes (3755) and relatively few samples per class (240), our fully supervised DNN beats the current state-of-the-art methods by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Traffic signs</head><p>Recognizing traffic signs is essential for the automotive industry's efforts in the field of driver's assistance, and for many other traffic-related applications. We use the GTSRB traffic sign dataset <ref type="bibr" target="#b35">[36]</ref>.</p><p>The original color images contain one traffic sign each, with a border of 10% around the sign. They vary in size from 15 ? 15 to 250 ? 250 pixels and are not necessarily square. The actual traffic sign is not always centered within the image; its bounding box is part of the annotations. The training set consists of 26640 images; the test set of 12569 images. We crop all images and process only within the bounding box. Our DNN implementation requires all training images to be of equal size. After visual inspection of the image size distribution we resize all images to 48 ? 48 pixels. As a consequence, scaling factors along both axes are different for traffic signs with rectangular bounding boxes. Resizing forces them to have square bounding boxes.</p><p>Our MCDNN is the only artificial method to outperform humans, who produced twice as many errors. Since traffic signs greatly vary in illumination and contrast, standard image preprocessing methods are used to enhance/normalize them ( <ref type="figure" target="#fig_2">Fig. 3a and supplementary material)</ref>. For each dataset five DNN are trained (architecture: 3x48x48-100C7-MP2-150C4-150MP2-250C4-250MP2-300N-43N), resulting in a MCDNN with 25 columns, achieving an error rate of 0.54% on the test set. <ref type="figure" target="#fig_2">Figure 3b</ref> depicts all errors, plus ground truth and first and second predictions. Over 80% of the 68 errors are associated with correct second predictions. Erroneously predicted class probabilities tend to be very low-here the MCDNN is quite unsure about its classifications. In general, however, it is very confident-most of its predicted class probabilities are close to one or zero. Rejecting only 1% percent of all images (confidence below 0.51) results in an even lower error rate of 0.24%. To reach an error rate of 0.01% (a single misclassification), only 6.67% of the images have to be rejected (confidence below 0.94). Our method outperforms the second best algorithm by a factor of 3. It takes 37 hours to train the MCDNN with 25 columns on four GPUs. The trained MCDNN can check 87 images per second on one GPU (and 2175 images/s/DNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">CIFAR 10</head><p>CIFAR10 is a set of natural color images of 32x32 pixels <ref type="bibr" target="#b17">[18]</ref>. It contains 10 classes, each with 5000 training samples and 1000 test samples. Images vary greatly within each class. They are not necessarily   <ref type="bibr" target="#b6">[7]</ref> centered, may contain only parts of the object, and show different backgrounds. Subjects may vary in size by an order of magnitude (i.e., some images show only the head of a bird, others an entire bird from a distance). Colors and textures of objects/animals also vary greatly.</p><p>Our DNN input layers have three maps, one for each color channel (RGB). We use a 10-layer architecture with very small kernels: 3x32x32-300C3-MP2-300C2-MP2-300C3-MP2-300C2-MP2-300N-100N-10N. Just like for MNIST, the initial learning rate 0.001 decays by a factor of 0.993 after every epoch. Transforming CIFAR color images to gray scale reduces input layer complexity but increases error rates. Hence we stick to the original color images. As for MNIST, augmenting the training set with randomly (by at most 5%) translated images greatly decreases the error from 28% to 20% (the NN-inherent local translation invariance by itself is not sufficient). By additional scaling (up to ?15%), rotation (up to ?5 ? ), and up to ?15% translation, the individual net errors decrease by another 3% (Tab. 5). The above small maximal bounds prevent loss of too much information leaked beyond the 32 ? 32 pixels rectangle. We repeat the experiment with different random initializations and compute mean and standard deviation of the error, which is rather small for original images, showing that our DNN are robust. Our MCDNN obtains a very low error rate of 11.21%, greatly rising the bar for this benchmark.</p><p>The confusion matrix <ref type="figure" target="#fig_3">(Figure 4)</ref> shows that the MCDNN almost perfectly separates animals from artifacts, except for planes and birds, which seems natural, although humans easily distinguish almost all the incorrectly classified images, even if many are cluttered or contain only parts of the objects/animals (see false positive and false negative images in <ref type="figure" target="#fig_3">Figure 4</ref>). There are many confusions between different animals; the frog class collects most false positives from other animal classes, with very few false negatives. As expected, cats are hard to tell from dogs, collectively causing 15.25% of the errors.</p><p>The MCDNN with 8 columns (four trained on original data and one trained for each preprocessing used also for traffic signs) reaches a low 11.21% error rate, far better than any other algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">NORB</head><p>We test a MCDNN with four columns on NORB (jittered-cluttered) <ref type="bibr" target="#b20">[21]</ref>, a collection of stereo images of 3D models ( <ref type="figure" target="#fig_4">Figure 5</ref>). The objects are centrally placed on randomly chosen backgrounds, and there is also cluttering from a peripherally placed second object. This database is designed for experimenting with 3D object recognition from shape. It contains images of 50 toys belonging to 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. The objects were imaged by two cameras under 6 lighting conditions, 9 elevations (30 to 70 degrees every 5 degrees), and 18 azimuths (0 to 340 every 20 degrees). The training set has 10 folds of 29160 images each for a total of 291600 images; the testing set consists of two folds totalizing 58320 images.   <ref type="bibr" target="#b31">[32]</ref> No preprocessing is used for this dataset. We scale down images from the original 108x108 to 48x48 pixels. This size is big enough to preserve the details present in images and small enough to allow fast training. We perform two rounds of experiments, using only the first two folds (to compare with previous results that do not use the entire training data) and using all training data.</p><p>We tested several distortion parameters with small nets and found that maximum rotation of 15 ? , maximum translation of 15% and maximum scaling of 15% are good choices, hence we use them for all NORB experiments.</p><p>To compare to previous results, we first train only on the first 2-folds of the data. The net architecture is deep, but has few maps per layer: 2x48x48-50C5-MP2-50C5-MP2-50C4-MP2-300N-100N-6N. The learning rate setup is: eta start 0.001; eta factor 0.95; eta stop 0.000003. Due to small net size, training is fast at 156s/epoch for 114 epochs. Testing one sample requires 0.5ms. Even when we use less data to train, the MCDNN greatly improves the state of the art from 5% to 3.57% <ref type="table" target="#tab_5">(Table 6</ref>).</p><p>Our method is fast enough to process the entire training set though. We use the same architecture but double the number of maps when training with all 10 folds: 2x48x48-100C5-MP2-100C5-MP2-100C4-MP2-300N-100N-6N. The learning rate setup remains the same. Training time increases to 34min/epoch because the net is bigger, and we use five times more data. Testing one sample takes 1.3ms. All of this pays off, resulting in a very low 2.70% error rate, further improving the state of the art.</p><p>Although NORB has only six classes, training and test instances sometimes differ greatly, making classification hard. More than 50% of the errors are due to confusions between cars and trucks. Considering second predictions, too, the error rate drops from 2.70% to 0.42%, showing that 84% of the errors are associated with a correct second prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This is the first time human-competitive results are reported on widely used computer vision benchmarks. On many other image classification datasets our MCDNN improves the state-of-the-art by 30-80% (Tab. 7). We drastically improve recognition rates on MNIST, NIST SD 19, Chinese characters, traffic signs, CI-FAR10 and NORB. Our method is fully supervised and does not use any additional unlabeled data source. Single DNN already are sufficient to obtain new state-of-the-art results; combining them into MCDNNs yields further dramatic performance boosts. see <ref type="table" target="#tab_3">Table 4</ref> see <ref type="table" target="#tab_3">Table 4</ref> 30-80 HWDB1.0 on. The confusion matrix of the 62 characters task <ref type="figure">(Fig. 6)</ref> shows that most of the errors are due to confusions between digits and letters and between lower-and upper-case letters. <ref type="figure">Figure 6</ref>: Confusion matrix of the NIST SD 19 MCDNN trained on the 62-class task: correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown as relative percentages of the total error number. For convenience, class labels are written beneath the errors. Errors below 1% of the total error number are not detailed.</p><p>Not very surprisingly, the confusion matrix for the digit task ( <ref type="figure" target="#fig_5">Fig. 7)</ref> shows that confusions between fours and nines are the most common error source. For the 52 letter task (case sensitive) the confusion matrix ( <ref type="figure" target="#fig_6">Fig. 8)</ref> shows that the MCDNN has mainly problems with upper-and lower-case confusions of the same letter. Other hard-to-distinguish classes are: 'q' and 'g', 'l' and 'i'.  For the upper-case letter task the confusion matrix ( <ref type="figure" target="#fig_7">Figure 9</ref>) shows that the MCDNN has problems with letters of similar shape, i.e. 'D', and 'O', 'V' and 'U' etc. The total error of 1.82% is very low though. For the lower-case letter task the confusion matrix ( <ref type="figure" target="#fig_0">Fig. 10)</ref> shows that like with upper-case letters, the MCDNN has problems with letters of similar shapes, i.e. 'g', and 'q', 'v' and 'u' etc. But the total error is much higher (7.47%) than for the upper-case letters task.</p><p>For the merged-case letter task (37 classes) the confusion matrix ( <ref type="figure" target="#fig_0">Figure 11</ref>) shows that the MCDNN has mostly problems with letters of similar shapes, i.e. 'l', and 'i'. All upper-lower-case confusions of identical letters from the 52 class task vanish, the error shrinks by a factor of almost three down to 7.99%.</p><p>The experiments on different subsets of the 62 character task clearly show that it is very hard to distinguish between small and capital letters. Also, digits 0 and 1 are hard to separate from letters O and I. Many of these problems could be alleviated by incorporating context where possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Traffic signs</head><p>High contrast variation among the images calls for normalization. We test the following standard contrast normalizations:</p><p>? Image Adjustment (Imadjust) increases image contrast by mapping pixel intensities to new values such that 1% of the data is saturated at low and high intensities <ref type="bibr" target="#b23">[24]</ref>.</p><p>? Histogram Equalization (Histeq) enhances contrast by transforming pixel intensities such that the output image histogram is roughly uniform <ref type="bibr" target="#b23">[24]</ref>.</p><p>? Adaptive Histogram Equalization (Adapthisteq) operates (unlike Histeq) on tiles rather than the entire image, we tiled the image in 8 nonoverlapping regions of 6x6 pixels. Each tile's contrast is enhanced such that its histogram becomes roughly uniform <ref type="bibr" target="#b23">[24]</ref>. ? Contrast Normalization (Conorm) enhances edges, filtering the input image by a difference of Gaussians, using a filter size of 5x5 pixels <ref type="bibr" target="#b32">[33]</ref>.</p><p>Note that the above normalizations, except Conorm, are performed in a color space with image intensity as one of its components. For this purpose we transform the image from RGB-to Lab-space, then perform normalization, then transform the result back to RGB-space. The effect of the four different normalizations is summarized in <ref type="figure" target="#fig_0">Figure 12</ref>, where histograms of pixel intensities together with original and normalized images are shown.</p><p>The DNN have three maps for the input layer, one for each color channel (RGB). The rest of the net architecture is detailed in <ref type="table" target="#tab_8">Table 8</ref>. We use a 10-layer architecture with very small max-pooling kernels.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">CIFAR10</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) DNN architecture. (b) MCDNN architecture. The input image can be preprocessed by P 0 ? P n?1 blocks. An arbitrary number of columns can be trained on inputs preprocessed in different ways. The final predictions are obtained by averaging individual predictions of each DNN. (c) Training a DNN. The dataset is preprocessed before training, then, at the beginning of every epoch, the images are distorted (D block). See text for more explanations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Handwritten digits from the training set (top row) and their distorted versions after each epoch (second to fifth row). (b) The 23 errors of the MCDNN, with correct label (up right) and first and second best predictions (down left and right). (c) DNN architecture for MNIST. Output layer not drawn to scale; weights of fully connected layers not displayed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Preprocessed images, from top to bottom: original, Imadjust, Histeq, Adapthisteq, Conorm. (b) The 68 errors of the MCDNN, with correct label (left) and first and second best predictions (middle and right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Confusion matrix for the CIFAR10 MCDNN: correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown both as relative percentages of the total error number, and in absolute value. Left -images of all birds classified as planes. Right -images of all planes classified as birds. Confusion sub-matrix for animal classes has a gray backround.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Twenty NORB stereo images (left image -up, right image -down).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Confusion matrix for the NIST SD 19 MCDNN trained on the digit task: correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown as relative percentages of the total error number. Class labels are shown beneath the errors. Errors below 1% of the total error number are shown as dots without any details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Confusion matrix for the NIST SD 19 MCDNN trained on all 52 letters: correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown as relative percentages of the total error number. Class labels are shown beneath the errors. Errors below 1% of the total error number are shown as dots without any details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Confusion matrix for the NIST SD 19 MCDNN trained on uppercase letters: correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown as relative percentages of the total error number. Class labels are shown beneath the errors. Errors below 1% of the total error number are shown as dots without any details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :</head><label>10</label><figDesc>Confusion matrix for the NIST SD 19 MCDNN trained on lowercase letters: correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown as relative percentages of the total error number. Class labels are shown beneath the errors. Errors below 1% of the total error number are shown as dots without any details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 11 :</head><label>11</label><figDesc>Confusion matrix for the NIST SD 19 MCDNN trained on merged letters (37 classes): correct labels on vertical axis; detected labels on horizontal axis. Square areas are proportional to error numbers, shown as relative percentages of the total error number. Class labels are shown beneath the errors. Errors below 1% of the total error number are shown as dots without any details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 12 :</head><label>12</label><figDesc>Preprocessing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 :</head><label>13</label><figDesc>Confusion matrix for the NORB: correct labels on vertical axis; detected labels on horizontal axis.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Test error rate [%] of the 35 NNs trained on MNIST. Wxx -width of the character is normalized to xx pixels</figDesc><table><row><cell>Trial</cell><cell>W10</cell><cell>W12</cell><cell>W14</cell><cell>W16</cell><cell>W18</cell><cell>W20</cell><cell>ORIGINAL</cell></row><row><cell>1</cell><cell>0.49</cell><cell>0.39</cell><cell>0.40</cell><cell>0.40</cell><cell>0.39</cell><cell>0.36</cell><cell>0.52</cell></row><row><cell>2</cell><cell>0.48</cell><cell>0.45</cell><cell>0.45</cell><cell>0.39</cell><cell>0.50</cell><cell>0.41</cell><cell>0.44</cell></row><row><cell>3</cell><cell>0.59</cell><cell>0.51</cell><cell>0.41</cell><cell>0.41</cell><cell>0.38</cell><cell>0.43</cell><cell>0.40</cell></row><row><cell>4</cell><cell>0.55</cell><cell>0.44</cell><cell>0.42</cell><cell>0.43</cell><cell>0.39</cell><cell>0.50</cell><cell>0.53</cell></row><row><cell>5</cell><cell>0.51</cell><cell>0.39</cell><cell>0.48</cell><cell>0.40</cell><cell>0.36</cell><cell>0.29</cell><cell>0.46</cell></row><row><cell>avg.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>08</cell><cell>0.47?0.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">35-net average error: 0.44?0.06</cell><cell></cell><cell></cell></row><row><cell>5 columns</cell><cell>0.37</cell><cell>0.26</cell><cell>0.32</cell><cell>0.33</cell><cell>0.31</cell><cell>0.26</cell><cell>0.46</cell></row><row><cell>MCDNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">35-net MCDNN: 0.23%</cell><cell></cell><cell></cell></row></table><note>0.52?0.05 0.44?0.05 0.43?0.03 0.40?0.02 0.40?0.06 0.39?0.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on MNIST dataset.</figDesc><table><row><cell>Method</cell><cell cols="2">Paper Error rate[%]</cell></row><row><cell>CNN</cell><cell>[35]</cell><cell>0.40</cell></row><row><cell>CNN</cell><cell>[28]</cell><cell>0.39</cell></row><row><cell>MLP</cell><cell>[5]</cell><cell>0.35</cell></row><row><cell>CNN committee</cell><cell>[6]</cell><cell>0.27</cell></row><row><cell>MCDNN</cell><cell>this</cell><cell>0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Average test error rate [%] of MCDNN trained on y preprocessed datasets.</figDesc><table><row><cell cols="3">y # MCDNN Average Error[%]</cell></row><row><cell>1</cell><cell>7</cell><cell>0.33?0.07</cell></row><row><cell>2</cell><cell>21</cell><cell>0.27?0.02</cell></row><row><cell>3</cell><cell>35</cell><cell>0.27?0.02</cell></row><row><cell>4</cell><cell>35</cell><cell>0.26?0.02</cell></row><row><cell>5</cell><cell>21</cell><cell>0.25?0.01</cell></row><row><cell>6</cell><cell>7</cell><cell>0.24?0.01</cell></row><row><cell>7</cell><cell>1</cell><cell>0.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Average error rates of MCDNN for all experiments, plus results from the literature. * case insensitive</figDesc><table><row><cell>Data</cell><cell>MCDNN</cell><cell cols="2">Published results</cell></row><row><cell>(task)</cell><cell>error [%]</cell><cell cols="2">Error[%] and paper</cell></row><row><cell>all (62)</cell><cell>11.63</cell><cell></cell><cell></cell></row><row><cell>digits (10)</cell><cell>0.77</cell><cell>3.71 [12]</cell><cell>1.88 [25]</cell></row><row><cell>letters (52)</cell><cell>21.01</cell><cell>30.91[16]</cell><cell></cell></row><row><cell>letters* (26)</cell><cell>7.37</cell><cell>13.00 [4]</cell><cell>13.66[16]</cell></row><row><cell>merged (37)</cell><cell>7.99</cell><cell></cell><cell></cell></row><row><cell>uppercase (26)</cell><cell>1.83</cell><cell>10.00 [4]</cell><cell>6.44 [9]</cell></row><row><cell>lowercase (26)</cell><cell>7.47</cell><cell cols="2">16.00 [4] 13.27 [16]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Error rates, averages and standard deviations for 10 runs of a 10 layer DNN on the CIFAR10 test set. The nets in the first row are trained on preprocessed images (see traffic sign preprocessing), whereas those in the second row are trained on original images.</figDesc><table><row><cell>preprocessing</cell><cell>errors for 8 runs [%]</cell><cell></cell><cell>mean[%]</cell></row><row><cell>yes</cell><cell>16.47 19.20 19.72</cell><cell>20.31</cell><cell>18.93 ? 1.69</cell></row><row><cell>no</cell><cell>15.63 15.85 16.13</cell><cell>16.05</cell><cell>15.91 ? 0.22</cell></row><row><cell></cell><cell cols="2">8-net average error: 17.42?1.96%</cell><cell></cell></row><row><cell></cell><cell cols="2">8-net MCDNN error: 11.21%</cell><cell></cell></row><row><cell></cell><cell cols="3">previous state of the art: 18.50% -[8]; 19.51% -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Error rates, averages and standard deviations over 4 runs of a 9 layer DNN on the NORB test set.</figDesc><table><row><cell>training</cell><cell>errors for 4 runs [%]</cell><cell>mean[%]</cell></row><row><cell>set size</cell><cell></cell><cell></cell></row><row><cell>first</cell><cell cols="2">4.49 4.71 4.82 4.85 4.72 ? 0.16</cell></row><row><cell>2 folds</cell><cell cols="2">4-net MCDNN error: 3.57%</cell></row><row><cell>all</cell><cell cols="2">3.32 3.18 3.73 3.36 3.40 ? 0.23</cell></row><row><cell>10 folds</cell><cell cols="2">4-net MCDNN error: 2.70%</cell></row><row><cell cols="3">previous state of the art: 5.00% -[8]; 5.60% -</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Results and relative improvements on different datasets.</figDesc><table><row><cell>Dataset</cell><cell>Best result</cell><cell>MCDNN</cell><cell>Relative</cell></row><row><cell></cell><cell>of others [%]</cell><cell>[%]</cell><cell>improv. [%]</cell></row><row><cell>MNIST</cell><cell>0.39</cell><cell>0.23</cell><cell>41</cell></row><row><cell>NIST SD 19</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>10 layer DNN architecture used for recognizing traffic signs.</figDesc><table><row><cell>Layer</cell><cell>Type</cell><cell># maps &amp; neurons</cell><cell>kernel</cell></row><row><cell>0</cell><cell>input</cell><cell>3 maps of 48x48 neurons</cell><cell></cell></row><row><cell>1</cell><cell>convolutional</cell><cell>100 maps of 42x42 neurons</cell><cell>7x7</cell></row><row><cell>2</cell><cell>max pooling</cell><cell>100 maps of 21x21 neurons</cell><cell>2x2</cell></row><row><cell>3</cell><cell>convolutional</cell><cell>150 maps of 18x18 neurons</cell><cell>4x4</cell></row><row><cell>4</cell><cell>max pooling</cell><cell>150 maps of 9x9 neurons</cell><cell>2x2</cell></row><row><cell>5</cell><cell>convolutional</cell><cell>250 maps of 6x6 neurons</cell><cell>4x4</cell></row><row><cell>6</cell><cell>max pooling</cell><cell>250 maps of 3x3 neurons</cell><cell>2x2</cell></row><row><cell>9</cell><cell>fully connected</cell><cell>300 neurons</cell><cell>1x1</cell></row><row><cell>10</cell><cell>fully connected</cell><cell>43 neurons</cell><cell>1x1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>10 layer DNN architecture used for CIFAR 10.</figDesc><table><row><cell>Layer</cell><cell>Type</cell><cell># maps &amp; neurons</cell><cell>kernel</cell></row><row><cell>0</cell><cell>input</cell><cell>3 maps of 32x32 neurons</cell><cell></cell></row><row><cell>1</cell><cell>convolutional</cell><cell>300 maps of 30x30 neurons</cell><cell>3x3</cell></row><row><cell>2</cell><cell>max pooling</cell><cell>300 maps of 15x15 neurons</cell><cell>2x2</cell></row><row><cell>3</cell><cell>convolutional</cell><cell>300 maps of 14x14 neurons</cell><cell>2x2</cell></row><row><cell>4</cell><cell>max pooling</cell><cell>300 maps of 7x7 neurons</cell><cell>2x2</cell></row><row><cell>5</cell><cell>convolutional</cell><cell>300 maps of 6x6 neurons</cell><cell>2x2</cell></row><row><cell>6</cell><cell>max pooling</cell><cell>300 maps of 3x3 neurons</cell><cell>2x2</cell></row><row><cell>7</cell><cell>convolutional</cell><cell>300 maps of 2x2 neurons</cell><cell>2x2</cell></row><row><cell>8</cell><cell>max pooling</cell><cell>300 maps of 1x1 neurons</cell><cell>2x2</cell></row><row><cell>9</cell><cell>fully connected</cell><cell>300 neurons</cell><cell>1x1</cell></row><row><cell>10</cell><cell>fully connected</cell><cell>100 neurons</cell><cell>1x1</cell></row><row><cell>11</cell><cell>fully connected</cell><cell>10 neurons</cell><cell>1x1</cell></row><row><cell>5.1.4 NORB</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1991 486DX-33 MHz, 2011 i7-990X 3.46 GHz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hierarchical Neural Networks for Image Interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2766</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Greedy layer-wise training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Parallel and serial neural mechanisms for visual search in macaque area V4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Bichot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Desimone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="529" to="534" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An implicit segmentation-based method for recognition of handwritten strings of characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Cavalin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>De Souza Britto</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bortolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E S</forename><surname>De Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SAC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="836" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural network committees for handwritten character classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1250" to="1254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Flexible, high performance convolutional neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1237" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The importance of encoding versus training with sparse coding and vector quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Overfitting in the selection of classifier ensembles: a comparative study between pso and ga</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maupin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Genetic and Evolutionary Computation</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1423" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Why does unsupervised pretraining help deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><forename type="middle">M P V</forename><surname>Dumitru Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="625" to="660" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Supervised Learning of Fuzzy ARTMAP Neural Networks Through Particle Swarm Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henniges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="27" to="60" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">NIST special database 19 -Handprinted forms and characters database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Grother</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
		<respStmt>
			<orgName>National Institute of Standards and Thechnology (NIST</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Field Guide to Dynamical Recurrent Neural Networks</title>
		<editor>S. C. Kremer and J. F. Kolen</editor>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Receptive fields, binocular interaction, and functional architecture in the cat&apos;s visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">160</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="106" to="154" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unconstrained handwritten character recognition using metaclasses of characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Koerich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Kalva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intl. Conf. on Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="542" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Self-Organization and Associative Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Une proc?dure d&apos;apprentissage pour r?seau a seuil asymmetrique (a learning scheme for asymmetric threshold networks)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Cognitiva 85</title>
		<meeting>Cognitiva 85<address><addrLine>Paris, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="599" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998-11" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning methods for generic object recognition with invariance to pose and lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning algorithms for classification: A comparison on handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">A</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sackinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: The Statistical Mechanics Perspective</title>
		<editor>J. H. Oh, C. Kwon, and S. Cho</editor>
		<imprint>
			<publisher>World Scientific</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="261" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Chinese Handwriting Recognition Contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Chinese Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
		<idno>MATLAB. version 7.10.0</idno>
	</analytic>
	<monogr>
		<title level="j">The MathWorks Inc</title>
		<imprint>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating accurate multi-class probabilities with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Milgram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabourin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. on Neural Networks</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1906" to="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><forename type="middle">B</forename><surname>Fu Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Computer Vision and Pattern Recognition Conference</title>
		<meeting>of Computer Vision and Pattern Recognition Conference</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised learning of invariant feature hierarchies with applications to object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</title>
		<meeting>Computer Vision and Pattern Recognition Conference (CVPR&apos;07)</meeting>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient learning of sparse representations with an energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Poultney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS 2006)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hierarchical models of object recognition in cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat. Neurosci</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1019" to="1025" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel distributed processing: explorations in the microstructure of cognition</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighborhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>of the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Evaluation of pooling operations in convolutional architectures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scherer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multi-scale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Joint Conference on Neural Networks (IJCNN&apos;11)</title>
		<meeting>International Joint Conference on Neural Networks (IJCNN&apos;11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The German Traffic Sign Recognition Benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Performance and scalability of gpu-based convolutional neural networks. Parallel, Distributed, and Network-Based Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Strigl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kofler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Podlipnig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Euromicro Conference on</title>
		<imprint>
			<biblScope unit="volume">0</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale object recognition with CUDA-accelerated hierarchical neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Uetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Computing and Intelligent Systems (ICIS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974" />
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Receptive fields of single neurones in the cat&apos;s striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Wiesel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Hubel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Physiol</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="574" to="591" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">How patterned neural connections can be set up by selforganization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Willshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malsburg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. R. Soc. London B</title>
		<imprint>
			<biblScope unit="volume">194</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="431" to="445" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
