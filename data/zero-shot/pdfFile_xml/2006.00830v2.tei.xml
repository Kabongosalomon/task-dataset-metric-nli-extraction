<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Aggregate Representations for Long-Range Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fadime</forename><surname>Sener</surname></persName>
							<email>sener@cs.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipika</forename><surname>Singhania</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yao</surname></persName>
							<email>ayao@comp.nus.edu.sg</email>
							<affiliation key="aff1">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Aggregate Representations for Long-Range Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>action anticipation, temporal aggregation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Future prediction, especially in long-range videos, requires reasoning from current and past observations. In this work, we address questions of temporal extent, scaling, and level of semantic abstraction with a flexible multi-granular temporal aggregation framework. We show that it is possible to achieve state of the art in both next action and dense anticipation with simple techniques such as max-pooling and attention. To demonstrate the anticipation capabilities of our model, we conduct experiments on Breakfast, 50Salads, and EPIC-Kitchens datasets, where we achieve state-of-the-art results. With minimal modifications, our model can also be extended for video segmentation and action recognition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We tackle long-range video understanding, specifically anticipating not-yet observed but upcoming actions. When developing intelligent systems, one needs not only to recognize what is currently taking place -but also predict what will happen next. Anticipating human actions is essential for applications such as smart surveillance, autonomous driving, and assistive robotics.</p><p>While action anticipation is a niche (albeit rapidly growing) area, the key issues that arise are germane to long-range video understanding as a whole. How should temporal or sequential relationships be modelled? What temporal extent of information and context needs to be processed? At what temporal scale should they be derived, and how much semantic abstraction is required? The answers to these questions are not only entangled with each other but also depend very much on the videos being analyzed. Here, one needs to distinguish between clipped actions, e.g. of UCF101 <ref type="bibr" target="#b31">[32]</ref>, versus the multiple actions in long video streams, e.g. of the Breakfast <ref type="bibr" target="#b17">[18]</ref>. In the former, the actions and video clips are on the order of a few seconds, while in the latter, it is several minutes. As such, temporal modelling is usually not necessary for simple action recognition <ref type="bibr" target="#b12">[13]</ref>, but more relevant for understanding complex activities <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Temporal models that are built into the architecture <ref type="bibr">[6,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref> are generally favoured because they allow frameworks to be learned end-to-end. However, this means that the architecture also dictates the temporal extent that can be accounted for. This tends to be short, either due to difficulties in memory retention or model size. As a result, the context for anticipation can only be drawn from a limited extent of recent observations, usually on the order of seconds <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b25">26]</ref>. This, in turn, limits the temporal horizon and granularity of the prediction.</p><p>One way to ease the computational burden, especially under longer temporal extents, is to use higher-level but more compact feature abstractions, e.g. by using detected objects, people <ref type="bibr" target="#b41">[42]</ref> or sub-activity labels <ref type="bibr">[1,</ref><ref type="bibr" target="#b14">15]</ref> based on the outputs of video segmentation algorithms <ref type="bibr" target="#b28">[29]</ref>. Such an approach places a heavy load on the initial task of segmentation and is often difficult to train end-toend. Furthermore, since labelling and segmenting actions from video are difficult tasks, their errors may propagate onwards when anticipating future actions.</p><p>Motivated by these questions of temporal modelling, extent, scaling, and level of semantic abstraction, we propose a general framework for encoding long-range video. We aim for flexibility in frame input, i.e. ranging from low-level visual features to high-level semantic labels, and the ability to meaningfully integrate recent observations with long-range context in a computationally efficient way. To do so, we split video streams into snippets of equal length and max-pool the frame features within the snippets. We then create ensembles of multi-scale feature representations that are aggregated bottom-up based on scaling and temporal extent. Temporal aggregation <ref type="bibr" target="#b15">[16]</ref> is a form of summarization used in database systems. Our framework is loosely analogous as it summarizes the past observations through aggregation, so we name it "temporal aggregates". We summarize our main contributions as follows:</p><p>-We propose a simple and flexible single-stage framework of multi-scale temporal aggregates for videos by relating recent to long-range observations. -Our representations can be applied to several video understanding tasks; in addition to action anticipation, it can be used for recognition and segmentation with minimal modifications and is able to achieve competitive results. -Our model has minimal constraints regarding the type of anticipation (dense or next action), type of the dataset (instructional or daily activities), and type of input features (visual features or frame-level labels). -We conduct experiments on Breakfast <ref type="bibr" target="#b17">[18]</ref>, 50Salads <ref type="bibr" target="#b33">[34]</ref> and EPIC-Kitchens <ref type="bibr">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Action recognition has advanced significantly with deep networks in recent years. Notable works include two steam networks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b39">40]</ref>, 3D convolutional networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr">3]</ref>, and RNNs <ref type="bibr">[7,</ref><ref type="bibr" target="#b43">44]</ref>. These methods have been designed to encode clips of a few seconds and are typically applied to the classification of trimmed videos containing a single action <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b13">14]</ref>. In our paper, we work with long untrimmed sequences of complex activities. Such long videos are not simply a composition of independent short actions, as the actions are related to each other with sequence dynamics. Various models for complex activity understanding have been addressed before <ref type="bibr">[6,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b29">30]</ref>; these approaches are designed to work on instructional videos by explicitly modelling their sequence dynamics. These models are not flexible enough to be extended to daily activities with loose orderings. Also, when only partial observations are provided, e.g. for anticipation, these models cannot be trained in a single stage.</p><p>Action anticipation aims to forecast actions before they occur. Prior works in immediate anticipation were initially limited to movement primitives like reaching <ref type="bibr" target="#b16">[17]</ref> or interactions such as hugging <ref type="bibr" target="#b37">[38]</ref>. <ref type="bibr" target="#b23">[24]</ref> presents a model for predicting both the next action and its starting position. <ref type="bibr">[5]</ref> presents a large daily activities dataset, along with a challenge for anticipating the next action one second before occurrence. <ref type="bibr" target="#b25">[26]</ref> proposes next action anticipation from recent observations. Recently, <ref type="bibr" target="#b9">[10]</ref> proposed using an LSTM to summarize the past and another LSTM for future prediction. These works assume near-past information, whereas we make use of long-range past.</p><p>Dense anticipation predicts actions multiple steps into the future. Previous methods <ref type="bibr">[1,</ref><ref type="bibr" target="#b14">15]</ref> to date require having already segmented temporal observations. Different than these, our model can perform dense anticipation in a single stage without any pre-segmented nor labelled inputs.</p><p>The role of motion and temporal dynamics has been well-explored for video understanding, though the focus has been on short clips <ref type="bibr" target="#b21">[22,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b12">13]</ref>. Some works use longer-term temporal contexts, though still in short videos <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b34">35]</ref>. Recently, Wu et al. <ref type="bibr" target="#b41">[42]</ref> proposed integrating long-term features with 3D CNNs in short videos and showed the importance of temporal context for action recognition. Our model is similar in spirit to <ref type="bibr" target="#b41">[42]</ref> in that we couple the recent with the long-range past using attention. One key difference is that we work with ensembles of multiple scalings and granularities, whereas <ref type="bibr" target="#b41">[42]</ref> work at a single frame-level granularity. As such, we can handle long videos up to tens of minutes, while they are only able to work on short videos. Recently, Feichtenhofer et al. <ref type="bibr">[9]</ref> proposed SlowFast networks, which, similar to our model, encode time-wise multi-scale representations. These approaches are limited to short videos and cannot be extended to minutes-long videos due to computational constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Representations</head><p>We begin by introducing the representations, which are inputs to the building blocks of our framework, see <ref type="figure">Fig. 1</ref>. We had two rationales when designing our network. First, we relate recent to long-range observations, since some past actions directly determine future actions. Second, to represent recent and longrange past at various granularities, we pool snippets over multiple scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Pooling</head><p>For a video of length T , we denote the feature representation of a single video frame indexed at time t as f t ? R D , 1 ? t ? T . f t can be derived from lowlevel features, such as IDT <ref type="bibr" target="#b38">[39]</ref>   <ref type="figure">Fig. 1</ref>. Model overview: In this example we use 3 scales for computing the "spanning past" features SK 1 , SK 2 , SK 3 , and 2 starting points to compute the "recent past" features, Ri 1 , Ri 2 , by max-pooling over the frame features in each snippet. Each recent snippet is coupled with all the spanning snippets in our Temporal Aggregation Block (TAB). An ensemble of TAB outputs is used for dense or next action anticipation.</p><p>sub-activity labels derived from segmentation algorithms. To reduce computational load, we work at a snippet-level and define a snippet feature F ij;K as the concatenation of max-pooled features from K snippets, where snippets are partitioned consecutively from frames i to j:</p><formula xml:id="formula_0">F ij;K = [F i,i+k , F i+k+1,i+2k , ..., F j?k+1,j ], where (F p,q ) d = max p?t?q {f t } d , 1 ? d ? D, k = (j ? i) /K.<label>(1)</label></formula><p>Here, F p,q indicates the maximum over each dimension d of the frame features in a given snippet between frames p and q, though it can be substituted with other alternatives. In the literature, methods representing snippets or segments of frames range from simple sampling and pooling strategies to more complex representations such as learned pooling <ref type="bibr" target="#b22">[23]</ref> and LSTMs <ref type="bibr" target="#b26">[27]</ref>. Especially for long snippets, it is often assumed that a learned representation is necessary <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b19">20]</ref>, though their effectiveness over simple pooling is still controversial <ref type="bibr" target="#b39">[40]</ref>. The learning of novel temporal pooling approaches goes beyond the scope of this work and is an orthogonal line of development. We verify established methods (see Sec. 5.2) and find that a simple max-pooling is surprisingly effective and sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Recent vs. Spanning Representations</head><p>Based on different start and end frames i and j and number of snippets K, we define two types of snippet features: "recent" features {R} from recent observations, and "spanning" features {S} drawn from the entire video. The recent snippets cover a couple of seconds (or up to a minute, depending on the temporal granularity) before the current time point while spanning snippets refer to the long-range past and may last up to ten minutes. For "recent" snippets, the end frame j is fixed to the current time point t, and the number of snippets is fixed to K R . The recent snippet features R can be defined as a feature bank of snippet features with different start frames i, i.e.</p><formula xml:id="formula_1">R = {F i1t;K R , F i2t;K R , ..., F i R t;K R } = {R i1 , R i2 , ..., R i R },<label>(2)</label></formula><p>where R i ? R D?K R is a shorthand to denote F i t;K R , since endpoint t and number of snippets K R are fixed. In <ref type="figure">Fig. 1</ref> we use two starting points to compute the "recent" features and represent each with K R = 3 snippets ( &amp; ). For "spanning" snippets, i and j are fixed to the start of the video and current time point, i.e. i = 0, j = t. Spanning snippet features S are defined as a feature bank of snippet features with varying number of snippets K, i.e.</p><formula xml:id="formula_2">S = {F 0 t;K1 , F 0 t;K2 , ..., F 0 t;K S } = {S K1 , S K2 , ..., S K S },<label>(3)</label></formula><p>where S K ? R D?K is a shorthand for F 0 t;K . In <ref type="figure">Fig. 1</ref> we use three scales to compute the "spanning" features with K = {7, 5, 3} ( , &amp; ). Key to both types of representations is the ensemble of snippet features from multiple scales. We achieve this by varying the number of snippets K for the spanning past. For the recent past, it is sufficient to keep the number of snippets K R fixed, and vary only the start point i, due to redundancy between R and S for the snippets that overlap. For our experiments, we work with snippets ranging from seconds to several minutes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Framework</head><p>In <ref type="figure">Fig. 2</ref> we present an overview of the components used in our framework, which we build in a bottom-up manner, starting with the recent and spanning features R and S, which are coupled with non-local blocks (NLB) (Sec. 4.1) within coupling blocks (CB) (Sec. 4.2). The outputs of the CBs from different scales are then aggregated inside temporal aggregation blocks (TAB) (Sec. 4.3). Outputs of different TABs can then be chained together for either next action anticipation or dense anticipation (Secs. 5.3, 5.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Non-Local Blocks (NLB)</head><p>We apply non-local operations to capture relationships amongst the spanning snippets and between spanning and recent snippets. Non-local blocks <ref type="bibr" target="#b40">[41]</ref> are a flexible way to relate features independently from their temporal distance and thus capture long-range dependencies. We use the modified non-local block from <ref type="bibr" target="#b41">[42]</ref>, which adds layer normalization <ref type="bibr">[2]</ref> and dropout <ref type="bibr" target="#b32">[33]</ref> to the original one in <ref type="bibr" target="#b39">[40]</ref>. <ref type="figure">Fig. 2</ref> (left) visualizes the architecture of the block, the operation of which we denote as NLB(?, ?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Coupling Block (CB)</head><p>Based on the NLB, we define attention-reweighted spanning and recent outputs:</p><formula xml:id="formula_3">S K = N LB(S K , S K ) and R i,K = N LB(S K , R i ).<label>(4)</label></formula><p>R i,K is coupled with either R i or S K via concatenation and a linear layer. This results in the fixed-length representations R i,K and S i,K , where i is the starting point of the recent snippet and K is the scale of the spanning snippet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Temporal Aggregation Block (TAB)</head><p>The final representation for recent and spanning past is computed by aggregating outputs from multiple CBs. For the same recent starting point i, we concatenate R i,K1 , ..., R i,K S for all spanning scales and pass the concatenation through a linear layer to compute R i . The final spanning representation S i is a max over all S i,K1 , ..., S i,K S . We empirically find that taking the max outperforms other alternatives like linear layers and/or concatenation for the spanning past (Sec. 5.2). TAB outputs, by varying recent starting points {i} and scales of spanning snippets {K}, are multi-granular video representations that aggregate and encode both the recent and long-range past. We name these temporal aggregate representations. <ref type="figure">Fig.1</ref> shows an example with 2 recent starting points and 3 spanning scales. These representations are generic and can be applied in various video understanding tasks (see Sec. 4.4) from long streams of video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Prediction Model</head><p>Classification: For single-label classification tasks such as next action anticipation, temporal aggregate representations can be used directly with a classification layer (linear + softmax). A cross-entropy loss based on ground truth labels Y can be applied to the predictions? i , where Y is either the current action label for recognition, or the next action label for next action prediction (see <ref type="figure" target="#fig_0">Fig. 3</ref>). When the individual actions compose a complex activity (e.g. "take bowl" and "pour milk" as part of "making cereal" in Breakfast <ref type="bibr" target="#b17">[18]</ref>), we can add an additional loss based on the complex activity label Z. Predicting Z as an auxiliary task helps with anticipation. For this we concatenate S i1 , . . . , S i R from all TABs and pass them through a classification layer to obtain?. The overall loss is a sum of the cross entropies over the action and complex activity labels:</p><formula xml:id="formula_4">L cl = L comp + L action = ? N Z n=1 Z n log(?) n ? R r=1 N Y n=1 Y n log(? ir ) n ,<label>(5)</label></formula><p>where i r is one of the R recent starting points, and N Y and N Z are the total number of actions and complex activity classes respectively. During inference, the predicted scores are summed for a final prediction, i.e.? = max n ( R r=1? ir ) n . We frame sequence segmentation as a classification task and predict framelevel action labels of complex activities. Multiple sliding windows with fixed start and end times are generated and then classified into actions using Eq.5.</p><p>Sequence prediction: The dense anticipation task predicts frame-wise actions of the entire future sequence. Previously, <ref type="bibr">[1]</ref> predicted future segment labels via classification and regressed the durations. We opt to estimate both via classification. The sequence duration is discretized into N D intervals and represented as one-hot encodings D ? {0, 1} N D . For dense predictions, we perform multistep estimates. We first estimate the current action and complex activity label, as per Eq. 6. The current duration D is then estimated via a classification layer applied to the concatenation of recent temporal aggregates R i1 , ..., R i R .</p><p>For future actions, we concatenate all recent and spanning temporal aggregates R i1 , ..., R i R and S i1 , ..., S i R and the classification layer outputs? i1 , ...,? i R , and pass the concatenation through a linear layer before feeding the output to a one-layer LSTM. The LSTM predicts at each step m an action vector? m and a duration vectorD m (see <ref type="figure" target="#fig_0">Fig. 3</ref>). The dense anticipation loss is a sum of the cross-entropies over the current action, its duration, future actions and durations, and task labels respectively:</p><formula xml:id="formula_5">L dense = L cl ? N D n=1 D n log(D) n ? 1 M M m=1 N Y n=1 Y m n log(? m ) n + N D n=1 D m n log(D m ) n<label>(6)</label></formula><p>During inference we sum the predicted scores (post soft-max) for all starting points i r to predict the current action as max n ( R r=1? ir ) n . The LSTM is then applied recurrently to predict subsequent actions and durations.  <ref type="table">Table 1</ref>. Dataset details and our respective model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Features</head><p>We experiment on Breakfast <ref type="bibr" target="#b17">[18]</ref>, 50Salads <ref type="bibr" target="#b33">[34]</ref> and EPIC-Kitchens <ref type="bibr">[5]</ref>. The sequences in each dataset reflect realistic durations and orderings of actions, which is crucial for real-world deployment of anticipation models. Relevant datasets statistics are given in <ref type="table">Table 1</ref>. One notable difference between these datasets is the label granularity; it is very fine-grained for EPIC, hence their 2513 action classes, versus the coarser 48 and 17 actions of Breakfast and 50Salads. As a result, the median action segment duration is 8-16x shorter.</p><p>Feature-wise, we use pre-computed Fisher vectors <ref type="bibr">[1]</ref> and I3D <ref type="bibr">[3]</ref> for Breakfast, Fisher vectors for 50Salads, and appearance, optical flow and object-based features provided by <ref type="bibr" target="#b9">[10]</ref> for EPIC. Results for Breakfast and 50Salads are averaged over the predefined 4 and 5 splits respectively. Since 50Salads has only a single complex activity (making salad) we omit complex activity prediction for it. For EPIC, we report results on the test set. Evaluation measures are class accuracy (Acc.) for next action prediction and mean over classes <ref type="bibr">[1]</ref> for dense prediction. We report Top-1 and Top-5 accuracies to be consistent with <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Hyper-parameters for spanning {K}, recent scales K R and recent starting points {i} are given in <ref type="table">Table 1</ref>. We cross validated the parameters on different splits of 50Salads and Breakfast; on EPIC, we select parameters with the validation set <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Component validation</head><p>We verify each component's utility via a series of ablation studies summarized in <ref type="table" target="#tab_4">Table 2</ref>. As our main motivation was to develop a representation for anticipation in long video streams, we validate on Breakfast for next action anticipation. Our full model gets a performance of 40.1% accuracy averaged over actions.</p><p>Video Representation: Several short-term feature representations have been proposed for video, e.g. 3D convolutions <ref type="bibr" target="#b35">[36]</ref>, or combining CNNs and RNNs for sequences <ref type="bibr" target="#b43">[44,</ref><ref type="bibr">7]</ref>. For long video streams, it becomes difficult to work with all the raw features. Selecting representative features can be as simple as sub-sampling the frames <ref type="bibr">[9,</ref><ref type="bibr" target="#b42">43]</ref>, or pooling <ref type="bibr" target="#b39">[40]</ref>, to more complex RNNs <ref type="bibr" target="#b43">[44]</ref>. Current findings in the literature are not in agreement. Some propose learned strategies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b19">20]</ref>, while others advocate pooling <ref type="bibr" target="#b39">[40]</ref>. Our experiments align with the latter, showing that max-pooling is superior to both sampling (+8%) and the GRU (+2.2%) or bi-directional LSTM <ref type="bibr">[4]</ref>   BiLSTM are comparable to average-pooling, but require much longer training and inference time. For us, max-pooling works better than average pooling; this contradicts the findings of <ref type="bibr" target="#b39">[40]</ref>. We attribute this to the fact that we pool over minutes-long snippets and it is likely that mean-smooths away salient features that are otherwise preserved by max-pooling. We conducted a similar ablations on EPIC, where we observed a 1.3% increase with max-over mean-pooling.</p><p>Recent and Spanning Representations: In our ablations, unless otherwise stated, an ensemble of 3 spanning scales K = {10, 15, 20} and 3 recent starting points i = {t ? 10, t ? 20, t ? 30} are used. Table 2 (a) compares single starting points for the recent snippet features versus an ensemble. With a single starting point, points too near to and too far from the current time decrease the performance. The worst individual result is with i 4 = 0, i.e. using the entire sequence; the peak is at i 2 = t ? 20, though an ensemble is still best. In <ref type="table" target="#tab_4">Table 2</ref> (b), we show the influence of spanning snippet scales. These scales determine the temporal snippet granularity; individually, results are not significantly different across the scales, but as we begin to aggregate an ensemble, the results improve. The ensemble with 4 scales is best but only marginally better than 3, at the expense of a larger network, so we choose K = {10, 15, 20}. In <ref type="table" target="#tab_4">Table 2</ref> (c), we show the influence of recent snippet scales, we find K R = 5 performs best.</p><p>Coupling Blocks: Previous studies on simple video understanding have shown the benefits of using features from both the recent and long-range past <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b41">42]</ref>. A na?ve way to use both is to simply concatenate, though combining the two in a learned way, e.g. via attention, is superior (+6.4%). To incorporate attention, we apply NLBs <ref type="bibr" target="#b40">[41]</ref>, which is an adaptation of the attention mechanism that is popularly used in machine translation. When we replace our CBs with concatenation and a linear layer, there is a drop of 6.7%. When we do not use coupling but separately pass the R i and S K through concatenation and a linear layer, there is a drop of 7.5%. We find also that coupling the recent R i and long range S K information is critical. Coupling only recent information (-5.9%) does not  <ref type="table">Table 3</ref>. Next action anticipation comparisons on Breakfast and 50Salads, given different frame inputs frame inputs, GT action labels, Fisher vectors and I3D features.</p><p>keep sufficient context, whereas coupling only long-range past (-5%) does not leave sufficient representation for the more relevant recent aspects. Temporal Aggregation Blocks (TAB) are the most critical component. Omitting them and classifying a single CB's outputs significantly decreases accuracy (-8%). The strength of the TAB comes from using ensembles of coupling blocks as input (single, -2.1%) and using the TABs in an ensemble (single, -2.4%).</p><p>Additional ablations: When we omit the auxiliary complex activity prediction, i.e. removing the Z term from Eq. 6 ("no Z"), we observe a slight performance drop of 1.1%. In our model we max pool over all S i,K1 , ..., S i,K S in our TABs. When we replace the max-pooling with concatenation + linear, we reach an accuracy of 37.4. We also try to disentangle the ensemble effect from the use of multi-granular representations. When we fix the spanning past scales K to {15, 15, 15} and all the starting points to i = t ? 20, we observe a drop of 1.2% in accuracy which indicates the importance of our multi-scale representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Anticipation on Procedural Activities -Breakfast &amp; 50 Salads</head><p>Next Action Anticipation predicts the action that occurs 1 second from the current time t. We compare to the state of the art in <ref type="table">Table 3</ref> with two types of frame inputs: spatio-temporal features (Fisher vectors or I3D) and frame-wise action labels (either from ground truth or via a separate segmentation algorithm) on Breakfast. Compared to previous methods using only visual features as input, we outperform CNN (FC7) features <ref type="bibr" target="#b37">[38]</ref> and spatio-temporal features R(2+1)D <ref type="bibr" target="#b25">[26]</ref> by a large margin (+32.3% and +8.1%). While the inputs are different, R(2+1)D features were shown to be comparable to I3D features <ref type="bibr" target="#b36">[37]</ref>. Since <ref type="bibr" target="#b25">[26]</ref> uses only recent observations, we conclude that incorporating the spanning past into the prediction model is essential.</p><p>Our method degrades when we replace I3D with the weaker Fisher vectors (40.1% vs 29.7%). Nevertheless, this result is competitive with methods using action labels <ref type="bibr">[1]</ref> (30.1% with RNN) derived from segmentation algorithms <ref type="bibr" target="#b28">[29]</ref> using Fisher vectors as input. For fair comparison, we report a variant without the complex activity prediction ("no Z"), which has a slight performance drop (-0.5%). If we use action labels as inputs instead of visual features, our performance  improves from 40.1% to 43.1%; merging labels and visual features gives another 4% boost to 47%. In this experiment we use segmentation results from our own framework, (see Sec. 5.6). However, if we substitute ground truth instead of segmentation labels, there is still a 17% gap. This suggests that the quality of the segmentation matters. When the segmentation is very accurate, adding additional features does not help and actually slightly deteriorates results (see <ref type="table">Table 3</ref> "frame GT" vs. "frame GT + I3D").</p><p>In <ref type="table">Table 3</ref>, we also report results for 50Salads. Using Fisher vectors we both outperform the state of the art by 1.8% and the baseline with CNN features <ref type="bibr" target="#b37">[38]</ref> by 25.4%. Using I3D features improves the accuracy by 9.1% over Fisher vectors.</p><p>Dense Anticipation predicts frame-wise actions; accuracies are given for specific portions of the remaining video (Pred.) after observing a given percentage of the past (Obs.). We refer the reader to the supplementary for visual results. Competing methods <ref type="bibr">[1]</ref> and <ref type="bibr" target="#b14">[15]</ref> have two stages; they first apply temporal video segmentation and then use outputs <ref type="bibr" target="#b28">[29]</ref>, i.e. frame-wise action labels, as inputs for anticipation. We experiment with both action labels and visual features.</p><p>For Breakfast <ref type="table" target="#tab_7">(Table 4</ref>, left), when using GT frame labels, we outperform the others, for shorter prediction horizons. For 50Salads <ref type="table" target="#tab_7">(Table 4</ref>, right), we outperform the state of the art for the observed 20%, and our predictions are more accurate on long-range anticipation (Pred. 50%). We outperform <ref type="bibr">[1]</ref> when we use visual features as input (B Features (Fisher)). When using the segmentations (from <ref type="bibr" target="#b28">[29]</ref>, which has a frame-wise temporal segmentation accuracy of 36.8% and 42.9% for the observed 20% and 30% of video respectively), we are comparable to state of the art <ref type="bibr" target="#b14">[15]</ref>. We further merge visual features with action labels for dense anticipation. With Fisher vectors and the frame labels obtained from <ref type="bibr" target="#b28">[29]</ref>, we observe a huge performance increase in performance compared to only using the frame labels (up to +7%) in Breakfast. In 50Salads, this increase is not significant nor consistent. This may be due to the better performing segmentation algorithm on 50Salads (frame-wise accuracy of 66.8% and 66.7% for 20% and 30% observed respectively). We observe further improvements on Breakfast once we substitute Fisher vectors with I3D features and segmentations from our own framework (I3D + ours seg.). Similar to next action anticipation, performance drops when using only visual features as input (I3D is better than Fisher vectors). When using I3D features and the frame label outputs of our segmentation method, we obtain our model's best performance, with a slight increase over using only frame label outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">How much spanning past is necessary?</head><p>We vary the duration of spanning snippets (Eq. 3) with start time i as fractions of the current time t; i = 0 corresponds to the full sequence, i.e. 100% of the spanning past, while i = t corresponds to none, i.e. using only recent snippets since the end points j remain fixed at t. Using the entire past is best for Breakfast ( <ref type="figure" target="#fig_1">Fig. 4 left)</ref>. Interestingly, this effect is not observed on EPIC <ref type="figure" target="#fig_1">(Fig. 4 right)</ref>.</p><p>Though we see a small gain by 1.2% until 40% past for the appearance features (rgb), beyond this, performance saturates. We believe this has to do with the fine granularity of labels in EPIC; given that the median action duration is only 1.9s, one could observe as many as 16 actions in 30 seconds. Given that the dataset has only 28.5K samples split over 2513 action classes, we speculate that the model cannot learn all the variants of long-range relationships beyond 30 seconds. Therefore, increasing the scope of the spanning past does not further increase the performance. Based on experiments on the validation set, we set the spanning scope to 6 seconds for EPIC for the rest of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Recognition and Anticipation on Daily Activities -EPIC</head><p>The anticipation task of EPIC requires anticipating the future action ? ? = 1s before it starts. For fair comparison to the state of the art <ref type="bibr" target="#b9">[10]</ref> (denoted by "RU"), we directly use features (appearance, motion and object) provided by the authors. We train our model separately for each feature modality with the same hyper-parameters and fuse predictions from the different modalities by  <ref type="table">Table 5</ref>. Action anticipation and recognition on EPIC tests sets S1 and S2</p><p>voting. Note that for experiments on this dataset we do not use the entire past for computing our spanning snippet features (see <ref type="bibr">Section 5.4)</ref>. Results on holdout test data of EPIC are given in <ref type="table">Table 5</ref> for seen kitchens (S1) with the same environments as the training data and unseen kitchens (S2) of held out environments. We outperform state of the art, RU <ref type="bibr" target="#b9">[10]</ref>, in the Top-1 and Top-5 action accuracies by 2% and 2.7% on S1 and by 1.8% and 2.3% on S2 using the same features suggesting superior temporal reasoning abilities of our model. When we add verb and noun classification to our model as auxiliary tasks to help with anticipation, "ours v+n", our performance improves for action and especially for noun and verb scores. For challenge results see supplementary.</p><p>For recognition, we classify pre-trimmed action segments. We adjust the scope of our spanning and recent snippets according to the action start and end times t s and t e . Spanning features are computed on a range of [t s ? 6, t e + 6]; the first recent snippet scope is fixed to [t s , t e ] and the rest to [t s ? 1, t e + 1], [t s ? 2, t e + 2] and [t s ? 3, t e + 3]. Remaining hyper-parameters are kept the same. In <ref type="table">Table 5</ref>, we compare to state of the art; we outperform all other methods including SlowFast networks with audio data <ref type="bibr" target="#b42">[43]</ref> (+5.4% on S1, +2.2% on S2 for Top-1) and LFB <ref type="bibr" target="#b41">[42]</ref>, which also uses non-local blocks (+8.6% on S1, +5% on S2 for Top-1) and RU <ref type="bibr" target="#b9">[10]</ref> by approximately +7% on both S1 and S2. Together with the anticipation results we conclude that our method generalizes to both anticipation and recognition tasks and is able to achieve state-of-the-art results on both, while <ref type="bibr" target="#b9">[10]</ref> performs very well on anticipation but poorly on recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Temporal Video Segmentation</head><p>We compare our performance against the state of the art, MS-TCN (I3D) <ref type="bibr">[8]</ref>, in <ref type="table">Table 6</ref> on Breakfast. We test our model with 2s and 5s windows. We report the frame-wise accuracy (Acc), segment-wise edit distance (Edit) and F1 scores at overlapping thresholds of 10%, 25% and 50%. In the example sequences, in the F1 scores and edit distances in <ref type="table">Table 6</ref>, we observe more fragmentation in our segmentation for 2s than for 5s. However, for 2s, our model produces better accuracies, as the 5s windows are smoothing the predictions at action boundaries.  <ref type="table">Table 6</ref>. Exemplary segmentation and comparisons on Breakfast.</p><p>Additionally we provide our model's upper bound, "ours I3D GT.seg.", for which we classify GT action segments instead of sliding windows. The results indicate that there is room for improvement, which we leave as future work. We show that we are able to easily adjust our method from its main application and already get close to the state of the art with slight modifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion &amp; Conclusion</head><p>This paper presented a temporal aggregate model for long-range video understanding. Our method computes recent and spanning representations pooled from snippets of video that are related via coupled attention mechanisms. Validating on three complex activity datasets, we show that temporal aggregates are either comparable or outperform the state of the art on three video understanding tasks: action anticipation, recognition and temporal video segmentation.</p><p>In developing our framework, we faced questions regarding temporal extent, scaling, and level of semantic abstraction. Our experiments show that maxpooling is a simple and efficient yet effective way of representing video snippets; this is the case even for snippets as long as two minutes. For learning temporal relationships in long video, attention mechanisms relating the present to long range context can successfully model and anticipate upcoming actions. The extent of context that is beneficial, however, may depend on the nature of activity (instructional vs. daily) and label granularity (coarse vs. fine) of the dataset.</p><p>We found significant advantages to using ensembles of multiple scales, both in recent and spanning snippets. Our aggregates model is flexible and can take as input either visual features or frame-wise action labels. We achieve competitive performance with either form of input, though our experiments confirm that higher levels of abstraction such as labels are more preferable for anticipation. Nevertheless, there is still a large gap between what can be anticipated with inputs from current segmentation algorithms in comparison to ground truth labels, leaving room for segmentation algorithms to improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary: Temporal Aggregate</head><p>Representations for Long-Range Video Understanding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">More on Datasets and Features</head><p>We provide more statistics about the datasets used in our paper to show a broader comparison about their scale and label granularity.</p><p>The Breakfast Actions dataset <ref type="bibr">[7]</ref> contains 1712 videos of 10 high level tasks like "making coffee", "making tea" and so on. There are in total 48 different actions, such as "pouring water" or "stirring coffee", with on average 6 actions per video. The average duration of the videos is 2.3 minutes. There are 4 splits and we report our results averaged over them. We use two types of frame-wise features: Fisher vectors computed as in <ref type="bibr">[1]</ref> and I3D features <ref type="bibr">[2]</ref>. The 50Salads dataset <ref type="bibr">[9]</ref> includes 50 videos and 17 different actions for a single task, namely making mixed salads. When training on this dataset, we therefore omit task prediction in our model. On average, 50Salads has 20 actions per video due to repetitions. The average video duration is 6.4 minutes. There are 5 splits, and we again average our results over them. We represent the frames using Fisher vectors as in <ref type="bibr">[1]</ref>. The EPIC-Kitchens dataset <ref type="bibr">[3]</ref> is a large first-person video dataset which contains 432 sequences and 39,594 action segments recorded by participants performing non-scripted daily activities in their kitchen. The average duration of the videos is 7.6 minutes ranging from 1 minute to 55 minutes. An action is defined as a combination of a verb and a noun, e.g. "boil milk". There are in total 125 verbs, 351 nouns and 2513 actions. The dataset provides a training and test set which contains 272 and 160 videos, respectively. The test set is divided into two splits: Seen Kitchens (S1) where sequences from the same environment are in the training data, and Unseen Kitchens (S2) where complete sequences of some participants are held out for testing. The labels for the test set are not shared, as there is an action anticipation challenge 3 and action recognition challenge 4 . We use the RGB, optical flow and object-based features provided by Furnari and Farinella et al. <ref type="bibr">[5]</ref>. The minimum and maximum snippet durations, over which we apply pooling, are 0.4s and 115.3s for 50Salads, 0.1s and 64.5s for Breakfast, and 1.2s and 3.0s for EPIC. Implementation Details : We train our model using the Adam optimizer <ref type="bibr">[6]</ref> with batch size 10, learning rate 10 ?4 and dropout rate 0.3. We train for 25 epochs and decrease the learning rate by a factor of 10 every 10 th epoch. We use 1024 dimensions for all non-classification linear layers for the Breakfast Actions and 50Salads datasets and 512 dimensions for the EPIC-Kitchens dataset. The LSTMs in dense anticipation have one layer and 512 hidden units. We use intervals of 20 seconds for Breakfast and 50Salads for discretizing the durations in dense anticipation. <ref type="table">Table 1</ref>. Model validation using GT labels for next action anticipation on the Breakfast Actions, presented are accuracies. We compare transition matrices (TM), lookup tables (LUT), LSTMs, and our temporal aggregates model (without complex activity prediction).</p><p>For validating our method's capabilities in modelling sequences, we make baseline comparisons. The simplest approach for solving the next action anticipation task is using a transition matrix (TM) <ref type="bibr">[8]</ref>, which encodes the transition from one action to the next. A more sophisticated solution is building a lookup table (LUT) of varying length sequences which allows encoding the context in a more explicit manner. The problem with LUTs is that their completeness depends on the coverage of the training data, and they rapidly grow with the number of actions. So far, for next step prediction, RNNs achieve good performance <ref type="bibr">[1]</ref>, as they learn modelling the sequences.</p><p>For our baseline comparisons, instead of frame features, we use the frame-level ground truth labels as input to our model. We compute the TM, LUT and RNN on the ground truth segment-level labels. In <ref type="table">Table 1</ref> we present comparisons on the Breakfast Actions for the next action anticipation per complex activity. Overall, transition matrices provide the worst results. LUTs improve the results, as they incorporate more contextual information. Both the RNN and our method outperform the other alternatives, while our method still performs better than the RNN on average. However, applying RNNs requires parsing the past into action sequences <ref type="bibr">[1]</ref>, which turns the problem into separate segmentation and prediction phases. Our model, on the other hand, can be trained end-to-end, and can represent the long-range observations good enough to outperform RNNs. We show that our model is doing better than simply learning pairwise statistics of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Visual Results</head><p>In <ref type="figure">Fig. 1</ref>, we provide qualitative results from our method for dense anticipation on the Breakfast Actions dataset. We show our method's predictions after observing 30% of the video. We compare our results when we use the GT labels and I3D features as input. In <ref type="figure">Fig. 2</ref>, we present qualitative results from our method for next action anticipation on the EPIC-Kitchens dataset for multiple anticipation times ? ? between 0.25 and 2 seconds. We show examples where our method is certain about the next action for all different times. We also show examples where our method's prediction gets more accurate when the anticipation time is closer.</p><p>In <ref type="figure" target="#fig_0">Fig. 3</ref>, we present some visualizations of regions attended by our nonlocal blocks. We show the five highest weighted spanning snippets (at different granularities). Our model attends different regions over the videos, for example for predicting 'fry egg' when making fried eggs, it attends regions both when pouring oil and cracking eggs. Pouring oil is an important long-range past action for frying eggs. Our method can encode long video durations while attending to salient snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Action Anticipation on EPIC-Kitchens</head><p>Furnari and Farinella <ref type="bibr">[5]</ref> reports prediction results at multiple anticipation times (? ? ) between 0.25s and 2s on EPIC. We compare in <ref type="table" target="#tab_4">Table 2</ref> on the validation set and note that our prediction scores are better than <ref type="bibr">[5]</ref> for all time points. Our improvements are greater when the anticipation time decreases.  <ref type="figure" target="#fig_0">Fig. 3</ref>. Attention visualization on the Breakfast Actions dataset for next action anticipation. Rectangles are the top 5 five spanning snippets (at different granularities where K = 10, <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20)</ref>, weighted highest by the attention mechanism in the Non-Local Blocks (NLB). Best viewed in color.</p><p>We report our results for hold-out test data on EPIC-Kitchens Egocentric Action Anticipation Challenge (2020) in <ref type="table">Table 3</ref> for seen kitchens (S1) with the same environments as in the training data and unseen kitchens (S2) of held out environments. The official ranking on the challenge is based on the Top-1 action accuracy. Our submission (Team "NUS CVML") is ranked first on S1 and third on S2 sets. We refer the reader to EPIC-Kitchens 2020 Challenges Report <ref type="bibr">[4]</ref> for details on the competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Action Recognition Challenge on EPIC-Kitchens</head><p>We present our results for the EPIC-Kitchens Egocentric Action Recognition Challenge 2020 in <ref type="table" target="#tab_7">Table 4</ref> for S1 and S2. Our team "NUS CVML" is ranked second on S1 and third on S2 sets. Please see EPIC-Kitchens 2020 Challenges Report <ref type="bibr">[4]</ref> for further details.  <ref type="table" target="#tab_4">Table 2</ref>. Action anticipation on EPIC validation set at different anticipation times.</p><p>Top-1 Accuracy% Top-5 Accuracy% Precision (%) Recall (%) Verb Noun Action Verb Noun Action Verb Noun Action Verb Noun Action 1st (S1) 37.87 24. <ref type="bibr" target="#b9">10</ref>   <ref type="table">Table 3</ref>. Action anticipation on EPIC tests sets, seen (S1) and unseen (S2)  <ref type="table" target="#tab_7">Table 4</ref>. Action recognition on EPIC tests sets, seen (S1) and unseen (S2)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Prediction models for classification (left) and sequence prediction (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Effect of spanning scope on instructional vs. daily activities. For EPIC we report Top-5 Acc. on the validation set with rgb, flow and object features and late fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>F1@{10, 25 ,</head><label>25</label><figDesc>50} Edit Acc. MS-TCN (I3D)[8]  52.6 48.1 37.9 61.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fadime Sener 1 , 2 , 2 1</head><label>122</label><figDesc>Dipika Singhania 2 , and Angela Yao University of Bonn, Germany 2 National University of Singapore {sener@cs.uni-bonn.de},{dipika16,ayao}@comp.nus.edu.sg</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>or I3D[3], or high-level abstractions, such as</figDesc><table><row><cell>input video sequence</cell><cell></cell><cell>? past future ?</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>spanning past</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(over frame features) max-pooling</cell><cell>recent past</cell><cell>2 1 3</cell><cell>TAB. TAB.</cell><cell>?</cell><cell>next action anticipation dense prediction</cell><cell>stir milk</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>(+1.4%). The performance of GRU and</figDesc><table><row><cell cols="3">Pooling type</cell><cell cols="3">frame sampling GRU BiLSTM mean-pooling max-pooling</cell></row><row><cell>Acc.</cell><cell></cell><cell></cell><cell>32.1</cell><cell>37.9 38.7</cell><cell>36.6</cell><cell>40.1</cell></row><row><cell cols="3">Influence of</cell><cell cols="2">Changes in components</cell><cell>Acc. (Drop)</cell></row><row><cell cols="6">Non-Local Blocks (NLB) replace all NLBs with concatenation + linear layer 33.7 (6.4% )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">only couple the SK and SK in CBs</cell><cell>35.1 (5.0% )</cell></row><row><cell cols="3">Coupling Blocks (CB)</cell><cell cols="2">only couple the Ri and Ri in CBs</cell><cell>34.2 (5.9% )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">replace CBs with concatenation + linear layer</cell><cell>33.4 (6.7% )</cell></row><row><cell cols="3">Temporal Aggregation Blocks (TAB)</cell><cell cols="3">a single CB is used in TABs three CBs are used in a single TAB a single a CB is used without any TABs</cell><cell>38.0 (2.1% ) 37.7 (2.4% ) 32.1 (8.0% )</cell></row><row><cell>Recent &amp; Spanning Repr.</cell><cell>(a) (b) (c)</cell><cell cols="4">starting points i i1 = t ? 10i2 = t ? 20i3 = t ? 30i4 = 0{i1, i2, i3} Acc. 36.9 37.7 37.2 35.1 40.1 spanning scales K {5} {10} {15} {20} {10, 15} {10,15,20}{5,10,15,20} Acc. 37.4 38.0 37.5 37.4 39.0 40.1 40.2 recent scales KR 1 3 5 10 Acc. 38.7 39.5 40.1 38.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Ablations on the influence of different model components.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Dense anticipation mean over classes on Breakfast and 50salads, given dif- ferent frame inputs frame inputs, GT action labels, Fisher vectors and I3D features.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Fig. 1. Qualitative results for dense anticipation on Breakfast Actions dataset when using the GT labels and I3D features. Best viewed in color.</figDesc><table><row><cell>Observed %30</cell><cell>Prediction %50</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>ours, GT labels ?</cell><cell></cell></row><row><cell>ours, I3D ?</cell><cell></cell></row><row><cell>Observed %30</cell><cell>Prediction %50</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>ours, GT labels ?</cell><cell></cell></row><row><cell>ours, I3D ?</cell><cell></cell></row><row><cell>Observed %30</cell><cell>Prediction %50</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>ours, GT labels ?</cell><cell></cell></row><row><cell>ours, I3D ?</cell><cell></cell></row><row><cell>Observed %30</cell><cell>Prediction %50</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>ours, GT labels ?</cell><cell></cell></row><row><cell>ours, I3D ?</cell><cell></cell></row><row><cell>Observed %30</cell><cell>Prediction %50</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>ours, GT labels ?</cell><cell></cell></row><row><cell>ours, I3D ?</cell><cell></cell></row><row><cell>Observed %30</cell><cell>Prediction %50</cell></row><row><cell>GT</cell><cell></cell></row><row><cell>ours, GT labels ?</cell><cell></cell></row><row><cell>ours, I3D ?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Fig. 2. Exemplary qualitative results for next action anticipation on EPIC-Kitchens dataset, showing the success of our method. We list our Top-5 predictions at different anticipation times, ??. The closer we are the better are our model's predictions. Best viewed in color.</figDesc><table><row><cell></cell><cell cols="2">GT action: open fridge</cell><cell></cell></row><row><cell>=2s</cell><cell>=1.5s</cell><cell>=1s</cell><cell>=0.5s</cell></row><row><cell>open fridge, open door, open</cell><cell>open fridge, open drawer,</cell><cell>open fridge, open drawer,</cell><cell>open fridge, open drawer,</cell></row><row><cell>drawer, take plate, rinse hand</cell><cell>take soy milk, open door, take</cell><cell>open door, take soy milk, take</cell><cell>open door, close fridge, turn-</cell></row><row><cell></cell><cell>bottle</cell><cell>national geographic</cell><cell>on light</cell></row><row><cell></cell><cell cols="2">GT action: pour oil</cell><cell></cell></row><row><cell>=2s</cell><cell>=1.5s</cell><cell>=1s</cell><cell>=0.5s</cell></row><row><cell>pour oil, put-down oil, close</cell><cell>pour oil, put-down oil, close</cell><cell>pour oil, put-down oil, close</cell><cell>pour oil, put-down oil, adjust</cell></row><row><cell>oil, turn-on hob, adjust hob</cell><cell>oil, take spoon, put-down pan</cell><cell>oil, pour salt, pour pepper</cell><cell>hob, pour pepper, put lid</cell></row><row><cell></cell><cell cols="2">GT action: open salt</cell><cell></cell></row><row><cell>=1.75s</cell><cell>=1.25s</cell><cell>=0.75s</cell><cell>=0.25s</cell></row><row><cell>take pepper, take spatula, stir</cell><cell>take pepper, stir onion, put-</cell><cell>take pepper, open salt, put-</cell><cell>take pepper, open salt, put-</cell></row><row><cell>onion, put-down spatula, take</cell><cell>down pepper, open oil, open</cell><cell>down, pepper, pour pepper,</cell><cell>down salt, take salt, put-</cell></row><row><cell>onion</cell><cell>salt</cell><cell>stir onion</cell><cell>down pepper</cell></row><row><cell></cell><cell cols="2">GT action: roll dough</cell><cell></cell></row><row><cell>=2s</cell><cell>=1.5s</cell><cell>=1s</cell><cell>=0.5s</cell></row><row><cell>put dough, knead dough, roll</cell><cell>put dough, knead dough, roll</cell><cell>roll dough, put dough, take</cell><cell>roll dough, put dough, brush</cell></row><row><cell>dough, take flour, spread</cell><cell>dough, take dough, squeeze</cell><cell>dough, rub hand, knead</cell><cell>flour, squeeze dough, knead</cell></row><row><cell>flour</cell><cell>dough</cell><cell>dough</cell><cell>dough</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>pour cereals SIL Making cereals POUR MILK crack egg pour oil SIL Making fried egg FRY EGG</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2</cell></row><row><cell cols="3">Making fruit salad</cell><cell></cell><cell></cell><cell></cell><cell>3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CUT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FRUIT</cell></row><row><cell></cell><cell>SIL</cell><cell>take</cell><cell>cut</cell><cell>take</cell><cell></cell><cell>cut fruit</cell><cell>put</cell></row><row><cell></cell><cell></cell><cell>knife</cell><cell>fruit</cell><cell>bowl</cell><cell></cell><cell>fruit2bowl</cell></row><row><cell cols="3">Making pancakes</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>POUR</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DOUGH</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TO PAN</cell></row><row><cell>SIL</cell><cell>crack</cell><cell cols="3">stir dough</cell><cell>spoon</cell><cell>pour milk</cell><cell>stir dough</cell></row><row><cell></cell><cell>egg</cell><cell></cell><cell></cell><cell></cell><cell>flour</cell></row><row><cell cols="2">Making juice</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TAKE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SQUEEZER</cell></row><row><cell></cell><cell>SIL</cell><cell></cell><cell></cell><cell>take glass</cell><cell></cell><cell>take knife</cell><cell>cut orange</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>16.64 79.74 53.98 36.06 36.41 25.20 9.64 15.67 22.01 10.05 3rd (S2) 29.50 16.52 10.04 70.13 37.83 23.42 20.43 12.95 4.92 8.03 12.84 6.26</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Verb Noun Action Verb Noun Action Verb Noun Action Verb Noun Action 2nd (S1) 66.56 49.60 41.59 90.10 77.03 64.11 59.43 45.62 25.37 41.65 46.25 26.98 3rd (S2) 54.56 33.46 26.97 80.40 60.98 46.43 33.60 30.54 14.99 25.28 28.39 17.97</figDesc><table><row><cell>Top-1 Accuracy%</cell><cell>Top-5 Accuracy%</cell><cell>Precision (%)</cell><cell>Recall (%)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://competitions.codalab.org/competitions/20071 4 https://competitions.codalab.org/competitions/20115</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5343" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised learning of universal sentence representations from natural language inference data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="670" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="753" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ms-tcn: Multi-stage temporal convolutional network for action segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3575" to="3584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Connectionist temporal modeling for weakly supervised action labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="137" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What makes a video a video: Analyzing temporal information in video understanding models and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7366" to="7375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Time-conditioned action anticipation in one shot</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Computing temporal aggregates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kline</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Snodgrass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Data Engineering</title>
		<meeting>the Eleventh International Conference on Data Engineering</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="page" from="222" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="14" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A hierarchical representation for future action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="689" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The 2nd youtube-8m large-scale video understanding challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Reade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<editor>Leal-Taix?, L., Roth, S.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="193" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04555</idno>
		<title level="m">Temporal modeling approaches for large-scale youtube-8m video understanding</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7083" to="7093" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nextvlad: An efficient neural network to aggregate framelevel features for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<editor>Leal-Taix?, L., Roth, S.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="206" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Joint prediction of activity labels and starting times in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5773" to="5782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06905</idno>
		<title level="m">Learnable pooling with context gating for video classification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Label denoising with large ensembles of heterogeneous neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ostyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Suvorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Aliev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sterkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Khomenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">I</forename><surname>Nikolenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<editor>Leal-Taix?, L., Roth, S.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="250" to="261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action detection using a statistical language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3131" to="3140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Weakly supervised action learning with rnn based fine-to-coarse modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="754" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning and segmentation of complex activities from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<ptr target="http://arxiv.org/abs/1212.0402" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Non-local netvlad encoding for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 Workshops</title>
		<editor>Leal-Taix?, L., Roth, S.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Leibe, B., Matas, J., Sebe, N., Welling, M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Long-Term Feature Banks for Detailed Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08740</idno>
		<title level="m">Audiovisual slowfast networks for video recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Model Validation cereal coffee f.egg juice milk panc. salat sand. s.egg tea</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abu</forename><surname>Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5343" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<editor>Ferrari, V., Hebert, M., Sminchisescu, C., Weiss, Y.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="753" to="771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Epic-kitchens-55 -2020 challenges report</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<ptr target="https://epic-kitchens.github.io/Reports/EPIC-KITCHENS-Challenges-2020-Report.pdf" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">What would you expect? anticipating egocentric actions with rolling-unrolling lstms and modality attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The language of actions: Recovering the syntax and semantics of goal-directed human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="780" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Leveraging the present to anticipate the future in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Combining embedded accelerometers with computer vision for recognizing food preparation activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Mckenna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 ACM international joint conference on Pervasive and ubiquitous computing</title>
		<meeting>the 2013 ACM international joint conference on Pervasive and ubiquitous computing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="729" to="738" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
