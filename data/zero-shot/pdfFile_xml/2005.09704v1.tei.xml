<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting Fig. 1: Inpainting results on ultra high-resolution images</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zili</forename><surname>Yi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shekoofeh</forename><surname>Azizi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daesik</forename><surname>Jang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada Co. Ltd</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Huawei Technologies Canada Co. Ltd</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting Fig. 1: Inpainting results on ultra high-resolution images</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Inpainting</term>
					<term>Image Completion</term>
					<term>Ultra high-resolution</term>
					<term>Gener- ative Adversarial Network</term>
					<term>Contextual Residual Aggregation</term>
					<term>Contextual Atten- tion</term>
					<term>Light-Weight Gated Convolution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently data-driven image inpainting methods have made inspiring progress, impacting fundamental image editing tasks such as object removal and damaged image repairing. These methods are more effective than classic approaches, however, due to memory limitations they can only handle low-resolution inputs, typically smaller than 1K. Meanwhile, the resolution of photos captured with mobile devices increases up to 8K. Naive up-sampling of the low-resolution inpainted result can merely yield a large yet blurry result. Whereas, adding a highfrequency residual image onto the large blurry image can generate a sharp result, rich in details and textures. Motivated by this, we propose a Contextual Residual Aggregation (CRA) mechanism that can produce high-frequency residuals for missing contents by weighted aggregating residuals from contextual patches, thus only requiring a low-resolution prediction from the network. Since convolutional layers of the neural network only need to operate on low-resolution inputs and arXiv:2005.09704v1 [cs.CV] 19 May 2020 outputs, the cost of memory and computing power is thus well suppressed. Moreover, the need for high-resolution training datasets is alleviated. In our experiments, we train the proposed model on small images with resolutions 512?512 and perform inference on high-resolution images, achieving compelling inpainting quality. Our model can inpaint images as large as 8K with considerable hole sizes, which is intractable with previous learning-based approaches. We further elaborate on the light-weight design of the network architecture, achieving realtime performance on 2K images on a GTX 1080 Ti GPU. Codes are available at: Atlas200dk/sample-imageinpainting-HiFill.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Smartphone users are interested to manipulate their photographs in any form of altering object positions, removing unwanted visual elements, or repairing damaged images. These tasks require automated image inpainting, which aims at restoring lost or deteriorated parts of an image given a corresponding mask. Inpainting has been an active research area for the past few decades, however, due to its inherent ambiguity and the complexity of natural images, general image inpainting remains challenging. High-quality inpainting usually requires generating visually realistic and semantically coherent content to fill the hole regions. Existing methods for image hole filling can be categorized into three groups. The first category which we call "fill through copying" attempts to explicitly borrow contents or textures from surroundings to fill the missing regions. An example is diffusion-based <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> methods which propagate local image appearance surrounding the target holes based on the isophote direction field. Another stream is relying on texture synthesis techniques, which fills the hole by both extending and borrowing textures from surrounding regions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Patch-based algorithms like <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b5">6]</ref> progressively fill pixels in the hole by searching the image patches from background regions that are the most similar to the pixels along the hole boundaries.</p><p>The second group attempts to "fill through modeling" and hallucinates missing pixels in a data-driven manner with the use of large external databases. These approaches learn to model the distribution of the training images and assume that regions surrounded by similar contexts likely to possess similar contents <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. For instance, PixelRNN <ref type="bibr" target="#b11">[12]</ref> uses a two-dimensional Recurrent Neural Network (RNN) to model the pixel-level dependencies along two spatial dimensions. More general idea <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15]</ref> is to train an encoder-decoder convolutional network to model the 2dimensional spatial contents. Rather than modeling the raw pixels, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14]</ref> train a convolutional network to model image-wide edge structure or foreground object contours, thus enabling auto-completion of the edge or contours. These techniques are effective when they find an example image with sufficient visual similarity to the query, but will easily fail if the database does not have similar examples. To overcome the limitation of copying-based or modeling-based methods, the third group of approaches attempts to combine the two <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. These methods learn to model the image distribution in a data-driven manner, and in the meantime, they develop mechanisms to explicitly borrow patches/features from background regions. <ref type="bibr" target="#b19">[20]</ref> introduces a novel contextual attention layer that enables borrowing features from distant spatial locations. <ref type="bibr" target="#b20">[21]</ref> further extends the contextual attention mechanism to multiple scales and all the way from feature-level to image-level. <ref type="bibr" target="#b16">[17]</ref> employs the patch-swap layer that propagates the high-frequency texture details from the boundaries to hole regions.</p><p>Most learning-based approaches belong to the second or third group. Compared to traditional methods, these techniques have strong ability to learn adaptive and highlevel features of disparate semantics and thus are more adept in hallucinating visually plausible contents especially when inpainting structured images like faces <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>, objects <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>, and natural scenes <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. Since existing methods employ convolutional layers directly on the original input, the memory usage could become extremely high and intractable when the input size is up to 8K. Another issue is that the quality deteriorates rapidly when hole size increases with image size. Even if the training is feasible, access to large amounts of high-resolution training data would be tedious and expensive.</p><p>To resolve these issues, we propose a novel Contextual Residual Aggregation (CRA) mechanism to enable the completion of ultra high-resolution images with limited resources. In specific, we use a neural network to predict a low-resolution inpainted result and up-sample it to yield a large blurry image. Then we produce the high-frequency residuals for in-hole patches by aggregating weighted high-frequency residuals from contextual patches. Finally, we add the aggregated residuals to the large blurry image to obtain a sharp result. Since the network only operates on low-resolution images, the cost of memory and computing time is significantly reduced. Moreover, as the model can be trained with low-resolution images, the need for high-resolution training datasets is alleviated. Furthermore, we introduce other techniques including slim and deep layer configuration, attention score sharing, multi-scale attention transfer, and Light-Weight Gated Convolutions (LWGC) to improve the inpainting quality, computation, and speed. Our method can inpaint images as large as 8K with satisfying quality, which cannot be handled by prior learning-based approaches. Exemplar results are shown in <ref type="figure" target="#fig_4">Figure 1</ref>. The contributions of the paper are summarized as follows:</p><p>-We design a novel and efficient Contextual Residual Aggregation (CRA) mechanism that enables ultra high-resolution inpainting with satisfying quality. The mechanism enables large images (up to 8K) with considerable hole sizes (up to 25%) to be inpainted with limited memory and computing resources, which is intractable for prior methods. Also, the model can be trained on small images and applied on large images, which significantly alleviates the requirements for highresolution training datasets. -We develop a light-weight model for irregular hole-filling that can perform realtime inference on images of 2K resolutions on a NVIDIA GTX 1080 Ti GPU, using techniques including slim and deep layer configuration, attention score sharing, and Light Weight Gated Convolution (LWGC). -We use attention transfer at multiple abstraction levels which enables filling holes by weighted copying features from contexts at multiple scales, improving the in- painting quality over existing methods by a certain margin even when tested on low-resolution images.</p><p>2 Related Works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Irregular Hole-filling &amp; Modified Convolutions</head><p>Vanilla convolutions are intrinsically troublesome for irregular hole-filling because convolutional filters treat all pixels the same as valid ones, causing visual artifacts such as color inconsistency, blurriness, and boundary artifacts. Partial convolution <ref type="bibr" target="#b15">[16]</ref> is proposed to handle irregular holes, where the convolution is masked and re-normalized to be conditioned on valid pixels. Gated convolution <ref type="bibr" target="#b18">[19]</ref> generalizes the partial convolution idea by providing a learnable dynamic feature selection mechanism for each channel and at each spatial location, achieving better visual performance. Here, we further improve the gated convolution through a lightweight design to improve efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Contextual Attention</head><p>Contextual attention <ref type="bibr" target="#b19">[20]</ref> is proposed to allow long-range spatial dependencies during inpainting, which enables borrowing pixels from distant locations to fill missing regions. The contextual attention layer has two phases: "match" and "attend". In the "match" phase, the attention scores are computed by obtaining region affinity between patches inside and those outside the holes. In the "attend" phase, holes are filled by copying and aggregating patches from weighted contexts by the attention scores. <ref type="bibr" target="#b20">[21]</ref> extends this idea by using a pyramid of contextual attention at multiple layers. In contrast to <ref type="bibr" target="#b20">[21]</ref>, we only compute the attention scores once and reuse them at multiple abstraction levels, which leads to fewer parameters and less computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Image Residuals</head><p>The difference between an image and the blurred version of itself represents the highfrequency image <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref>. Early works use the difference obtained by Gaussian blurring for low-level image processing tasks like edge detection, image quality assessment, and feature extraction <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. We employ this concept to decompose the input image into low-frequency and high-frequency components. The low-frequency component is obtained through averaging neighboring pixels, whereas the high-frequency component (i.e. image residuals) is obtained by subtracting the original image with its low-frequency component.</p><p>3 Method <ref type="figure" target="#fig_0">Figure 2</ref> illustrates the overall pipeline of the proposed CRA mechanism where the generator is the only trainable component in the framework. Given a high-resolution input image, we first down-sample the image to 512 ? 512 and then up-sample it to obtain a blurry large image of the same size as the raw input (Section 4.1). The height and width of the image are not necessary to be equal but must be multiples of 512. The generator takes the low-resolution image and fills the holes. Meanwhile, the attention scores are calculated by the Attention Computing Module (ACM) of the generator (Section 3.2). Also, the contextual residuals are computed by subtracting the large blurry image from the raw input, and the aggregated residuals in the mask region are then calculated from the contextual residuals and attention scores through an Attention Transfer Module (ATM) (Section 3.2). Finally, adding the aggregated residuals to the up-sampled inpainted result generates a large sharp output in the mask region while the area outside mask is simply a copy of the original raw input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Overall Pipeline</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Contextual Residual Aggregation (CRA)</head><p>Filling the missing region by using contextual information <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b26">27]</ref>, and contextual attention mechanism <ref type="bibr" target="#b19">[20]</ref> has been proposed previously. Similarly, we deploy the CRA mechanism to borrow information from contextual regions. However, the CRA mechanism borrows from contexts not only features but also residuals. In particular, we adopt the idea of contextual attention <ref type="bibr" target="#b19">[20]</ref> in calculating attention scores by obtaining region affinity between patches inside/outside missing regions. Thus contextually relevant features and residuals outside can be transferred into the hole. Our mechanism involves two key modules: Attention Computing Module and Attention Transfer Module.</p><p>Attention Computing Module (ACM) The attention scores are calculated based on region affinity from a high-level feature map (denoted as P in <ref type="figure" target="#fig_0">Figure 2</ref>). P is divided into patches and ACM calculates the cosine similarity between patches inside and outside missing regions:</p><formula xml:id="formula_0">c i,j = p i p i , p j p j (1)</formula><p>where p i is the i th patch extracted from P outside mask, p j is the j th patch extracted from P inside the mask. Then softmax is applied on the similarity scores to obtain the attention scores for each patch:</p><formula xml:id="formula_1">s i,j = e ci,j ? N i=1 e ci,j<label>(2)</label></formula><p>where N is the number of patches outside the missing hole. In our framework, each patch size is 3 ? 3 and P is 32 ? 32, thus a total number of 1024 patches can be extracted. In practice, the number of in-hole patches could vary for different hole sizes. We uniformly use a matrix of 1024 ? 1024 to save affinity scores between any possible pair of patches, though only a fraction of them are useful.</p><p>Attention Transfer Module (ATM) After obtaining the attention scores from P , the corresponding holes in the lower-level feature maps (P l ) can be filled with contextual patches weighted by the attention scores:</p><formula xml:id="formula_2">p l j = ? N i=1 s i,j p l i<label>(3)</label></formula><p>where l ? 1, 2, 3 is the layer number and p l i is the i th patch extracted from P l outside masked regions, and p l j is the j th patch to be filled inside masked regions. N indicates the number of contextual patches (background). After calculating all in-hole patches, we can finally obtain a filled feature P l . As the size of feature maps varies by layer, the size of patches should vary accordingly. Assuming the size of the feature map is 128 2 and the attention scores are computed from 32 2 patches, then the patch sizes should be greater or equal to (128/32) 2 = 4 2 so that all pixels can be covered. If the patch sizes are greater than 4 ? 4, then certain pixels are overlapped, which is fine as the following layers of the network can learn to adapt. Multi-scale attention transfer and score sharing. In our framework, we apply attention transfer multiple times with the same set of attention scores <ref type="figure" target="#fig_0">(Figure 2</ref>). The sharing of attention scores leads to fewer parameters and better efficiency in terms of memory and speed.</p><p>Residual Aggregation The target of Residual Aggregation is to calculate residuals for the hole region so that sharp details of the missing contents could be recovered. The residuals for the missing contents can be calculated by aggregating the weighted contextual residuals obtained from previous steps:</p><formula xml:id="formula_3">R j = ? N i=1 s i,j R i (4)</formula><p>where R is the residual image and R i is the i th patch extracted from contextual residual image outside the mask, and R j is j th patch to be filled inside the mask. The patch sizes are properly chosen to exactly cover all pixels without overlapping, to ensure the filled residuals being consistent with surrounding regions. Once the aggregated residual image is obtained, we add it to the up-sampled blurry image of the generator, and obtain a sharp result ( <ref type="figure" target="#fig_0">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Architecture of Generator</head><p>The network architecture of the generator is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. We use a two-stage coarse-to-fine network architecture where the coarse network hallucinates rough missing contents, and the refine network predicts finer results. The generator takes an image and a binary mask indicating the hole regions as input and predicts a completed image. The input and output sizes are expected to be 512 ? 512. In order to enlarge the perceptive fields and reduce computation, inputs are down-sampled to 256?256 before convolution in the coarse network, different from the refine network who operates on 512?512. The prediction of the coarse network is naively blended with the input image by replacing the hole region of the latter with that of the former as the input to the refine network. Refine network computes contextual attention scores with a high-level feature map and performs attention transfer on multiple lower-level feature maps, thus distant contextual information can be borrowed at multiple abstraction levels. We also adopt dilated convolutions <ref type="bibr" target="#b9">[10]</ref> in both coarse and refine networks to further expand the size of the receptive fields. To improve the computational efficiency, our inpainting network is designed in a slim and deep fashion, and the LWGC is applied for all layers of the generator. Other implementation considerations include: (1) using 'same' padding and ELUs <ref type="bibr" target="#b27">[28]</ref> as activation for all convolution layers, (2) removing batch normalization layer as they deteriorate color coherency <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Light Weight Gated Convolution</head><p>Gated Convolutions (GC) <ref type="bibr" target="#b18">[19]</ref> leverages the art of irregular hole inpainting. However, GC almost doubles the number of parameters and processing time in comparison to vanilla convolution. In our network, we proposed three modified versions of GC called Light Weight Gated Convolutions (LWGC), which reduces the number of parameters and processing time while maintaining the effectiveness. The output of the original GC can be expressed as:</p><formula xml:id="formula_4">G = conv(W g , I) F = conv(W f , I) O = ?(G) ?(F )<label>(5)</label></formula><p>where ? is Sigmoid function thus the output values are within [0, 1]. ? is an activation function which are set to ELU in our experiments. w g and w f are two different set of convolutional filters, which are used to compute the gates and features respectively. GC enables the network to learn a dynamic feature selection mechanism. The three </p><formula xml:id="formula_5">G = conv depth?separable (W g , I)<label>(6)</label></formula><formula xml:id="formula_6">G = conv pixelwise (W g , I) (7) G = conv(W g , I), G is single-channel (8)</formula><p>The depth-separable LWGC employs a depth-wise convolution followed by a 1 ? 1 convolution to compute gates. The pixelwise LWGC uses a pixelwise or 1 ? 1 convolution to compute the gates. The single-channel LWGC outputs a single-channel mask that is broadcast to all feature channels during multiplication. The single-channel mask is similar to partial convolution, whereas the mask of partial convolutions is hard-wired and untrainable, and generates a binary mask instead of a soft mask. Given that the height (H k ) and width (W k ) of kernels, and numbers of input channels (C in ) and output channels (C out ), we compare the number of parameters needed to calculate gates in <ref type="table" target="#tab_0">Table 1</ref>. We used the single-channel LWGC for all layers of the coarse network and depth-separable or pixelwise LWGC for all layers of the refine network, which has been proved to be equally effective as regular GC but more efficient (Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Training of the network</head><p>Training Losses Without the degradation of performance, we also significantly simplify the training objectives as two terms: the adversarial loss and the reconstruction loss. We use the WGAN-GP loss as our adversarial loss <ref type="bibr" target="#b28">[29]</ref>, which enforces global consistency in the second-stage refinement network. The discriminator and generator are alternatively trained with the losses defined in Equation 9 and Equation 10:</p><formula xml:id="formula_7">L d = Ex ?Pg [D(x)] ? E x?Pr [D(x)]+ ?Ex ?Px [ x D(x) 2 ? 1] 2<label>(9)</label></formula><p>where D(.) is the discriminator output and G(.) is the generator output.</p><p>x,x,x, are real images, generated images, and interpolations between them, respectively. P g , P r , Px are the corresponding distributions of them respectively. </p><formula xml:id="formula_8">L adv = ?Ex ?Pg [D(x)]<label>(10)</label></formula><p>We also add the L1 loss to force the consistency of the prediction with the original image. In contrast to <ref type="bibr" target="#b19">[20]</ref>, we avoid the computationally expensive spatially-discounted reconstruction loss. For simplicity, we just assign a smaller constant weight for the reconstruction loss of all in-hole pixels. The reconstruction loss is thus written as: <ref type="bibr" target="#b12">13)</ref> where ? 1 and ? 2 are coefficients for the in-hole term and contextual term (? 1 = 1, and ? 2 = 1.2). The coarse network is trained with the reconstruction loss explicitly, while the refinement network is trained with a weighted sum of the reconstruction and GAN losses. The coarse network and refine network are trained simultaneously with merged losses as shown in <ref type="bibr">Equation 14</ref>.</p><formula xml:id="formula_9">L in?hole = |G(x, m) ? x| m (11) Lcontext = |G(x, m) ? x| (1 ? m) (12) Lrec = ?1L in?hole + ?2Lcontext<label>(</label></formula><formula xml:id="formula_10">L g = L rec + ?L adv<label>(14)</label></formula><p>where ? is the coefficient for adversarial loss (? = 10 ?4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Random Mask Generation</head><p>To diversify the inpainting masks, we use two methods to generate irregular masks on-the-fly during training. The first one is <ref type="bibr" target="#b15">[16]</ref>, which simulates tears, scratches, spots or manual erasing with brushes. The second approach generates masks by randomly manipulating the real object shape templates, accounting for the object removal scenario. These shape templates are obtained from object segmentation masks and including a wide range of categories such as single, multiple or crowded objects. We also randomly rotate, flip and scale the templates with a random scale ratio. In practice, the aforementioned two methods can be combined or separated, depending on specific needs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>We evaluate the proposed method on three datasets including Places2 <ref type="bibr" target="#b29">[30]</ref>, CelebA-HQ <ref type="bibr" target="#b30">[31]</ref>, and DIV2K <ref type="bibr" target="#b31">[32]</ref>. Our model is trained on two NVIDIA 1080 Ti GPUs with images of resolution 512 ? 512 with batch size of 8. For DIV2K and CelebA-HQ, images are down-sampled to 512 ? 512. For Places2, images are randomly cropped to 512 ? 512. After training, we test the models on images of various resolutions of 512 to 8K on a GPU. The final model has a total of 2.7M parameters and implemented on TensorFlow v1.13 with CUDNN v7.6 and CUDA v10.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Analysis of CRA Design</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, the CRA mechanism involves one down-sampling and two upsampling operations outside of the generator. Choosing different methods for downsampling and up-sampling may affect the final results. To explore this, we experimented   with four down-sampling methods: Nearest-Neighbor, Bilinear, Bicubic, and Averaging. Averaging evenly splits the input into target patches and average all pixels in each patch to obtain a 512?512 image. We also explored three up-sampling methods including Nearest-Neighbor, Bilinear or Bicubic. Note that the two up-sampling operations must be consistent, so we do not consider inconsistent combinations. Experimental results on an HD dataset indicate that Averaging performs the best for down-sampling and Bilinear or Bicubic performs equally well for up-sampling ( <ref type="figure" target="#fig_1">Figure 3</ref>). For simplicity, we use Averaging down-sampling and Bilinear up-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis of Light Weight Gated Convolutions</head><p>We propose three types of LWGC, which are faster than the original GC. We experimented how they affect inpainting quality and efficiency on the CelebA-HQ dataset to explore the influence of LWGC on the results, by replacing the original GCs with LWGCs for the coarse/refine networks. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, the LWGC sc +LWGC sc configuration brings visible artifacts while the other five configurations perform equally well in terms of quality. Considering LWGC sc +LWGC pw requires fewer parameters than the other four, we adopt the LWGC sc +LWGC pw configuration in the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons With Learning-based Methods</head><p>We compared our method with other state-of-the-art learning-based inpainting methods including Global-local GAN <ref type="bibr" target="#b9">[10]</ref>, DeepFillV1 <ref type="bibr" target="#b19">[20]</ref>, DeepFillV2 <ref type="bibr" target="#b18">[19]</ref>, PEN-Net <ref type="bibr" target="#b20">[21]</ref> and Partial Convolution <ref type="bibr" target="#b15">[16]</ref>. To make fair comparisons, we attempted to use the same settings for all experiments, though not fully guaranteed. The official pre-trained Deep-FillV1 <ref type="bibr" target="#b19">[20]</ref> model was trained for 100M iterations with the batch size of 16 and the global-local GAN <ref type="bibr" target="#b9">[10]</ref> was trained for 300K iterations with the batch size of 24. Both of them were trained on 256?256 images with rectangular holes of maximum size 128?128. All the other models were trained for 300K iterations with the batch size of 8 on 512?512 images with irregular holes up to 25% area of the whole image. The original DeepFillV2 <ref type="bibr" target="#b18">[19]</ref> model attached a sketch channel to the input to facilitate image manipulation, we simply removed the sketch channel and re-trained the model. For all these methods, no specific post-processing steps are performed other than pasting the filled contents back to the original image.  Qualitative comparisons <ref type="figure" target="#fig_5">Figure 5</ref> shows our model performs equally good or slightly better than previous methods on 512 ? 512 images. Partial convolution <ref type="bibr" target="#b15">[16]</ref> and globallocal GAN <ref type="bibr" target="#b9">[10]</ref> performs well when the mask is small and narrow but exert severe artifacts when the hole size becomes bigger. Global-local GAN <ref type="bibr" target="#b9">[10]</ref> is problematic in maintaining the color consistency of filled contents with surroundings. DeepFillV1 <ref type="bibr" target="#b19">[20]</ref> generates plausible results, but occasionally the artifacts inside the hole region are visible, implying its vulnerability to irregular masks. DeepFillV2 <ref type="bibr" target="#b18">[19]</ref> generates incoherent textures when the hole size goes up. Nevertheless, when tested on larger images with bigger hole sizes, our model performs consistently good while the inpainting results of other methods deteriorate dramatically (e.g. severe hole-shaped artifacts in <ref type="figure" target="#fig_5">Figure 5</ref>). Quantitative comparisons <ref type="table" target="#tab_1">Table 2</ref> reports our quantitative evaluation results in terms of mean L1 error, MS-SSIM <ref type="bibr" target="#b33">[34]</ref>, Inception Score (IS) <ref type="bibr" target="#b34">[35]</ref>, and Frechet Inception Distance (FID) <ref type="bibr" target="#b35">[36]</ref>. It also shows the average inference time per image on a NVIDIA GTX 1080 Ti GPU. These metrics are calculated over all 36,500 images of the Places2 validation set. Each image is assigned a randomly generated irregular mask. To examine the performance on various image sizes, we linearly scale images and masks to various dimensions. <ref type="table" target="#tab_1">Table 2</ref> shows that our proposed model achieves the lowest L1 loss and FID on 512?512 images. When the input images are greater than or equal to 1024?1024, our proposed model achieves the best results on all metrics. In addition, the proposed approach significantly outperforms other learning-based methods in terms of speed. In specific, it is 28.6% faster for 512?512 images, 3.5 times faster for 1024?1024 images, and 5.9 times faster for 2048?2048 images than the second-fastest method. Furthermore, the proposed model can inpaint 4096?4096 images in 87.3 milliseconds which is intractable with other learning-based methods due to limits of GPU memory. <ref type="figure" target="#fig_6">Figure 6</ref>, compares the high-resolution results of our CRA with those obtained via various super-resolution techniques. After obtaining a 512?512 inpainted result, we up-sample the output to the original size using different up-sampling methods including SRGAN <ref type="bibr" target="#b36">[37]</ref>, Nearest Neighbor, and Bicubic, then, we paste the filled contents to the original image. SRGAN <ref type="bibr" target="#b36">[37]</ref> is a learning-based method that can perform 4? superresolution and the official pre-trained model was trained on DIV2K. We can observe from that the hole region generated by CRA is generally sharper and visually more consistent with surrounding areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparisons of CRA with Super-resolution</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Comparisons With Traditional Methods</head><p>Moreover, we compare our method with two commercial products based on Patch-Match <ref type="bibr" target="#b37">[38]</ref> (Photoshop, Inpaint) and an open-source PatchMatch implementation for image inpainting <ref type="bibr" target="#b32">[33]</ref>  <ref type="figure" target="#fig_7">(Figure 7</ref>). We discover that PatchMatch-based methods are able to generate clear textures but with distorted structures incoherent with surrounding regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We presented a novel Contextual Residual Aggregated technique that enables more efficient and high-quality inpainting of ultra high-resolution images. Unlike other data-driven methods, the increase of resolutions and hole size does not deteriorate the inpainting quality and does not considerably increase the processing time in our framework. When tested on high-resolution images between 1K and 2K, our model is extremely efficient where it is 3x?6x faster than the state-of-the-art on images of the same size. Also, it achieves better quality by reducing FID by 82% compared to the state-of-the-art. We also compared our method with commercial products that showed significant superiority in certain scenarios. So far, our method is the only learningbased technique that can enable end-to-end inpainting on the ultra-high-resolution image (4K?8K). In the future, we will explore similar mechanisms for other tasks such as image expansion, video inpainting and image blending.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>More Test Results on Places2</head><p>More test results on places2 are presented in <ref type="figure" target="#fig_9">Figures 8, 9</ref>, 10, with input size 512?512, 1024?1024, 2048?2048 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sources of High-Resolution Images</head><p>Sources of the HD images in the main paper that are crawled from the internet are presented in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure Examples &amp; Limitation</head><p>Some failure examples of our model are presented in <ref type="figure" target="#fig_4">Figure 11</ref>. Our model is prone to fail when the majority parts of a background object are missing (Referring to the bicycle and dog face in <ref type="figure" target="#fig_2">Figure. 4</ref>).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The overall pipeline of the method: (top) CRA mechanism, (bottom) the architecture of the generator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Comparing down-sampling and up-sampling operators: (top) using Bilinear upsampling along with Averaging down-sampling generates more coherent textures with the surroundings. (bottom) using the Averaging down-sampling along with Nearest Neighbor produces tiling artifacts while Bilinear and Bicubic up-sampling perform equally well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Comparisons of different Gated Convolutions configurations. For example, the notation of LWGC sc +LWGC ds means: the coarse network uses single-channel LWGC and the refine network uses depth-separable LWGC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Training</head><label></label><figDesc>Procedure During training, color values of all images are linearly scaled to [?1, 1] in all experiments, and the mask uses 1 to indicate the hole region and 0 to indicate background. The masked image is constructed as x (1 ? m), where x is the input image and m is the binary mask, and represents dot product. Inpainting generator G takes concatenation of the masked image and mask as input, and predicts y = G(x, m) of the same size as the input image. The entire training procedure is illustrated in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 :</head><label>1</label><figDesc>Training of our proposed network initialization; while G has not converged do for i = 1,...,5 do Sampling batch images x from training data; Generating random masks m for x; Getting inpainted y ? G(x, m); Pasting backx ? y m + x (1 ? m); Sampling a random number ? ? U [0, 1]; Getting interpolationx ? (1 ? ?)x + ?x; Updating the discriminator D with loss L d ; end Sampling batch images x from training data; Generating random masks m for x; Getting Inpainted y ? G(x, m); Pasting backx ? y m + x (1 ? m); Updating generator G with loss Lg; end</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparisons using 512?512 (top) and 1024?1024 (bottom) images from Places2 validation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Comparisons of different super-resolution methods: the red squares area are zoomed-in for more details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Comparisons of our method with Inpaint (software), Photoshop content-aware fill and an open-source PatchMatch implementation<ref type="bibr" target="#b32">[33]</ref>. The masks for Photoshop and Inpaint are manually drawn, thus not guaranteed to be the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure ID in the main paper Image Source Figure 3</head><label>3</label><figDesc>top http://www.sohu.com/a/117062677_189010 Figure 6 top http://ow.ly/u8Wff Figure 6 bottom https://www.mafengwo.cn/yj/14103/s-0-0-0-0-1-0.htmlFigure 1 topright https://www.champaignoutdoors.com/kilimanjaro Images in demo.pps http://www.imecchina.com/news/1293274.html http://www.zdqx.com/wall/57962_6.html https://www.xuehua.us/2018/06/03/%E5%92%8C%E9%AB%98%E5%B0%94%E5%A4 %AB%E5%98%89%E6%97%85%E4%B8%80%E9%81%93-%E6%8E%A2%E5%AF%BB%E4%BB %99%E6%B9%96%E8%BE%B9%E7%9A%84%E6%85%A2%E7%94%9F%E6%B4%BB/zh-tw/ https://you.autohome.com.cn/details/68005/727cc0cec7214dd62e92d8f009e7adf9 https://www.reyfoto.com/ Attention Transfer Branch (P l=1 ): [P l=1 ] -ATM -K3S1C32 -K3S1C32D2 -concat Discriminator: K3S2C64 -K3S2C128 -K3S2C256 -K3S2C256 -K3S2C256 -K3S2C256 -fully connected to 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Test results on places2 validation datasets with input size of 512 ? 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 :</head><label>9</label><figDesc>Test results on places2 validation datasets with input size of 1024 ? 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 10 :Fig. 11 :</head><label>1011</label><figDesc>Test results on places2 validation datasets with input size of 2048 ? 2048. Failure examples of our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of parameters needed to compute gates Method Parameters Calculation H k , W k = 3 Cin, Cout = 32 GC [19] H k ? W k ? Cin ? Cout 9216 LWGC ds H k ? W k ? Cin + Cin ? Cout 1312 LWGC pw Cin ? Cout 1024 LWGC sc H k ? W k ? Cin ? 1 288 variations of LWGC that we propose are named as: depth-separable LWGC (LWGC ds ), pixelwise LWGC (LWGC pw ), and single-channel LWGC (LWGC sc ). They differ by the computation of the gate branch, G:</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Quantitative evaluation results on Places2 validation set. Note that certain models cause Out-Of-Memory (OOM) error when tested on 2K or 4K images, thus the corresponding cells are left empty.</figDesc><table><row><cell>Image Size</cell><cell>512 ? 512</cell><cell>1024 ? 1024</cell><cell cols="2">2048 ? 2048</cell><cell></cell><cell></cell><cell></cell><cell cols="2">4096 ? 4096</cell></row><row><cell>Metrics</cell><cell cols="10">L1 MS-SSIM FID IS Time L1 MS-SSIM FID IS Time L1 MS-SSIM FID IS Time L1 MS-SSIM FID IS Time</cell></row><row><cell cols="3">DeepFillV1[20] 6.733 0.8442 7.541 17.56 62ms 7.270 0.8424 10.21 17.69 663ms -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">DeepFillV2[19] 6.050 0.8848 4.939 18.20 78ms 6.942 0.8784 8.347 17.04 696ms -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">PEN-Net[21] 9.732 0.8280 14.13 14.51 35ms 10.42 0.8128 19.36 12.51 289ms -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">PartialConv[16] 8.197 0.8399 29.32 13.13 35ms 11.19 0.8381 32.20 13.53 110ms 16.19 0.8373 41.23 11.17 920ms -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="8">Global-local[10] 8.617 0.8469 21.27 13.48 53ms 9.232 0.8392 26.23 13.05 219ms 9.308 0.8347 27.09 12.61 219ms -</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Ours</cell><cell cols="10">5.439 0.8840 4.898 17.72 25ms 5.439 0.8840 4.899 17.72 31ms 5.492 0.8840 4.893 17.85 37ms 5.503 0.8840 4.895 17.81 87.3ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Sources of some HD images used for test</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>We want to acknowledge Peng Deng, Shao Hua Chen, Xinjiang Sun, Chunhua Tian and other colleagues in Huawei Technologies for their support in the project.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Network Architectures</head><p>In addition to Section 3.3 and <ref type="figure">Figure 2</ref> in the main paper, we report more details of our network architectures. For simplicity, we denote them with K (kernel size), S (stride size), C (channel number) and D (dilation rate). D is neglected when D=1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 27th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Region filling and object removal by exemplar-based image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1200" to="1212" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fragment-based image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Drori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yeshurun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="303" to="312" />
			<date type="published" when="2003" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Statistics of patch offsets for image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="16" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hole filling through photomontage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilczkowiak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tordoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2005-Proceedings of the British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image inpainting by patch propagation using patch sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1153" to="1165" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Texture synthesis by non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh IEEE international conference on computer vision</title>
		<meeting>the seventh IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1033" to="1038" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">107</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Edge-aware context encoder for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3156" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Foreground-aware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High-resolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contextual-based image inpainting: Infer, match, and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Jay Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Free-form image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning pyramid-context encoder network for highquality image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The laplacian pyramid as a compact image code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Burt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on communications</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="532" to="540" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep generative image models using laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image fusion and image quality assessment of fused images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bhosale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Image Processing (IJIP)</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">484</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A classified and comparative study of edge detection algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharifi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Mahmoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. International conference on information technology: Coding and computing</title>
		<meeting>International conference on information technology: Coding and computing</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="117" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image fusion by a ratio of low-pass pyramid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="245" to="253" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<title level="m">Multi-scale context aggregation by dilated convolutions</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>intelli- gence 40</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1452" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haris</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
	<note>Ntire 2018 challenge on single image super-resolution: Methods and results</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">mage completion using patchmatch algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Fu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">F E</forename></persName>
		</author>
		<ptr target="https://github.com/YuanTingHsieh/Image_CompletionAc-cessed" />
		<imprint>
			<biblScope unit="page" from="2019" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Multiscale structural similarity for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thrity-Seventh Asilomar Conference on Signals, Systems &amp; Computers</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1398" to="1402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Patchmatch: A randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2009" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">K3S1C3 -clip Attention Computing Branch</title>
		<idno>K3S1C64 -K3S1C64 -concat -upsample(2?) -K3S1C32 -K3S1C32 -concat -upsample(2?</idno>
	</analytic>
	<monogr>
		<title level="m">ACM -ATM Attention Transfer Branch (P l=3 ): [P l=3 ] -ATM -K3S1C128 -concat Attention Transfer Branch</title>
		<imprint/>
	</monogr>
	<note>P l= -concat -K3S1C128 -upsample(2?. P l=3 ] -downsample (2?) -[P. P l=2. P l=2 ] -ATM -K3S1C64 -K3S1C64D2 -concat</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
