<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pattern Recognition Letters FFAVOD: Feature Fusion Architecture for Video Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-09-15">15 Sep 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hughes</forename><surname>Perreault</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chemin de Polytechnique</orgName>
								<orgName type="institution">Polytechnique Montr?al</orgName>
								<address>
									<postCode>2500, H3T 1J4</postCode>
									<settlement>Montr?al</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume-Alexandre</forename><surname>Bilodeau</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chemin de Polytechnique</orgName>
								<orgName type="institution">Polytechnique Montr?al</orgName>
								<address>
									<postCode>2500, H3T 1J4</postCode>
									<settlement>Montr?al</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Saunier</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Chemin de Polytechnique</orgName>
								<orgName type="institution">Polytechnique Montr?al</orgName>
								<address>
									<postCode>2500, H3T 1J4</postCode>
									<settlement>Montr?al</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maguelonne</forename><surname>H?ritier</surname></persName>
							<affiliation key="aff1">
								<address>
									<addrLine>2280 Boulevard Alfred Nobel</addrLine>
									<postCode>H4S 2A4</postCode>
									<settlement>Genetec, Montr?al</settlement>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pattern Recognition Letters FFAVOD: Feature Fusion Architecture for Video Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-09-15">15 Sep 2021</date>
						</imprint>
					</monogr>
					<note>1 journal homepage: www.elsevier.com</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A significant amount of redundancy exists between consecutive frames of a video. Object detectors typically produce detections for one image at a time, without any capabilities for taking advantage of this redundancy. Meanwhile, many applications for object detection work with videos, including intelligent transportation systems, advanced driver assistance systems and video surveillance. Our work aims at taking advantage of the similarity between video frames to produce better detections. We propose FFAVOD, standing for feature fusion architecture for video object detection. We first introduce a novel video object detection architecture that allows a network to share feature maps between nearby frames. Second, we propose a feature fusion module that learns to merge feature maps to enhance them. We show that using the proposed architecture and the fusion module can improve the performance of three base object detectors on two object detection benchmarks containing sequences of moving road users. Additionally, to further increase performance, we propose an improvement to the SpotNet attention module. Using our architecture on the improved SpotNet detector, we obtain the state-of-the-art performance on the UA-DETRAC public benchmark as well as on the UAVDT dataset. Code is available at https://github.com/hu64/FFAVOD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection as meant in this paper is the task of finding rectangular bounding boxes that tightly bound objects of interest in an image. Video object detection uses the temporal information of a video in order to gain an edge over single frame detection. Object detection in general has been largely dominated by deep learning approaches in the last few years. It is quite a vibrant subject that has seen a lot of research. Its cousin task, video object detection, has seen a lot less activity on the other hand. To capitalize on video information, some methods aim to speed up the inference process by estimating feature maps <ref type="bibr" target="#b21">(Liu and Zhu, 2018)</ref>, and some aim to merge feature maps by optical flow warping <ref type="bibr" target="#b46">(Zhu et al., 2017)</ref>. In contrast, in this paper, we develop an end-to-end architecture and train it to merge feature maps without external methods or explicit knowledge of temporal relations.</p><p>The applications for video object detection are certainly not lacking, for instance in intelligent transportation systems, such e-mail: hughes.perreault@polymtl.ca (Hughes Perreault) <ref type="figure">Fig. 1</ref>: In this image from the UA-DETRAC dataset <ref type="bibr" target="#b41">(Wen et al., 2015)</ref>, one can see multiple examples where the proposed architecture applied on Spot-Net <ref type="bibr" target="#b29">(Perreault et al., 2020a)</ref> (blue) outperforms its baseline (yellow). Detections by both models are shown in green (considering a minimum IOU score of 0.8 to match them). Examples of improved performance include cases of occlusion and of smaller objects (top center and both top corners). The blurred rectangles represent regions excluded for the evaluation, but where the proposed architecture nonetheless can make better detections.</p><p>as advanced driver assistance systems and video surveillance. In a world where robotics and automation are growing, the number of these applications can only increase.</p><p>Using multiple frames can improve results in situations that include occlusions, blur and smaller objects (see in <ref type="figure">figure 1</ref>). Indeed, there are several ways in which using multiple frames can help to detect objects on a target frame. In a video sequence, the objects of interest appear over and over again under different lighting, angles and occlusion conditions. Over a number of consecutive frames, one of them will contain the best view of an object of interest. One can even go even further, for each pixel location in our target frame, one of the temporally close frames will contain the best object features for this location. Of course, the features cannot be too far temporally because the object of interest might have moved too much in that case. FFAVOD aims to learn an operation to best merge the feature maps of several frames, at each image location. We propose three main contributions: an adaptable video object detection architecture that can be inserted into multiple object detection methods, a module for the fusion of feature maps in order to enrich them, and an improvement to the Spot-Net <ref type="bibr" target="#b29">(Perreault et al., 2020a)</ref> attention module. The result of these contributions is a framework called FFAVOD, which is trainable end-to-end for video object detection and classification. This work generalizes the work published in <ref type="bibr" target="#b30">(Perreault et al., 2020b)</ref> by extending it and testing it with several baseline object detector architectures.</p><p>The evaluation of our method is focused on traffic surveillance scenes, since they contain most of the challenges we aim to solve with our method and they are our target application. The method is evaluated on two datasets, UA-DETRAC <ref type="bibr" target="#b41">(Wen et al., 2015)</ref> and UAVDT <ref type="bibr" target="#b6">(Du et al., 2018)</ref>. We tested our method by incorporating it into several base networks and comparing them to their corresponding baselines and other state-ofthe-art methods on each dataset. A consistent and significant improvement over the base networks is demonstrated, as well as strong overall results on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Object Detection</head><p>The object detection benchmarks have been systematically dominated by deep learning-based methods in the last few years. They can be broadly divided in three categories: twostage, single-stage and anchor-free.</p><p>The two-stage category contains methods that use an object proposal phase where object candidates are proposed, and then refined into final predictions. R-CNN <ref type="bibr" target="#b10">(Girshick et al., 2014)</ref> is the first dominant detector to make use of a CNN. However, the object proposal method it uses is an external one, selective search. The CNN is used to extract features for every object proposal. Those features are then classified using an SVM. Fast R-CNN <ref type="bibr" target="#b9">(Girshick, 2015)</ref> improves upon it by passing the image into a CNN only once, and then cropping the features from the resulting feature map for the corresponding region of the image for each object proposal. The third iteration of the method, Faster R-CNN <ref type="bibr" target="#b35">(Ren et al., 2015)</ref>, removes the external object proposal method. It does so by using two CNNs, one that proposes object candidates called the region proposal network (RPN), and another that classifies and refines the bounding box for each candidate. The two CNNs share most parameters, making Faster R-CNN very efficient with accurate results. R-FCN <ref type="bibr" target="#b5">(Dai et al., 2016)</ref> is a variant of Faster R-CNN, where the objects are detected as a grid of parts of objects where each cell of the grid votes, making it better for accurately positioning objects. The method Evolving Boxes  is an efficient vehicle detection network that includes a proposal subnetwork and an early discard sub-network. It generates candidates with multiple representations and later refines and classifies those candidates.</p><p>The single-stage category improves over the two-stage methods by speeding up the inference process to eventually arrive to real-time detection. Single-stage refers to the removal of the object proposal phase. The original method YOLO <ref type="bibr" target="#b32">(Redmon et al., 2016)</ref> is the first CNN-based method to reach real-time speed. It divides the image into a grid, and makes each cell predict two bounding boxes using regression. Two subsequent versions of YOLO were proposed, YOLOv2 <ref type="bibr" target="#b32">(Redmon et al., 2016)</ref> and YOLOv3 <ref type="bibr" target="#b34">(Redmon and Farhadi, 2018)</ref>, with various improvements. SSD <ref type="bibr" target="#b23">(Liu et al., 2016)</ref> addresses the challenge of detecting objects at multiple scales by using feature maps at multiple levels in the CNN. A sliding window approach is then used to perform classification and regression with anchor boxes at fixed aspect ratios and scales. RetinaNet <ref type="bibr" target="#b18">(Lin et al., 2018)</ref> is an improvement upon SSD that uses a different loss function, the focal loss, that aims to fix the asymmetry between positive and negative examples during training. RetinaNet also incorporates a feature pyramid network (FPN) , a network that builds a pyramid of features at different scales by using lateral and vertical connections on feature maps of the CNN. Using a sliding window, RetinaNet then classifies and regresses on these pyramid levels using anchor boxes.</p><p>More recently, a different approach to object detection was proposed. It is called anchor-free since it replaces the use for anchor boxes by detecting objects as keypoints. Cor-nerNet <ref type="bibr" target="#b13">(Law and Deng, 2018)</ref> first introduced this approach by detecting objects as a pair of keypoints, the top-left and the bottom-right corners. A learned embedding allows the method to later pair the corresponding corners using the similarity between the embedding vectors. CenterNet (keypoint triplets) <ref type="bibr" target="#b7">(Duan et al., 2019)</ref> builds upon this idea by adding a third learned keypoint, the center of the object, which is used to remove false positives, since two corners without a center are not likely to be part of an object. CenterNet (objects as points) <ref type="bibr" target="#b45">(Zhou et al., 2019)</ref> uses a different approach, and instead trains a network to detect objects as a single center keypoint, using center heatmaps for each label, as well as regressions for the width and height and the offset. A variant of Center-Net (objects as points), SpotNet <ref type="bibr" target="#b29">(Perreault et al., 2020a)</ref>, makes use of semi-supervised segmentation annotations to train a selfattention mechanism within the network and thus increase its performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Object Detection</head><p>A first way to perform video object detection is to combine the features of several frames. Flow Guided Feature Aggregation (FGFA) <ref type="bibr" target="#b46">(Zhu et al., 2017)</ref> makes use of optical flow warping to merge temporally close frames to improve accuracy. In MANet <ref type="bibr" target="#b38">(Wang et al., 2018)</ref>, an optical flow estimation is done and two networks are trained, one to do pixel-level calibration (detailed motion adjustments) and another one for instance-level calibration (global motion adjustments). Some works take advantage of recurrent neural networks, for example STMM (Xiao and Jae Lee, 2018) that models the motion and appearance of an object within a video sequence. In <ref type="bibr" target="#b21">(Liu and Zhu, 2018)</ref>, Long Short-Term Memories (LSTMs) are used to interpolate feature maps, which increases the inference speed greatly. Multi-frame Single Shot Detector <ref type="bibr" target="#b3">(Broad et al., 2018)</ref> builds upon SSD by adding a temporal information with a recurrent convolutional module. <ref type="bibr" target="#b2">(Bertasius et al., 2018)</ref> used deformable convolutions to compute offsets between temporally close frames. Using these offsets they can share some features from neighboring frames to better perform detection. 3D-DETNet <ref type="bibr" target="#b15">(Li and Chen, 2018</ref>) makes use of 3D convolutions on temporally close frames that are concatenated in order to produce better feature maps. <ref type="bibr" target="#b28">(Perreault et al., 2019)</ref> experimented with training networks on image pairs. Since no pre-trained weights were available, they could only outperform the single frame baseline when training from scratch. The same problem is faced when using 3D convolutions. In contrast to these previous methods, we also merge feature maps, but instead of aligning the feature maps or using 3D convolutions, we train a network to fuse directly the raw features maps from several frames.</p><p>Another possible avenue is to combine detection and tracking. TrackNet <ref type="bibr" target="#b14">(Li et al., 2019)</ref> extends the Faster R-CNN framework by directly detecting a 3D cube bounding a moving object. In Joint detection and tracking in videos with identification features <ref type="bibr" target="#b25">(Munjal et al., 2020)</ref>, the authors train a multitask model by joint optimization of detection, tracking and re-identification. The Global Correlation Network <ref type="bibr" target="#b20">(Lin et al., 2021)</ref> also jointly trains the detection and the tracking task by first training the detection module before fine-tuning the whole network. Finally, motion information can be integrated to the network. Illuminating Vehicles With Motion Priors For Surveillance Vehicle Detection <ref type="bibr" target="#b40">(Wang et al., 2020)</ref> integrates motion in a network in order the illuminate the vehicles and suppress false positives. To better detect tiny objects, MMA  proposes a dual stream network, an appearance stream and a motion stream. They also integrate a memory attention module to help select discriminating features using the temporal information. MFMNet <ref type="bibr" target="#b24">(Liu et al., 2019</ref>) uses a motion from memory module to encode the temporal context. This module contains a separate dynamic memory for every input sequence, and produces motion features for every frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature Fusion Strategies</head><p>Since we consider combining the features of several frames, we also briefly review common feature fusion strategies. Most methods consider the concatenation of features or their sum. In the inception network <ref type="bibr" target="#b36">(Szegedy et al., 2015)</ref>, convolutions with kernels of various sizes are used to produce a set of feature maps which are then combined using concatenation. In the feature pyramid network , pyramid levels are combined using upsampling and addition, using a top-down approach. In FSSD <ref type="bibr" target="#b16">(Li and Zhou, 2017)</ref>, a variant of the feature pyramid is proposed where feature maps from all levels are first concatenated together, and later used to create all the pyramid levels. In ExFuse , low-level and high-level semantic information are merged after introducing spatial information into the high-level features and semantic information into the low-level features. The feature maps are combined using additions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The task we aim to solve can be described as placing a bounding box and label on every object of interest in a target image, using the target image as well as n frames both before and after the target frame. To do so, two main contributions are presented. First, we design an architecture for object detection that allows the use of temporally close frames and that is adaptable to multiple base networks. Second, we propose a fusion module to merge feature maps of the same dimension from temporally close frames. In order to achieve high accuracy with less training, we specifically designed this architecture to use pre-trained weights from state-of-the-art single frame methods. In order to achieve state-of-the-art detection results, we also propose an improvement to the SpotNet attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Frame Fusion Architecture</head><p>The idea behind our architecture is to compute feature maps for frames in a video only once for each frame and, when performing detection for a target frame, use the feature maps that are already computed for the temporally close frames to enhance the target frame feature map. Thanks to this idea of computing once and reusing, we also propose a novel fusion module that is used deep in the base network. We do not have to merge early in order the save computation time since we reuse the feature maps for detecting in the next frames. It is inserted between the backbone and the regression and classification heads of a base object detection network.</p><p>Our frame fusion architecture for video object detection (FFAVOD) takes multiple images as input, and they are merged as one feature map deeper in the network, as shown in figure 2. For a target frame at time t, we use a window of 2n + 1 frames, that is n frames before and after the frame t. We cannot use a n that is too high because the positions of objects of interest at the different frames will become too different and it will not be possible to share the information. When facing "boundary conditions", meaning that we are too close to the beginning or end of a sequence to take frame t ? n or t + n for instance, we simply duplicate the first or last frame. For example, not having access to t ? 2 but to t ? 1, with n = 2, we would use the five frames t ? 1, t ? 1, t, t + 1 and t + 2.</p><p>The way feature maps are merged is adapted to each base detector. For example, for a network like RetinaNet, the three outputs used to create the feature pyramid network are merged. For networks like CenterNet, the outputs of the double stacked hourglass network used as backbone are merged. The merged feature map is used to create the center keypoint heatmaps. Details about which layers are merged are provided for three networks in section 4.1. The fusion process is done with our custom fusion module described below, that enhances the target frame feature map. During the inference process, we can reuse feature maps already computed as we progress sequentially in the video, thus saving time for computing detections at every frame. However, during the training process, multiple images must e used for every ground-truth example, thus requiring more memory and time to train. We do believe that the extra time required to train the model is worth the better detection performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Fusion Module</head><p>A small trainable module is implemented to merge feature maps of temporally close frames (see <ref type="figure" target="#fig_1">figure 3</ref>). The idea for combining feature maps is the intuitive way a human would approach the task. For instance, when you look at one location for a given channel, you might want to average responses over the feature maps from the different frames, or look for the maximum response and only keep this one (like in a max pooling). That would mean the feature would come from the frame where it is best seen. Taking the average responses or the maximum response for merging the feature maps depends on the situation. Therefore, we let the neural network learn the merging operation by itself, using 1 ? 1 convolutions over the channels.</p><p>1 ? 1 convolutions are often used to adapt the dimension of feature maps. In GoogLeNet, 1 ? 1 convolutions are used to reduce the dimension of tensors, allowing the network to be deeper and remaining efficient. Recently, the network Bor-derDet <ref type="bibr" target="#b31">(Qiu et al., 2020)</ref> uses 1 ? 1 convolutions to learn to produce border sensitive feature maps. In contrast to these previous works, we use 1 ? 1 convolutions to combine features from several frames.</p><p>The fusion module receives 2n + 1 feature maps, each of dimension w * h * c, as input (see <ref type="figure" target="#fig_1">figure 3)</ref>. Its output is one merged feature map of dimension w * h * c. The feature maps taken as input come from the same backbone networks that share parameters. For 2n + 1 feature maps of dimension w * h * c, we slice every c channels and concatenate them. The result is c tensors of shape w * h * (2n + 1). A two dimensional convolution is performed on these tensors with a kernel of shape 1 ? 1 (1 * 1 * (2n + 1)) and an output depth of 1, resulting in c tensors of shape w * h. We finally concatenate the resulting tensors channel-wise to obtain the w * h * c feature map that is our output. This module thus learns the optimal operation to combine feature maps for the domain on which it is fine tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">SpotNet improvement</head><p>In order to further push the detection accuracy, we propose an improvement to the SpotNet (Perreault et al., 2020a) attention module. Instead of using three 3?3 convolutions to produce the saliency map, we designed and trained a small U-Net segmentation network. The U-Net has four levels and thus reduces the spatial resolution by half four times while doubling the channel resolution four times also, before reversing these changes and returning to the original resolution. This allows the network to produce finer saliency maps and improves detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Base object detectors</head><p>The proposed architecture is implemented with several base object detector networks. It was first tested using Reti-naNet <ref type="bibr" target="#b18">(Lin et al., 2018)</ref> for its speed and accuracy, along with two other state-of-the-art object detection methods, CenterNet (objects as points) <ref type="bibr" target="#b45">(Zhou et al., 2019)</ref> and SpotNet <ref type="bibr" target="#b29">(Perreault et al., 2020a)</ref>. For the RetinaNet base model, the backbone is a VGG-16 network <ref type="bibr" target="#b22">(Liu and Deng, 2015)</ref>. For the CenterNet and SpotNet base models, the backbone is a stacked hourglass network <ref type="bibr" target="#b26">(Newell et al., 2016)</ref>. For RetinaNet, we merged the three outputs that are used to create the feature pyramid network. The network is the same otherwise. For CenterNet and SpotNet, we merged the outputs of the double stacked hourglass network used as backbone for these detectors. The merged feature map is used to create the center keypoint heatmaps, but the other heads use the target frame feature map. Experiments showed that this worked better than if all the heads used the merged feature maps, maybe due to the fact that the center heatmaps use general spatial features more, and the other heads might be more specialized in some specific semantic features in the feature map that do not answer well to merging between frames. The fusion process is done with our custom fusion module as described above, that enhances the target feature map by merging. Illustrations of the three evaluated models can be found at https://github.com/hu64/FFAVOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets</head><p>Since our method relies on temporally close frames, it must be assessed on video datasets. The chosen evaluation domain is traffic surveillance, since it is of great interest to us and there are many applications for video object detection. Two video datasets in the traffic surveillance domain were selected: UA-DETRAC <ref type="bibr" target="#b41">(Wen et al., 2015)</ref> (recorded with a fixed camera) and the Unmanned Aerial Vehicle Benchmark (UAVDT) <ref type="bibr" target="#b6">(Du et al., 2018)</ref> (recorded with a mobile camera). The versatility of our architecture is demonstrated by using videos from both fixed and mobile cameras. These two datasets are largely different, UA-DETRAC contains sequences taken by cameras fixed above highways and intersections, with medium sized objects. UAVDT, on the other hand, contain sequences taken by drones that hover over roads at different altitudes, but generally with much higher viewpoints than UA-DETRAC. Therefore the objects are significantly smaller and denser in that dataset, and also the background is changing across the sequence, making it more challenging to merge feature maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementations Details</head><p>The neural networks are implemented in Keras <ref type="bibr" target="#b4">(Chollet et al., 2015)</ref> using the TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref> backend for our RetinaNet base detector. Our CenterNet and SpotNet base detectors are implemented using Pytorch <ref type="bibr" target="#b27">(Paszke et al., 2017)</ref>.</p><p>The same training protocol was used for the all the base detectors to demonstrate the contribution of our approach. The training process is done in two steps. First, the base detector is fine-tuned on each dataset starting from pre-trained weights on MS COCO <ref type="bibr" target="#b19">(Lin et al., 2014)</ref>. Second, the shallower layers of the backbone are frozen and the fusion module as well as the network heads are trained. The reason for freezing the shallower layers is that the network seems to have difficulty training the fusion module while also training every other layers. Doing it in two steps seems to facilitate the learning.</p><p>A VGG-16 backbone with a feature pyramid of five levels is used for RetinaNet. RetinaNet takes the outputs of the last three blocks of VGG-16 to build the five-level feature pyramid, three levels of the same dimension of the three VGG-16 blocks, and two smaller. The fusion module is inserted between the VGG-16 and the feature pyramid. As a result, our fusion module is duplicated three times in the network.</p><p>A double stacked hourglass network is used as the backbone network for CenterNet and SpotNet. The fusion module is inserted after the end of the second hourglass. During training, the five frames are therefore passed through the same double stacked hourglass (all the parameters are shared), then passed through the fusion module, and the network continues as usual after that.</p><p>To determine the number of frames n used by our model, an ablation study is performed and the results are shown in figure 4. We end up using n = 2, i.e. a window of five frames in total, which showed the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance Evaluation</head><p>For the evaluation process, the test set is predetermined on each dataset. The training data is split into training and validation. The split is done by video sequence and not by frame to prevent overfitting on the validation data. The same split of three sets is employed for all of our experiments. We trained by monitoring the validation loss every epoch and select the best model according by the validation loss. Results were then computed on the test set.</p><p>The results are evaluated following the dataset protocols, using the Mean Average Precision (mAP). The mAP is the mean of the average precisions for every class. The average precision is the average precision under different recall values, which can also be described as the area under the precision-recall curve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results and Discussion</head><p>The results on the UA-DETRAC dataset are reported in table 1. The proposed FFAVOD applied to the three base detectors consistently outperforms them. The FFAVOD applied on SpotNet achieves the state-of-the-art (SOTA) result on this dataset. Our approach improves the performance of the base detectors particularly well on harder categories like "hard" and "rainy", hinting that indeed our FFAVOD allows the network to overcome challenges present on the harder examples, but has less impact on the easier ones. There is a significant improvement of the detection results when using our FFAVOD with the RetinaNet detector as well as on the CenterNet detector. It is also true for SpotNet, but the improvement is smaller. This shows that our proposed feature fusion architecture can capitalize on multiple frames to improve object detection, and that our approach is applicable to several networks. In general, the performance is never reduced when using FFAVOD, except in the "Sunny" case for SpotNet where the mAP is virtually identical, due to the examples being easy. Even though they achieve good results, no other detector using multiple frames can outperform the results that we obtained with modern detectors (FFAVOD-CenterNet and FFAVOD-SpotNet). This shows that our approach capitalizes better on the multiple frames since it can be integrated with SOTA single frame detectors to improve them. Also, our SpotNet with U-Net outperforms the original SpotNet by around 0.5 to 1% in both datasets, showing the significance of using a better attention module.</p><p>The results on the UAVDT dataset are reported in table 2. Our FFAVOD detectors consistently outperform their respective base detectors. On the SpotNet architecture, the FFAVOD achieves also the state-of-the-art result on this dataset.</p><p>The improvement is smaller on SpotNet. Our hypothesis for why that is is that the attention module of SpotNet might help detect many of the same objects (small or occluded) that our fusion module does. Nevertheless, the improvement is still significant. On UA-DETRAC the improvement is almost zero for the "Easy" category, but close 1% for the "Hard" category, which is very consistent with the idea that the fusion module helps overcome challenges on hard examples. On UAV, the improvement is consistent with UA-DETRAC. It is also important to note that the better we perform on a dataset, the harder it becomes to improve our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head><p>An ablation study was designed to evaluate the contribution of the different parts of the proposed FFAVOD. The two contributions were isolated and analyzed. On the UA-DETRAC dataset using SpotNet, the fusion module was removed and replaced with a simple concatenation followed by a convolution. Instead of channel by channel grouping and convolution then reordering, we simply concatenate the feature maps and reduce the dimension with a convolution. Replacing the fusion module with concatenation decreased the performance by a large margin as shown in table 3 (Concatenation). We believe that combining feature maps in this fashion is noisy and the model might require more parameters in order to learn how to combine <ref type="figure">Fig. 4</ref>: mAP reported on the UA-DETRAC test set for different values of n of FFAVOD them. The fusion module on the other hand is a much more direct and efficient way of combining feature maps. As baseline fusion methods, we also tried using a mean, maximum and a median operation, and we show that the results are worse than the single frame method. Additionally, to justify the use of past and future frames, we did an experiment where we retrain our model using 2n past frames with the target frame for n = 1 and n = 2. This decreased performance dramatically, as the fusion module has a hard time focusing on the target frame and the positions of the objects get too different (features are not aligned). Indeed, the distribution of the spatial information is no longer centered around the target frame, and this causes misalignment of the bounding boxes.</p><p>In order to select the number of frames, FFAVOD was applied by varying the number of frames used by the network (see <ref type="figure">figure 4)</ref>. Each model using a particular n is re-trained by freezing the base network weights and training only the fusion module weights. We show that we obtain the highest result by using 5 frames, and thus that is what we used to produce our main results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Limitations</head><p>One obvious limitation of our architecture is the fact that it needs video sequences with temporally close frames in order to work. This limitation is mitigated by the fact that a lot of applications for object detection rely on such data. Another limitation is the memory usage increase in inference, where FFAVOD has to keep feature maps stored in memory for several iterations before releasing them when they are no longer in the target frame window. This makes our architecture not ideal for embedded applications. Finally, without some sort of tracking, the temporal scope is also somewhat limited, making bigger improvements in accuracy difficult.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We introduce FFAVOD, a new method for video object detection which can be applied and used with most standard object detectors. Using the proposed approach, we trained and evaluated our fusion module on two datasets from the traffic surveillance domain. We demonstrate with three different base detectors that performance can be significantly increased by helping solve challenges in the harder examples of the datasets. Additionally, we propose an improvement to the SpotNet attention <ref type="table">Table 1</ref>: mAP of FFAVOD applied to base detectors on the UA-DETRAC test set compared their respective base detectors, as well as classic state-of-the-art detectors. FFAVOD uses n = 2. Results for FFAVOD and their base detectors are generated using the official toolkit from the UA-DETRAC website. Boldface indicates the best result overall, Underline indicates the best result within a section, while Italic indicates the baseline and *indicates the use of multiple frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Detector</head><p>Overall  SSD <ref type="bibr" target="#b23">(Liu et al., 2016)</ref> 33.62%</p><p>Faster-RCNN <ref type="bibr" target="#b35">(Ren et al., 2015)</ref> 22.32% RON <ref type="bibr" target="#b12">(Kong et al., 2017)</ref> 21.59% module that increases the detection accuracy. Several ideas may be tried to further increase performance, for example by adding temporal coherence in the form of re-identification of features across frames, or by integrating tracking in parallel to detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgment</head><p>We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC), , and the support of Genetec.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>A visual representation of FFAVOD with a window of 5 frames (n = 2). Frames are passed through the backbone network of the base object detection network, and the fusion module takes their outputs as input. Finally, the fusion module outputs a fused feature map compatible with the base object detection network, and the base object detection heads are applied to the fused feature map to classify the object categories and regress the bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>The fusion module. Channels are represented by colors. The fusion module is composed of channel grouping, concatenation followed by 1 ? 1 convolution and a final re-ordering of channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>SpotNet with U-Net 88.10% 97.82% 92.84% 79.14% 91.25% 89.55% 82.85% 91.72% SpotNet (Perreault et al., 2020a) with U-Net 87.76% 97.78% 92.57% 78.59% 90.88% 89.28% 82.47% 91.83% Liu et al., 2019) 69.10% 90.49% 75.21% 53.53% 83.66% 73.97% 56.11% 72.15% *RN-D (Perreault et al., 2019) 54.69% 80.98% 59.13% 39.23% 59.88% 54.62% 41.11% 77.53% *3D-DETnet (Li and Chen, 2018) 53.30% 66.66% 59.26% 43.22% 63.30% 52.90% 44.27% 71.26% SOTA methods (single frame) FG-BR Net (Fu et al., 2019) 79.96% 93.49% 83.60% 70.78% 87.36% 78.42% 70.50% 89.8% HAT (Wu et al., 2019) 78.64% 93.44% 83.09% 68.04% 86.27% 78.00% 67.97% 88.78% GP-FRCNNm (Amin and Galasso, 2017) 77.96% 92.74% 82.39% 67.22% 83.23% 77.75% 70.17% 86.56% R-FCN (Dai et al., 2016) 69.87% 93.32% 75.67% 54.31% 74.38% 75.09% 56.21% 84.08% EB (Wang et al., 2017) 67.96% 89.65% 73.12% 53.64% 72.42% 73.93% 53.40% 83.73% Faster R-CNN (Ren et al., 2015) 58.45% 82.75% 63.05% 44.25% 66.29% 69.85% 45.16% 62.34% YOLOv2 (Redmon and Farhadi, 2017) 57.72% 83.28% 62.25% 42.44% 57.97% 64.53% 47.84% 69.75%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="3">Easy Medium Hard</cell><cell cols="2">Cloudy Night</cell><cell>Rainy</cell><cell>Sunny</cell></row><row><cell>SpotNet</cell><cell>*FFAVOD-SpotNet (Perreault et al., 2020a)</cell><cell cols="8">86.80% 97.58% 92.57% 76.58% 89.38% 89.53% 80.93% 91.42%</cell></row><row><cell>CenterNet</cell><cell>*FFAVOD-CenterNet</cell><cell cols="8">86.85% 97.47% 92.58% 76.51% 89.76% 89.52% 80.80% 90.91%</cell></row><row><cell></cell><cell>CenterNet(Duan et al., 2019)</cell><cell cols="8">83.48% 96.50% 90.15% 71.46% 85.01% 88.82% 77.78% 88.73%</cell></row><row><cell>RetinaNet</cell><cell>*FFAVOD-RetinaNet</cell><cell cols="8">70.57% 87.50% 75.53% 58.04% 80.69% 69.56% 56.15% 83.60%</cell></row><row><cell></cell><cell>RetinaNet (Lin et al., 2018)</cell><cell cols="8">69.14% 86.82% 73.70% 56.74% 79.88% 66.57% 55.21% 82.09%</cell></row><row><cell></cell><cell>*Joint (Munjal et al., 2020)</cell><cell>83.80%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>*Illuminating (Wang et al., 2020)</cell><cell cols="8">80.76% 94.56% 85.90% 69.72% 87.19% 80.68% 71.06% 89.74%</cell></row><row><cell></cell><cell>*MMA (Hu et al., 2019)</cell><cell>74.88%</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SOTA methods (multiple frames)</cell><cell>*Global (Lin et al., 2021)</cell><cell cols="4">74.04% 91.57% 81.45% 59.43%</cell><cell>-</cell><cell cols="3">78.50% 65.38% 83.53%</cell></row><row><cell></cell><cell>*Perceiving Motion (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>mAP of our FFAVOD applied to detectors on the UAVDT test set compared their respective base detectors. FFAVOD uses n = 2. Results for our FFAVOD and their base detectors are generated using the official Matlab toolkit provided by the authors. The other results are taken from their respective papers. Boldface indicates the best result overall, Underline indicates the best result within a section, while Italic indicates the baseline and *indicates the use of multiple frames.</figDesc><table><row><cell></cell><cell>Detector</cell><cell>Overall</cell></row><row><cell></cell><cell>*FFAVOD-SpotNet with U-Net</cell><cell>53.76%</cell></row><row><cell>SpotNet</cell><cell cols="2">SpotNet (Perreault et al., 2020a) with U-Net 53.38%</cell></row><row><cell></cell><cell>SpotNet (Perreault et al., 2020a)</cell><cell>52.80%</cell></row><row><cell>CenterNet</cell><cell>*FFAVOD-CenterNet</cell><cell>52.07%</cell></row><row><cell></cell><cell>CenterNet (Zhou et al., 2019)</cell><cell>51.18%</cell></row><row><cell>RetinaNet</cell><cell>*FFAVOD-RetinaNet</cell><cell>39.43%</cell></row><row><cell></cell><cell>RetinaNet(Lin et al., 2018)</cell><cell>38.26%</cell></row><row><cell></cell><cell>LRF-NET (Wang et al., 2019)</cell><cell>37.81%</cell></row><row><cell></cell><cell>R-FCN (Dai et al., 2016)</cell><cell>34.35%</cell></row><row><cell>SOTA methods</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Different fusion strategies to conduct an ablation study. Results are generated using the official Matlab toolbox provided by the authors. Boldface indicates the best result overall</figDesc><table><row><cell>n</cell><cell>Fusion Method</cell><cell>mAP</cell></row><row><cell>2</cell><cell>Learned (ours, as proposed)</cell><cell>88.10%</cell></row><row><cell>0</cell><cell>None (baseline)</cell><cell>87.76%</cell></row><row><cell>2</cell><cell>Max</cell><cell>87.09%</cell></row><row><cell>2</cell><cell>Mean</cell><cell>87.09%</cell></row><row><cell>2</cell><cell>Median</cell><cell>87.08%</cell></row><row><cell>2</cell><cell>Concatenation</cell><cell>83.44%</cell></row><row><cell cols="3">2 Learned (ours) (past frames only) 75.20%</cell></row><row><cell cols="3">1 Learned (ours) (past frames only) 76.02%</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<ptr target="http://tensorflow.org/.softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Geometric proposals for faster r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS), IEEE</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="331" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Recurrent multi-frame single shot detector for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Broad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
			<biblScope unit="page">94</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://keras.io" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The unmanned aerial vehicle benchmark: Object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="370" to="386" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Foreground gating and background refining network for surveillance object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="6077" to="6090" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mma: motion memory attention network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on image and graphics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="167" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Ron: Reverse connection with objectness prior networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="734" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Tracknet: Simultaneous object detection and tracking and its application in traffic video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01466</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d-detnet: a single stage video-based vehicle detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third International Workshop on Pattern Recognition, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">108280</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Fssd: feature fusion single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00960</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Global correlation network: End-to-end joint multi-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12511</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mobile video object detection with temporally-aware feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5686" to="5695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very deep convolutional neural network based image classification using small training sample size</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACPR.2015.7486599</idno>
		<idno>doi:10.1109/ ACPR.2015.7486599</idno>
	</analytic>
	<monogr>
		<title level="m">3rd IAPR Asian Conference on Pattern Recognition (ACPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="730" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Perceiving motion from dynamic memory for vehicle detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="3558" to="3567" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Joint detection and tracking in videos with identification features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Munjal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Aftab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Brandlmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">103932</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gravel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12049</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Road user detection in videos</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Spotnet: Selfattention multi-task network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>H?ritier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 17th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rnvid: A feature fusion architecture for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heritier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gravel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="125" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Borderdet: Border feature for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.11056</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Evolving boxes for fast vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1135" to="1140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully motion-aware network for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="542" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning rich features at high-speed for single-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1971" to="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Illuminating vehicles with motion priors for surveillance vehicle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2021" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
		<title level="m">UA-DETRAC: A New Benchmark and Protocol for Multi-Object Detection and Tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>arXiv CoRR abs/1511.04136</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Hierarchical attention for part-aware face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="page" from="560" to="578" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video object detection with an aligned spatialtemporal memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="485" to="501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exfuse: Enhancing feature fusion for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
