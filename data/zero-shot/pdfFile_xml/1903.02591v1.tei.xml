<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deren</forename><surname>Lei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
							<email>xiaoxiao.guo@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
							<email>william@cs.ucsb.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University of California</orgName>
								<orgName type="institution" key="instit2">Santa Barbara * IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Imposing Label-Relational Inductive Bias for Extremely Fine-Grained Entity Typing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing entity typing systems usually exploit the type hierarchy provided by knowledge base (KB) schema to model label correlations and thus improve the overall performance. Such techniques, however, are not directly applicable to more open and practical scenarios where the type set is not restricted by KB schema and includes a vast number of free-form types. To model the underlying label correlations without access to manually annotated label structures, we introduce a novel label-relational inductive bias, represented by a graph propagation layer that effectively encodes both global label co-occurrence statistics and word-level similarities. On a large dataset with over 10,000 free-form types, the graph-enhanced model equipped with an attention-based matching module is able to achieve a much higher recall score while maintaining a high-level precision. Specifically, it achieves a 15.3% relative F1 improvement and also less inconsistency in the outputs. We further show that a simple modification of our proposed graph layer can also improve the performance on a conventional and widely-tested dataset that only includes KB-schema types. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Fine-grained entity typing is the task of identifying specific semantic types of entity mentions in given contexts. In contrast to general entity types (e.g., organization, event), fine-grained types (e.g., political party, natural disaster) are often more informative and can provide valuable prior knowledge for a wide range of NLP tasks, such as coreference resolution <ref type="bibr" target="#b2">(Durrett and Klein, 2014)</ref>, relation extraction <ref type="bibr">(Yaghoobzadeh et al., 2016)</ref> and question answering <ref type="bibr" target="#b6">(Lee et al., 2006;</ref><ref type="bibr">Yavuz et al., 2016)</ref>. <ref type="table">Table 1</ref>: Examples of inconsistent predictions produced by existing entity typing system that does not model label correlations. We use different subscript symbols to indicate contradictory type pairs and show the ground-truth types in italics.</p><p>In practical scenarios, a key challenge of entity typing is to correctly predict multiple ground-truth type labels from a large candidate set that covers a wide range of types in different granularities. In this sense, it is essential for models to effectively capture the inter-label correlations. For instance, if an entity is identified as a "criminal", then the entity must also be a "person", but it is less likely for this entity to be a "police officer" at the same time. When ignoring such correlations and considering each type separately, models are often inferior in performance and prone to inconsistent predictions. As shown in <ref type="table">Table 1</ref>, an existing model that independently predicts different types fails to reject predictions that include apparent contradictions.</p><p>Existing entity typing research often address this aspect by explicitly utilizing a given type hierarchy to design hierarchy-aware loss functions <ref type="bibr" target="#b15">(Ren et al., 2016b;</ref><ref type="bibr">Xu and Barbosa, 2018)</ref> or enhanced type label encodings <ref type="bibr" target="#b17">(Shimaoka et al., 2017)</ref> that enable parameter sharing between related types. These methods rely on the assump-tion that the underlying type structures are predefined in entity typing datasets. For benchmarks annotated with the knowledge base (KB) guided distant supervision, this assumption is often valid since all types are from KB ontologies and naturally follow tree-like structures. However, since knowledge bases are inherently incomplete <ref type="bibr" target="#b9">(Min et al., 2013)</ref>, existing KBs only include a limited set of entity types. Thus, models trained on these datasets fail to generalize to lots of unseen types. In this work, we investigate entity typing in a more open scenario where the type set is not restricted by KB schema and includes over 10,000 free-form types <ref type="bibr" target="#b1">(Choi et al., 2018)</ref>. As most of the types do not follow any predefined structures, methods that explicitly incorporate type hierarchies cannot be straightforwardly applied here.</p><p>To effectively capture the underlying label correlations without access to known type structures, we propose a novel label-relational inductive bias, represented by a graph propagation layer that operates in the latent label space. Specifically, this layer learns to incorporate a label affinity matrix derived from global type co-occurrence statistics and word-level type similarities. It can be seamlessly coupled with existing models and jointly updated with other model parameters. Empirically, on the Ultra-Fine dataset <ref type="bibr" target="#b1">(Choi et al., 2018)</ref>, the graph layer alone can provide a significant 11.9% relative F1 improvement over previous models. Additionally, we show that the results can be further improved (11.9% ? 15.3%) with an attention-based mention-context matching module that better handles pronouns entity mentions. With a simple modification, we demonstrate that the proposed graph layer is also beneficial to the widely used OntoNotes dataset, despite the fact that samples in OntoNotes have lower label multiplicity (i.e., average number of ground-truth types for each sample) and thus require less labeldependency modeling than the Ultra-Fine dataset.</p><p>To summarize, our major contribution includes:</p><p>? We impose an effective label-relational bias on entity typing models with an easy-toimplement graph propagation layer, which allows the model to implicitly capture type dependencies;</p><p>? We augment our graph-enhanced model with an attention-based matching module, which constructs stronger interactions between the mention and context representations;</p><p>? Empirically, our model is able to offer significant improvements over previous models on the Ultra-Fine dataset and also reduces the cases of inconsistent type predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Fine-Grained Entity Typing The task of finegrained entity typing was first thoroughly investigated in <ref type="bibr" target="#b7">(Ling and Weld, 2012)</ref>, which utilized Freebase-guided distant supervision (DS) <ref type="bibr" target="#b10">(Mintz et al., 2009)</ref> for entity typing and created one of the early large-scale datasets. Although DS provides an efficient way to annotate training data, later work <ref type="bibr" target="#b3">(Gillick et al., 2014)</ref> pointed out that entity type labels induced by DS ignore entities' local context and may have limited usage in contextaware applications. Most of the following research has since focused on testing in context-dependent scenarios. While early methods <ref type="bibr" target="#b3">(Gillick et al., 2014;</ref><ref type="bibr">Yogatama et al., 2015)</ref> on this task rely on well-designed loss functions and a suite of handcraft features that represent both context and entities, <ref type="bibr" target="#b16">Shimaoka et al. (2016)</ref> proposed the first attentive neural model which outperformed featurebased methods with a simple cross-entropy loss.</p><p>Modeling Entity Type Correlations To better capture the underlying label correlations, Shimaoka et al. (2017) employed a hierarchical label encoding method and AFET <ref type="bibr" target="#b14">(Ren et al., 2016a)</ref> used the predefined label hierarchy to identify noisy annotations and proposed a partial-label loss to reduce such noise. A recent work (Xu and Barbosa, 2018) proposed hierarchical loss normalization which alleviated the noise of too specific types. Our work differs from these works in that we do not rely on known label structures and aim to learn the underlying correlations from data. <ref type="bibr" target="#b13">Rabinovich and Klein (2017)</ref> recently proposed a structure-prediction approach which used type correlation features. The inference on their learned factor graph is approximated by a greedy decoding algorithm, which outperformed unstructured methods on their own dataset. Instead of using an explicit graphical model, we enforce a relational bias on model parameters, which does not introduce extra burden on label decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Task Definition</head><p>Specifically, the task we consider takes a raw sentence C as well as an entity mention span M inside C as inputs, and aims to predict the correct type labels T m of M from a candidate type set T , which includes more than 10,000 free-form types. The entity span M here can be named entities, nominals and also pronouns. The ground-truth type set T m here usually includes more than one types (approximately five types on average), making this task a multi-label classification problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>In this section, we first briefly introduce the neural architecture to encode raw text inputs. Then we describe the matching module we use to enhance the interaction between the mention span and the context sentence. Finally, we move to the label decoder, on which we impose the label-relational bias with a graph propagation layer that encodes type co-occurrence statistics and word-level similarities. <ref type="figure" target="#fig_0">Figure 1</ref> provides a graphical overview of our model, with 1a) illustrating both the text encoders and the matching module, and 1b) showing an example of graph propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Representation Model</head><p>Our base model to encode the context and the mention span follows existing neural approaches <ref type="bibr" target="#b16">(Shimaoka et al., 2016;</ref><ref type="bibr">Xu and Barbosa, 2018;</ref><ref type="bibr" target="#b1">Choi et al., 2018)</ref>. To encode the context, we first apply a standard Bi-LSTM, which takes GloVe <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref> embeddings and position embeddings (three vectors representing positions before, inside or after the mention span) as inputs and outputs the hidden states at each time step t ? [1, l c ]. With the derived hidden states C h ? R lc?hc , we then apply a self-attentive encoder <ref type="bibr" target="#b8">(McCann et al., 2017)</ref> on the top to get the final context representation C. For the entity mention span, we concatenate the features derived by a character-level CNN and a similar self-attentive encoder. We denote the final mention representation as M. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Mention-Context Interaction</head><p>Since most previous datasets only consider named entities, a simple concatenation of the two features [C; M] followed by a linear output layer <ref type="bibr" target="#b16">(Shimaoka et al., 2016</ref><ref type="bibr" target="#b17">(Shimaoka et al., , 2017</ref> usually works reasonably well when making predictions. This suggests that M itself provides important information for recognizing entity types. However, as in our target dataset, a large portion of entity mentions are actually pronouns, such as "he" or "it", this kind of mentions alone provide only limited clues about general entity types (e.g., "he" is a "person") but little information about fine-grained types. In this case, directly appending representation of pronouns does not provide extra useful information for making fine-grained predictions. Thus, instead of using the concatenation operator, we propose to construct a stronger interaction between the mention and context with an attentionbased matching module, which has shown its effectiveness in recent natural language inference models <ref type="bibr" target="#b11">(Mou et al., 2016;</ref><ref type="bibr" target="#b0">Chen et al., 2017)</ref>. Formally consider the mention representation M ? R hm and context's hidden feature C h ? R lc?hc , where l c indicates the number of tokens in the context sentence and h m , h c denote feature dimensions. We first project the mention feature M into the same dimension space as C h with a linear layer (W 1 ? R hm?hc ) and a tanh function 3 :</p><formula xml:id="formula_0">m proj = tanh(W T 1 M),<label>(1)</label></formula><p>then we perform bilinear attention matching between m proj and C h , resulting in an affinity matrix A with dimension A ? R 1?lc :</p><formula xml:id="formula_1">A = m proj ? W a ? C h ,<label>(2)</label></formula><p>where W a ? R hc?hc is a learnable matrix. If we consider the mention feature as query and the context as memory, we can use the affinity matrix to retrieve the relevant parts in the context:</p><formula xml:id="formula_2">A = sof tmax(A) (3) r c =? ? C h .<label>(4)</label></formula><p>With the projected mention representation m proj and the retrieved context feature r c , we define the following interaction operators:</p><formula xml:id="formula_3">r = ?(W r [r c ; m proj ; r c ? m proj ]) (5) g = ?(W g [r c ; m proj ; r c ? m proj ]) (6) o = g * r + (1 ? g) * m proj ,<label>(7)</label></formula><p>where ?(?) is a gaussian error linear unit <ref type="bibr" target="#b4">(Hendrycks and Gimpel, 2016)</ref> and r is the fused context-mention feature; ?(?) indicates a sigmoid function and g is the resulting gating function, which controls how much information in mention span itself should be passed down. We expect the model to focus less on the mention representation when it is not informative. The concatenation [r c ; m proj ; r c ? m proj ] here is supposed to capture different aspects of the interactions. To emphasize the context's impact, we finally concatenate the extracted context feature (C) with the output (o) of the matching module (f = [o; C]) for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Imposing Label-Relational Inductive Bias</head><p>For approaches that ignore the underlying label correlations, the type predictions are considered as N independent binary classification problems, with N being the number of types. If we denote the feature extracted by any arbitrary neural model 3 tanh here is used to make mproj in the same scale as C h , which was the output of a tanh function inside LSTM.</p><p>as f ? R d f , then the probability of being any given type is calculated by:</p><formula xml:id="formula_4">p = ?(W o f ), W o ? R N ?d f .<label>(8)</label></formula><p>We can see that every row vector of W o is responsible for predicting the probability of one particular type. We will refer the row vectors as type vectors for the rest of this paper. As these type vectors are independent, the label correlations are only implicitly captured by sharing the model parameters that are used to extract f . We argue that the paradigm of parameter sharing is not enough to impose strong label dependencies and the values of type vectors should be better constrained.</p><p>A straightforward way to impose the desired constraints is to add extra regularization terms on W o . We first tested several auxiliary loss functions based on the heuristics from GloVe <ref type="bibr" target="#b12">(Pennington et al., 2014)</ref>, which operates on the type co-occurrence matrix. However, the auxiliary losses only offer trivial improvements in our experiments. Instead, we find that directly imposing a model-level inductive bias on the type vectors turns out to be a more principled solution. This is done by adding a graph propagation layer over randomly initialized W o and generating the updated type vectors W o , which is used for final prediction. Both W o and the graph convolution layer are learned together with other model parameters. We view this layer as the key component of our model and use the rest of this section to describe how we create the label graph and compute the propagation over the graph edges.</p><p>Label Graph Construction In KB-supervised datasets, the entity types are usually arranged in tree-like structures. Without any prior about type structures, we consider a more general graph-like structure. While the nodes in the graph straightforwardly represent entity types, the meaning of the edges is relatively vague, and the connections are also unknown. In order to create meaningful edges using training data as the only resource, we utilize the type co-occurrence matrix: if two type t 1 and t 2 both appear to be the true types of a particular entity mention, we will add an edge between them. In other words, we are using the co-occurrence statistics to approximate the pair-wise dependencies and the co-occurrence matrix now serves as the adjacent matrix. Intuitively, if t 2 co-appears with t 1 more often than another type t 3 , the probabilities of t 1 and t 2 should have stronger depen- dencies and the corresponding type vectors should be more similar in the vector space. In this sense, we expect each type vector to effectively capture the local neighbor structure on the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation Encoding via Graph Convolution</head><p>To encode the neighbor information into each node's representation, we follow the propagation rule defined in Graph Convolution Network (GCN) <ref type="bibr" target="#b5">(Kipf and Welling, 2016)</ref>. In particular, with the adjacent or co-occurrence matrix A, we define the following propagation rule on W o :</p><formula xml:id="formula_5">W o =D ? 1 2?D ? 1 2 W o T (9) A = A + I N .<label>(10)</label></formula><p>Here T ? R d f ?d f is the transformation matrix and I N is an identity matrix used to add selfconnected edges.D is a diagonal degree matrix withD ii = j? ij , which is used to normalize the feature vectors such that the number of neighbors does not affect the scale of transformed feature vectors. In our experiments, we find that an alternative propagation rule</p><formula xml:id="formula_6">W o =D ?1? W o T<label>(11)</label></formula><p>works similarly well and is more efficient as it involves less matrix multiplications. If we look closely and take each node out, the propagation can be written as</p><formula xml:id="formula_7">W o [i, :] = 1 j? ij ( j? ij W o [j, :]T ).<label>(12)</label></formula><p>From this formula, we can see that the propagation is essentially gathering features from the firstorder neighbors. In this way, the prediction on type t i is dependent on its neighbor types. Compared to original GCNs that often use multi-hop propagations (i.e., multiple graph layers connected by nonlinear functions) to capture higher-order neighbor structures. We only apply one-hop propagation and argue that high-order label dependency is not necessarily beneficial in our scenario and might introduce false bias. A simple illustration is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We can see that propagating 2-hop information introduces undesired inductive bias, since types that are more than 1-hop away (e.g., "Engineer" and "Politician") usually do not have any dependencies. In fact, some of the 2-hop type pairs can be contradictory types (e.g., "police" and "prisoner"). This hypothesis is consistent with our experiment results: adding more than one graph layer leads to worse results. Additionally, we also omit GCN's nonlinear activation which introduces unnecessary constraints on the scale of W o , with which we calculate the unscaled scores before calculating the probability via a sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Leveraging Label Word Embeddings</head><p>As the type labels are all written as text phrases, an interesting question is whether we can exploit the semantics provided by pre-trained word embeddings to improve entity typing. We explore this possibility by using the cosine similarity of word embeddings. We first calculate type embeddings by simply summing the embeddings of all tokens in the type name. Then we build a label affinity matrix A word by calculating pair-wise cosine similarities. With the assumption that word-level similarity measures some degree of label dependency, we propose to integrate A word into the graph convolution layer following</p><formula xml:id="formula_8">A word = (A word + 1)/2 (13) W o =D ?1 (? + ?A word )W o T.<label>(14)</label></formula><p>Here Equation 13 scales the similarity value into (0, 1] to avoid negative edge weights, which might introduce numerical issues when calculatingD ?1 . ? is a trainable parameter used to weight the impact of word-level similarities. As will be shown in Section 5, this simple augmentation provides further improvement over our original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Datasets Our experiments mainly focus on the Ultra-Fine entity typing dataset which has 10,331 labels and most of them are defined as freeform text phrases. The training set is annotated with heterogeneous supervisions based on KB, Wikipedia and head words in dependency trees, resulting in about 25.2M 4 training samples. This dataset also includes around 6,000 crowdsourced samples. Each of these samples has five groundtruth labels on average. For a fair comparison, we use the original test split of the crowdsourced data for evaluation. To better understand the capability of our model, we also test our model on the commonly-used OntoNotes <ref type="bibr" target="#b3">(Gillick et al., 2014)</ref> benchmark. It is worth noting that this dataset is much smaller and has lower label multiplicity than the Ultra-Fine dataset, i.e., each sample only has around 1.5 labels on average. <ref type="figure" target="#fig_2">Figure 3</ref> shows a comparison of these two datasets.</p><p>Baselines For the Ultra-Fine dataset, we compare our model with AttentiveNER <ref type="bibr" target="#b16">(Shimaoka et al., 2016</ref>) and the multi-task model proposed with the Ultra-Fine dataset. Note that other models that require pre-defined type hierarchy are not applicable to this dataset. For experiments on OntoNotes, in addition to the two neural baselines for Ultra-Fine, we compare with several existing methods that explicitly utilize the pre-defined type structures in loss functions. Namely, these methods are AFET <ref type="bibr" target="#b14">(Ren et al., 2016a)</ref>, LNR <ref type="bibr" target="#b15">(Ren et al., 2016b)</ref> and <ref type="bibr">NFETC (Xu and Barbosa, 2018)</ref>.</p><p>Evaluation Metrics On Ultra-Fine, we first evaluate the mean reciprocal rank (MRR), macro precision(P), recall (R) and F1 following existing research. As P, R and F1 all depend on a chosen threshold on probabilities, we also consider a more transparent comparison using precisionrecall curves. On OntoNotes, we use the standard metrics used by baseline models: accuracy, macro, and micro F1 scores.</p><p>Implementation Details Most of the model hyperparameters, such as embedding dimensions, learning rate, batch size, dropout ratios on context and mention representations are consistent with existing models. Since the mention-context matching module brings more parameters, we apply a dropout layer over the extracted feature f to avoid overfitting. We list all the hyperparameters in the appendix. Models for OntoNotes are trained with standard binary cross-entropy (BCE) losses defined on all candidate labels. When training on Ultra-Fine, we adopt the multi-task loss proposed in <ref type="bibr" target="#b1">Choi et al. (2018)</ref> which divides the cross-entropy loss into three separate losses over different type granularities. The multi-task objective avoids penalizing false negative types and can achieve higher recalls.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on the Ultra-Fine Dataset</head><p>We report the results on Ultra-Fine in <ref type="table" target="#tab_1">Table 2</ref>. It is worth mentioning that our model, denoted as LA-BELGCN, is trained using the unlicensed training set which is smaller than the one used by compared baselines. Even though our model significantly outperforms the baselines, for a fair comparison, we first test our model using the same decision threshold (0.5) used by previous models. In terms of F1, our best model (LABELGCN) outperforms existing methods by a large margin. Compared to <ref type="bibr" target="#b1">Choi et al. (2018)</ref>, our model improves on both precision and recall significantly. Compared to the AttentiveNER trained with standard BCE loss, our model achieves much higher recall but performs worse in precision. This is due to the fact that when trained with BCE loss, the model usually retrieves only one label per sample and these types are mostly general types 5 which are easier to predict. With higher recalls or more retrieved types, achieving high precision requires being accurate on fine-grained types, which are often harder to predict. As the precision and recall scores both rely on the decision threshold, different models or different metrics can have different optimal thresholds. As shown by the "LABELGCN + thresh tuning" entry in <ref type="table" target="#tab_1">Table 2</ref>, with threshold tuning, our model beats baselines in all metrics. We also see that recall is usually lagging behind precision on this dataset, indicating that F1 score is mainly affected     <ref type="table">Table 3</ref>: Decomposed validation performance on pronouns and the other entities. Each entry is obtained using the best threshold among the 50 equal-interval thresholds. The corresponding PR curves can be found in the appendix ( <ref type="figure">Figure 5</ref>).</p><p>by the recall and tuning towards recall can usually lead to higher F1 scores. For more transparent comparisons, we show the precision-recall curves in <ref type="figure" target="#fig_3">Figure 4</ref>. These data points are based on the validation performance given by 50 equal-interval thresholds between 0 and 1. We can see there is a clear margin between our model and the multi-task baseline method (LabelGCN vs <ref type="bibr">Choi et al.)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>To quantify the effect of different model components, we report the performance of model variants in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="figure" target="#fig_3">Figure 4</ref>. We can clearly see that the graph convolution layer is the most essential component. The information provided by word embedding is useful and can further improve both precision and recall. Although <ref type="table" target="#tab_1">Table 2</ref> seems to indicate the interaction module decreases the precision, we can see from <ref type="figure" target="#fig_3">Figure 4</ref> that with a proper threshold, the enhanced interaction actually improves both precision and recall. In term of this, we recommend future research to use PR curves for more accurate model analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Fine-Grained Performance for Pronouns</head><p>As discussed in Section 4.2, the mention representation of pronouns provide limited information about fine-grained types. We investigate the effect of the enhanced mention-context interaction by analyzing the decomposed performance on pronouns and other kinds of entities. From the results in <ref type="table">Table 3</ref>, we can see that the enhanced interaction offers consistent improvements over pronouns entities and also maintains the performance on other kinds of entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Analysis</head><p>To gain insights on the improvements provided by our model, we manually analyze 100 error cases 6 of the baseline model <ref type="bibr" target="#b1">(Choi et al. (2018)</ref> with threshold 0.5) and see if our model can generate high-quality predictions. We first observe that many errors actually results from incomplete annotations. This suggests models' precision scores are often underestimated in this dataset. We discuss several typical error cases shown in <ref type="table" target="#tab_4">Table 4</ref> and list more samples in the appendix <ref type="table">(Table 7)</ref>.</p><p>A key observation is that while the baseline model tends to make inconsistent predictions (see examples 1, 2, 3), our model can avoid predicting such inconsistent type pairs. This indeed validates our model's ability to encode label correlations. We also notice that our model is more sensitive to gender information indicated by pronouns, while 1) Context   the baseline model sometimes holds the genderindicating predictions and predict other types, our model predicts the gender-indicating types more often (examples 3, 4, 5). We conjecture that our model learns this easy way to maintain precision. For cases that both models fail, some of them actually require background knowledge (example 4) to make accurate predictions. Another typical case is that both models predict some other entities in the context (example 5). We think this potentially results from the data bias introduced by the head-word supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Evaluation on OntoNotes</head><p>To better understand the requirements for applying our model, we further evaluate on the OntoNotes dataset. Here we do not apply the proposed mention-context matching module as this dataset does not include any pronoun entities. To obtain more reliable co-occurrence statistics, we use the augmented training data released by <ref type="bibr" target="#b1">Choi et al. (2018)</ref>. However, since the training set is still much smaller than that of the Ultra-Fine dataset, the derived co-occurrence statistics are relatively noisy and might introduce undesired bias. We thus add an additional residual connection to our graph convolution layer, which allows the model to selectively use co-occurrence statistics. This indeed gives us improvements over previous state-of-thearts, as shown in <ref type="table" target="#tab_5">Table 5</ref>. However, compared to Ultra-Fine, the margin of the improvement is smaller. In view of the key differences of these two datasets, we highlight two key requirements for our proposed model to offer substantial improvements. First, there should be a large-scale training set so that the derived co-occurrence statistics can reasonably reflect the true label correlations. Second, the samples themselves should also have higher label multiplicity. In fact, most of the samples in OntoNotes only have 1 or 2 labels. This property actually alleviates the need for models to capture label dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present an effective method to impose label-relational inductive bias on finegrained entity typing models. Specifically, we utilize a graph convolution layer to incorporate type co-occurrence statistics and word-level type similarities. This layer implicitly captures the label correlations in the latent vector space. Along with an attention-based mention-context matching module, we achieve significant improvements over previous methods on a large-scale dataset. As our method does not require external knowledge about the label structures, we believe our method is general enough and has the potential to be applied to other multi-label tasks with plain-text labels. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the process to make predictions on the type "person". a) Modules used to extract mention and context aware representations. b) An illustration of the graph layer operating over the type vector of "person".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A snippet of the underlying type cooccurrence graph. Multiple edges between nodes are omitted here for clarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Label multiplicity distribution of the datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Precision-recall curves on Ultra-Fine. The trivial point derived by threshold 0 is omitted here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison with baseline models on the Ultra-Fine dataset. Threshold-tuning gives better performance on all metrics compared to both baselines.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Today, Taiwan is manifesting the elegance of a democratic island, once again attracting global attention, as the people on this land create a new page in our history. Groundtruth time, date, day, today, present Prediction Baseline: {day ? , person , organization, religion} Ours: {day} 2) Context A gigantic robot emerges, emitting a sound that paralyzes humans and disrupts all electrical systems in New York City. Groundtruth object, device, machine, mechanism Prediction Baseline: {object, person ? , robot } Ours: {object, robot} 3) Context He also has been accused of genocide in Bosnia and other war crimes in Croatia, but the date to try those two indictments together has not been set. Groundtruth person Prediction Baseline:{person, god ? , title, criminal } Ours: {person, politician, criminal, male, prisoner} 4) Context Her status was uncertain for Wimbledon, which begins June 23. Groundtruth person, athlete, adult, player, professional, tennis player, contestant Prediction Baseline: {person, female, woman, spouse} Ours: {person, artist, female, woman} 5) Context For eight years he treated thousands of wounded soldiers of the armed forces led by the CPC. Groundtruth person, doctor, caretaker, nurse Prediction Baseline: {person, soldier, suspect, serviceman} Ours: {person, soldier, man}</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Qualitative analysis of validation samples. We use different colors and subscript symbols to mark inconsistencies. The bottom two rows show error cases for both models.</figDesc><table><row><cell>Model</cell><cell cols="3">Accuracy Macro-F1 Micro-F1</cell></row><row><cell>AttentiveNER</cell><cell>51.7</cell><cell>71.0</cell><cell>64.9</cell></row><row><cell>AFET</cell><cell>55.1</cell><cell>71.1</cell><cell>64.7</cell></row><row><cell>LNR</cell><cell>57.2</cell><cell>71.5</cell><cell>66.1</cell></row><row><cell>NFETC</cell><cell>60.2</cell><cell>76.4</cell><cell>70.2</cell></row><row><cell>Choi et al. (2018)</cell><cell>59.5</cell><cell>76.8</cell><cell>71.8</cell></row><row><cell>LABELGCN</cell><cell>59.6</cell><cell>77.8</cell><cell>72.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results on OntoNotes. Upper rows show the results of baselines that explicitly use the hierarchical type structures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Peng Xu and Denilson Barbosa. 2018. Neural finegrained entity type classification with hierarchyaware loss. arXiv preprint arXiv:1803.03378. Yadollah Yaghoobzadeh, Heike Adel, and Hinrich Sch?tze. 2016. Noise mitigation for neural entity typing and relation extraction. arXiv preprint arXiv:1612.07495. Semih Yavuz, Izzeddin Gur, Yu Su, Mudhakar Srivatsa, and Xifeng Yan. 2016. Improving semantic parsing via answer type inference. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 149-159. Dani Yogatama, Daniel Gillick, and Nevena Lazic. 2015. Embedding methods for fine grained entity type classification. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), volume 2, pages 291-296.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Please refer to<ref type="bibr" target="#b17">(Shimaoka et al., 2017)</ref> and<ref type="bibr" target="#b1">(Choi et al., 2018)</ref> for more detailed descriptions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4"><ref type="bibr" target="#b1">Choi et al. (2018)</ref> use the licensed Gigaword to build part of the dataset, while in our experiments we only use the open-sourced training set which has approximately 6M training samples.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">According to the results of our own implementation of BCE-trained model which achieves similar performance as AttentiveNER.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The baseline model achieves the lowest precision on these 100 samples.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research was supported in part by DARPA Grant D18AP00044 funded under the DARPA YFA program. The authors are solely responsible for the contents of the paper, and the opinions expressed in this publication do not reflect those of the funding agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendix a) PR curves of pronoun entities a) PR curves of non-pronoun entities  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Enhanced lstm for natural language inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen-Hua</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1657" to="1668" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04905</idno>
		<title level="m">Ultra-fine entity typing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fine-grained named entity recognition using conditional random fields for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changki</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Gyu</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyo-Jung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soojong</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeong</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Hee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeon-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Hyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Gil</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia Information Retrieval Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="581" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="94" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learned in translation: Contextualized word vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6294" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction with an incomplete knowledge base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonan</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Grishman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Gondek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="777" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distant supervision for relation extraction without labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Mintz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bills</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rion</forename><surname>Snow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</title>
		<meeting>the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1003" to="1011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Natural language inference by tree-based convolution and heuristic matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07751</idno>
		<title level="m">Fine-grained entity typing with high-multiplicity assignments</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Afet: Automatic finegrained entity typing by hierarchical partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Label noise reduction in entity typing by heterogeneous partial-label embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clare</forename><forename type="middle">R</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1825" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An attentive neural architecture for fine-grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Automated Knowledge Base Construction</title>
		<meeting>the 5th Workshop on Automated Knowledge Base Construction</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architectures for fine-grained entity type classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonse</forename><surname>Shimaoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kentaro</forename><surname>Inui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1271" to="1280" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Groundtruth location, place, country, area, nation, region Prediction Baseline: {location ? , person , agency, artist, cemetery, country, language, title, republic} Ours:{nationality, location, place, country, area, license, nation} Context The committee undertook its work on Saturday 16/2/1426 A. H . The following is noteworthy : Groundtruth group, organization, agency, company, institution, administration, body, management, party Prediction Baseline: {committee ? , person , organization, government} Ours: {group, government, committee} Context They are accused of helping Libya develop a nuclear weapons programme and were alleged to have been in contact with Abdul Qadeer Khan , the disgraced father of Pakistan &apos;s nuclear programme. Groundtruth group, terrorist Prediction Baseline: {military, person ? , group, country } Ours: {person, politician, prisoner, serviceman} Context It also marked the first major roundup of Islamist leaders by a government eager to demonstrate its commitment to the anti-terror fight waged by the United States. Groundtruth event, consequence Prediction Baseline:{internet ? , event, art , writing} Ours:{event} Context If you have ever watched a keynote speech by Steve Jobs, you know that he was the best of the best in launching a product. Groundtruth person, adult, businessman, celebrity, professional Prediction Baseline: {person, artist, athlete ? , author, musician } Ours:{person} Context They dined together, this time in Benedict&apos;s house, before the pope was driven back to his temporary residence in Regensburg &apos;s St Wolfgang Seminary. Groundtruth adult, man, supporter, serviceman Prediction Baseline: {person, adult, female, woman} Ours: {person} Context Topic : I am grateful to the University of Science and Technology Groundtruth person, individual, student Prediction Baseline:{person, politician, employee, leader, minister, traveler, announcer, clergyman} Ours:{person, student} Context &quot;I didn&apos;t think the speech was that long</title>
	</analytic>
	<monogr>
		<title level="m">serviceman} Ours:{person} Context</title>
		<imprint/>
	</monogr>
	<note>Groundtruth person Prediction Baseline: {person ? , art , writing, convict, felon} Ours: {person, female, woman} Context The monument is located in Pioneer Park Cemetery in the Convention Center District of downtown Dallas, Texas, USA, next to the Dallas Convention Center and Pioneer Plaza. Pataki said. Groundtruth person, speaker Prediction Baseline: {person, actor, politician, spokesperson, woman} Ours: {person, adult} Context &quot;This is touching our troops,&quot; she said. Groundtruth person, adult, female, reporter, woman Prediction Baseline: {person, politician, official, spokesperson, communicator} Ours: {female, official, reporter, strategist, communicator</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Our model is able to give more accurate type predictions and also reduce the inconsistency in the output type set</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note>More sample predictions</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
