<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lulu</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">BAAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">BAAI</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">BAAI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Point-BERT: Pre-training 3D Point Cloud Transformers with Masked Point Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Point-BERT, a new paradigm for learning Transformers to generalize the concept of BERT [8] to 3D point cloud. Inspired by BERT, we devise a Masked Point Modeling (MPM) task to pre-train point cloud Transformers. Specifically, we first divide a point cloud into several local point patches, and a point cloud Tokenizer with a discrete Variational AutoEncoder (dVAE) is designed to generate discrete point tokens containing meaningful local information. Then, we randomly mask out some patches of input point clouds and feed them into the backbone Transformers. The pre-training objective is to recover the original point tokens at the masked locations under the supervision of point tokens obtained by the Tokenizer. Extensive experiments demonstrate that the proposed BERT-style pretraining strategy significantly improves the performance of standard point cloud Transformers. Equipped with our pretraining strategy, we show that a pure Transformer architecture attains 93.8% accuracy on ModelNet40 and 83.1% accuracy on the hardest setting of ScanObjectNN, surpassing carefully designed point cloud models with much fewer hand-made designs. We also demonstrate that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. The code and pre-trained models are available at https: //github.com/lulutang0608/Point-BERT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Compared to conventional hand-crafted feature extraction methods, Convolutional Neural Networks (CNN) <ref type="bibr" target="#b19">[20]</ref> is dependent on much less prior knowledge. Transformers <ref type="bibr" target="#b50">[51]</ref> have pushed this trend further as a step towards no inductive bias with minimal man-made assumptions, such as translation equivalence or locality in CNNs. Recently, the structural superiority and versatility of standard Transformers are proved in both language <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer Encoder</head><p>Predicted Tokens</p><p>Reconstructed Point Cloud</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5, 1967, 49</head><p>Tokenizer Decoder <ref type="figure">Figure 1</ref>. Illustration of our main idea. Point-BERT is designed for pre-training of standard point cloud Transformers. By training a dVAE via point cloud reconstruction, we can convert a point cloud into a sequence of discrete point tokens. Then we are able to pre-train the Transformers with a Mask Point Modeling (MPM) task by predicting the masked tokens.</p><p>image tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b67">68]</ref>, and the capability of diminishing the inductive biases is also justified by enabling more parameters, more data <ref type="bibr" target="#b8">[9]</ref>, and longer training schedules. While Transformers produce astounding results in Natural Language Processing (NLP) and image processing, it is not well studied in the 3D community. Existing Transformerbased point cloud models <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b64">65]</ref> bring in certain inevitable inductive biases from local feature aggregation <ref type="bibr" target="#b64">[65]</ref> and neighbor embedding <ref type="bibr" target="#b10">[11]</ref>, making them deviate from the mainstream of standard Transformers. To this end, we aim to apply standard Transformers on point cloud directly with minimal inductive bias, as a stepping stone to a neat and unified model for 3D representation learning. Apparently, the straightforward adoption of Transformers does not achieve satisfactory performance on point cloud tasks (see <ref type="figure" target="#fig_2">Figure 5</ref>). This discouraging result is partially attributed to the limited annotated 3D data since pure Transformers with no inductive bias need massive training data. For example, ViT <ref type="bibr" target="#b8">[9]</ref> uses ImageNet <ref type="bibr" target="#b19">[20]</ref> (14M images) and JFT <ref type="bibr" target="#b42">[43]</ref> (303M images) to train vision Transformers. In contrast, accurate annotated point clouds are relatively insufficient. Despite the 3D data acquisition is getting easy with the recent proliferation of modern scanning devices, labeling point clouds is still time-consuming, error-prone, and even infeasible in some extreme real-world scenarios. The difficulty motivates a flux of research into learning from unlabelled 3D data. Self-supervised pre-  <ref type="figure">Figure 2</ref>. Masked point clouds reconstruction using our Point-BERT model trained on ShapeNet. We show the reconstruction results of synthetic objects from ShapeNet test set with block masking and random masking in the first two groups respectively. Our model also generalize well to unseen real scans from ScanObjectNN (the last two groups).</p><p>training thereby becomes a viable technique to unleash the scalability and generalization of Transformers for 3D point cloud representation learning. Among all the Transformer-based pre-training models, BERT <ref type="bibr" target="#b7">[8]</ref> achieved state-of-the-art performance at its released time, setting a milestone in the NLP community. Inspired by BERT <ref type="bibr" target="#b7">[8]</ref>, we seek to exploit the BERT-style pretraining for 3D point cloud understanding. However, it is challenging to directly employ BERT on point clouds due to a lack of pre-existing vocabulary. In contrast, the language vocabulary has been well-defined (e.g., WordPiece in <ref type="bibr" target="#b7">[8]</ref>) and off-the-shelf for model pre-training. In terms of point cloud Transformers, there is no pre-defined vocabulary for point clouds. A naive idea is to treat every point as a 'word' and mimic BERT <ref type="bibr" target="#b7">[8]</ref> to predict the coordinates of masked points. Such a point-wise regression task surges computational cost quadratically as the number of tokens increases. Moreover, a word in a sentence contains basic contextual semantic information, while a single point in a point cloud barely entails semantic meaning.</p><p>Nevertheless, a local patch partitioned from a holistic point cloud contains plentiful geometric information and can be treated as a component unit. What if we build a vocabulary where different tokens represent different geometric patterns of the input units? At this point, we can represent a point cloud as a sequence of such tokens. Now, we can favorably adopt BERT and its efficient implementations almost out of the box. We hypothesize that bridging this gap is a key to extending the successful Transformers and BERT to the 3D vision domain.</p><p>Driven by the above analysis, we present Point-BERT, a new scheme for learning point cloud Transformers. Two essential components are conceived: 1) Point Tokenization: A point cloud Tokenizer is devised via a dVAE-based <ref type="bibr" target="#b38">[39]</ref> point cloud reconstruction, where a point cloud can be converted into discrete point tokens according to the learned vocabulary. We expect that point tokens should imply local geometric patterns, and the learned vocabulary should cover diverse geometric patterns, such that a sequence of such tokens can represent any point cloud (even never seen before). 2) Masked Point Modeling: A 'masked point modeling' (MPM) task is performed to pre-train Transformers, which masks a portion of input point cloud and learns to reconstruct the missing point tokens at the masked regions. We hope that our model enables reasoning the geometric relations among different patches of the point cloud, capturing meaningful geometric features for point cloud understanding.</p><p>Both two designs are implemented and justified in our experiments. We visualize the reconstruction results both on the synthetic (ShapeNet <ref type="bibr" target="#b4">[5]</ref>) and real-world (ScanOb-jectNN <ref type="bibr" target="#b48">[49]</ref>) datasets in <ref type="figure">Figure 2</ref>. We observe that Point-BERT correctly predicts the masked tokens and infers diverse, holistic reconstructions through our dVAE decoder. The results suggest that the proposed model has learned inherent and generic knowledge of 3D point clouds, i.e, geometric patterns or semantics. More significantly, our model is trained on ShapeNet, the masked point predictions on ScanObjectNN reflect its superior performance on challenging scenarios with both unseen objects and domain gaps.</p><p>Our Point-BERT with a pure Transformer architecture and BERT-style pre-training technique achieves 93.8% accuracy on ModelNet40 and 83.1% accuracy on the complicated setting of ScanObjectNN, surpassing carefully de-signed point cloud models with much fewer human priors. We also show that the representations learned by Point-BERT transfer well to new tasks and domains, where our models largely advance the state-of-the-art of few-shot point cloud classification task. We hope a neat and unified Transformer architecture across images and point clouds could facilitate both domains since it enables joint modeling of 2D and 3D visual signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised Learning (SSL). SSL is a type of unsupervised learning, where the supervision signals can be generated from the data itself <ref type="bibr" target="#b14">[15]</ref>. The core idea of SSL is to define a pretext task, such as jigsaw puzzles <ref type="bibr" target="#b30">[31]</ref>, colorization <ref type="bibr" target="#b20">[21]</ref>, and optical-flow <ref type="bibr" target="#b28">[29]</ref> in images. More recently, several studies suggested using SSL techniques for point cloud understanding <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59]</ref>. Example 3D pretext tasks includes orientation estimation <ref type="bibr" target="#b32">[33]</ref>, deformation reconstruction <ref type="bibr" target="#b0">[1]</ref>, geometric structural cues <ref type="bibr" target="#b44">[45]</ref> and spatial cues <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42]</ref>. Inspired by the jigsaw puzzles in images <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b40">[41]</ref> proposes to reconstruct point clouds from the randomly rearranged parts. A contrastive learning framework is proposed by DepthContrast <ref type="bibr" target="#b63">[64]</ref> to learn representations from depth scans. More recently, OcCo <ref type="bibr" target="#b51">[52]</ref> describes an encoder-decoder mechanism to reconstruct the occluded point clouds. Different from these studies, we attempt to explore a point cloud SSL model following the successful Transformers <ref type="bibr" target="#b50">[51]</ref>.</p><p>Transformers. Transformers <ref type="bibr" target="#b50">[51]</ref> have become the dominant framework in NLP <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref> due to its salient benefits, including massively parallel computing, longdistance characteristics, and minimal inductive bias. It has intrigued various vision tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19]</ref>, such as object classification <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, detection <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b67">68]</ref> and segmentation <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b65">66]</ref>. Nevertheless, its applications on point clouds remain limited. Some preliminary explorations have been implemented <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b64">65]</ref>. For instance, <ref type="bibr" target="#b64">[65]</ref> applies the vectorized self-attention mechanism to construct a point Transformer layer for 3D point cloud learning. <ref type="bibr" target="#b10">[11]</ref> uses a more typical Transformer architecture with neighbor embedding to learn point clouds. Nevertheless, prior efforts for Transformerbased point cloud models more or less involve some inductive biases, making them out of the line with standard Transformers. In this work, we seek to continue the success of standard Transformers and extend it to point cloud learning with minimal inductive bias.</p><p>BERT-style Pre-training. The main architecture of BERT <ref type="bibr" target="#b7">[8]</ref> is built upon a multi-layer Transformer encoder, which is first designed to pre-train bidirectional representations from the unlabeled text in a self-supervised scheme. The primary ingredient that helps BERT stand out and achieve impres-sive performance is the pretext of Masked Language Modeling (MLM), which first randomly masks and then recovers a sequence of input tokens. The MLM strategy has also inspired a lot of pre-training tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b47">48]</ref>. Take BEiT <ref type="bibr" target="#b1">[2]</ref> for example, it first tokenizes the input image into discrete visual tokens. After that, it randomly masks some image patches and feeds the corrupted images into the Transformer backbone. The model is trained to recover the visual tokens of the masked patches. More recently, MAE <ref type="bibr" target="#b12">[13]</ref> presents a masked autoencoder strategy for image representation learning. It first masks random patches of the input image and then encourages the model to reconstruct those missing pixels. Our work is greatly inspired by BEiT <ref type="bibr" target="#b1">[2]</ref>, which encodes the image into discrete visual tokens so that a Transformer backbone can be directly applied to these visual tokens. However, it is more challenging to acquire tokens for point clouds due to the unstructured nature of point clouds, which subsequently hinders the straightforward use of BERT on point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Point-BERT</head><p>The overall objective of this work is to extend the BERTstyle pre-training strategy to point cloud Transformers. To achieve this goal, we first learn a Tokenizer to obtain discrete point tokens for each input point cloud. Mimicking the 'MLM' strategy in BERT <ref type="bibr" target="#b7">[8]</ref>, we devise a 'masked point modeling' (MPM) task to pre-train Transformers with the help of those discrete point tokens. The overall idea of our approach is illustrated in <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Point Tokenization</head><p>Point Embeddings. A naive approach treats per point as one token. However, such a point-wise reconstruction task tends to unbearable computational cost due to the quadratic complexity of self-attention in Transformers. Inspired by the patch embedding strategy in Vision Transformers <ref type="bibr" target="#b8">[9]</ref>, we present a simple yet efficient implementation that groups each point cloud into several local patches (sub-clouds). Specifically, given an input point cloud p ? R N ?3 , we first sample g center points from the holistic point cloud p via farthest point sampling (FPS). The k-nearest neighbor (kNN) algorithm is then used to select the n nearest neighbor points for each center point, grouping g local patches (sub-clouds) {p i } g i=1 . We then make these local patches unbiased by subtracting their center coordinates, disentangling the structure patterns and spatial coordinates of the local patches. These unbiased sub-clouds can be treated as words in NLP or image patches in the vision domain. We further adopt a mini-PointNet <ref type="bibr" target="#b33">[34]</ref> to project those sub-clouds into point embeddings. Following the practice of Transformers in NLP and 2D vision tasks, we represent a point cloud as a sequence of point embeddings {f i } g i=1 , which can be re- Input Token</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pred. Token</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local Centers</head><p>Mini-PointNet <ref type="figure">Figure 3</ref>. The pipeline of Point-BERT. We first partition the input point cloud into several point patches (sub-clouds). A mini-PointNet <ref type="bibr" target="#b33">[34]</ref> is then used to obtain a sequence of point embeddings. Before pre-training, a Tokenizer is learned through dVAE-based point cloud reconstruction (as shown in the right part of the figure), where a point cloud can be converted into a sequence of discrete point tokens; During pre-training, we mask some portions of point embeddings and replace them with a mask token. The masked point embeddings are then fed into the Transformers. The model is trained to recover the original point tokens, under the supervision of point tokens obtained by the Tokenizer. We also add an auxiliary contrastive learning task to help the Transformers to capture high-level semantic knowledge.</p><p>ceived as inputs to standard Transformers.</p><formula xml:id="formula_0">Point Tokenizer. Point Tokenizer takes point embed- dings {f i } g i=1</formula><p>as the inputs and converts them into discrete point tokens. Specifically, the Tokenizer</p><formula xml:id="formula_1">Q ? (z|f ) maps point embeddings {f i } g i=1 into discrete point tokens z = [z 1 , z 2 , ...., z g ] ? V 1 ,</formula><p>where V is the learned vocabulary with total length of N . In this step, the sub-clouds</p><formula xml:id="formula_2">{p i } g i=1 can be tokenized into point tokens {z i } g i=1</formula><p>, relating to effective local geometric patterns. In our experiments, DGCNN <ref type="bibr" target="#b53">[54]</ref> is employed as our Tokenizer network.</p><formula xml:id="formula_3">Point Cloud Reconstruction. The decoder P ? (p|z) of dVAE receives point tokens {z i } g i=1</formula><p>as the inputs and learns to reconstruct the corresponding sub-clouds {p i } g i=1 . Since the local geometry structure is too complex to be represented by the limited N situations. We adopt a DGCNN <ref type="bibr" target="#b53">[54]</ref> to build the relationship with neighboring point tokens, which can enhance the representation ability of discrete point tokens for diverse local structures. After that, a FoldingNet <ref type="bibr" target="#b58">[59]</ref> is used to reconstruct the sub-clouds.</p><p>The overall reconstruction objective can be written as</p><formula xml:id="formula_4">E z?Q ? (z|p) [ log P ? (p|z)]</formula><p>, and the reconstruction procedure can be viewed as maximizing the evidence lower bound (ELB) of the log-likelihood P ? (p|p) <ref type="bibr" target="#b36">[37]</ref>:</p><formula xml:id="formula_5">(p i ,p i )?D logP ? (pi|pi) ? (p i ,p i )?D (E z i ?Q ? (z|p i ) [ log P?(pi|zi)] ?DKL[Q ? (z|pi), P?(z|pi)]),<label>(1)</label></formula><p>where p denotes the original point cloud,p denotes the reconstructed point cloud. Since the latent point tokens are discrete, we cannot apply the reparameterization gradient to train the dVAE. Following <ref type="bibr" target="#b36">[37]</ref>, we use the Gumbelsoftmax relaxation <ref type="bibr" target="#b16">[17]</ref> and a uniform prior during dVAE training. Details about dVAE architecture and its implementation can be found in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Transformer Backbone</head><p>We adopt the standard Transformers <ref type="bibr" target="#b50">[51]</ref> in our experiments, consisting of multi-headed self-attention layers and FFN blocks. For each input point cloud, we first divide it into g local patches with center points {c i } g i=1 . Those local patches are then projected into point embeddings</p><formula xml:id="formula_6">{f i } g i=1</formula><p>via a mini-PointNet <ref type="bibr" target="#b33">[34]</ref>, which consists of only MLP layers and the global maxpool operation. We further obtain the positional embeddings {pos i } of each patch by applying an MLP on its center point {c i }. Formally, we define the input embeddings as</p><formula xml:id="formula_7">{x i } g i=1 , which is the combination of point embeddings {f i } g i=1 and positional embeddings {pos i } g i=1</formula><p>. Then, we send the input embeddings {x i } g i=1 into the Transformer. Following <ref type="bibr" target="#b7">[8]</ref>, we append a class token E[s] to the input sequences. Thus, the input sequence of Transformer can be expressed as</p><formula xml:id="formula_8">H 0 = {E[s], x 1 , x 2 , ? ? ? , x g }.</formula><p>There are L layers of Transformer block, and the output of the last layer</p><formula xml:id="formula_9">H L = h L s , h L 1 , ? ? ? , h L g</formula><p>represents the global feature, along with the encoded representation of the input sub-clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Masked Point Modeling</head><p>Motivated by BERT <ref type="bibr" target="#b7">[8]</ref> and BEiT <ref type="bibr" target="#b1">[2]</ref>, we extend the masked modeling strategy to point cloud learning and devise a masked point modeling (MPM) task for Point-BERT.</p><p>Masked Sequence Generation. Different from the ran-dom masking used in BERT <ref type="bibr" target="#b7">[8]</ref> and MAE <ref type="bibr" target="#b12">[13]</ref>, we adopt a block-wise masking strategy like <ref type="bibr" target="#b1">[2]</ref>. Specifically, we first choose a center point c i along with its sub-cloud p i , and then find its m neighbor sub-clouds, forming a continuous local region. We mask out all local patches in this region to generate the masked point cloud. In practice, we directly apply such a block-wise masking strategy like <ref type="bibr" target="#b1">[2]</ref> to the inputs of the Transformer. Formally, we mark the masked positions as M ? {1, ? ? ? , g} rg , where r is the mask ratio. Next, we replace all the masked point embeddings with a same learnable pre-defined mask embeddings E[M] while keeping its positional embeddings unchanged. Finally, the corrupted input embeddings</p><formula xml:id="formula_10">X M = {x i : i / ? M} g i=1 ? {E[M] + pos i : i ? M} g i=1</formula><p>are fed into the Transformer encoder.</p><p>Pretext Task Definition. The goal of our MPM task is to enable the model to infer the geometric structure of missing parts based on the remaining ones. The pre-trained dVAE (see section 3.1) encodes each local patch into discrete point tokens, representing the geometric patterns. Thus, we can directly apply those informative tokens as our surrogate supervision signal to pre-train the Transformer.</p><p>Point Patch Mixing. Inspired by the CutMix <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref> technique, we additionally devise a neat mixed token prediction task as an auxiliary pretext task to increase the difficulty of pre-training in our Point-BERT, termed as 'Point Patch Mixing'. Since the information of the absolute position of each sub-cloud has been excluded by normalization, we can create new virtual samples by simply mixing two groups of sub-clouds without any cumbersome alignment techniques between different patches, such as optimal transport <ref type="bibr" target="#b62">[63]</ref>. During pre-training, we also force the virtual sample to predict the corresponding tokens generated by the original subcloud to perform the MPM task. In our implementation, we generate the same number of virtual samples as the real ones to make the pre-training task more challenging, which is helpful to improve the training of Transformers with limited data as observed in <ref type="bibr" target="#b46">[47]</ref>.</p><p>Optimization Objective. The goal of MPM task is to recover the point tokens that are corresponding to the masked locations. The pre-training objective can be formalized as maximizing the log-likelihood of the correct point tokens z i given the masked input embeddings X M :</p><formula xml:id="formula_11">max X?D EM i?M logP zi|X M . (2)</formula><p>MPM task encourages the model to predict the masked geometric structure of the point clouds. Training the Transformer only with MPM task leads to an unsatisfactory understanding on high-level semantics of the point clouds, which is also pointed out by the recent work in 2D domain <ref type="bibr" target="#b66">[67]</ref>. So we adopt the widely used contrastive learning method MoCo <ref type="bibr" target="#b13">[14]</ref> as a tool to help the Transformers to better learn high-level semantics. With our point patch mixing technique, the optimization of contrastive loss encourages the model to pay attention to the high-level semantics of point clouds by making features of the virtual samples as closely as possible to the corresponding features from the original samples. Let q be the feature of a mixed sample that comes from two other samples, whose features are k + 1 and k + 2 ({k i } are extracted by the momentum feature encoder <ref type="bibr" target="#b13">[14]</ref>). Assuming the mixing ratio is r, the contrastive loss can be written as:</p><formula xml:id="formula_12">Lq = ?rlog exp(qk + 1 /? ) K i=0 exp(qki/? ) ?(1?r)log exp(qk + 2 /? ) K i=0 exp(qki/? ) ), (3)</formula><p>where ? is the temperature and K is the size of memory bank. Coupling MPM objective and contrastive loss enables our Point-BERT to simultaneously capture the local geometric structures and high-level semantic patterns, which are crucial in point cloud representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first introduce the setups of our pretraining scheme. Then we evaluate the proposed model with various downstream tasks, including object classification, part segmentation, few-shot learning and transfer learning. We also conduct an ablation study for our Point-BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training Setups</head><p>Data Setups. ShapeNet <ref type="bibr" target="#b4">[5]</ref> is used as our pre-training dataset, which covers over 50,000 unique 3D models from 55 common object categories. We sample 1024 points from each 3D model and divide them into 64 point patches (subclouds). Each sub-cloud contains 32 points. A lightweight PointNet <ref type="bibr" target="#b33">[34]</ref> containing two-layer MLPs is adopted to project each sub-cloud into 64 point embeddings, which are used as input both for dVAE and Transformer. dVAE Setups. We use a four-layer DGCNN <ref type="bibr" target="#b53">[54]</ref> to learn the inter-patch relationships, modeling the internal structures of input point clouds. During dVAE training, we set the vocabulary size N to 8192. Our decoder is also a DGCNN architecture followed by a FoldingNet <ref type="bibr" target="#b58">[59]</ref>. It is worth noting that the performance of dVAE is susceptible to hyper-parameters, which makes that the configurations of image-based dVAE <ref type="bibr" target="#b36">[37]</ref> cannot be directly used in our scenarios. The commonly used 1 -style Chamfer Distance loss is employed during the reconstruction procedure. Since the value of this 1 loss is numerically small, the weight of KLD loss in Eq.1 must be smaller than that in the image tasks. We set the weight of KLD loss to 0 in the first 10,000 steps and gradually increased to 0.1 in the following 100,000 steps. The learning rate is set to 0.0005 with a cosine learning schedule with 60,000 steps warming up. We decay the temperature in Gumble-softmax function from 1 to 0.0625 in 100,000 steps following <ref type="bibr" target="#b36">[37]</ref>. We train dVAE for a total of 150,000 steps with a batch size of 64.</p><p>MPM Setups. In our experiments, we set the depth for the Transformer to 12, the feature dimension to 384, and the number of heads to 6. The stochastic depth <ref type="bibr" target="#b15">[16]</ref> with a 0.1 rate is applied in our transformer encoder. During MPM pre-training, we fix the weights of Tokenizer learned by dVAE. 25% ? 45% input point embeddings are randomly masked out. The model is then trained to infer the expected point tokens at those masked locations. In terms of MoCo, we set the memory bank size to 16,384, temperature to 0.07, and weight momentum to 0.999. We employ an AdamW <ref type="bibr" target="#b26">[27]</ref> optimizer, using an initial learning rate of 0.0005 and a weight decay of 0.05. The model is trained for 300 epochs with a batch size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream Tasks</head><p>In this subsection, we report the experimental results on downstream tasks. Besides the widely used benchmarks, including classification and segmentation, we also study the model's capacity on few-shot learning and transfer learning.</p><p>Object Classification. We conduct classification experiments on ModelNet40 [55], In the classification task, a twolayer MLP with a dropout of 0.5 is used as our classification head. We use AdamW with a weight decay of 0.05 and a learning rate of 0.0005 under a cosine schedule to optimize the model. The batch size is set to 32.</p><p>The results are presented in <ref type="table" target="#tab_2">Table 1</ref>. We denote our  <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b64">65]</ref>. Additionally, we compare with a recent pre-training strategy OcCo <ref type="bibr" target="#b51">[52]</ref> as a strong baseline of our pre-training method. For fair comparisons, we follow the details illustrated in <ref type="bibr" target="#b51">[52]</ref> and use the Transfomer-based decoder PoinTr [61] to perform their pretext task. Combining our Transformer encoder and PoinTr's decoder, we conduct the completion task on ShapeNet, following the idea of OcCo. We term this model as 'Transformer+OcCo'. We see pre-training Transformer with OcCo improves 0.7%/1.0% over the baseline using 1024/4096 inputs. In comparison, our Point-BERT brings 1.8%/2.2% gains over that of training from scratch. We also observe that adding more points will not significantly improve the Transformer model without pre-training while Point-BERT models can be consistently improved by increasing the number of points. When we increase the density of inputs (4096), our Point-BERT achieves significantly better performance (93.4%) than that with the baseline (91.2%) and OcCo (92.2%). Given more input points (8192), our method can be further boosted to 93.8% accuracy on ModelNet40.</p><p>Few-shot Learning. We follow previous work <ref type="bibr" target="#b41">[42]</ref> to evaluate our model under the few-shot learning setting. A typical setting is "K-way N -shot", where K classes are first randomly selected, and then (N +20) objects are sampled for each class <ref type="bibr" target="#b41">[42]</ref>. The model is trained on K ? N samples (support set), and evaluated on the remaining 20K samples (query set). We compare Point-BERT with OcCo <ref type="bibr" target="#b51">[52]</ref>, which achieves state-of-the-art performance on this task. In our experiments, we test the performance under "5way 10shot", "5way 20shot", "10way 10shot" and "10way 20shot". We conduct 10 independent experiments under each setting and report the average performance as well as the standard deviation over the 10 runs. We also reproduce DGCNN-rand and DGCNN-OcCo under the same condition for a fair comparison. As shown in the <ref type="table" target="#tab_3">Table 2</ref>, Point-BERT achieves the best in the few-shot learning. It obtains an absolute improvement of 6.8%, 3.0%, 6.4%, 3.3% over the baseline and 0.6%, 0.4%, 1.6%, 0.3% over the OcCo-based method on the four settings. The strong results indicate that Point-BERT learns more generic knowledge that can be quickly transferred to new tasks with limited data.</p><p>Part Segmentation. Object part segmentation is a challenging task aiming to predict a more fine-grained class label for every point. We evaluate the effectiveness of Point-BERT on ShapeNetPart <ref type="bibr" target="#b59">[60]</ref>, which contains 16,881 models from 16 categories. Following PointNet <ref type="bibr" target="#b33">[34]</ref>, we sample 2048 points from each model and increase the group number g from 64 to 128 in the segmentation tasks. We design a segmentation head to propagate the group features to each point hierarchically. Specifically, features from 4 th , 8 th and the last layer of Transformer are selected, denoted as</p><formula xml:id="formula_13">{H 4 = {h 4 i } g i=1 , H 8 = {h 8 i } g i=1 , H 12 = {h 12 i } g i=1 }.</formula><p>Then we downsample the origin point cloud to 512 and 256 points via FPS, phrased as</p><formula xml:id="formula_14">P 4 = {p 4 i } 512 i=1 and P 8 = {p 8 i } 256 i=1</formula><p>. We follow PointNet++ <ref type="bibr" target="#b34">[35]</ref> to perform feature propagation between H 4 and P 4 , H 8 and P 8 . Here, we can obtain the upsampled feature map? 4 and? 8 , which represent the features for the points in P 4 and P 8 . Then, we can propagate the feature from H 12 to? 4 and finally to every point. Two types of mIoU are reported in <ref type="table">Table 3</ref>. It is clear that our Point-BERT outperforms PointNet, PointNet++, and DGCNN. Moreover, Point-BERT improves 0.69% and 0.5% mIoU over vanilla Transformers, while OcCo fails to improve baseline performance in part segmentation task.</p><p>Transfer to Real-World Dataset. We evaluate the generalization ability of the learned representation by pre-training the model on ShapeNet and fine-tuning it on ScanOb-jectNN <ref type="bibr" target="#b48">[49]</ref>, which contains 2902 point clouds from 15 categories. It is a more challenging dataset sampled from realworld scans containing background and occlusions. We follow previous works to conduct experiments on three main variants: OBJ-BG, OBJ-ONLY, and PB-T50-RS. The experimental results are reported in <ref type="table">Table 4</ref>. As we can see, Point-BERT improves the vanilla Transformers by about 7.57%, 7.57%, and 5.83% on three variants.</p><p>Comparing the classification results on ModelNet40 <ref type="table" target="#tab_2">(Table 1)</ref> and ScanObjectNN <ref type="table" target="#tab_3">(Table 2)</ref>, we observe that DGCNN outperforms PointNet++ (+2.4%) on the Model-Net40. While the superiority is degraded on the real-world dataset ScanObjectNN. As for Point-BERT, it achieves SOTA performance on both datasets, which strongly confirms the effectiveness of our method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>Pretext Task. We denote model A as our baseline, which is the Transformer training from scratch. Model B presents pre-training Transformer with MPM pretext task. Model C is trained with more samples coming from 'point patch mixing' technique. Model D (the proposed method) is trained under the setting of MPM, point patch mixing, and MoCo. As can be seen in the upper part of <ref type="table" target="#tab_5">Table 5</ref>, Model B with MPM improves the performance about 1.17% . By adopting point patch mixing strategy, Model C gets an improvement of 0.33%. With the help of MoCo <ref type="bibr" target="#b13">[14]</ref>, Model D further brings an improvement of 0.33%.</p><p>Masking Strategy. We visualize the point token prediction task in <ref type="figure">Figure 2</ref>. Our Transformer encoder can reasonably infer the point tokens of the missing patches. In practice, we reconstruct the local patches through the decoder of dVAE, based on the point tokens predicted by the Transformer encoder. Two masking strategies are explored: block-wise masking (block-mask) and random masking (rand-mask). The masking strategy determines the difficulty of the pretext task, influencing reconstruction quality and representations. We further investigate the effects of different masking strategies and provide the results in <ref type="table" target="#tab_5">Table 5</ref>. We see that Model D with block-mask works better at the ratio of 25% ? 45%. Unlike images, which can be split into regular non-overlapping patches, sub-clouds partitioned from the original point cloud often involve overlaps. Thus, randmask makes the task easier than block-mask, and further degrades the reconstruction performance. We also consider another type of augmentations: randomly replace some input embeddings with those from other samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization</head><p>We visualize the learned features of two datasets via t-SNE <ref type="bibr" target="#b49">[50]</ref> in <ref type="figure" target="#fig_1">Figure 4</ref>. In figure (a) and (b), the visualized features are from our Point-BERT (a) before fine-tuning and (b) after fine-tuning on ModelNet40. As can be seen, features from different categories can be well separated by our method even before fine-tuning. We also visualize the feature maps on the PB-T50-RS of ScanObjectNN in (c). We can see that separate clusters are formed for each category, indicating the transferability of learned representation to real-world scenarios. It further verifies that Point-BERT helps the Transformer to learn generic knowledge for 3D objects. We also visualize the learning curves of our baseline Transformers and the proposed Point-BERT in <ref type="figure" target="#fig_2">Figure 5</ref>. As can be seen, pre-training with our Point-BERT significantly improves the performance of baseline Transformers both in accuracy and speed on both synthetic and real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Discussions</head><p>We present a new paradigm for 3D point cloud Transformers through a BERT-style pre-training to learn both low-level structural information and high-level semantic feature. We observe a significant improvement for the Transformer on learning and generalization by comprehensive experiments on several 3D point cloud tasks. We show the potential of standard Transformers in 3D scenarios with appropriate pre-training strategy and look forward to further study on standard Transformers in the 3D domain.</p><p>We do not foresee any negative ethical/societal impacts at this moment. Although the proposed method can effectively improve the performance of standard Transformers on point clouds, the entire 'pre-training + fine-tuning' procedure is rather time-consuming, like other Transformers pre-training methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13]</ref>. Improving the efficiency of the training process will be an interesting future direction. where P represents the prediction point set and G represents the ground-truth point set. Except for the reconstruction loss, we follow <ref type="bibr" target="#b36">[37]</ref> to optimize the KL-divergence L KL between the predicted tokens' distribution and a uniform prior. The final objective function is</p><formula xml:id="formula_15">LdVAE = d 1 CD (P f ine , G) + d 1 CD (Pcoarse, G) + ?LKL.<label>(2)</label></formula><p>Experiment Setting: We report the default setting for dVAE training in <ref type="table" target="#tab_6">Table 7</ref>.</p><p>Hyper-parameters of dVAE: We set the size of the learnable vocabulary to 8192, and each 'word' in it is a 256-dim vector. The most important and sensitive hyper-parameters of dVAE are ? for L KL and the temperature ? for Gumbelsoftmax. We set ? to 0 in the first 18 epochs (about 10,000 steps) and gradually increase to 0.1 in the following 180 epochs (about 100,000 steps) using a cosine schedule. As for ? , we follow <ref type="bibr" target="#b36">[37]</ref> to decay it from 1 to 0.0625 using a cosine schedule in the first 180 epochs (about 100,000 steps).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Point-BERT</head><p>Architecture: We follow the standard Transformer <ref type="bibr" target="#b8">[9]</ref> architecture in our experiments. It contains a stack of Transformer blocks <ref type="bibr" target="#b50">[51]</ref>, and each block consists of a multi-head self-attention layer and a FeedForward Network (FFN). In these two layers, LayerNorm (LN) is adopted.</p><p>Multi-head Attention: Multi-head attention mechanism enables the network to jointly consider information from different representation subspaces <ref type="bibr" target="#b50">[51]</ref>. Specifically, given the input values V , keys K and queries Q, the multi-head attention is computed by:</p><formula xml:id="formula_16">MultiHead(Q, K, V ) = W o Concat(head1, ..., head h ),<label>(3)</label></formula><p>where W o is the weights of the last linear layer. The feature of each head can be obtained by:   <ref type="table">Table 9</ref>. Experiment setting for end-to-end finetuning. S represents segmentation task, C represents classification task.</p><formula xml:id="formula_17">head i = softmax( QW Q i (KW K i ) T ? d k )V W V i ,<label>(4)</label></formula><p>where W Q i , W K i and W V i are the linear layers that project the inputs to different subspaces and d k is the dimension of the input features.</p><p>Feed-forward network (FFN): Following <ref type="bibr" target="#b50">[51]</ref>, two linear layers with ReLU activations and dropout are adopted as the feed-forward network.</p><p>Point-BERT pre-training: We report the default setting for our experiments in Point-BERT pretraining in <ref type="table" target="#tab_8">Table 8</ref>. The pre-training is conducted on ShapeNet.</p><p>End-to-end finetuning: We finetune our Point-BERT model follow the common practice of supervised models strictly. The default setting for end-to-end finetuning is in <ref type="table">Table 9</ref>.</p><p>Hyper-parameters of Transformers: We set the number of blocks in the Transformer to 12. The number of heads in each multi-head self-attention layer is set to 6. The feature <ref type="figure">Figure 6</ref>. Two main operations of our segmentation head: 1) Upsampling: upsample the feature map for the sparse point cloud to the dense point cloud. 2) Propagation: propagate the feature hierarchically from deep layers to shallow layers for dense prediction.</p><p>dimension of the transformer layer is set to 384. We follow <ref type="bibr" target="#b46">[47]</ref> to adopt the stochastic depth strategy with a drop rate of 0.1.</p><p>Classification Head: A two-layer MLP with dropout is applied as our classification head. In classification tasks, we first take the output feature of [CLS] token out, and maxpool the rest of nodes' features. These two features are then combined together and sent into the classification head. The detailed architecture of the classification head is shown in <ref type="table">Table 6</ref>, where N cls is the number of classes for a certain dataset.</p><p>Segmentation Head: There are no downsampling layers in the standard Transformers, making it challenging to perform dense prediction based on a single-resolution feature map. We adopt an upsampling-propagation strategy to solve this problem, consisting of two steps: 1) Geometry-based feature upsampling and 2) Hierarchical feature propagation.</p><p>We extract features from different layers of the Transformer, where features from shallow layers tend to capture low-level information, while features from deeper layers involve more high-level information. To upsample the feature maps to different resolutions, we first apply FPS to the origin point cloud and obtain point clouds at various resolutions. Then we upsample the feature maps from different layers to different resolutions accordingly. As shown in the left part of <ref type="figure">Figure 6</ref>, 'A' is a point from the dense point cloud, and 'a','b','c' are its nearest points in the sparser point cloud, with distance of d a , d b and d c respectively. We obtain the point feature of 'A' based on the weighted addition of those features, which can be written as:</p><formula xml:id="formula_18">F A = MLP(Concat( i?[a,b,c] 1 di F i i?[a,b,c] 1 di , p A )),<label>(5)</label></formula><p>where p A represents the coordinates of point 'A'. After obtaining the feature maps at different resolutions, we perform feature propagation from coarse-grained feature maps to fine-grained feature maps. As shown in the right part of <ref type="figure">Figure 6</ref>, for a point 'A' in the dense point cloud, we find its k nearest points in the sparser point cloud. Then a lightweight DGCNN <ref type="bibr" target="#b53">[54]</ref> is used to update the feature of 'A'. We hierarchically update the feature with the resolution increases and finally obtain the dense feature map, which can be used for segmentation tasks. The detailed architecture for the segmentation head is shown in <ref type="table">Table 6</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Visualization of feature distributions. We show the t-SNE visualization of feature vectors learned by Point-BERT (a) after pre-training, (b) after fine-tuning on ModelNet40, and (c) after fine-tuning on ScanObjectNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Learning curve. We compare the performance of Transformers training from scratch (blue) and pre-training with Point-BERT (red) in terms of training loss and validation accuracy on synthetic and real-world object classification datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>g</head><label></label><figDesc>? p ,<ref type="bibr" target="#b0">(1)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of Point-BERT with of state-of-the-art models on ModelNet40. We report the classification accuracy (%) and the number of points in the input.[ST] and [T] represent the standard Transformers models and Transformer-based models with some special designs and more inductive biases, respectively.</figDesc><table><row><cell>Method</cell><cell>#point</cell><cell>Acc.</cell></row><row><cell>PointNet [34]</cell><cell>1k</cell><cell>89.2</cell></row><row><cell>PointNet++ [35]</cell><cell>1k</cell><cell>90.5</cell></row><row><cell>SO-Net [22]</cell><cell>1k</cell><cell>92.5</cell></row><row><cell>PointCNN [23]</cell><cell>1k</cell><cell>92.2</cell></row><row><cell>DGCNN [54]</cell><cell>1k</cell><cell>92.9</cell></row><row><cell>DensePoint [24]</cell><cell>1k</cell><cell>92.8</cell></row><row><cell>RSCNN [38]</cell><cell>1k</cell><cell>92.9</cell></row><row><cell>KPConv [46]</cell><cell>?6.8k</cell><cell>92.9</cell></row><row><cell>[T] PCT [11]</cell><cell>1k</cell><cell>93.2</cell></row><row><cell>[T] PointTransformer [65]</cell><cell>-</cell><cell>93.7</cell></row><row><cell>[ST] NPCT [11]</cell><cell>1k</cell><cell>91.0</cell></row><row><cell>[ST] Transformer</cell><cell>1k</cell><cell>91.4</cell></row><row><cell>[ST] Transformer + OcCo [52]</cell><cell>1k</cell><cell>92.1</cell></row><row><cell>[ST] Point-BERT</cell><cell>1k</cell><cell>93.2</cell></row><row><cell>[ST] Transformer</cell><cell>4k</cell><cell>91.2</cell></row><row><cell>[ST] Transformer + OcCo [52]</cell><cell>4k</cell><cell>92.2</cell></row><row><cell>[ST] Point-BERT</cell><cell>4k</cell><cell>93.4</cell></row><row><cell>[ST] Point-BERT</cell><cell>8k</cell><cell>93.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Few-shot classification results on ModelNet40. We report the average accuracy (%) as well as the standard deviation over 10 independent experiments.</figDesc><table><row><cell></cell><cell cols="2">5-way</cell><cell cols="2">10-way</cell></row><row><cell></cell><cell>10-shot</cell><cell>20-shot</cell><cell>10-shot</cell><cell>20-shot</cell></row><row><cell cols="5">DGCNN-rand [52] 31.6 ? 2.8 40.8 ? 4.6 19.9 ? 2.1 16.9 ? 1.5</cell></row><row><cell cols="5">DGCNN-OcCo [52] 90.6 ? 2.8 92.5 ? 1.9 82.9 ? 1.3 86.5 ? 2.2</cell></row><row><cell>DGCNN-rand  *</cell><cell cols="4">91.8 ? 3.7 93.4 ? 3.2 86.3 ? 6.2 90.9 ? 5.1</cell></row><row><cell>DGCNN-OcCo  *</cell><cell cols="4">91.9 ? 3.3 93.9 ? 3.1 86.4 ? 5.4 91.3 ? 4.6</cell></row><row><cell>Transformer-rand</cell><cell cols="4">87.8 ? 5.2 93.3 ? 4.3 84.6 ? 5.5 89.4 ? 6.3</cell></row><row><cell cols="5">Transformer-OcCo 94.0 ? 3.6 95.9 ? 2.3 89.4 ? 5.1 92.4 ? 4.6</cell></row><row><cell>Point-BERT</cell><cell cols="4">94.6 ? 3.1 96.3 ? 2.7 91.0 ? 5.4 92.7 ? 5.1</cell></row><row><cell cols="5">baseline model as 'Transformer', which is trained on Mod-</cell></row><row><cell cols="5">elNet40 with random initialization. Several Transformer-</cell></row><row><cell cols="5">based models are illustrated, where [ST] represents a stan-</cell></row><row><cell cols="5">dard Transformer architecture, and [T] denotes the Trans-</cell></row><row><cell cols="5">former model with some special designs or inductive bi-</cell></row><row><cell cols="5">ases. Although we mainly focus on pre-training for stan-</cell></row><row><cell cols="5">dard Transformers in this work, our MPM pre-training strat-</cell></row><row><cell cols="5">egy is also suitable for other Transformer-based point cloud</cell></row><row><cell>models</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Part segmentation results on the ShapeNetPart dataset. We report the mean IoU across all part categories mIoUC (%) and the mean IoU across all instance mIoUI (%) , as well as the IoU (%) for each categories. Methods mIoUC mIoUI aero bag cap car chair earphone guitar knife lamp laptop motor mug pistol rocket skateboard table Classification results on the ScanObjectNN dataset.</figDesc><table><row><cell>PointNet [34]</cell><cell>80.39</cell><cell>83.7</cell><cell cols="2">83.4 78.7 82.5 74.9 89.6</cell><cell>73.0</cell><cell cols="2">91.5 85.9 80.8 95.3</cell><cell>65.2</cell><cell>93</cell><cell>81.2</cell><cell>57.9</cell><cell>72.8</cell><cell>80.6</cell></row><row><cell>PointNet++ [35]</cell><cell>81.85</cell><cell>85.1</cell><cell cols="2">82.4 79 87.7 77.3 90.8</cell><cell>71.8</cell><cell>91</cell><cell>85.9 83.7 95.3</cell><cell cols="3">71.6 94.1 81.3</cell><cell>58.7</cell><cell>76.4</cell><cell>82.6</cell></row><row><cell>DGCNN [54]</cell><cell>82.33</cell><cell>85.2</cell><cell cols="2">84 83.4 86.7 77.8 90.6</cell><cell>74.7</cell><cell cols="2">91.2 87.5 82.8 95.7</cell><cell cols="3">66.3 94.9 81.1</cell><cell>63.5</cell><cell>74.5</cell><cell>82.6</cell></row><row><cell>Transformer</cell><cell>83.42</cell><cell>85.1</cell><cell cols="2">82.9 85.4 87.7 78.8 90.5</cell><cell>80.8</cell><cell cols="2">91.1 87.7 85.3 95.6</cell><cell cols="3">73.9 94.9 83.5</cell><cell>61.2</cell><cell>74.9</cell><cell>80.6</cell></row><row><cell cols="2">Transformer-OcCo 83.42</cell><cell>85.1</cell><cell cols="2">83.3 85.2 88.3 79.9 90.7</cell><cell>74.1</cell><cell cols="2">91.9 87.6 84.7 95.4</cell><cell cols="3">75.5 94.4 84.1</cell><cell>63.1</cell><cell>75.7</cell><cell>80.8</cell></row><row><cell>Point-BERT</cell><cell>84.11</cell><cell>85.6</cell><cell cols="2">84.3 84.8 88.0 79.8 91.0</cell><cell>81.7</cell><cell cols="2">91.6 87.9 85.2 95.6</cell><cell cols="3">75.6 94.7 84.3</cell><cell>63.4</cell><cell>76.3</cell><cell>81.5</cell></row><row><cell cols="4">We report the accuracy (%) of three different settings.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell></cell><cell cols="4">OBJ-BG OBJ-ONLY PB-T50-RS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointNet [34]</cell><cell></cell><cell>73.3</cell><cell>79.2</cell><cell>68.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SpiderCNN [58]</cell><cell></cell><cell>77.1</cell><cell>79.5</cell><cell>73.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointNet++ [35]</cell><cell></cell><cell>82.3</cell><cell>84.3</cell><cell>77.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointCNN [23]</cell><cell></cell><cell>86.1</cell><cell>85.5</cell><cell>78.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DGCNN [54]</cell><cell></cell><cell>82.8</cell><cell>86.2</cell><cell>78.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BGA-DGCNN [49]</cell><cell>-</cell><cell>-</cell><cell>79.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BGA-PN++ [49]</cell><cell>-</cell><cell>-</cell><cell>80.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell></cell><cell>79.86</cell><cell>80.55</cell><cell>77.24</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Transformer-OcCo</cell><cell>84.85</cell><cell>85.54</cell><cell>78.79</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Point-BERT</cell><cell></cell><cell>87.43</cell><cell>88.12</cell><cell>83.07</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Ablation study. We investigate the effects of different designs and report the classification accuracy (%) after fine-tuning on ModelNet40. All models are trained with 1024 points.</figDesc><table><row><cell>Pretext tasks</cell><cell>MPM</cell><cell cols="2">Point Patch Mixing Moco</cell><cell>Acc.</cell></row><row><cell>Model A</cell><cell></cell><cell></cell><cell></cell><cell>91.41</cell></row><row><cell>Model B</cell><cell></cell><cell></cell><cell></cell><cell>92.58 ?</cell></row><row><cell>Model C</cell><cell></cell><cell></cell><cell></cell><cell>92.91 ?</cell></row><row><cell>Model D</cell><cell></cell><cell></cell><cell></cell><cell>93.24 ?</cell></row><row><cell cols="2">Augmentation mask type</cell><cell>mask ratio</cell><cell>replace</cell><cell>Acc.</cell></row><row><cell>Model B</cell><cell>block mask</cell><cell>[0.25, 0.45]</cell><cell>No</cell><cell>92.58</cell></row><row><cell>Model B</cell><cell>block mask</cell><cell>[0.25, 0.45]</cell><cell>Yes</cell><cell>91.81 ?</cell></row><row><cell>Model B</cell><cell>rand mask</cell><cell>[0.25, 0.45]</cell><cell>No</cell><cell>92.34 ?</cell></row><row><cell>Model B</cell><cell>block mask</cell><cell>[0.55, 0.85]</cell><cell>No</cell><cell>92.52 ?</cell></row><row><cell>Model D</cell><cell>block mask</cell><cell>[0.25, 0.45]</cell><cell>No</cell><cell>93.16</cell></row><row><cell>Model D</cell><cell>block mask</cell><cell>[0.25, 0.45]</cell><cell>Yes</cell><cell>92.58 ?</cell></row><row><cell>Model D</cell><cell>rand mask</cell><cell>[0.25, 0.45]</cell><cell>No</cell><cell>92.91 ?</cell></row><row><cell>Model D</cell><cell>block mask</cell><cell>[0.55, 0.85]</cell><cell>No</cell><cell>92.59 ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>Experiment setting for training the dVAE.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW [28]</cell></row><row><cell>learning rate</cell><cell>5e-4</cell></row><row><cell>weight decay</cell><cell>5e-4</cell></row><row><cell>learning rate schedule</cell><cell>cosine [26]</cell></row><row><cell>warmingup epochs</cell><cell>10</cell></row><row><cell>augmentation</cell><cell>RandSampling</cell></row><row><cell>batch size</cell><cell>64</cell></row><row><cell>number of points</cell><cell>1024</cell></row><row><cell>number of patches</cell><cell>64</cell></row><row><cell>patch size</cell><cell>32</cell></row><row><cell>training epochs</cell><cell>300</cell></row><row><cell>dataset</cell><cell>ShapeNet [5]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>Experiment setting for Point-BERT pre-training</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>optimizer</cell><cell>AdamW</cell></row><row><cell>learning rate</cell><cell>5e-4</cell></row><row><cell>weight decay</cell><cell>5e-2</cell></row><row><cell>learning rate schedule</cell><cell>cosine</cell></row><row><cell>warmingup epochs</cell><cell>10</cell></row><row><cell>augmentation</cell><cell>ScaleAndTranslate</cell></row><row><cell>batch size</cell><cell>32(C),16(S)</cell></row><row><cell>number of points</cell><cell>1024(C),2048 (S)</cell></row><row><cell>number of patches</cell><cell>64(C),128(S)</cell></row><row><cell>patch size</cell><cell>32</cell></row><row><cell>training epochs</cell><cell>300</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Point tokens have two forms, discrete integer number and corresponding word embedding in V, which are equivalent.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: Implementation Details</head><p>A. Discrete VAE Architecture: Our dVAE consists of a tokenizer and a decoder. Specifically, the tokenizer contains a 4-layer DGCNN <ref type="bibr" target="#b53">[54]</ref>, and the decoder involves a 4-layer DGCNN followed by a FoldingNet <ref type="bibr" target="#b58">[59]</ref>. The detailed network architecture of our dVAE is illustrated in <ref type="table">Table 6</ref>, where C in and C out are the dimension of input and output features, C middle is the dimension of the hidden layers. N out is the number of point patches in each layer, and K is the number of neighbors in kNN operation. Additionally, FoldingLayer concatenates a 2D grids to the inputs and finally generates 3D point clouds. Optimization: During the training phase, we consider reconstruction loss and distribution loss simultaneously. For reconstruction, we follow PoinTr <ref type="bibr" target="#b60">[61]</ref> to supervise both coarse-grained prediction and fine-grained prediction with the ground-truth point cloud. The 1 -form Chamfer Distance is adopted, which is calculated as:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Selfsupervised learning for domain adaptation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Idan Achituve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chechik</surname></persName>
		</author>
		<idno>WACV, 2021. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Beit: Bert pre-training of image transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self-supervised learning on 3d point clouds by learning discrete generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<title level="m">Pct: Point cloud transformer. Computational Visual Media</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixing</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12556</idno>
		<title level="m">A survey on visual transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Piotr Doll ar, and Ross Girshick</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 3</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Aaai&apos;2020 keynotes turing award winners event</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pretraining by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">So-net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pointcnn: Convolution on x-transformed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densepoint: Learning densely contextual representation for efficient point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Shiming Xiang, and Chunhong Pan</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross pixel optical-flow similarity for self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Thewlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Self-supervised point cloud prediction using 3d spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Mersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyuanli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Behley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04076</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Self-supervised learning of point clouds via orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omid</forename><surname>Poursaeed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nayun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<idno>3DV. IEEE, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Point-net++ deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners. OpenAI blog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Global-local bidirectional reasoning for unsupervised representation learning of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discrete variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolfe</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Info3d: Representation learning on 3d objects using mutual information maximization and contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Sanghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-supervised deep learning on point clouds by reconstructing space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Sauder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjarne</forename><surname>Sievers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Self-supervised few-shot learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Point cloud pre-training by mixing and disentangling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00452</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Selfsupervised learning of local features in 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<title level="m">Kpconv: Flexible and deformable convolution for point clouds. ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, 2021. 1</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Selfie: Self-supervised pretraining for image embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Trieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02940</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<editor>Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Duc Thanh Nguyen, and Sai-Kit Yeung</editor>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Unsupervised point cloud pre-training via occlusion completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Lasenby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Max-deeplab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">TOG</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pretraining for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Self-supervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Pointr: Diverse point cloud completion with geometry-aware transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xumin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 2021. 3</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Pointcutmix: Regularization strategy for point cloud classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyujie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanmei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01461</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Self-supervised pretraining of 3d features on any point-cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Point transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihang</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07832</idno>
		<title level="m">Ibot: Image bert pre-training with online tokenizer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
