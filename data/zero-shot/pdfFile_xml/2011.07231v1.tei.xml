<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ActBERT: Learning Global-Local Video-Text Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
							<email>linchao.zhu@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">Baidu Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">ReLER</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ActBERT: Learning Global-Local Video-Text Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce ActBERT for self-supervised learning of joint video-text representations from unlabeled data. First, we leverage global action information to catalyze mutual interactions between linguistic texts and local regional objects. It uncovers global and local visual clues from paired video sequences and text descriptions for detailed visual and text relation modeling. Second, we introduce a TaNgled Transformer block (TNT) to encode three sources of information, i.e., global actions, local regional objects, and linguistic descriptions. Global-local correspondences are discovered via judicious clues extraction from contextual information. It enforces the joint video-text representation to be aware of fine-grained objects as well as global human intention. We validate the generalization capability of ActBERT on downstream video-and-language tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization. ActBERT significantly outperforms the stateof-the-art, demonstrating its superiority in video-text representation learning.actbct</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>While supervised learning has been successful in a variety of computer vision tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b29">30]</ref>, self-supervised representation learning from unlabeled data has attracted increasing attention in recent years <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28]</ref>. In self-supervised learning, a model is first pre-trained on a large amount of unlabeled data with a surrogate loss. The fine-tuning process further helps the pre-trained model to be specialized in downstream tasks. Recently, there has been rapid progress in self-supervised representation learning for texts <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b46">47]</ref>, where the Bidirectional Encoder Representations from Transformers (BERT) model <ref type="bibr" target="#b6">[7]</ref> generalizes remarkably to many natural language tasks, e.g., question answering <ref type="bibr" target="#b1">[2]</ref>.</p><p>Motivated by BERT's success in self-supervised training, we aim to learn an analogous model for video and text joint modeling. We exploit video-text relations based on narrated instructional videos, where the aligned texts are detected by off-the-shelf automatic speech recognition (ASR) models. These instructional videos serve as natural sources for video-text relationship studies. First, they are vastly available and freely accessible on YouTube and other platforms <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b34">35]</ref>. Second, the visual frames are aligned with the instructional narrations. The text narrations not only cover the objects in the scene explicitly but identify the salient action in the video clip.</p><p>To generalize BERT to video-and-language tasks, Sun et al. <ref type="bibr" target="#b34">[35]</ref> extended the BERT model by learning from quantized video frame features. The original BERT takes discrete elements as inputs and predicts the corresponding tokens as the output. In contrast, visual features are distributed representations with real value, while the real-value features cannot be directly categorized into discrete labels for "visual token" prediction. Sun et al. <ref type="bibr" target="#b34">[35]</ref> discretized visual features into visual words via clustering. These visual tokens can be directly passed to the original BERT model. However, detailed local information, e.g., interacting objects, human actions would be possibly lost during clustering. It prevents the model from uncovering fine-grained relations between video and text. In this paper, we propose ActBERT to learn a joint video-text representation that uncovers global and local visual clues from paired video sequences and text descriptions. Both the global and the local visual signals interact with the semantic stream mutually. ActBERT leverages profound contextual information and exploits fine-grained relations for video-text joint modeling.</p><p>First, ActBERT incorporates global actions, local regional objects and text descriptions in a joint framework. Actions, e.g., "cut", "rotate", "slice", are essential to various video-related downstream tasks. The recognition of human actions can demonstrate the model's capacity in motion understanding and complex human intention reasoning. It could be beneficial to explicitly model human actions during model pre-training. Long-term action sequences furthermore offer temporal dependencies about an instruc-tional task. Though action clues are important, they are largely ignored in previous self-supervised video-text training <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b26">27]</ref>, where actions are treated identically to objects. To model human actions, we first extract verbs from the text descriptions and construct an action classification dataset from the original dataset. Then, a 3D convolution network is trained to predict the action labels. The features from the optimized network are used as the action embedding. In this way, clip-level actions are represented, and the corresponding action label is inserted. Besides global action information, we incorporate local regional information to provide fine-grained visual cues <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b4">5]</ref>. Object regions provide detailed visual clues about the whole scene, including the regional object feature, the position of the object. The language model can benefit from the regional information for better language-and-visual alignment.</p><p>Second, we introduce a TaNgled Transformer block (TNT) to encode features from three sources, i.e., global actions, local regional objects, and linguistic tokens. Previous studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b35">36]</ref> consider two modalities when designing the new transformer layers, i.e., fine-grained object information from image and natural language. Lu et al. <ref type="bibr" target="#b21">[22]</ref> introduced a co-attentional transformer layer, where the key-value pairs from one modality are passed to the other modality's attention block to act as the new key-value pairs. However, in our scenario, there are three sources of inputs. The two sources, i.e., local regional features and linguistic texts, offer detailed descriptions of the occurring event in the clip. The other global action feature provides the human intention in time-series as well as a straightforward clue for contextual inferring. We design a new tangled transformer block for cross-modality feature learning from three sources. To enhance the interactions between two visual cues and linguistic features, we use a separate transformer block <ref type="bibr" target="#b41">[42]</ref> to encode each modality. The mutual cross-modal communication is later enhanced with two additional multi-head attention blocks. The action feature catalyzes mutual interactions. With the guidance from the action features, we inject visual information to the linguistic transformer, and incorporate linguistic information to the visual transformers. The tangled transformer dynamically selects judicious cues its context to facilitate the target prediction.</p><p>Furthermore, we design four surrogate tasks to train Act-BERT, i.e., masked language modeling with global and local visual cues, masked action classification, masked object classification and cross-modal matching. The pre-trained ActBERT is transferred to five video-related downstream tasks, i.e., video captioning, action segmentation, text-video clip retrieval, action step localization, and video question answering. We quantitatively show ActBERT achieves the state-of-the-art performance with a clear margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video and language. There are many existing video-andlanguage tasks to evaluate the model's capacities in joint video-text representation learning, e.g., video question answering <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b55">56]</ref>, video captioning <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b53">54]</ref>, textvideo retrieval <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b25">26]</ref>, video grounding <ref type="bibr" target="#b51">[52]</ref>. In video and language modeling, it can be difficult to learn relations between ordered video frames and their corresponding descriptions, where video temporal information and the interactions between multiple objects spatio-temporally requires to be incorporated. The dominant approach for multi-modal modeling is to leverage Recurrent Neural Networks (RNNs) and their variants, e.g., Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU), to model sequence relations, e.g., <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b54">55]</ref>. Zhou et al. <ref type="bibr" target="#b53">[54]</ref> leveraged masked transformers in both the encoder and the decoder for dense video captioning. Most of these works are conducted on well-annotated datasets where the descriptions are manually generated, requiring considerable human interference. There are other works to learn video representations from limited annotated data <ref type="bibr" target="#b56">[57]</ref>. The video data is a natural source to learn cross-modal representations. The text descriptions are automatically generated by off-the-shelf automatic speech recognition (ASR) models. This is more scalable and general to the model's deployment in real-world applications. In this paper, we focus on learning joint videotext representation in a self-supervised way.</p><p>Cross-modal pre-training. In the past year, many works extended BERT to model cross-modal data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref>. The recent BERT model for video-text modeling <ref type="bibr" target="#b34">[35]</ref> introduces visual words for video frames encoding, where local regional information is largely ignored. The synchronized video-audio signal is also a good test-bed for cross-modal representation learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b15">16]</ref>. However, they leveraged low-level audio signals and only considered the synchronization nature of video data. In this work, we focus on video-text joint representation learning. Our ActBERT leverages multi-source information and achieves remarkable performance in many downstream video-text tasks.</p><p>Instructional videos. Learning from instructional videos is challenging due to its data complexity across various tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b26">27]</ref>. These videos are collected from many domains, e.g., cooking, sports, gardening. Many works also regard the transcriptions generated from instructional videos as a source of supervision <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b26">27]</ref>. However, we employ ActBERT to explicitly model human actions, local regions in a unified framework. We improve <ref type="bibr" target="#b26">[27]</ref> with more specific relation modeling between videos and their description. We quantitatively demonstrated that ActBERT is more suitable for unsupervised video-text modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>We first illustrate the original BERT <ref type="bibr" target="#b6">[7]</ref> model. BERT <ref type="bibr" target="#b6">[7]</ref> pre-trains a language model on large corpora in an unsupervised way. The pre-trained model is found to be flexible and beneficial to a variety of downstream tasks, e.g., question answering <ref type="bibr" target="#b1">[2]</ref>.</p><p>In BERT <ref type="bibr" target="#b6">[7]</ref>, the input entities are processed by a multilayer bidirectional transformer <ref type="bibr" target="#b41">[42]</ref>. The embeddings of each input are processed with stacked self-attention layers to aggregate contextual features. The attention weights are adaptively generated. The output features contain contextual information about the original input sequence. In selfattention, the generated features are irrelevant to input sequence order, and it enables the output representation to be permutation-invariant. The output representation is not affected when the input sequence is shuffled. A position embedding is commonly applied to each input entity for the incorporation of sequential order clues.</p><p>In the original BERT, Devlin et al. introduced two tasks for pre-training. In the task of masked language modeling (MLM), a portion of input words are randomly masked out. These masked-out words are replaced by a special token "[MASK]". The task is to predict the masked words based on the observations from the contextual contents. The contextual contents are unmasked elements that provide useful relevant cues for the prediction of the masked word.</p><p>The other task, i.e., Next Sentence Prediction (NSP), models order information between two sentences. Two sentences are sampled from a document, and NSP aims to identify if the second sentence is adjacent to the first sentence with the correct order. The two sentences are concatenated via a token "[SEP]", so that the models can be aware of the inputs being separated sentences. The prediction is made upon the output features of the first token "[CLS]". This is a binary classification problem, and a simple sigmoid classifier is used. A prediction of "1" indicates the sentences are consecutive, and the second sentence is right after the first sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">ActBERT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Input Embeddings</head><p>There are four types of input elements in ActBERT. They are actions, image regions, linguistic descriptions and special tokens. Special tokens are used to distinguish different inputs.</p><p>Each input sequence starts with a special token "[CLS]" and ends with another token "[SEP]". We put the linguistic descriptions after "[CLS]". There are the action inputs followed by local regional features. We denote the action features as a 1 , . . . , a L , the frame region fea-tures as r 1 , . . . , r M . The sequential text descriptions is denoted as w 1 , . . . , w N .</p><p>The whole sequence is denoted as {[CLS], w 1 , . . . , w N , [SEP], a 1 , . . . , a L , [SEP], r 1 , . . . , r M , [SEP]}. " <ref type="bibr">[SEP]</ref>" is also inserted between different sentences. We also insert "[SEP]" between regions that are from different clips, which can help the model to identify the clip boundaries. For each input step, the final embedding feature consists of four different embeddings. The embeddings are position embedding, segment embedding, token embedding, visual feature embedding. We added a few new tokens to distinguish action features and regional object features. The visual embedding is introduced to extract visual and action information. These embeddings are added to be the final feature of ActBERT. We explain them in detail as follows. Position embedding. Following <ref type="bibr" target="#b6">[7]</ref>, we incorporate a learnable position embedding to every input in the sequence. Since self-attention does not consider order information, position encoding offers a flexible way to embed a sequence when the sequence order matters. For the actions in different clips, the position embeddings are different as the video clips are ordered. For the regions extracted from the same frame, we use the same position embedding. To distinguish regions from the same frame, we consider spatial position embedding for different spatial positions. The details will be described in "Visual (action) embedding". Segment embedding. We consider multiple video clips for long-term video context modeling. Each video clip or video segment has a corresponding segment embedding. The elements, i.e., action inputs, regional object inputs, linguistic descriptions, have the same segment embedding in the same video clip. Token embedding. Each word is embedded with Word-Piece embeddings <ref type="bibr" target="#b43">[44]</ref> with a 30,000 vocabulary. In addition to the special tokens mentioned above ("[CLS]", "[MASK]", "[SEP]"), we introduce "[ACT]" and "[RE-GION]" to represent the action features and the region features extracted from video frames, respectively. Note that all action inputs have the identical token embedding, which reveals the modality of the inputs. Visual (action) embedding. We now explain the visual (action) embedding in details. We first illustrate the procedure to obtain the action embedding. For each video clip, we extract verbs from its corresponding descriptions. For simplicity, we remove clips that do not have any verbs. We then build a vocabulary from all the extracted verbs. After verb vocabulary construction, each video clip has one or multiple category labels. We train a 3D convolutional neural network on this constructed dataset. The inputs to the 3D network is a tensor that contains an additional temporal dimension. We leverage a softmax classifier on top of the convolutional neural network. For clips with multiple labels, we normalize the one-hot label with 1 -norm, where the scores for all labels are summed to be 1. After the model is trained, we extract the features after global average pooling as the action features. This feature can well represent the actions that occurred in the video clip. To obtain regional object features, we extract bounding boxes and the corresponding visual features from a pretrained object detection network. Similar to Lu et al. <ref type="bibr" target="#b21">[22]</ref>, we utilized pre-trained Faster R-CNN network <ref type="bibr" target="#b29">[30]</ref> to extract the categorical distribution under the COCO vocabulary <ref type="bibr" target="#b20">[21]</ref>. The image region features offer detailed visual information for visual and text relation modeling. For each region, the visual feature embeddings are the feature vectors before the output layer in the pre-trained network. Following <ref type="bibr" target="#b21">[22]</ref>, we incorporate spatial position embeddings to represent region locations with a 5-D vector. This vector consists of four box coordinates and the fraction of the region area. Specifically, we denote the vector</p><formula xml:id="formula_0">as ( x1 W , y1 H , x2 W , y2 H , (x2?x1) * (y2?y1) W * H ),</formula><p>where W is the frame width, H is the frame height, and (x 1 , y 1 ) and (x 2 , y 2 ) are the top-left and bottom-right coordinates, respectively.</p><p>This vector is then embedded to match the dimension of the visual feature. The final regional object feature is the summation of the spatial position embedding and the object detection feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Tangled Transformer</head><p>We design a TaNgled Transformer (TNT) to better encode three sources of information, i.e., action features, regional object features and linguistic features.</p><p>Instead of using only one transformer that treats the visual and text features equally, our tangled transformer consists of three transformers. The three transformers take three sources of features, respectively. To enhance the interactions between visual and linguistic features, we propose to inject visual information to the linguistic transformer and incorporate linguistic information to the visual transformers. With cross-modal interactions, the tangled transformer can dynamically select judicious cues for target prediction.</p><p>We denote the intermediate representa-</p><formula xml:id="formula_1">tions at transformer block l as h l = {(h l w0 , . . . , h l w N ), (h l a0 , . . . , h l a L ), (h l r0 , . . . , h l r M )}. For simplicity, we denote h l w = {h l w0 , . . . , h l w N }, h l a = {h l a0 , .</formula><p>. . , h l a L )}, and h l r = {h l r0 , . . . , h l r M )}, which are processed by w-transfomer, a-transformer, and r-transformer, respectively ( <ref type="figure" target="#fig_9">Figure 1</ref>). Besides the standard multi-head attention encoding features from the same modality, we leverage the other two multi-head attention blocks to enhance mutual interactions between the transformer blocks. Specifically, we utilize h l a to catalyze mutual interactions. We denote the multi-head attention as output = M ultihead(Q, K, V ), where Q is the query, K is the key, V is the value. The details of multi-head          attention can be found in <ref type="bibr" target="#b41">[42]</ref>. We use h l a as a query to attend judicious cues from h l w and h l r :</p><formula xml:id="formula_2">V T D K / 8 T K g k R a 7 Y Y l G Y S o I x m f 1 O B k J z h n J i C W V a 2 F s J G 1 F N G d q E S j Y E b / n l V d K 6 q H p u 1 b u 7 r N S v 8 z i K c A K n c A 4 e 1 K A O t 9 C A J j A Y w z O 8 w p u T O C / O u / O</formula><formula xml:id="formula_3">I 5 G v / z V G 8 Q s j b h C J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 1 L F Y 2 4 8 b P 5 u V N y Z p U B C W N t S y G Z q 7 8 n M h o Z M 4 k C 2 x l R H J l l b y b + 5 3 V T D K / 8 T K g k R a 7 Y Y l G Y S o I x m f 1 O B k J z h n J i C W V a 2 F s J G 1 F N G d q E S j Y E b / n l V d K 6 q H p u 1 b u 7 r N S v 8 z i K c A K n c A 4 e 1 K A O t 9 C A J j A Y w z O 8 w p u T O C / O u / O</formula><formula xml:id="formula_4">I 5 G v / z V G 8 Q s j b h C J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 1 L F Y 2 4 8 b P 5 u V N y Z p U B C W N t S y G Z q 7 8 n M h o Z M 4 k C 2 x l R H J l l b y b + 5 3 V T D K / 8 T K g k R a 7 Y Y l G Y S o I x m f 1 O B k J z h n J i C W V a 2 F s J G 1 F N G d q E S j Y E b / n l V d K 6 q H p u 1 b u 7 r N S v 8 z i K c A K n c A 4 e 1 K A O t 9 C A J j A Y w z O 8 w p u T O C / O u / O</formula><formula xml:id="formula_5">I 5 G v / z V G 8 Q s j b h C J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 1 L F Y 2 4 8 b P 5 u V N y Z p U B C W N t S y G Z q 7 8 n M h o Z M 4 k C 2 x l R H J l l b y b + 5 3 V T D K / 8 T K g k R a 7 Y Y l G Y S o I x m f 1 O B k J z h n J i C W V a 2 F s J G 1 F N G d q E S j Y E b / n l V d K 6 q H p u 1 b u 7 r N S v 8 z i K c A K n c A 4 e 1 K A O t 9 C A J j A Y w z O 8 w p u T O C / O u / O</formula><formula xml:id="formula_6">F l O B M 4 q / R T j Q l l E z r C n q W S R q j 9 b H 7 u j J x Z Z U j C W N m S h s z V 3 x M Z j b S e R o H t j K g Z 6 2 U v F / / z e q k J r / 2 M y y Q 1 K N l i U Z g K Y m K S / 0 6 G X C E z Y m o J Z Y r b W w k b U 0 W Z s Q l V b A j</formula><formula xml:id="formula_7">F l O B M 4 q / R T j Q l l E z r C n q W S R q j 9 b H 7 u j J x Z Z U j C W N m S h s z V 3 x M Z j b S e R o H t j K g Z 6 2 U v F / / z e q k J r / 2 M y y Q 1 K N l i U Z g K Y m K S / 0 6 G X C E z Y m o J Z Y r b W w k b U 0 W Z s Q l V b A j</formula><formula xml:id="formula_8">F l O B M 4 q / R T j Q l l E z r C n q W S R q j 9 b H 7 u j J x Z Z U j C W N m S h s z V 3 x M Z j b S e R o H t j K g Z 6 2 U v F / / z e q k J r / 2 M y y Q 1 K N l i U Z g K Y m K S / 0 6 G X C E z Y m o J Z Y r b W w k b U 0 W Z s Q l V b A j</formula><formula xml:id="formula_9">F l O B M 4 q / R T j Q l l E z r C n q W S R q j 9 b H 7 u j J x Z Z U j C W N m S h s z V 3 x M Z j b S e R o H t j K g Z 6 2 U v F / / z e q k J r / 2 M y y Q 1 K N l i U Z g K Y m K S / 0 6 G X C E z Y m o J Z Y r b W w k b U 0 W Z s Q l V b A j</formula><formula xml:id="formula_10">c w = M ultihead(W 1 q h l a , W w k h l w , W w v h l w ),<label>(1)</label></formula><formula xml:id="formula_11">c r = M ultihead(W 2 q h l a , W r k h l r , W r v h l r ),<label>(2)</label></formula><p>where W * * are learnable weights. c w is the blended feature from linguistic representations, while c r is the guided feature from regional object representation. We then generate a new key-value pair from c w using a linear layer. This generated key-value pair is stacked with the key-value pairs from the original a-transformer and r-transformer. Similarly, we generate a new key-value pair from c r , which is stacked with key-value pair in w-transformer. With this form tangled transformer, visual and linguistic features are further associated.</p><p>Note that our tangled transformer is different from the co-attentional transformer block in <ref type="bibr" target="#b21">[22]</ref> in several ways. First, the co-attentional transformer block simply passes the keys and values from one modality to the other modality's attention block, without further pre-processing. Second, <ref type="bibr" target="#b21">[22]</ref> treats the two modalities equally, while our tangled block utilizes a global cue to guide the selection of local hints from linguistic and visual features. Third, the keys and values from different modalities replace the origin keyvalues in <ref type="bibr" target="#b21">[22]</ref>, while our tangled transformer stacks the keyvalue with the original one. In this way, both the linguistic and visual features are incorporated during transformer encoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">ActBERT Training</head><p>We introduce four tasks for ActBERT pre-training. Our framework is presented in <ref type="figure">Figure 2</ref>. We naturally extend [ACT] <ref type="bibr">[REGION]</ref> [SEP] ? <ref type="figure">Figure 2</ref>: Our ActBERT framework. We incorporate three sources of information during pre-training, i.e., global actions, local regional objects, and text descriptions. The yellow grid indicates that the action or the region object is masked out.</p><p>the Masked Language Modeling in our cross-modal setting. There are some existing extensions for image and language pre-training <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>, and video and language pretraining <ref type="bibr" target="#b34">[35]</ref>. Compared to <ref type="bibr" target="#b34">[35]</ref>, we explicitly model actions and regional information in a unified framework. Masked Language Modeling with Global and Local Visual Cues. We extend the Masked Language Modeling (MLM) task in BERT to our setting. We leverage visual cues from local regional objects and global actions to uncover the relationships between visual and linguistic entities. As described in Section 3.1, each word in the input sentence is randomly masked with a fixed probability. The task forces the model to learn from contextual descriptions, and at the same time, extract relevant visual features to facilitate prediction. When a verb word is masked out, the model should exploit the action features for a more accurate prediction. When a description of an object is masked out, local regional features can provide more contextual information. Thus, the strong model needs to align visual and linguistic inputs locally and globally. The output feature is then appended with a softmax classifier over the whole linguistic vocabulary. Masked Action Classification. Similarly, in Masked Action Classification, the action features are masked out. The task is to predict the masked action label based on linguistic features and object features. Explicit action prediction can be beneficial in two perspectives. First, action sequential cues can be exploited in the long-term. For example, for a video with action sequences of "get into", "rotate", "add", this task can better exploit the temporal order information regarding performing this instructional assignment. Second, the regional objects and linguistic texts are leveraged for better cross-modality modeling. Note that in Masked Action Classification, the goal is to predict the categorical label of the masked-out action feature. This task can enhance the action recognition capability of the pre-trained model, which can be further generalized to many downstream tasks, e.g., video question answering. Masked Object Classification. In Masked Object Classification, the regional object features are randomly masked out. We follow <ref type="bibr" target="#b21">[22]</ref> to predict a distribution over fixed vocabulary for the masked-out image region. The target distribution of the masked-out region is calculated as the softmax activation that is extracted by forwarding the region to the same pre-trained detection model in the feature extraction stage. The KL divergence between the two distributions is minimized.</p><p>Cross-modal matching. Similar to the Next Sentence Pre-diction (NSP) task, we apply a linear layer on top of the output of the first token "[CLS]". It is followed by a sigmoid classifier, indicating the relevance score of the linguistic sentences and the visual features. If the score is high, it shows that the text well-describes the video clips. The model is optimized via a binary cross-entropy loss. To train this cross-modal matching task, we sample negative videotext pairs from the unlabeled dataset. We follow <ref type="bibr" target="#b26">[27]</ref> for sampling positive pairs and negative pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate ActBERT in multiple downstream video-and-language tasks. We quantitatively evaluate the generalization capability of ActBERT on five challenging tasks, i.e., text-video clip retrieval, video captioning, video question answering, action segmentation, and action step localization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ActBERT implementation details</head><p>HowTo100M. We pre-train ActBERT on the HowTo100M dataset <ref type="bibr" target="#b26">[27]</ref>. The HowTo100M dataset is constructed by querying YouTube API. The top 200 search results are kept. This dataset covers a total of 23,611 tasks, e.g., maintenance and repair, animal rescue, food preparation. This dataset is biased towards actions, where the verbs like "go", "make", "come" being the most frequent. The nouns are also distributed in a long-tailed way, where objects like "water", "cup" are ranked top. Each video has a corresponding narration that is extracted from video subtitles. As the association between video clips and texts are not manually annotated, the video-text connection can sometimes be weak. There are cases of noisy correspondences, where the actors sometimes talk about unrelated things. Though noisy, we found pre-training on HowTo100M can still significantly improve the performance of downstream tasks. Pre-training details. To construct video-text inputs for ActBERT pre-training, we sample video clips from the HowTo100M dataset. Instead of only using one clip for video-text joint training, we leverage multiple adjacent clips to cover a longer context. This enables ActBERT to model relations in different segments. We sample 10 adjacent video clips, and the temporal-aligned linguistic tokens are extracted to form a video-text pair.</p><p>To obtain the local regional features, we use Faster R-CNN pre-trained on the Visual Genome <ref type="bibr" target="#b16">[17]</ref> dataset following <ref type="bibr" target="#b21">[22]</ref>. The backbone is ResNet-101 <ref type="bibr" target="#b8">[9]</ref>. We use the frame rate of 1 FPS to extract the regional features. Each region feature is RoI-pooled from the convolutional feature from that region. We set the detection confidence threshold as 0.4, and each frame contains at most five boxes. Transformer and co-attentional transformer blocks in the visual stream have hidden state size of 1024 and 8 attention heads.</p><p>To obtain the action features, we first construct an action classification dataset. We sample frames at 8 FPS. For each clip, we extract the verb from its text descriptions. Then, we train a ResNet-3D <ref type="bibr" target="#b40">[41]</ref> network with a softmax classification loss. We initialized the weights of the ResNet-3D model from a pre-trained model on Kinetics <ref type="bibr" target="#b11">[12]</ref>. The Kinetics dataset covers 400 actions from YouTube videos. The 3D convolutional network converges faster using when it is pre-trained on Kinetics. The input clip length to ResNet-3D is 32. The clip covers a 4-second video duration. The spatial shape of the input frame is 224?224. The initial learning rate is set to 0.001. The batch size is 16. We decay the learning rate by 0.1 at iteration 100,000, and the total number of training iterations is 1,000,000. We keep other training settings unchanged following <ref type="bibr" target="#b40">[41]</ref>. During feature extraction, we sample the central clip, and each frame is central cropped. We use the feature after global average pooling as the clip representation.</p><p>During ActBERT pre-training, 15% of input features are randomly masked out. ActBERT has 12 layers of transformer blocks. Each transformer block has a hidden unit size of 768. We initialize the linguistic transformer with the BERT model pre-trained on the BookCorpus <ref type="bibr" target="#b57">[58]</ref> and English Wikipedia. The other two transformers are randomly initialized. The network is optimized by Adam optimizer. We set the learning rate to be 10 ?5 . We trained the model for five epochs due to the large-scale data.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Video captioning</head><p>We compare our ActBERT to VideoBERT [35] on the video captioning task. We take the pre-trained action transformer as the video encoder. We follow the setup from <ref type="bibr" target="#b53">[54]</ref> that takes the video clips from YouCook2 <ref type="bibr" target="#b52">[53]</ref> as input, and a transformer decoder is used to decode videos to captions. We do not use the regional object transformer to fairly compare to <ref type="bibr" target="#b34">[35]</ref>. Similar to <ref type="bibr" target="#b34">[35]</ref>, we cross-validate the hyperparameters on the training set. We report the standard evaluation metrics for captioning, i.e., BLEU, METEOR, and ROUGE, on the validation set. The model is optimized by Adam optimizer for 40k iterations. We set the initial learning rate to 1.0 ? 10 ?3 , and the batch size is 128. The results are shown in <ref type="table" target="#tab_2">Table 1</ref>. We outperform VideoBERT <ref type="bibr" target="#b34">[35]</ref>   <ref type="table">Table 2</ref>: Action segmentation results on COIN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Action segmentation</head><p>The action segmentation task in COIN is to design an action label for a video at the frame-level. To apply ActBERT to action segmentation, we fine-tune ActBERT by adding a linear classifier upon the output features for dense frame labeling. We do not feed the text descriptions during the fine-tuning process. The results are shown in <ref type="table">Table 2</ref>. The baseline methods are conducted by <ref type="bibr" target="#b36">[37]</ref>. Notably, ActBERT significantly outperforms the baselines with more than 20% improvements. It shows that the pre-trained ActBERT can deal with only visual inputs when linguistic descriptions are absent. When we remove the regional information, we observe a performance drop compared to our full model. It shows that detailed local cues are important to the dense frame labeling task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Action step localization</head><p>We evaluate action step localization on CrossTask. To fairly compare to <ref type="bibr" target="#b26">[27]</ref>, we do not fine-tune on the target dataset. We regard the step action label as the text description and directly feed the text-video pair to ActBERT. We regard the prediction for the first token "[CLS]" as the relevance score of this clip belonging to the label. We choose the action with the max relevance score as the final prediction. The results are shown in <ref type="table" target="#tab_5">Table 3</ref>. ActBERT significantly outperforms TVJE <ref type="bibr" target="#b26">[27]</ref> with a large margin, i.e., the average improvement is 7%. We achieve even better than the supervised baseline. We remove the region cues to have a fair comparison to <ref type="bibr" target="#b26">[27]</ref>, as <ref type="bibr" target="#b26">[27]</ref> does not use object detection features for video and text matching. The results of "Act-BERT w/o region cues" also substantially outperform <ref type="bibr" target="#b26">[27]</ref>, demonstrating the effectiveness of ActBERT pre-training. Our full ActBERT model further improves performance by 4%. This validates that regional information is an important source that provides detailed local object features for text-and-video matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.5">Text-video clip retrieval</head><p>We evaluate ActBERT on the task of video clip retrieval with natural language queries. Given a linguistic query, it aims to rank the video clips from a gallery video set. We   <ref type="table">Table 4</ref>: Text-video clip retrieval results on YouCook2 and MSR-VTT. "FT" denotes fine-tuning on the training set.</p><p>use the following metrics for evaluation <ref type="bibr" target="#b26">[27]</ref>, i.e., Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10) and the median rank (Median R). We evaluate ActBERT on YouCook2 and MSR-VTT. We followed <ref type="bibr" target="#b26">[27]</ref> to conduct the YouCook2 evaluation. The results are shown in <ref type="table">Table 4</ref>. ActBERT significantly outperforms TVJE <ref type="bibr" target="#b26">[27]</ref> and other baselines. TVJE trains a ranking loss on the HowTo100M dataset. It shows ActBERT is a better pre-training framework for video-text joint representation learning. Notably, our pretrained model achieves better retrieval performance than the finetuned TVJE model ("TVJE +FT") on YouCook2. It shows the superiority of ActBERT in self-supervised videotext representation learning. In MSR-VTT, ActBERT outperforms TVJE by 1.1% on R@1 when no labeled data is accessed. Note that JSFusion <ref type="bibr" target="#b48">[49]</ref> is a supervised method that leverages labeled video and text pairs for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.6">Video question answering.</head><p>We evaluate ActBERT on VideoQA tasks. For multi-choice VideoQA, we fine-tune the pre-trained ActBERT on the Method Accuracy LSTM-fusion <ref type="bibr" target="#b48">[49]</ref> 38.3 C+LSTM+SA-FC7 <ref type="bibr" target="#b38">[39]</ref> 60.2 VSE-LSTM <ref type="bibr" target="#b13">[14]</ref> 67.3 SNUVL <ref type="bibr" target="#b49">[50]</ref> 65.4 EITanque <ref type="bibr" target="#b10">[11]</ref> 65.5 CT-SAN <ref type="bibr" target="#b50">[51]</ref> 66.4 MLB <ref type="bibr" target="#b12">[13]</ref> 76.1 JSFusion <ref type="bibr" target="#b48">[49]</ref> 83.4</p><p>ActBERT 85.7 <ref type="table">Table 5</ref>: Video question answering (multiple-choices) results on MSR-VTT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Text-only BLSTM <ref type="bibr" target="#b22">[23]</ref> 32.0 Text-only Human <ref type="bibr" target="#b22">[23]</ref> 30.2 GoogleNet-2D + C3D <ref type="bibr" target="#b22">[23]</ref> 35.7 Merging-LSTM <ref type="bibr" target="#b23">[24]</ref> 34.2 SNUVL <ref type="bibr" target="#b49">[50]</ref> 38.0 CT-SAN <ref type="bibr" target="#b50">[51]</ref> 41.9 LR/RL LSTMs <ref type="bibr" target="#b24">[25]</ref> 40.9 JSFusion <ref type="bibr" target="#b48">[49]</ref> 45.5</p><p>ActBERT 48.6 <ref type="table">Table 6</ref>: Video question answering (fill-in-the-blank) results on LMSDC.</p><p>MSR-VTT training set. The video-text pairs are fed to Act-BERT. We use a linear classifier upon the output feature. We use a small learning rate of 0.0001 and use Adam optimizer for training. At the inference time, we fed each candidate with the video clip to ActBERT. The final choice is made by selecting the candidates with the max matching score. The results are shown in <ref type="table">Table 5</ref>. We compare to many baselines in this task. Without fancy joint modeling, Act-BERT significantly outperforms JSFusion <ref type="bibr" target="#b48">[49]</ref> by 2.3%. It shows ActBERT's strong generalization from a large-scale dataset. We additionally evaluate on another VideoQA task on LSMDC, i.e., fill-in-the-blank VideoQA. We report the prediction accuracy on the public test set and the results are shown in <ref type="table">Table 6</ref>. It shows ActBERT is capable of learning generalizable features that it achieves considerable gains when the target video domains are movies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce ActBERT for joint video-text modeling in a self-supervised way. We directly model both global and local visual cues for fine-grained visual and linguistic relation learning. ActBERT takes three sources of information as input, i.e., global actions, local regional objects, and linguistic descriptions. The novel tangled transformer further enhances the communications between the three sources. Quantitative results on five video-text benchmarks demonstrate the effectiveness of ActBERT. In the future, we will consider evaluating ActBERT on video action recognition and detection. We will also improve Act-BERT by designing more powerful modules for video and text modeling.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>t e x i t s h a 1 _ b a s e 6 4 = " L 0 e s R c J f V a f G d m b p G r k V J l z J A F Y = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V 7 A e 0 s W y 2 m 3 b p Z h N 2 J 0 o J / R F e P C j i 1 d / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G N z O / / c i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l 9 u h B 9 r O n a b 9 c c a v u H G S V e D m p Q I 5 G v / z V G 8 Q s j b h C J q k x X c 9 N 0 M + o R s E k n 5 Z 6 q e E J Z W M 6 5 F 1 L F Y 2 4 8 b P 5 u V N y Z p U B C W N t S y G Z q 7 8 n M h o Z M 4 k C 2 x l R H J l l b y b + 5 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>x a C 0 4 + c w x / I H z + Q O i m o / A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L 0 e s R c J f V a f G d m b p G r k V J l z J A F Y = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V 7 A e 0 s W y 2 m 3 b p Z h N 2 J 0 o J / R F e P C j i 1 d / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G N z O / / c i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l 9 u h B 9 r O n a b 9 c c a v u H G S V e D m p Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>x a C 0 4 + c w x / I H z + Q O i m o / A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L 0 e s R c J f V a f G d m b p G r k V J l z J A F Y = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V 7 A e 0 s W y 2 m 3 b p Z h N 2 J 0 o J / R F e P C j i 1 d / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G N z O / / c i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l 9 u h B 9 r O n a b 9 c c a v u H G S V e D m p Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>x a C 0 4 + c w x / I H z + Q O i m o / A &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L 0 e s R c J f V a f G d m b p G r k V J l z J A F Y = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 V 7 A e 0 s W y 2 m 3 b p Z h N 2 J 0 o J / R F e P C j i 1 d / j z X / j t s 1 B W x 8 M P N 6 b Y W Z e k E h h 0 H W / n c L a + s b m V n G 7 t L O 7 t 3 9 Q P j x q m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G N z O / / c i 1 E b G 6 x 0 n C / Y g O l Q g F o 2 i l 9 u h B 9 r O n a b 9 c c a v u H G S V e D m p Q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>x a C 0 4 + c w x / I H z + Q O i m o / A &lt; / l a t e x i t &gt; h l a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 4 V w G 3 u Z o 2 A a E c Z n E M S O + R X z R J g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W C b b T b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V l L V o L G L V D V A z w S V r G W 4 E 6 y a K Y R Q I 1 g k m t 7 n f e W J K 8 1 g + m G n C / A h H k o e c o r F S Z / w o B h n O B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M K Z p x K S h A r X u e W 5 i / A y V 4 V S w W a W f a p Y g n e C I 9 S y V G D H t Z / N z Z + T M K k M S x s q W N G S u / p 7 I M N J 6 G g W 2 M 0 I z 1 s t e L v 7 n 9 V I T X v s Z l 0 l q m K S L R W E q i I l J / j s Z c s W o E V N L k C p u b y V 0 j A q p s Q l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a Q G E C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 4 E s j 6 o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 4 V w G 3 u Z o 2 A a E c Z n E M S O + R X z R J g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W C b b T b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V l L V o L G L V D V A z w S V r G W 4 E 6 y a K Y R Q I 1 g k m t 7 n f e W J K 8 1 g + m G n C / A h H k o e c o r F S Z / w o B h n O B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M K Z p x K S h A r X u e W 5 i / A y V 4 V S w W a W f a p Y g n e C I 9 S y V G D H t Z / N z Z + T M K k M S x s q W N G S u / p 7 I M N J 6 G g W 2 M 0 I z 1 s t e L v 7 n 9 V I T X v s Z l 0 l q m K S L R W E q i I l J / j s Z c s W o E V N L k C p u b y V 0 j A q p s Q l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a Q G E C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 4 E s j 6 o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 4 V w G 3 u Z o 2 A a E c Z n E M S O + R X z R J g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W C b b T b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V l L V o L G L V D V A z w S V r G W 4 E 6 y a K Y R Q I 1 g k m t 7 n f e W J K 8 1 g + m G n C / A h H k o e c o r F S Z / w o B h n O B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M K Z p x K S h A r X u e W 5 i / A y V 4 V S w W a W f a p Y g n e C I 9 S y V G D H t Z / N z Z + T M K k M S x s q W N G S u / p 7 I M N J 6 G g W 2 M 0 I z 1 s t e L v 7 n 9 V I T X v s Z l 0 l q m K S L R W E q i I l J / j s Z c s W o E V N L k C p u b y V 0 j A q p s Q l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a Q G E C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 4 E s j 6 o = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K 4 V w G 3 u Z o 2 A a E c Z n E M S O + R X z R J g = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W C b b T b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V l L V o L G L V D V A z w S V r G W 4 E 6 y a K Y R Q I 1 g k m t 7 n f e W J K 8 1 g + m G n C / A h H k o e c o r F S Z / w o B h n O B t W a W 3 f n I K v E K 0 g N C j Q H 1 a / + M K Z p x K S h A r X u e W 5 i / A y V 4 V S w W a W f a p Y g n e C I 9 S y V G D H t Z / N z Z + T M K k M S x s q W N G S u / p 7 I M N J 6 G g W 2 M 0 I z 1 s t e L v 7 n 9 V I T X v s Z l 0 l q m K S L R W E q i I l J / j s Z c s W o E V N L k C p u b y V 0 j A q p s Q l V b A j e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a Q G E C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 4 E s j 6 o = &lt; / l a t e x i t &gt; h l r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M P w Y p J M Y 3 u F e 4 3 k s Y Z J / s x W q Q G M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i t 1 x o 9 i k K n Z o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f x i y N U B o m q N Y 9 z 0 2 M n 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 5 s B j 7 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M P w Y p J M Y 3 u F e 4 3 k s Y Z J / s x W q Q G M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i t 1 x o 9 i k K n Z o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f x i y N U B o m q N Y 9 z 0 2 M n 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 5 s B j 7 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M P w Y p J M Y 3 u F e 4 3 k s Y Z J / s x W q Q G M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i t 1 x o 9 i k K n Z o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f x i y N U B o m q N Y 9 z 0 2 M n 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 5 s B j 7 s = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M P w Y p J M Y 3 u F e 4 3 k s Y Z J / s x W q Q G M = " &gt; A A A B 7 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c K 9 g P a W D b b S b t 0 s w m 7 G 6 G E / g g v H h T x 6 u / x 5 r 9 x 0 + a g r Q 8 G H u / N M D M v S A T X x n W / n d L a + s b m V n m 7 s r O 7 t 3 9 Q P T x q 6 z h V D F s s F r H q B l S j 4 B J b h h u B 3 U Q h j Q K B n W B y m / u d J 1 S a x / L B T B P 0 I z q S P O S M G i t 1 x o 9 i k K n Z o F p z 6 + 4 c Z J V 4 B a l B g e a g + t U f x i y N U B o m q N Y 9 z 0 2 M n 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>e 8 s u r p H 1 R 9 9 y 6 d 3 9 Z a 9 w U c Z T h B E 7 h H D y 4 g g b c Q R N a w G A C z / A K b 0 7 i v D j v z s e i t e Q U M 8 f w B 8 7 n D 5 s B j 7 s = &lt; / l a t e x i t &gt; h l+1 r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 9 l V A p z k 9 t Y W l a W V Y q q N G O B + J / 8 = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 0 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H z 5 p k T c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 9 l V A p z k 9 t Y W l a W V Y q q N G O B + J / 8 = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 0 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H z 5 p k T c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 9 l V A p z k 9 t Y W l a W V Y q q N G O B + J / 8 = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 0 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H z 5 p k T c = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " s 9 l V A p z k 9 t Y W l a W V Y q q N G O B + J / 8 = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 0 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H z 5 p k T c = &lt; / l a t e x i t &gt; h l+1 a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i g + i M A c m I x W m d m c / q N A O E y l x E 0 g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 o 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H y S U k S Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i g + i M A c m I x W m d m c / q N A O E y l x E 0 g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 o 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f Wv P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H y S U k S Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i g + i M A c m I x W m d m c / q N A O E y l x E 0 g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 o 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p J B i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j v z s e 8 d c U p Z o 7 g D 5 z P H y S U k S Y = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " i g + i M A c m I x W m d m c / q N A O E y l x E 0 g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 4 W e t X 1 a O X x S I I Q k l E 0 G P R i 8 c K 9 g P a W D b b T b t 0 s w m 7 E 6 G E / A w v H h T x 6 q / x 5 r 9 x 2 + a g r Q 8 G H u / N M D M v S K Q w 6 L r f z s r q 2 v r G Z m m r v L 2 z u 7 d f O T h s m T j V j D d Z L G P d C a j h U i j e R I G S d x L N a R R I 3 g 7 G t 1 O / / c S 1 E b F 6 w E n C / Y g O l Q g F o 2 i l 7 u g x k + d e 3 s 9 o 3 q 9 U 3 Z o 7 A 1 k m X k G q U K D R r 3 z 1 B j F L I 6 6 Q S W p M 1 3 M T 9 D O q U T D J 8 3 I v N T y h b E y H v G u p o h E 3 f j Y 7 O S e n V h m Q M N a 2 F J K Z + n s i o 5 E x k y i w n R H F k V n 0 p u J / X j f F 8 N r P h E p S 5 I r N F 4 W p JB i T 6 f 9 k I D R n K C e W U K a F v Z W w E d W U o U 2 p b E P w F l 9 e J q 2 L m u f W v P v L a v 2 m i K M E x 3 A C Z + D B F d T h D h r Q B A Y x P M M r v D n o v D j vz s e 8 d c U p Z o 7 g D 5 z P H y S U k S Y = &lt; / l a t e x i t &gt; h l+1 w &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 9 t q 2 E D k V G f D n 8 R L Y a R X X F h + J g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j x W s B + Q x r L Z b t q l m 9 2 w u 1 F K y M / w 4 k E R r / 4 a b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p n 6 n U e q N J P i 3 k w S G s R 4 K F j E C D Z W 8 k c P G T / z 8 n 7 2 l P e r N b f u z o C W i V e Q G h R o 9 q t f v Y E k a U y F I R x r 7 X t u Y o I M K 8 M I p 3 m l l 2 q a Y D L G Q + p b K n B M d Z D N T s 7 R i V U G K J L K l j B o p v 6 e y H C s 9 S Q O b W e M z U g v e l P x P 8 9 P T X Q V Z E w k q a G C z B d F K U d G o u n / a M A U J Y Z P L M F E M X s r I i O s M D E 2 p Y o N w V t 8 e Z m 0 z + u e W / f u L m q N 6 y K O M h z B M Z y C B 5 f Q g F t o Q g s I S H i G V 3 h z j P P i v D s f 8 9 a S U 8 w c w h 8 4 n z 9 G A p E 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 9 t q 2 E D k V G f D n 8 R L Y a R X X F h + J g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j x W s B + Q x r L Z b t q l m 9 2 w u 1 F K y M / w 4 k E R r / 4 a b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p n 6 n U e q N J P i 3 k w S G s R 4 K F j E C D Z W 8 k c P G T / z 8 n 7 2 l P e r N b f u z o C W i V e Q G h R o 9 q t f v Y E k a U y F I R x r 7 X t u Y o I M K 8 M I p 3 m l l 2 q a Y D L G Q + p b K n B M d Z D N T s 7 R i V U G K J L K l j B o p v 6 e y H C s 9 S Q O b W e M z U g v e l P x P 8 9 P T X Q V Z E w k q a G C z B d F K U d G o u n / a M A U J Y Z P L M F E M X s r I i O s M D E 2 p Y o N w V t 8 e Z m 0 z + u e W / f u L m q N 6 y K O M h z B M Z y C B 5 f Q g F t o Q g s I S H i G V 3 h z j P P i v D s f 8 9 a S U 8 w c w h 8 4 n z 9 G A p E 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 9 t q 2 E D k V G f D n 8 R L Y a R X X F h + J g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j x W s B + Q x r L Z b t q l m 9 2 w u 1 F K y M / w 4 k E R r / 4 a b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p n 6 n U e q N J P i 3 k w S G s R 4 K F j E C D Z W 8 k c P G T / z 8 n 7 2 l P e r N b f u z o C W i V e Q G h R o 9 q t f v Y E k a U y F I R x r 7 X t u Y o I M K 8 M I p 3 m l l 2 q a Y D L G Q + p b K n B M d Z D N T s 7 R i V U G K J L K l j B o p v 6 e y H C s 9 S Q O b W e M z U g v e l P x P 8 9 P T X Q V Z E w k q a G C z B d F K U d G o u n / a M A U J Y Z P L M F E M X s r I i O s M D E 2 p Y o N w V t 8 e Z m 0 z + u e W / f u L m q N 6 y K O M h z B M Z y C B 5 f Q g F t o Q g s I S H i G V 3 h z j P P i v D s f 8 9 a S U 8 w c w h 8 4 n z 9 G A p E 8 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 4 9 t q 2 E D k V G f D n 8 R L Y a R X X F h + J g = " &gt; A A A B 8 n i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B Z B E E o i g h 6 L X j x W s B + Q x r L Z b t q l m 9 2 w u 1 F K y M / w 4 k E R r / 4 a b / 4 b t 2 0 O 2 v p g 4 P H e D D P z w o Q z b V z 3 2 y m t r K 6 t b 5 Q 3 K 1 v b O 7 t 7 1 f 2 D t p a p I r R F J J e q G 2 J N O R O 0 Z Z j h t J s o i u O Q 0 0 4 4 v p n 6 n U e q N J P i 3 k w S G s R 4 K F j E C D Z W 8 k c P G T / z 8 n 7 2 l P e r N b f u z o C W i V e Q G h R o 9 q t f v Y E k a U y F I R x r 7 X t u Y o I M K 8 M I p 3 m l l 2 q a Y D L G Q + p b K n B M d Z D N T s 7 R i V U G K J L K l j B o p v 6 e y H C s 9 S Q O b W e M z U g v e l P x P 8 9 P T X Q V Z E w k q a G C z B d F K U d G o u n / a M A U J Y Z P L M F E M X s r I i O s M D E 2 p Y o N w V t 8 e Z m 0 z + u e W / f u L m q N 6 y K O M h z B M Z y C B 5 f Q g F t o Q g s I S H i G V 3 h z j P P i v D s f 8 9 a S U 8 w c w h 8 4 n z 9 G A p E 8 &lt; / l a t e x i t &gt; -transformer -transformer w &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 8 o 8 k 6 X 9 N K 9 G x C P F w p L s h v p 7 c 5 Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 o J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 L W M + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 8 o 8 k 6 X 9 N K 9 G x C P F w p L s h v p 7 c 5 Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 o J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 L W M + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 8 o 8 k 6 X 9 N K 9 G x C P F w p L s h v p 7 c 5 Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 o J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 L W M + w = = &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 8 o 8 k 6 X 9 N K 9 G x C P F w p L s h v p 7 c 5 Y = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E 0 G P R i 8 c W 7 A e 0 o W y 2 k 3 b t Z h N 2 N 0 o J / Q V e P C j i 1 Z / k z X / j t s 1 B W x 8 M P N 6 b Y W Z e k A i u j e t + O 4 W 1 9 Y 3 N r e J 2 a W d 3 b / + g f H j U 0 n G q G D Z Z L G L V C a h G w S U 2 D T c C O 4 l C G g U C 2 8 H 4 d u a 3 H 1 F p H s t 7 M 0 n Q j + h Q 8 p A z a q z U e O q X K 2 7 V n Y O s E i 8 n F c h R 7 5 e / e o O Y p R F K w w T V u u u 5 i f E z q g x n A q e l X q o x o W x M h 9 i 1 V N I I t Z / N D 5 2 S M 6 s M S B g r W 9 K Q u f p 7 I q O R 1 p M o s J 0 R N S O 9 7 M 3 E / 7 x u a s J r P + M y S Q 1 K t l g U p o K Y m M y + J g O u k B k x s Y Q y x e 2 t h I 2 o o s z Y b E o 2 B G / 5 5 V X S u q h 6 b t V r X F Z q N 3 k c R T i B U z g H D 6 6 g B n d Q h y Y w Q H i G V 3 h z H p w X 5 9 3 5 W L Q W n H z m G P 7 A + f w B 5 L W M + w = = &lt; / l a t e x i t &gt; a &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 7 / v C s 5 z e 5 K t V d 6 6 W 3 y y A L Y B f b k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p S Q f l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / D X Y z l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 7 / v C s 5 z e 5 K t V d 6 6 W 3 y y A L Y B f b k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p S Q f l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / D X Y z l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 7 / v C s 5 z e 5 K t V d 6 6 W 3 y y A L Y B f b k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p S Q f l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / D X Y z l &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b 7 / v C s 5 z e 5 K t V d 6 6 W 3 y y A L Y B f b k = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p S Q f l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / D X Y z l &lt; / l a t e x i t &gt; r &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D + v I j Y I Y i u Y B q f G N J B m X Y b U Z J b 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p q Q b l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / d I Y z 2 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D + v I j Y I Y i u Y B q f G N J B m X Y b U Z J b 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p q Q b l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / d I Y z 2 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D + v I j Y I Y i u Y B q f G N J B m X Y b U Z J b 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p q Q b l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / d I Y z 2 &lt; / l a t e x i t &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " D + v I j Y I Y i u Y B q f G N J B m X Y b U Z J b 0 = " &gt; A A A B 6 H i c b V B N S 8 N A E J 3 U r 1 q / q h 6 9 L B b B U 0 l E q M e i F 4 8 t 2 A 9 o Q 9 l s J + 3 a z S b s b o Q S + g u 8 e F D E q z / J m / / G b Z u D t j 4 Y e L w 3 w 8 y 8 I B F c G 9 f 9 d g o b m 1 v b O 8 X d 0 t 7 + w e F R + f i k r e N U M W y x W M S q G 1 C N g k t s G W 4 E d h O F N A o E d o L J 3 d z v P K H S P J Y P Z p q g H 9 G R 5 C F n 1 F i p q Q b l i l t 1 F y D r x M t J B X I 0 B u W v / j B m a Y T S M E G 1 7 n l u Y v y M K s O Z w F m p n 2 p M K J v Q E f Y s l T R C 7 W e L Q 2 f k w i p D E s b K l j R k o f 6 e y G i k 9 T Q K b G d E z V i v e n P x P 6 + X m v D G z 7 h M U o O S L R e F q S A m J v O v y Z A r Z E Z M L a F M c X s r Y W O q K D M 2 m 5 I N w V t 9 e Z 2 0 r 6 q e W / W a 1 5 X 6 b R 5 H E c 7 g H C 7 B g x r U 4 R 4 a 0 A I G C M / w C m / O o / P i v D s f y 9 a C k 8 + c w h 8 4 n z / d I Y z 2 &lt; / l a t e x i t &gt; -transformer</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 1 :</head><label>1</label><figDesc>Our tangled transformer takes three sources of information as inputs, which enhances the interactions between linguistic features and visual features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Video captioning results on YouCook2. We outperform VideoBERT<ref type="bibr" target="#b34">[35]</ref> across all the metrics. For text-video clip retrieval, following<ref type="bibr" target="#b48">[49]</ref>, we use 1,000 pairs text-video for evaluation. LSMDC: We evaluate fill-in-the-blank video question answering on LSMDC<ref type="bibr" target="#b31">[32]</ref>. This task is to predict a single answer given a video clip and a sentence with a blank in it. In LSMDC fill-inthe-blank, there are 296,960 training question-answer pairs, 21,689 validation pairs, and 30,349 testing pairs on public test set. The accuracy is reported on the public test set.</figDesc><table><row><cell>In YouCook2, there are 89 types of recipes and totally 14k</cell></row><row><cell>clips described with linguistic texts. Following [27], we</cell></row><row><cell>evaluate the text-video clip retrieval task on the validation</cell></row><row><cell>clips of YouCook2. MSR-VTT: We evaluate text-video</cell></row><row><cell>clip retrieval and video question answering on MSR-VTT.</cell></row><row><cell>The MSR-VTT dataset [46] is a general video dataset col-</cell></row><row><cell>lected from YouTube with text descriptions. For the video</cell></row><row><cell>question answering task, we evaluate the multiple-choice</cell></row><row><cell>VideoQA following [49]. There are 2,990 questions in total</cell></row><row><cell>for testing. Each test video is associated with a ground-truth</cell></row><row><cell>caption, a correct answer, and four mismatched descrip-</cell></row><row><cell>tions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>across all metrics, achieving a 1.36 improvement on METEOR. It demonstrates that our pre-trained transformer learns a better video representation. It also indicates the effectiveness of ActBERT in modeling video sequences by considering both global and local video cues. Our transformer generalizes better in video captioning.</figDesc><table><row><cell>Method</cell><cell>Frame Accuracy (%)</cell></row><row><cell>NN-Viterbi [31]</cell><cell>21.17</cell></row><row><cell>VGG [33]</cell><cell>25.79</cell></row><row><cell>TCFPN-ISBA [8]</cell><cell>34.30</cell></row><row><cell>ActBERT w/o region cues</cell><cell>52.10</cell></row><row><cell>ActBERT</cell><cell>56.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>10.6 7.5 14.2 9.3 11.8 17.3 13.1 6.4 12.9 27.2 9.2 15.7 8.6 16.3 13.0 23.2 7.4 13.3 Zhukov et al. [59] 13.3 18.0 23.4 23.1 16.9 16.5 30.7 21.6 4.6 19.5 35.3 10.0 32.3 13.8 29.5 37.6 43.0 13.3 22.4 Supervised [59] 19.1 25.3 38.0 37.5 25.7 28.2 54.3 25.8 18.3 31.2 47.7 12.0 39.5 23.4 30.9 41.1 53.4 17.3 31.6 TVJE [27] 33.5 27.1 36.6 37.9 24.1 35.6 32.7 35.1 30.7 28.5 43.2 19.8 34.7 33.6 40.4 41.6 41.9 27.4 33.6 ActBERT w/o region cues 37.4 29.5 39.0 42.2 29.8 37.5 35.5 37.8 33.2 32.8 48.4 25.2 37.4 35.6 42.4 47.0 46.1 30.4 37.1 ActBERT 41.8 33.6 42.7 46.8 33.4 43.0 40.8 41.8 38.3 37.4 52.5 30.1 41.2 40.4 46.1 51.0 49.7 35.1 41.4</figDesc><table><row><cell></cell><cell>Make</cell><cell>Kimchi Rice</cell><cell>Pickle</cell><cell>Cucumber</cell><cell>Make Banana</cell><cell>Ice Cream</cell><cell>Grill</cell><cell>Steak</cell><cell>Jack Up</cell><cell>Car</cell><cell>Make</cell><cell>Jello Shots</cell><cell>Change</cell><cell>Tire</cell><cell>Make</cell><cell>Lemonade</cell><cell>Add Oil</cell><cell>to Car</cell><cell>Make</cell><cell>Latte</cell><cell>Build</cell><cell>Shelves</cell><cell>Make</cell><cell>Taco Salad</cell><cell>Make</cell><cell>French Toast</cell><cell>Make</cell><cell>Irish Coffee</cell><cell>Make</cell><cell>Strawberry Cake</cell><cell>Make</cell><cell>Pancakes</cell><cell>Make</cell><cell>Meringue</cell><cell>Make</cell><cell>Fish Curry</cell><cell>Average</cell></row><row><cell>Alayrac et al. [1]</cell><cell cols="2">15.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Action step localization results on CrossTask<ref type="bibr" target="#b58">[59]</ref>.</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell cols="4">R@1 R@5 R@10 Median R</cell></row><row><cell>HGLMM [15]</cell><cell>YouCook2</cell><cell>4.6</cell><cell>14.3</cell><cell>21.6</cell><cell>75</cell></row><row><cell>TVJE [27]</cell><cell>YouCook2</cell><cell>4.2</cell><cell>13.7</cell><cell>21.5</cell><cell>65</cell></row><row><cell>TVJE +FT [27]</cell><cell>YouCook2</cell><cell>8.2</cell><cell>24.5</cell><cell>35.3</cell><cell>24</cell></row><row><cell>ActBERT</cell><cell>YouCook2</cell><cell>9.6</cell><cell>26.7</cell><cell>38.0</cell><cell>19</cell></row><row><cell cols="2">C+LSTM+SA [39] MSR-VTT</cell><cell>4.2</cell><cell>12.9</cell><cell>19.9</cell><cell>55</cell></row><row><cell>VSE-LSTM [14]</cell><cell>MSR-VTT</cell><cell>3.8</cell><cell>12.7</cell><cell>17.1</cell><cell>66</cell></row><row><cell>SNUVL [50]</cell><cell>MSR-VTT</cell><cell>3.5</cell><cell>15.9</cell><cell>23.8</cell><cell>44</cell></row><row><cell cols="2">Kaufman et al. [11] MSR-VTT</cell><cell>4.7</cell><cell>16.6</cell><cell>24.1</cell><cell>41</cell></row><row><cell>CT-SAN [51]</cell><cell>MSR-VTT</cell><cell>4.4</cell><cell>16.6</cell><cell>22.3</cell><cell>35</cell></row><row><cell>JSFusion [49]</cell><cell cols="2">MSR-VTT 10.2</cell><cell>31.2</cell><cell>43.2</cell><cell>13</cell></row><row><cell>TVJE [27]</cell><cell>MSR-VTT</cell><cell>7.5</cell><cell>21.2</cell><cell>29.6</cell><cell>38</cell></row><row><cell>TVJE +FT [27]</cell><cell cols="2">MSR-VTT 14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9</cell></row><row><cell>ActBERT</cell><cell>MSR-VTT</cell><cell>8.6</cell><cell>23.4</cell><cell>33.1</cell><cell>36</cell></row><row><cell>ActBERT +FT</cell><cell cols="2">MSR-VTT 16.3</cell><cell>42.8</cell><cell>56.9</cell><cell>10</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on video-and-text tasks</head><p>We evaluate ActBERT on five downstream tasks, i.e., action step localization, action segmentation, text-video clip retrieval, video captioning, and video question answering. We evaluate the five tasks on CrossTask <ref type="bibr" target="#b58">[59]</ref>, COIN <ref type="bibr" target="#b36">[37]</ref>, YouCook2 <ref type="bibr" target="#b52">[53]</ref>, MSR-VTT <ref type="bibr" target="#b45">[46]</ref> and LSMDC <ref type="bibr" target="#b31">[32]</ref>. Videos from the test sets of these datasets are removed during pretraining on HowTo100M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets</head><p>CrossTask: We evaluate action step localization on the CrossTask <ref type="bibr" target="#b58">[59]</ref> dataset. CrossTask <ref type="bibr" target="#b58">[59]</ref> contains 83 tasks and 4.7k videos related to cooking, car maintenance, crafting, etc. We use the recall metric described in <ref type="bibr" target="#b58">[59]</ref>, which is defined by the number of step assignments that fall into the ground-truth interval, divided by the total number of steps in the video. COIN: We evaluate the action segmentation task on the recent COIN <ref type="bibr" target="#b36">[37]</ref> dataset. COIN <ref type="bibr" target="#b36">[37]</ref> contains 180 tasks and 11,827 videos. This dataset consists of 46,354 annotated segments. The videos are collected from YouTube. YouCook2: We evaluate text-video clip retrieval and video captioning on YouCook2. YouCook2 is a cooking video dataset collected from YouTube, covering a large variety of cooking styles, methods, ingredients and cookwares <ref type="bibr" target="#b52">[53]</ref>. Acknowledgements. This work is supported by ARC DP200100938.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised learning from narrated instruction videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A bert baseline for the natural questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08634</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Scaling egocentric vision: The epic-kitchens dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weakly-supervised action segmentation with iterative soft boundary assignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6508" to="6516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Temporal tessellation: A unified approach for video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dotan</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woosang</forename><surname>Kyoung-Woon On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeonghee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung-Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04325</idno>
		<title level="m">Hadamard product for low-rank bilinear pooling</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.01696</idno>
		<title level="m">Tvqa: Localized, compositional video question answering</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuejian</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.06066</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A dataset and exploration of models for understanding video data through fill-in-theblank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Video fill in the blank with merging lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04062</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video fill in the blank using lr/rl lstms with spatial-temporal attentions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neuralnetwork-viterbi: A framework for weakly supervised video learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahsan</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08530</idno>
		<title level="m">Vl-bert: Pre-training of generic visuallinguistic representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Coin: A large-scale dataset for comprehensive instructional video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yansong</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dajun</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongming</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lili</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Movieqa: Understanding stories in movies through questionanswering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning language-visual embedding for movie understanding with natural-language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08124</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to compose topic-aware mixture of experts for zero-shot video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xlnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.08237</idno>
		<title level="m">Generalized autoregressive pretraining for language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Video captioning and retrieval models with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02947</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<meeting><address><addrLine>Jason J Corso, and Marcus Rohrbach</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Bidirectional multirate reconstruction for temporal modeling in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Uncovering the temporal context for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongwen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="421" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aligning books and movies: Towards story-like visual explanations by watching movies and reading books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Crosstask weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
