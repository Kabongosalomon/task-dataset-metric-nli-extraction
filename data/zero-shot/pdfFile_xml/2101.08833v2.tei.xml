<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Duke</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdalla</forename><surname>Ahmed</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Universit? de Lyon</orgName>
								<orgName type="institution" key="instit2">INSA-Lyon</orgName>
								<address>
									<settlement>LIRIS</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parham</forename><surname>Aarabi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Modiface, Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Guelph</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SSTVOS: Sparse Spatiotemporal Transformers for Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper we introduce a Transformer-based approach to video object segmentation (VOS). To address compounding error and scalability issues of prior work, we propose a scalable, end-to-end method for VOS called Sparse Spatiotemporal Transformers (SST). SST extracts per-pixel representations for each object in a video using sparse attention over spatiotemporal features. Our attention-based formulation for VOS allows a model to learn to attend over a history of multiple frames and provides suitable inductive bias for performing correspondence-like computations necessary for solving motion segmentation. We demonstrate the effectiveness of attention-based over recurrent networks in the spatiotemporal domain. Our method achieves competitive results on YouTube-VOS and DAVIS 2017 with improved scalability and robustness to occlusions compared with the state of the art. Code is available at https: //github.com/dukebw/SSTVOS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video object segmentation (VOS) involves simultaneous tracking and segmentation of one or more objects throughout a video clip. VOS is a challenging task in which algorithms must overcome object appearance changes, occlusion and disocclusion, as well as distinguish similar objects in motion over time.</p><p>A highly performant VOS system is important in downstream tracking applications where pixelwise tracking information is useful, such as player tracking in sports analytics, person tracking in security footage, and car and road obstacle tracking in self-driving vehicle applications. VOS methods are also relevant in interactive annotation of video data, where annotator time can be used more efficiently by using automatic video object segmentation in an annotatepredict-refine loop. Our work uses VOS as a proxy task to investigate scalable algorithms for extracting spatiotempo- <ref type="figure">Figure 1</ref>: We propose a Transformer-based model for video object segmentation featuring self-attention over time and over space. To segment an output frame, the model learns to look up similar regions in the temporal history and to search for reference masks. We address the high computational complexity of the problem with a sparse Transformer formulation, which allows each cell to attend to each other cell over one or multiple hops. Here, interactions propagate from a given feature cell via our sparse spatiotemporal attention variants: (a) grid attention, and (b) strided attention. ral representations from video in general, and these algorithms can be re-used for yet other video prediction tasks.</p><p>Previous methods that attempt to solve VOS can be divided into three major categories: online finetuning, mask refinement, and temporal feature propagation. Each of these categories, reviewed in detail in ?2, has inherent drawbacks.</p><p>Online finetuning methods cannot adapt to changes in object appearance throughout a sequence. The dominant mask refinement and temporal feature propagation methods are recurrent. Due to their sequential nature, recurrent methods for VOS exhibit compounding error over time, and are not parallelizable across a single example.</p><p>Motivated by the success of Transformer architectures in NLP (see ?2) we propose a novel method for semisupervised VOS that overcomes the drawbacks of online finetuning and sequential methods. Our method, Sparse Spatiotemporal Transformers (SST), processes videos in a single feedforward pass of an efficient attention-based network. At every layer of this net, each spatiotemporal feature vector simultaneously interacts with all other feature vectors in the video. SST does not require online finetuning, although it may benefit from this practice at the cost of the aforementioned runtime penalty. Furthermore, since SST is feedforward, it avoids the compounding error issue inherent in recurrent methods. Finally, SST is fully parallelizable across a single example and can therefore take advantage of the scalability of current and future compute architectures.</p><p>Applying spatiotemporal attention operators to VOS raises two challenges: computational complexity and distinguishing foreground objects. Na?ve spatiotemporal attention is square in the dimensionality of the video feature tensor, i.e., O((T HW ) 2 C). We resolve this computational complexity issue with sparse attention operators, of which we compare two promising candidates. SST reduces feature matching FLOPs by an order of magnitude, and achieves an overall score of 81.8 on the official YouTube-VOS 2019 validation set, comparing favourably with prior work. Furthermore, we observed qualitatively improved robustness to occlusions using SST's temporal buffer of preceding frame embeddings.</p><p>Contributions -We propose a Transformer-based model for VOS, and link its inductive bias to correspondence calculations. While there is work on Transformers for representation learning in video <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref>, these models attend over time and not densely over space. There is also recent work that adapts Transformers to video action recognition <ref type="bibr" target="#b13">[14]</ref>, however, we are unaware of work that uses Transformers in VOS, which requires dense predictions. We also contribute empirical evaluation of Transformer models applied to VOS, and argue superiority over recurrent models.</p><p>We address computational complexity using sparse attention operator variants, making it possible to apply selfattention on high-resolution videos. We extend sparse attention variants to video so that they can be used for VOS. Specifically we extend to 3D, with two spatial axes and one temporal axis, Criss-Cross Attention <ref type="bibr" target="#b17">[18]</ref> from 2D semantic segmentation, and Sparse Attention <ref type="bibr" target="#b5">[6]</ref> from 1D language translation. Our sparse video attention operators are not VOS specific, and could be applied to other dense video prediction tasks. We provide our implementation <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our work is related to previous efforts in VOS. We are motivated by work on Transformer architectures in language translation, as well as by orthogonal work on correspondence matching in computer vision.</p><p>Video Object Segmentation -In the VOS literature, online finetuning approaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> do one-shot, or semi-supervised, VOS by finetuning a semantic segmentation network on an initial frame. A number of methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b46">47]</ref> performed VOS on independent frames using finetuning alone without explicitly modeling temporal coherence. Maninis et al. <ref type="bibr" target="#b30">[31]</ref> built on the original usage of this approach by Caelles et al. <ref type="bibr" target="#b2">[3]</ref> by adding instance-level semantic information, while Voigtlaender and Leibe <ref type="bibr" target="#b46">[47]</ref> added adaptive training during the sequence. Offline methods must use temporal information to produce future segmentations from past frames, as done using optical flow by Jang and Kim <ref type="bibr" target="#b19">[20]</ref> and Tsai et al. <ref type="bibr" target="#b42">[43]</ref>. Our method is in principle able to take advantage of online finetuning to improve performance, and also performs competitively using offline training alone.</p><p>Research into temporal coherence in VOS splits into two categories: approaches that refine masks, and those that propagate temporal features.</p><p>Mask refinement approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20</ref>] refine a previous mask using feedforward models. Early work <ref type="bibr" target="#b22">[23]</ref> implemented mask refinement by a recurrent connection on the concatenated frame t?1 output masks and frame t RGB inputs where the recurrent connection is a VGG <ref type="bibr" target="#b37">[38]</ref> network. An extension concatenated the feature map from the first frame <ref type="bibr" target="#b32">[33]</ref>. Yang et al. <ref type="bibr" target="#b52">[53]</ref> used a spatial prior on the target location, with a channel-wise attention mechanism and meta-learning to adapt the network to the object given in the first frame. Bao et al. <ref type="bibr" target="#b1">[2]</ref> propagated masks by approximate inference in an MRF, with temporal dependencies based on optical flow, and spatial dependencies using a CNN. Optical flow has also been used to add motion information via jointly training optical flow and VOS <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Temporal feature propagation approaches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> improve upon mask refinement by increasing the expressive power of the mask feature representation. At the time of writing, all such methods have used RNNs to encode and propagate spatiotemporal representations through time. Our approach falls under the temporal feature propagation category. We use sparse attention operators to propagate features temporally in a single feedforward operation.</p><p>FEELVOS <ref type="bibr" target="#b45">[46]</ref> is a simple and fast method for solving the VOS problem. Unlike most other VOS methods, FEELVOS trains end-to-end using a pixel-wise embedding. FEELVOS also uses global and local matching to the reference and previous frames to predict masks for the video sequence. Our work shares similarities with FEELVOS in that both methods are end-to-end trainable and conceptually simple. Our method has the added advantages of simultaneously extracting features from multiple frames using attention, and using positional encodings to learn spatiotemporal position-aware representations.</p><p>Self-attention and Correspondence Matching -Endto-end attention networks known as Transformers <ref type="bibr" target="#b43">[44]</ref> are a dominant current approach to a multitude of text natural language processing (NLP) tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>, vision and language <ref type="bibr" target="#b39">[40]</ref> as well as speech recognition tasks <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b23">24]</ref>. Recent work has explored ties between attention heads and different reasoning functions <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b21">22]</ref>. More recently, Transformers have also been applied in computer vision with success <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b0">1]</ref>. In the context of VOS, we argue that self-attention also has the potential to overcome the shortcomings of the traditionally used recurrent methods <ref type="bibr" target="#b44">[45]</ref>. RNNs and variants are based on a Markovian assumption, where a flat vectorial hidden state is propagated over time through the sequence. Our Transformer based model takes a history of several frames and reference or predicted masks as input and allows each output region to attend to any region in the input history. This makes the propagated representation inherently structured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our proposed method for VOS consists of propagating a history of ? frames over the video sequence, and allowing the model to perform spatio-temporal attention inside this history. We argue, that the proposed high-level SST architecture ( ?3) provides inductive bias well suited for the reasoning skills required for VOS, namely computing optical flow (attending to past similar frames) and propagating reference masks over time (attending to given frames with similar appearance). We solve the challenge of computation complexity with two variants of sparse spatiotemporal attention, the "grid" and "strided" modules ( ?3).</p><p>SST Architecture -The canonical text-based Transformer architecture <ref type="bibr" target="#b43">[44]</ref> from which we drew motivation bears both similarities and differences with our VOS architecture. Like NLP Transformers, SST consists of a hierarchy of self-attention layers that form an encoder. In contrast to the encoder of an NLP Transformer, which takes as input embeddings extracted from a text sequence, SST's encoder input consists of embeddings extracted from the frames of the video to segment. As in NLP Transformers, the SST encoder's output feeds into a decoder. However, SST's decoder is unlike NLP Transformer decoders, which consist of cross-attention layers that take the output sequence embeddings as input. Instead, SST's decoder is a generic convolutional segmentation network that takes as input a concatenated set of features: current frame embeddings, attention values produced from SST's encoder's hierarchy of atten-tion maps, and embedding output by SST's encoder. For the purpose of fair comparison with state-of-the-art work, in ?4 we use CFBI's decoder module <ref type="bibr" target="#b53">[54]</ref>.</p><p>The SST architecture <ref type="figure">(Fig. 2</ref>) takes a length T sequence of H ? W RGB video frames S ? R 3?T ?H?W as input. From S a CNN feature extractor f extracts</p><formula xml:id="formula_0">T = f (S),<label>(1)</label></formula><p>a per-frame embedding T ? R C?T ?H ?W at reduced spatial resolution H ? W and with embedding channels C. In our experiments we used a ResNet-101 as f . In order to meet hardware resource constraints, and supposing that a given frame's relation to past frames decreases with time, any given frame embedding attends to a temporal buffer of its ? preceding frame embeddings. Denote the truncated frame embedding buffer by T ? . We optionally add information about the spatiotemporal position of cells in a video tensor by the positional encoding sum</p><formula xml:id="formula_1">T = T ? + P<label>(2)</label></formula><p>where P ? R C?T ?H ?W encodes absolute position. We can encode absolute position P using various priors, such as sinusoids or learned embeddings <ref type="bibr" target="#b43">[44]</ref>, or as a zero tensor in the case of no positional encoding. The SST encoder processes positionally encoded perframe embeddings T with L self-attention layers, adding temporal context to the video representation. The SST encoder passes two outputs to the SST decoder, the first of which is spatiotemporal features T L ? R C?? ?H ?W . A composition of L layers g l computes features T L as</p><formula xml:id="formula_2">T L = g L ? g L?1 ? ? ? ? ? g 1 ( T).<label>(3)</label></formula><p>Each encoder layer g l consists of a sequence of multihead attention and spatiotemporal position-wise feedforward components combined with skip connections and normalization ( <ref type="figure">Fig. 2)</ref>. The output T L of the SST encoder feeds directly into the decoder as the representation containing spatiotemporal context. The SST decoder's other input arises as an intermediate tensor computed by the multi-head attention component of each encoder layer. Each sparse multi-head attention operation computes an attention map, which we refer to as an object affinity tensor. The role of the object affinity tensor is to propagate segmentation information from past frames (either reference or predicted) to the future using the attention distributions of the Transformer heads. This can be seen as inductive bias for the model allowing it to more easily tie attention to semantically relevant motion. Key to the procedure are tensors I o p , which correspond to the pixels in the sparse connectivity pattern of feature cell p, and which belong to object o. Connectivity pattern I p determines which other feature cells are "connected to" and thus interact with</p><formula xml:id="formula_3">Figure 2:</formula><p>We propagate a history of ? frames over a video sequence and perform spatio-temporal self attention as a suitable bias for video object segmentation, allowing the model to attend to previous video frames for optical-flow like calculation, and to attend to reference frames. Computational complexity is addressed through two different sparse variants. feature cell p <ref type="figure">(Fig. 1</ref>). The SST encoder uses connectivity patterns to compute the decoder's second input: object affinity values.</p><p>Object affinity values A v ? R L?O?? ?H ?W represent the affinity of each of the ? ?H ?W cells in the spatiotemporal feature tensor with each of the O generalized objects -all reference objects plus the background. Each object affinity value A l v (p) is calculated as the maximum attention score in the object affinity tensor A l ? R |Ip|?? ?H ?W , i.e., the score of the pixel belonging to object o and most attended to by the attention head. Each of the L attention layers in the SST encoder computes its own object affinity value using a reduction operation over its object affinity tensor <ref type="figure">(Fig. 2)</ref>. To enforce causality, feature cell p's object affinity is computed only over previous timesteps. Furthermore, each feature cell p attends only to feature cells in its connectivity pattern I p . We define object affinity values as</p><formula xml:id="formula_4">A l v (p) = max I o p ?{0} A l<label>(4)</label></formula><p>where I o p denotes the (possibly empty) set of feature cells belonging to object o, in connectivity pattern I p , and defaulting to zero. By taking the T th temporal slice of object affinity values A v we obtain object-discriminative features used to infer the current frame object segmentation.</p><p>Due to the form of the multi-head attention computation described in ?3, attention maps A l contain pairwise dot products between feature cells and other feature cells within their respective connectivity patterns. From these dot products we can compute Euclidean distance or normalized cross correlation. Intuitively, by doing so we use the attention map features to compare the distances or angles between per-frame embeddings in the affine subspaces projected to by each attention head. Taking all attention heads in the encoder together forms a hierarchy of such distance (or angle) features. This improves the expressiveness of the model compared with straightforward Euclidean distance between the per-frame embeddings T from Equation 1.</p><p>The SST decoder <ref type="figure">(Fig. 2)</ref> is a convolutional decoder module that takes the spatiotemporal context features T L and object affinity features A 1..L v of all encoder layers as input. The final layer of the SST decoder produces the video object segmentation probability or masks Y ? R H?W from the final object-discriminative representation for a given frame. It applies a scoring convolution followed by sigmoid at training time or argmax at test time. In the case of multiple objects we have probability scoremaps in R O?H?W , i.e., probabilities for each generalized object (including background) for each pixel in the video. An inference protocol reduces these scoremaps to a tensor in R ? ?H?W of object integer labels. We use the "na?ve" inference protocol <ref type="bibr" target="#b32">[33]</ref> and take, for each pixel, the argmax over all object probabilities including the background probability.</p><p>Sparse Attention -In this section we use T to denote a generic temporal dimension, but as described in ?3, we actually operate on a reduced sized buffer of length ? .</p><p>Attention is a dense operator that allows each element of a tensor to interact with all other elements at each attention layer. In VOS, attention can capture long-range dependencies without recurrence, and can be viewed intuitively as a cross-correlation operator that uses CNN features for correspondence <ref type="bibr" target="#b26">[27]</ref>, similar to prior work that used matching layers for optical flow <ref type="bibr" target="#b8">[9]</ref>.</p><p>Formally, we follow <ref type="bibr" target="#b43">[44]</ref> in defining attention as</p><formula xml:id="formula_5">Attention Q, K, V = softmax QK V,<label>(5)</label></formula><p>where query Q, key K, and value V are all matrices in R S?C for flattened spatiotemporal dimensions S = T HW . As we alluded to in ?3, we use spatiotemporal features T as query, key, and value, i.e., we do self-attention. Intuitively we increase the spatiotemporal context of each feature cell p by doing a lookup in the spatiotemporal features connected to p. We adapted for VOS characteristic components of the Transformer architecture as described by Vaswani et al. <ref type="bibr" target="#b43">[44]</ref>, including multi-head attention and positional encodings. We compare the effectiveness of positional encoding schemes applied to VOS in ?4. We did not normalize the softmax argument in <ref type="figure">Equation 5</ref> by the inverse square root of channels, as we found this scaling factor reduced model effectiveness. The difference in impact of scaling factor between our VOS attention and Vaswani et al.'s NLP attention could be due to our attention operator having a comparatively low number of channels.</p><p>A computational barrier prevents na?vely using Equation 5 to perform our desired self-attention operation on spatiotemporal features T. The attention operation given in <ref type="bibr">Equation 5</ref> is O((T HW ) 2 C), which poses a problem for video object segmentation since for dense prediction tasks such as segmentation, model performance tends to improve with greater input resolution <ref type="bibr" target="#b55">[56]</ref>. As an illustration of the infeasibility of using na?ve attention for VOS, consider that a single layer of attention on a 16 frame video with a 64?64 feature map with 32 channels would cost more than 137 billion FLOPs, far more than the most computationally expensive CNNs in the literature at the time of writing <ref type="bibr" target="#b40">[41]</ref>. We propose to use sparse spatiotemporal attention operators to overcome this computational barrier to applying attention for VOS. We define sparse attention operators using a connectivity pattern set I = {I p0 , . . . , I p S } where I p is a set of coordinates (i, j, k) that index a 3D tensor. Here again, connectivity pattern I p determines which other feature cells interact with feature cell p.</p><p>For query Q, key K, and value V tensors all in R C?T ?H?W , a sparse attention operator is defined as</p><formula xml:id="formula_6">SparseAttn Q, K, V) p = softmax Q p K Ip V Ip . (6)</formula><p>We adapt two different sparse attention methods from 1D or 2D to 3D to make our attention operator computationally tractable at our desired framerate and resolution. We achieve computational tractability by careful selection of the connectivity patterns of our sparse attention operators.</p><p>Grid Attention -We adapted our first sparse attention operator from Huang et al., who also noted the computational complexity issue when applying attention for semantic segmentation <ref type="bibr" target="#b17">[18]</ref>. In VOS, however, the computational complexity issue is exacerbated by the addition of the time dimension. We refer to the generalized operator described below as "grid attention" since the moniker "criss-cross attention" is no longer fitting in more than two dimensions.</p><p>At each layer of grid attention, each feature cell of the spatiotemporal feature tensor aggregates information from other feature cells along its X, Y , and T axes independently. Each feature cell interacts once with every other feature cell in the spatiotemporal feature tensor that shares at least two of its X, Y , and T coordinates.</p><p>Grid attention implements Equation 6 with a connectivity pattern I p consisting of (T + H + W ? 2) feature cell indices. I p contains all feature cells along the horizontal, vertical, or temporal axes incident to location p ? (x, y, t). The grid attention weights softmax Q p K Ip are then a matrix in R S?(T +H+W ?2) , each row of which contains weights of a convex combination. For a feature cell p in the spatiotemporal feature tensor, the grid attention weights are over all feature cells along p's temporal, vertical, and horizontal axes. By multiplying by the grid attention weights we attend over p's grid connectivity pattern. Note that we implemented our grid attention operator in place, so we incur no overhead from indexing tensors by p.</p><p>In <ref type="figure">Figure 1</ref> (top) we illustrate how grid attention propagates interactions from a single attended feature cell to all other feature cells in three sequential layers. The first grid attention layer propagates information to other feature cells in the same frame vertically and horizontally, and to feature cells at the same spatial location in all other frames. The second layer propagates interactions to the entire current frame, and vertical and horizontal axes in other frames. Finally, the third layer propagates information to all feature cells in the video feature tensor.</p><p>We can show that composing three applications of grid self-attention on spatiotemporal feature tensor T produces an output tensor where each spatiotemporal feature cell with coordinates (x, y, t) is composed of a weighted sum</p><formula xml:id="formula_7">W i=1 H j=1 T k=1 T xyt T iyt T iyt T ijt T ijt T ijk T ijk (7)</formula><p>over other feature cells in T with coordinates (i, j, k). Equation 10 shows that grid attention propagates information along "routes" through the spatiotemporal feature tensor. For a feature cell at position (x, y, t) to interact with another feature cell at an arbitrary position (i, j, k), interactions must propagate along a "route" composed of pairs of similar feature cells. Just as we might give travel directions through a city grid such as "first walk ten blocks North, then walk three blocks East", grid attention interactions must propagate a fixed number of feature cells in the X, Y , and T directions, in some order, before connecting the source feature cell with its target feature cell.</p><p>By replacing dense attention with grid attention we reduced the computational complexity of video attention from O(C(T HW ) 2 ) to O(C(T + H + W )T HW ), achieving our goal of making attention tractable for video. Strided Attention -We investigated strided attention as an alternative sparse attention method in addition to grid attention. Drawing inspiration from sparse Transformers for sequences <ref type="bibr" target="#b5">[6]</ref>, information propagates by following paths of locations through sparse connectivity patterns in the spatiotemporal feature tensor. We define strided attention's connectivity pattern I p as a generalization of Child et al.'s strided attention from 1D to 3D. Our strided attention uses two different connectivity patterns I 1 p and I 2 p corresponding to separate, sequential heads of multihead attention. The first connectivity pattern I 1 p routes to all feature cells in a cube of side-length h from p, i.e., I 1 p = (p + (l x , l y , l z ) : l x , l y , l z &lt; h). The subsequent connectivity pattern I 2 p routes to all feature cells in the video tensor that can reach p by taking steps of size h along each axis, i.e., I 2 p = (p + (l x , l y , l z ) : l x , l y , l z mod h = 0). We choose h ? ? H to reduce the computational complexity by a square root from O(C(T HW ) 2 )</p><p>to O(C(T HW ) 3/2 ). We visualize strided attention's connectivity pattern in <ref type="figure">Figure 1 (bottom)</ref>. The relative efficiency of grid and strided attention depends on the size of T , since we assume that H and W are both large relative to T . In a setup where H, W ? {64, 128}, and T ? 8, strided attention costs about 1.3 to 1.4 times as many operations as grid attention.</p><p>Runtime - <ref type="table" target="#tab_0">Table 1</ref> provides a runtime and parameter analysis. We computed the multiply-accumulates (MACs) of SST for a 3-frame temporal buffer, input resolution of 465 ? 465, 128 channels, and 3 Transformer layers. We show MACs in both absolute GFLOPs and as slowdown relative to DeepLab-v3 backbone MACs. We also compare to CFBI's local/global matching. Note that SST's local temporal window (? = 3) is larger than CFBI's (? = 1). Finally, we compare to na?ve attention. Both na?ve attention and CFBI's global matching attend pairwise to an entire feature map, costing 39.0% and 66.6% slowdown relative to DeepLab-v3's runtime. In contrast, SST factorizes the computation by attending to all other spatiotemporal feature cells over sequential layers <ref type="figure">(Fig. 1)</ref>. SST reduces slowdown by more than an order of magnitude to ? 2%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>We present benchmark experiment results against stateof-the-art (SOTA) methods on the DAVIS 2017 <ref type="bibr" target="#b34">[35]</ref> and YouTube-VOS <ref type="bibr" target="#b51">[52]</ref> datasets. We further analyze the effect   <ref type="bibr" target="#b51">[52]</ref> 2018 and 2019. Region similarity over seen (J seen ) and unseen (J unseen ) categories, and overall score G are computed as in standard benchmarks <ref type="bibr" target="#b33">[34]</ref>. We distinguish methods by those that use online finetuning (O-Ft) and/or synthetic data (S), and those that do not.</p><p>of different sparse attention operators, history sizes and positional encodings through ablation studies on DAVIS 2017. YouTube-VOSis a large scale VOS dataset comprised of 4453 YouTube video clips spanning 94 object categories <ref type="bibr" target="#b51">[52]</ref>. YouTube-VOS includes an official validation set of 507 videos with held out labels, which can be evaluated only through an evaluation server. YouTube-VOS has been the basis of challenges in 2018 and 2019, yielding two versions of the validation set and evaluation server. The 2019 version contains new and corrected annotations. The YouTube-VOS validation set contains 26 object categories that are unique to the validation set, used to test the generalization capability of VOS models to object classes unseen in the training set. The convention is to compute region similarity J and contour accuracy F as defined by Perazzi et al. <ref type="bibr" target="#b33">[34]</ref>. As a single metric for comparing results, it is also customary to compute overall score G as the average of four values comprising region similarity and contour accuracy scores for seen and unseen classes.</p><p>In <ref type="table" target="#tab_2">Table 2</ref> we present our model's results on YouTube-VOS 2018 and 2019 alongside previous SOTA results. Our model (SST) performs favourably against all previous methods in overall score G, even methods that use online fine-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head><p>25% 50% 75% 100%  <ref type="table">Table 3</ref>: Comparison with SOTA methods on DAVIS 2017 <ref type="bibr" target="#b34">[35]</ref>. We denote online finetuning methods by O-Ft, and synthetic data methods by S. We report results trained only on the DAVIS 2017 training set, and pre-trained on YouTube-VOS. YouTube-VOS pre-training is denoted by ? .</p><p>tuning (denoted by O-Ft) and pre-training on synthetic data (denoted by S). Note that our unique method performs competitively against recurrent methods that have undergone multiple research and development cycles where one method builds on the foundation of another, for example <ref type="bibr" target="#b20">[21]</ref> extends <ref type="bibr" target="#b32">[33]</ref>, which in turn extends <ref type="bibr" target="#b22">[23]</ref>. DAVIS 2017is the latest dataset in the DAVIS initiative to promote VOS research. DAVIS 2017 comprises 150 sequences, which include 376 separately annotated objects <ref type="bibr" target="#b34">[35]</ref>. We additionally evaluate our method on DAVIS 2017 <ref type="bibr" target="#b34">[35]</ref>, and compare our results with SOTA in <ref type="table">Table 3</ref>. We report our DAVIS results following the traditionally used region similarity J and contour accuracy F metrics as well as their mean J &amp;F. Our DAVIS 2017 evaluation provides additional experimental evidence that SST performs favourably compared with existing SOTA methods, since SST achieves a mean J &amp;F score of 78.4, whereas previous SOTA scored a J &amp;F of 74.9 under a comparable experimental setup (without online finetuning or synthetic data).</p><p>We evaluate only on DAVIS 2017 and not DAVIS 2016 <ref type="bibr" target="#b33">[34]</ref> because DAVIS 2017 is strictly a more challenging superset of DAVIS 2016. Furthermore DAVIS 2016 contains only single object annotations and therefore we could make only limited evaluation of SST's ability to handle multi-object context using DAVIS 2016.</p><p>Ablation Studies - <ref type="figure">Figure 3</ref> shows a qualitative example on the YouTube-VOS validation set of SST handling foreground occlusion, where one fish entirely occludes another before the second fish becomes disoccluded again.</p><p>We used DAVIS 2017 to perform ablation studies on interesting components of our method, including sparse attention operators, positional encodings, and temporal history size. We first describe and compare different design choices for our positional encodings.</p><p>We investigated sinusoidal positional encodings for the temporal dimension, as used in Transformers for language translation <ref type="bibr" target="#b43">[44]</ref>. We hypothesized that sinusoidal positional encodings would be superior to learned positional embeddings because of the data imbalance of temporal positions in VOS datasets, which are skewed towards lower frame numbers. Sinusoidal positional encodings can be interpolated or extrapolated to generalize to underrepresented absolute frame numbers, whereas positional embeddings have no such generalization mechanism.</p><p>We present our positional encoding results in <ref type="table" target="#tab_5">Table 4</ref>     ing the J &amp;F score on the DAVIS 2017 validation set. The positional encoding labeled "None" is our baseline attention with no positional information, while "Sinusoidal" uses sinusoidal positional encodings for all spatiotemporal dimensions X, Y , and T . To evaluate robustness to hyperparameters, we computed the mean and variance of J &amp;F over multiple runs for each positional encoding scheme, varying the number of Transformer layers and the temporal history. Sinusoidal temporal positional encodings performed best, achieving both a higher mean score and lower variance. The superiority of positional encodings supports our hypothesis that information about distance-from-reference is important in positional encodings for VOS. The lower variance indicates that sinusoidal positional encodings form a robust prior for finding correspondences in VOS.</p><p>In <ref type="table" target="#tab_6">Table 5</ref> we evaluate the effect of increasing SST's temporal history ? . We varied the temporal history while keeping other hyperparameters fixed, and computed the variance over multiple runs. We observed that even a modest increase in temporal history improves the J &amp;F score. We expect that efficiency improvements further increasing the temporal history size will improve the effectiveness of SST's temporal history mechanism even further.</p><p>In <ref type="table" target="#tab_7">Table 6</ref> we compare the performance of SST using different sparse attention variants. We expected that increasing the number of layers would improve grid attention's performance due to the increasing receptive field of each feature cell. A larger receptive field should improve the effectiveness of the object affinity value from multi-head attention. Grid attention's J &amp;F score increased as expected from one to two layers, but dropped off for three layers possibly due to overfitting. We also expected that local attention should be effective for DAVIS 2017's fast framerate compared to strided attention, both of which we describe in ?3. Local-Strided attention provides a tradeoff between the local and global context windows of local and strided attention, respectively. So that LocalStrided attention can attend transitively to all feature cells in just two consecutive sparse attention layers, we set the kernel size equal to the square root of the feature tensor width. For fair comparison, we kept the same kernel size for all strided attention variants. In general, local and strided attention outperformed grid attention, showing that local and strided connectivity patterns form superior priors for VOS compared with grid attention.</p><p>Discussion -We present a method for VOS purely based on end-to-end attention. Future work could be analogous to Transformer models' progression on language translation tasks, where researchers successfully applied Transformers to increasingly long sequences. For example, Dai et al. combined recurrence with attention to translate arbitrary-length sequences <ref type="bibr" target="#b6">[7]</ref>, and Kitaev et al. introduced locality-sensitive hashing instead of dotproduct attention, to reduce computational complexity from squared to O(N log N ) while using reversible networks to model arbitrary-length sequences with constant memory usage <ref type="bibr" target="#b24">[25]</ref>. In order to evaluate VOS on long sequences the VOS community would have to overcome a dataset creation challenge, since the current benchmark dataset YouTube-VOS contains sequences with at most 36 labeled frames, sampled at 6 fps. We propose that future work could use interactive and semi-automatic annotation methods, based on the existing high-quality VOS models, to create datasets with longer and therefore more challenging sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We presented Sparse Spatiotemporal Transformers (SST), which, up to our knowledge, constitutes the first application of an entirely attention-based model for video object segmentation (VOS). We evaluated the positive effect of positional encodings and the advantage of attending over a history of multiple frames, suggesting a superiority of a spatiotemporally structured representation over the flat hidden representation of recurrent models. We showed that SST is capable of state-of-the-art results on the benchmark VOS dataset YouTube-VOS, attaining an overall score of G = 81.8, while having superior runtime scalability compared with previous state of the art, including methods based on recurrence. We provide code <ref type="bibr" target="#b10">[11]</ref> to reproduce all experiments in our work, including sparse video-attention operator implementations <ref type="bibr" target="#b9">[10]</ref>, so that the community can build on the promising idea of using attention-based models for video. Open challenges are the memory requirements inherent in a model with only weak Markovian assumptions, which for the moment prevents the increase of history size to a desirable longer extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Additional Results</head><p>In <ref type="figure">Figures 4 and 5</ref>, we compare SST qualitatively to CFBI <ref type="bibr" target="#b53">[54]</ref> and STM <ref type="bibr" target="#b31">[32]</ref>. SST produces superior tracking in these challenging sequences, which contain occlusions and disocclusions. The positional encoding in the Transformer representations may enable SST to distinguish similar instances under occlusions, using positional information. Whereas CFBI confuses instances that are far apart, SST remains robust to these nonlocal failures. This further supports the effectiveness of SST's use of positional information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Grid Attention Routing</head><p>To demonstrate mathematically information propagation in grid attention we consider, for sake of clarity, a sparse attention function SparseAttn Q, K, V) p = Q p K Ip V Ip .</p><p>where we replace the softmax by an identity function. Further assume that our query, key, and value all are our video feature tensor, i.e., Q = T, K = T, and V = T. The first layer outputs, for each pixel p,</p><formula xml:id="formula_9">GridAttn Q, K, V) p xyt = W i=1 T xyt T iyt T iyt + H j=1 j =y T xyt T xjt T xjt + T k=1 k =t T xyt T xyk T xyk .<label>(9)</label></formula><p>We can show that composing three applications of selfattention on T, which we denote for brevity as GridAttn 3 , produces</p><formula xml:id="formula_10">GridAttn 3 T, T, T xyt = W i=1 H j=1 T k=1 T xyt T iyt T iyt T ijt T ijt T ijk T ijk + ? ? ? ,<label>(10)</label></formula><p>where ? ? ? represents other similar third order terms. We show in Equation 10 that grid attention propagates information along "routes" through the video feature tensor: for a pixel at position (x, y, t) to interact with another pixel at an arbitrary position (i, j, k), interactions must propagate along a "route" through the video feature tensor of pairs of similar pixels. Just as we might give travel directions through a city grid such as "first walk ten blocks North, then walk three blocks East", grid attention interactions must propagate a fixed number of pixels in the X, Y and T directions, in some order, before connecting the interaction source pixel with its target pixel.</p><p>Consider what happens if we replace the value T ijk returned by the inner cross-attention in Equation 10 with a foreground mask value m ijk . We see that the output routes reference mask values m ijk over paths of feature vectors in the video tensor T that transitively correspond to reference features T ijk .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>0%</head><p>25% 50% 75% 100% STM <ref type="figure">Figure 4</ref>: Fish tank. This challenging YouTube-VOS 2019 validation set example contains many occlusions and disocclusions by similar-looking instances of the same fish class. SST makes relatively few errors relatively later in the sequence when compared with CFBI <ref type="bibr" target="#b53">[54]</ref> or STM <ref type="bibr" target="#b31">[32]</ref>. For clarity we labeled errors with yellow dotted boxes (best viewed digitally, with zoom and in colour).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CFBI SST</head><p>0% 25% 50% 75% 100% STM <ref type="figure">Figure 5</ref>: Jazz band. In this YouTube-VOS 2019 validation set example the saxophone player self-occludes and disoccludes their saxophone while playing. SST maintains the correct saxophone segmentation throughout the sequence. In contrast, CFBI <ref type="bibr" target="#b53">[54]</ref> and STM <ref type="bibr" target="#b31">[32]</ref> confuse the saxophone with the saxophone player's upper body after disocclusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Runtime and parameter analysis.</figDesc><table><row><cell>Model</cell><cell>MACs (GFLOPs)</cell><cell>Slowdown (%)</cell><cell>Params (M)</cell></row><row><cell>DeepLab-v3</cell><cell>255.4</cell><cell>-</cell><cell>66.5</cell></row><row><cell>Matching (CFBI)</cell><cell>99.6</cell><cell>39.0</cell><cell>0</cell></row><row><cell>SST (Local)</cell><cell>5.34</cell><cell>2.1</cell><cell>0.3</cell></row><row><cell>SST (Strided)</cell><cell>1.89</cell><cell>0.7</cell><cell>0.3</cell></row><row><cell>SST (Grid)</cell><cell>1.45</cell><cell>0.6</cell><cell>0.3</cell></row><row><cell>Na?ve Attention</cell><cell>170.1</cell><cell>66.6</cell><cell>0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Comparison with SOTA methods on YouTube- VOS</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Figure 3: A qualitative example from YouTube-VOS validation showing SST handling occlusion. In this challenging example, SST's temporal history enables robust tracking of all three fish, while CFBI confuses two fish once they overlap.</figDesc><table><row><cell>Method</cell><cell>O-Ft S</cell><cell>J &amp;F (%)</cell><cell>J (%)</cell><cell>F (%)</cell></row><row><cell cols="3">DAVIS 2017 Validation Split</cell><cell></cell><cell></cell></row><row><cell>OSVOS-S [31]</cell><cell></cell><cell cols="3">68.0 64.7 71.3</cell></row><row><cell>OSVOS [3]</cell><cell></cell><cell cols="3">60.3 56.6 63.9</cell></row><row><cell>OnAVOS [47]</cell><cell></cell><cell cols="3">65.4 61.6 69.1</cell></row><row><cell>CINM [2]</cell><cell></cell><cell cols="3">70.6 67.2 74.0</cell></row><row><cell>PReMVOS [29]</cell><cell></cell><cell cols="3">77.7 73.9 81.7</cell></row><row><cell>RGMP [33]</cell><cell></cell><cell cols="3">66.7 64.8 68.6</cell></row><row><cell>STM ? [32]</cell><cell></cell><cell cols="3">71.6 69.2 74.0</cell></row><row><cell>STM  ? [32]</cell><cell></cell><cell cols="3">81.8 79.2 84.3</cell></row><row><cell>KMN [37]</cell><cell></cell><cell cols="3">76.0 74.2 77.8</cell></row><row><cell>OSMN [53]</cell><cell></cell><cell cols="3">54.8 52.5 57.1</cell></row><row><cell>FAVOS [4]</cell><cell></cell><cell cols="3">58.2 54.6 61.8</cell></row><row><cell>VM [17]</cell><cell></cell><cell cols="2">-56.5</cell><cell>-</cell></row><row><cell>DyeNet [26]</cell><cell></cell><cell cols="3">69.1 67.3 71.0</cell></row><row><cell>A-GAME  ? [21]</cell><cell></cell><cell cols="3">70.0 67.2 72.7</cell></row><row><cell>FEELVOS  ? [46]</cell><cell></cell><cell cols="3">71.5 69.1 74.0</cell></row><row><cell>CFBI [54]</cell><cell></cell><cell cols="3">74.9 72.1 77.7</cell></row><row><cell>CFBI  ? [54]</cell><cell></cell><cell cols="3">81.9 79.3 84.5</cell></row><row><cell>SST (Local)</cell><cell></cell><cell cols="3">78.4 75.4 81.4</cell></row><row><cell>SST (Local)  ?</cell><cell></cell><cell cols="3">82.5 79.9 85.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Positional encodings on DAVIS 2017 val.</figDesc><table><row><cell>?</cell><cell>J &amp;F (%)</cell></row><row><cell cols="2">1 75.8 ? 0.3</cell></row><row><cell cols="2">2 76.2 ? 0.3</cell></row><row><cell cols="2">3 76.5 ? 0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Temporal histories ? on DAVIS 2017 val.</figDesc><table><row><cell cols="3">Sparse Attention Layers J &amp;F</cell><cell>J</cell><cell>F</cell></row><row><cell>Grid</cell><cell>1</cell><cell cols="2">65.3 62.3 68.4</cell></row><row><cell>Grid</cell><cell>2</cell><cell cols="2">66.1 62.6 69.5</cell></row><row><cell>Grid</cell><cell>3</cell><cell cols="2">64.2 61.0 67.4</cell></row><row><cell>Local</cell><cell>2</cell><cell cols="2">76.2 72.8 79.5</cell></row><row><cell>Strided</cell><cell>2</cell><cell cols="2">69.5 65.7 73.3</cell></row><row><cell>LocalStrided</cell><cell>2</cell><cell cols="2">72.3 69.1 75.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>SST sparse attention variants on DAVIS 2017 val.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements -C. Wolf acknowledges support from ANR through grant "Remember" (ANR-20-CHIA-0018) of the call "AI chairs in research and teaching".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anonymous</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">openreview ICLR 2020 submissino</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CNN in MRF: video object segmentation via inference in A cnn-based higher-order spatio-temporal MRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast and accurate online video object segmentation via tracking parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Segflow: Joint learning for video object segmentation and optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingchun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno>2019. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<idno>De- cember 2015. 4</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Sparse spatiotemporal transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Duke</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Duke</forename><surname>Sstvos</surname></persName>
		</author>
		<ptr target="https://github.com/dukebw/SSTVOS,2021" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fusionseg: Learning to combine motion and appearance for fully automatic segmentation of generic objects in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Suyog Dutt Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video Action Transformer Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Carl Doersch, and Andrew Zisserman. Video action transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Motion-guided cascaded refinement network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maskrnn: Instance level video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Videomatch: Matching based video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Ting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Online video object segmentation via convolutional trident network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A generative appearance model for end-to-end video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emil</forename><surname>Brissman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How Transferrable are Reasoning Patterns in VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kervadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaunet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Antipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vuillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<idno>2021. 3</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved Vocal Tract Length Perturbation for a State-of-the-Art End-to-End Speech Recognition System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanwoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhananjaya</forename><surname>Gowda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation with joint re-identification and attention-aware mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Premvos: Proposal-generation, refinement and merging for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">RWTH ASR Systems for LibriSpeech: Hybrid vs Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>L?scher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kitza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wilfried</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schl?ter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Video object segmentation without temporal information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recurrent neural networks for semantic instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Baradad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongje</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Euntai</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for Large-Scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning Video Representations using Contrastive Bidirectional Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno>arxiv:1906.05743</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning video object segmentation with visual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">BoLTVOS: Box-Level Tracking for Video Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04552</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Non-local Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Youtube-vos: Sequence-to-sequence video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Youtubevos: A large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos</forename><forename type="middle">K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Collaborative video object segmentation by foreground-background integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Exploring Self-attention for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Icnet for real-time semantic segmentation on high-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
