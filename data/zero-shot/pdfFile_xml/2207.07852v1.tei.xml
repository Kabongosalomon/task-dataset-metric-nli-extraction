<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>2?</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luhui</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengming</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<address>
									<settlement>Tencent</settlement>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
							<email>qjin@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TS2-Net: Token Shift and Selection Transformer for Text-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>text-video retrieval</term>
					<term>token shift</term>
					<term>token selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-Video retrieval is a task of great practical value and has received increasing attention, among which learning spatial-temporal video representation is one of the research hotspots. The video encoders in the state-of-the-art video retrieval models usually directly adopt the pre-trained vision backbones with the network structure fixed, they therefore can not be further improved to produce the fine-grained spatialtemporal video representation. In this paper, we propose Token Shift and Selection Network (TS2-Net), a novel token shift and selection transformer architecture, which dynamically adjusts the token sequence and selects informative tokens in both temporal and spatial dimensions from input video samples. The token shift module temporally shifts the whole token features back-and-forth across adjacent frames, to preserve the complete token representation and capture subtle movements. Then the token selection module selects tokens that contribute most to local spatial semantics. Based on thorough experiments, the proposed TS2-Net achieves state-of-the-art performance on major text-video retrieval benchmarks, including new records on MSRVTT, VATEX, LSMDC, Activ-ityNet, and DiDeMo. Code will be available at https://github.com/ yuqi657/ts2_net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1</ref><p>. The text-video retrieval examples that require fine-grained video representation. Left: the small object 'hat' is important for correctly retrieving the target video. Right: the subtle movement of 'talking' is crucial for the correct retrieval of the target video. Green boxes depict the positive video result, while red boxes are negative candidates proposed <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>, which usually formulate the task as a learning and matching task based on a similarity function between the text query and candidate videos in the corpus. With the success of deep neural networks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b46">47]</ref>, deep learned features have replaced manually-designed features. A text-video retrieval engine is generally composed of a text encoder and a video encoder, which maps the text query and the video candidate to the same embedding space, where the similarity can be easily computed using a distance metric.</p><p>Building a powerful video encoder to produce spatial-temporal feature encoding for videos, that can simultaneously capture motion between video frames, as well as entities in video frames, has been one of the research focuses for text-video retrieval in recent years <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b32">33]</ref>. Lately, Transformer has become the dominant visual encoder architecture, and it enables the training of video-language models with raw video and text data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref>. Various video transformers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref>, considering both spatial and temporal representations, have achieved superior performance on major benchmarks. However, these models still lack fine-grained representation capacity in either spatial or temporal dimension. For example, the video encoder in models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref> normally consists of a single-frame feature extraction module followed by a global feature aggregation module, which lacks fine-grained interaction between adjacent frames and only aggregates the frame-level semantic information. Although the video encoder in Frozen <ref type="bibr" target="#b3">[4]</ref> employs divided space-time attention, it uses only one [CLS] token as the video representation, failing to capture the find-grained spatial-temporal details. In general, all these models can effectively represent obvious motions and categorical spatial semantics in the video, but still lack the capacity for subtle movement and small objects. They will fail in cases such as illustrated in <ref type="figure">Fig.1</ref>, where the video encoder needs to capture the small object ('hat') and subtle movement ('talking') in order to retrieve the correct target videos.</p><p>Based on the structure of video transformer, video sequence is spatially and temporally divided into consecutive patches. To enhance modeling of small objects and subtle movements, patch enhancement is an intuitive and straightforward solution. This motivates us to find a feasible way to incorporate spatial-temporal patch contexts into encoded features. The shift operation is introduced in TSM <ref type="bibr" target="#b28">[29]</ref>, which shifts parts of the channel along temporal dimension. Shift Transformer <ref type="bibr" target="#b51">[52]</ref> applies shift in visual transformer to enhance temporal modeling. However, the architecture of transformer is different from CNN, such partial shift operation damages the completeness of each token representation. Therefore, in this paper, we propose TS2-Net, a novel token shift and selection transformer network, to realize local patch feature enhancement. Specifically, we first adopt the token shift module in TS2-Net, which shifts the whole spatial token features back-and-forth across adjacent frames, in order to capture local movement between frames. We then design a token selection module to select top-K informative tokens to enhance the salient semantic feature modeling capability. Our token shift module treats the features of each token as a whole, and iteratively swaps token features at the same location with neighbor frames, to preserve the complete local token representation and capture local temporal semantics at the same time. The token selection module estimates the importance of each token feature of patches with a selection network, which relies on the correlation between all spatial-temporal patch features and [CLS] tokens. It then selects tokens which contributes most to local spatial semantics. Finally, we align cross-modal representation in a fine-grained manner, where we calculate the similarity between text and each frame-wise video embedding and aggregate them together. TS2-Net is optimized with video-language contrastive learning.</p><p>We conduct extensive experiments on several text-video retrieval benchmarks to evaluate our model, including MSRVTT, VATEX, LSMDC, ActivityNet, and DiDeMo. Our proposed TS2-Net achieves the state-of-the-art performance on most of the benchmarks. The ablation experiments demonstrate that the proposed token shift and token selection modules both improve the fine-grained text-video retrieval accuracy. The main contributions of this work are as follows:</p><p>-We propose a new perspective of video-language learning with local patch enhancements to improve the text-video retrieval. -We introduce two modules, token shift transformer and token selection transformer, to better model video representation temporally and spatially. -We report new records of retrieval accuracy on several text-video retrieval benchmarks. Thorough ablation studies demonstrate the merits of our patch enhancement concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Video Retrieval</head><p>Various approaches have been proposed to deal with text-video retrieval task, which usually consist of off-line feature extractors and feature fusion module <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">45]</ref>. MMT <ref type="bibr" target="#b20">[21]</ref> uses a cross-modal encoder to aggregate feature extracted by different experts. MDMMT <ref type="bibr" target="#b16">[17]</ref> further utilizes knowledge learned from multi-domain datasets. Recent works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12]</ref> attempt to train text-video model in an end-to-end manner. ClipBERT <ref type="bibr" target="#b25">[26]</ref> is the pioneering end-to-end text-video pretrain model. Its promising results show that jointly train high-level semantic alignment network with low-level feature extractor is beneficial. CLIP4Clip <ref type="bibr" target="#b34">[35]</ref> and CLIP2Video <ref type="bibr" target="#b18">[19]</ref> transfer knowledge from pretrained CLIP <ref type="bibr" target="#b37">[38]</ref> to video retrieval task. However, these models still lack finegrained representation capacity in either spatial or temporal dimension. Different from previous works, we aim to model fine-grained spatial and temporal information to enhance text-video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Visual-Language Pre-training</head><p>Viusal-language pre-training models has shown promising results in visual-andlanguage tasks such as image retrieval, image caption and video retrieval. In works such as Unicoder-VL <ref type="bibr" target="#b26">[27]</ref>, VL-BERT <ref type="bibr" target="#b42">[43]</ref> and VLP <ref type="bibr" target="#b52">[53]</ref>, text and visual sequence are input into a shared transformer encoder. In Hero <ref type="bibr" target="#b27">[28]</ref>, ClipBERT <ref type="bibr" target="#b25">[26]</ref> and Univl <ref type="bibr" target="#b33">[34]</ref>, text and visual sequence are encoded independently, then a crossencoder is used to fuse different modality. While in Frozen <ref type="bibr" target="#b3">[4]</ref>, CLIP <ref type="bibr" target="#b37">[38]</ref>, text and visual sequence are encoded independently and a contrastive loss is used to align text and visual embedding. Our work use the two-stream structure, where text feature and video feature are encoded independently, then a cross-modal contrastive loss is used to align them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Video Representation Learning</head><p>Early works use 2D or 3D-CNN to encode video feature <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>. Recently, Visual Transformer(ViT) <ref type="bibr" target="#b15">[16]</ref> has shown great potential in image modeling. Many works attempt to transfer ViT into video domain <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33]</ref>. TimeSformer <ref type="bibr" target="#b4">[5]</ref> and ViViT <ref type="bibr" target="#b2">[3]</ref> propose variants of spatial-temporal video transformer. There are several works exploring shift operation to enable 2D network learn temporal information, including TSM <ref type="bibr" target="#b28">[29]</ref> and Shift Transformer <ref type="bibr" target="#b51">[52]</ref>. They shift parts of the channel along the temporal dimension. Different from previous work, we consider token shift operation, which we shift all channels of selected visual tokens to the temporal dimension rather than partial shift (i.e. shift some channels). Token selection has been used to reduce redundancy problem in transformer based visual model. Dynamic ViT <ref type="bibr" target="#b39">[40]</ref> and STTS <ref type="bibr" target="#b43">[44]</ref> use token selection for efficiency. Perturbed maximum is proposed in <ref type="bibr" target="#b5">[6]</ref> to make top-K differentiable. Based on differential top-K <ref type="bibr" target="#b12">[13]</ref>, our work designs a light-weight token selection module to select informative tokens for effective temporal-spatial modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>The goal of text-video retrieval is to find the best matching videos based on the text query. <ref type="figure" target="#fig_0">Fig.2</ref> illustrates the overall structure of the proposed TS2-Net model for the text-video retrieval task, which consists of three key components: the text encoder, the video encoder, and the text-video matching. The text encoder encodes the sequence of query words into a query representation q. In this paper, we use GPT <ref type="bibr" target="#b38">[39]</ref> model as the text encoder. By adding a special token [EOS] at the end of query word sequence, we employ the encoding of [EOS] by the GPT encoder as the query representation q. The video encoder encodes the sequence of video frames into a sequence of frame-wise video representation v = {f 1 , f 2 , . . . , f t }. Based on the query and video representation, q and v, the text-video matching computes the cross-modal similarity between the query and video candidate. In following sections, we first elaborate the core ingredients of our video encoder, namely the token shift transformer (Sec.3.1) and the token selection transformer (Sec.3.2), and finally present our text-video matching strategy in details (Sec.3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Token Shift Transformer</head><p>Token shift transformer is based on Vision Transformer (ViT) <ref type="bibr" target="#b15">[16]</ref>. It inserts a token shift module in the transformer block. Let's review ViT model first, and then describe our modification to ViT. Given an image I, ViT first splits I into N patches {p 0 , p 1 , . . . , p n?1 }. To eliminate ambiguity, we use token to represent patch below. After adding a [CLS] token p cls , the token sequence {p cls , p 0 , p 1 , . . . , p n?1 } is fed into a stack of transformer blocks. Then the image embedding is generated by either averaging all the visual tokens or using the [CLS] token p cls . In this work, we use p cls as the image embedding. Token shift transformer aims to effectively model subtle movements in a video. The proposed token shift operation is a parameter-free operation, as illustrated in <ref type="figure" target="#fig_1">Fig.3</ref>. Suppose we have a video V ? R T ?N ?C , where T represents the number of frames, N refers to the number of tokens per frame, and C represents the feature dimension. We feed T frames into ViT to encode frame feature. In certain ViT layer, we shift some tokens from adjacent frames to the current frame to In Shift-Transformer <ref type="bibr" target="#b51">[52]</ref>, tokens are shifted along the channel dimension, while our proposed Token Shift Module does not compromise the integrity of a video token exchange information of adjacent frames. Note that we use a bi-directional token shift in our implementation. By token shift operation across adjacent frames, our model is able to capture subtle movements in the local temporal interval.</p><p>Shift-Transformer <ref type="bibr" target="#b51">[52]</ref> has also explored several shift variants on the visual transformer architecture. <ref type="figure" target="#fig_1">Fig.3</ref> visualizes the difference between these shift variants and our proposed token shift. A naive channel temporal shift swaps part of channels of a frame tensor along temporal dimension, as shown in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>. Shift-Transformer <ref type="bibr" target="#b51">[52]</ref> also presents [VIS] channel temporal shift and [CLS] channel temporal shift, as shown in <ref type="figure" target="#fig_1">Fig.3</ref>(b)(c). They fix tensor in token dimension and shift parts of channels for chosen token along the temporal dimension. Different from these works, our token shift transformer emphasizes the token dimension, where we shift whole channels of a token back-and-forth across adjacent frames, as shown in <ref type="figure" target="#fig_1">Fig.3(d)</ref>. We believe our token shift is better for ViT architecture, because different from the CNN architecture, each token in ViT is independent and contains unique spatial information with respect to its location. Thus shifting parts of channels destroys the integrity of the information contained in a token. On the contrast, shifting a whole token with all channels can preserve complete information contained in a token and enable cross-frame interaction.</p><p>However, if we shift most of the tokens in every ViT layer, it damages the spatial modeling ability, and the information contained in these tokens is no longer accessible in the current frame. We therefore use a residual connection between original feature and token shift feature, as illustrated in <ref type="figure" target="#fig_0">Fig.2</ref>. In addition, we assume that shallow layers are more important to model spatial features, so shifting in shallow layers could harm spatial modeling. We thus choose to apply token shift operation only in deeper layers in our implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Token Selection Transformer</head><p>Aggregating information from each frame is a necessary step in building the video representation. A naive solution to aggregate per-frame information is by adding some temporal transformer layers, or by mean pooling as CLIP4Clip <ref type="bibr" target="#b34">[35]</ref>. We argue that aggregation with only the [CLS] token leads to missing important spatial information (i.e. some objects). An alternative way is using all tokens from all frames to aggregate information, but this introduces redundancy problem, leading to the pitfall of some background tokens with irrelevant information dominating the final video representation.</p><p>In this work, we propose the token selection transformer by inserting a token selection module, which aims to select informative tokens per frame, especially those tokens containing salient semantics of objects, for video feature aggregation. As shown in <ref type="figure" target="#fig_2">Fig.4</ref>, top-K informative tokens are selected via the trainable token selection module every frame. The input of the token selection module is a sequence of tokens of each frame I = {p cls , p 0 , p 1 , . . . , p n?1 } ? R (N +1)?C . We first apply an MLP over I for channel dimension reduction and output</p><formula xml:id="formula_0">I ? = {p ? cls , p ? 0 , p ? 1 , . . . , p ? n?1 } ? R (N +1)? C 2 .</formula><p>We then use p ? cls as a global frame feature and concatenate it with each local token</p><formula xml:id="formula_1">p ? i ,p i = [p ? cls , p ? i ] , 0 ? i &lt; N .</formula><p>We finally feed all the concatenated token features to another MLP followed by a Softmax layer to predict the importance scores, which can be formulated as:</p><formula xml:id="formula_2">S = Softmax(MLP(p)) ? R (N +1) .<label>(1)</label></formula><p>We select indices of K most informative tokens based on S, denoting as M ? {0, 1} (N +1)?K , where each column in M is a one-hot (N + 1) dimensional indicator. We extract top-K most informative tokens by:</p><formula xml:id="formula_3">I = M T I,<label>(2)</label></formula><p>After top-K token select on every frame, we input the selected tokens from all frames to a joint spatial-temporal transformer, to learn global spatial-temporal video representation. We also pick the most informative token from each frame as the frame-wise video encoding.</p><p>Differentiable TopK. Until now, both top-K operation and one-hot operation are non-differentiable. To make token selection module differentiable, we employ the perturbed maximum method proposed in <ref type="bibr" target="#b5">[6]</ref>. Specifically, a discrete optimization problem with input S ? R (N +1) (S is the importance score matrix in Eq.1) and optimization variable M ? R (N +1)?K (M is the index indicator matrix in Eq. 2) can be formulated as:</p><formula xml:id="formula_4">F (S) = max M?C ?M, S?, M * (S) = arg max M?C ?M, S?,<label>(3)</label></formula><p>where F (S) represents the top-K selection operation, M * (S) represents the optimal value. Based on Eq.3, we can select top-K informative tokens by F (S). We calculate forward and backward pass following <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Text-Video Matching</head><p>The similarity between the text query and video candidate is computed by integrating the similarity between the query and each video frame. To be specific, given the query representation q and a sequence of frame-wise video representa-</p><formula xml:id="formula_5">tion v = {f 1 , f 2 , .</formula><p>.., f t }, we compute the frame-level similarity as follows:</p><formula xml:id="formula_6">s i = q ? f i ?q? ?f i ? .<label>(4)</label></formula><p>The final text-video matching similarity is defined as the weighted combination of frame-level similarities:</p><formula xml:id="formula_7">s = n i=1 ? i s i ,<label>(5)</label></formula><p>where ? i = exp(?si) n i=1 exp(?si) and ? is a temperature parameter. We set ? as 4 empirically in our experiments.</p><p>Symmetric cross-entropy loss is adopted as our training objective function. For each training step with B text-video pairs, we calculate symmetric crossentropy loss as follows:</p><formula xml:id="formula_8">L t2v t = ? 1 B B i log exp (? ? sim (q i , v i )) B j=1 exp (? ? sim (q i , v j )) ,<label>(6)</label></formula><formula xml:id="formula_9">L v2t t = ? 1 B B i log exp (? ? sim (q i , v i )) B j=1 exp (? ? sim (q j , v i )) ,<label>(7)</label></formula><formula xml:id="formula_10">L = 1 2 (L t2v + L v2t ) ,<label>(8)</label></formula><p>where ? is a trainable scaling parameter and sim (q, v) is calculated using Eq.5. During inference, we calculate the matching score between each text and video based on Eq.5, and return videos with the highest ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>In this section, we carry out text-video retrieval evaluations on multiple benchmark datasets to validate our proposed model TS2-Net. We first ablate the core ingredients of our video encoder, the token shift transformer and the token selection transformer, on the dominant MSR-VTT dataset. We then compare our model with other state-of-the-art models on multiple benchmark datasets quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets. To demonstrate the effectiveness and generalization ability of our model, we conduct evaluations on five popular text-video benchmarks, including MSR-VTT <ref type="bibr" target="#b47">[48]</ref>, VATEX <ref type="bibr" target="#b45">[46]</ref>, LSMDC <ref type="bibr" target="#b40">[41]</ref>, ActivityNet-Caption <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref>, DiDeMo <ref type="bibr" target="#b1">[2]</ref>. All these datasets are collected from different scenarios with various amounts of captions. Videos in different datasets also have different content styles and different lengths.</p><p>? MSR-VTT <ref type="bibr" target="#b47">[48]</ref>   <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b25">26]</ref>, we concatenate all descriptions of a video to retrieval videos with paragraphs.</p><p>Evaluation Metrics. We measure the retrieval performance using standard text-video retrieval metrics: Recall at K (R@K, higher is better), Median Rank (MdR, lower is better) and Mean Rank (MnR, lower is better). R@K calculates the fraction of correct videos among the top K retrieved videos. Similar to previous works <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b11">12]</ref>, we use K=1,5,10 for different datasets. We also sum up all the R@K results as rsum to reflect the overall retrieval performance. MedR calculates the median rank of correct results in the retrieval ranking list and MeanR calculates the mean rank of correct results in the retrieval ranking list. Implementation Details. The layer of GPT, token shift transformer and token selection transformer is 12, 12 and 4, respectively. The dimension of text embedding and frame embedding is 512. We initialize transformer layers in GPT, token shift transformer and token selection transformer with pre-trained weight from CLIP(ViT-B/32) <ref type="bibr" target="#b37">[38]</ref>, using parameters with similar dimension, while other modules are initialized randomly. We choose 4 most informative tokens in MSR-VTT, VATEX, ActivityNet-Caption, DiDeMo, and 1 in LSMDC. We set the max query text length as 32 and max video frame length as 12 in MSR-VTT, VATEX, LSMDC. For ActivityNet-Caption and DiDeMo, we set the max query text length and max video frame length as 64. We train our model with Adam <ref type="bibr" target="#b23">[24]</ref> optimizer and adopt a warmup <ref type="bibr" target="#b22">[23]</ref> setting. We choose a batch size of 128. The learning rate of GPT and token shift transformer is 1e-7 and the learning rate of token selection transformer is 1e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Experiments</head><p>In this section, we evaluate the proposed token shift transformer and token selection transformer under different settings to validate their effectiveness. We conduct ablation experiments with the 1k-A test split on MSR-VTT <ref type="bibr" target="#b47">[48]</ref>. We set our baseline model as the degraded TS2-Net model which removes the token shift and token selection modules from TS2-Net. <ref type="figure">Fig. 5</ref>. The text-video retrieval results of different network architecture. Left: with token shift transformer, our model is able to distinguish 'shake hands', while the baseline model retrieves an incorrect video. Right: with token selection transformer, our model retrieves the correct video, although 'bag' is only shown in small part of video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Green boxes: correct target video; red boxes: incorrect target video</head><p>Ablation of Token Shift Transformer. We first analyze the impact of some factors on the token shift module in Tab.1, including shift layer and shift ratio. Shift layer (in which layers should we insert token shift) and shift ratio (how many tokens should we shift) are two main factors that affect the final retrieval performance. The backbone of our token shift transformer is the 12-layer ViT.</p><p>We thus experiment to insert the token shift module in different layers. As shown in Tab.1, shift operation in deeper layers (i.e. 11-12 layers) brings retrieval performance improvement. But if we shift more layers (i.e. 9-12 layers), it hurts the retrieval performance, and it hurts more if we operate shift in shallower layers (i.e. 1-12 layers). Our explanation is that shallow layers in ViT are more important in modeling spatial information, so shift in shallow layers damages spatial modeling ability. We thus choose to insert the token shift module in the 11-12 layers in the following experiments. In terms of shift ratio, we find that shifting 25% tokens back-and-forth across frames achieves the best retrieval performance. Despite some slight fluctuations, token shift with different ratios achieve better results than the baseline model. The improvement is more obvious especially for R@1. We further conduct experiments to compare our proposed token shift module with other shift operation variants proposed in Shift-ViT <ref type="bibr" target="#b51">[52]</ref>. As shown in Tab.2, our proposed token shift module outperforms all other shift operation variants. This is because our token shift operation can preserve the integrity of the token feature, posing minor impact on the spatial modeling ability. We visualize the retrieval results from the baseline model and the model with token shift transformer in <ref type="figure">Fig.5(a)</ref>. With token shift transformer, the model is able to capture subtle movement such as 'shake hand'. Ablation of Token Selection Transformer. The token selection transformer follows the token shift transformer to select the most informative tokens for the next transformer propagation. We conduct experiment to verify what proportion of tokens is beneficial to the final retrieval in Tab.3. As can be observed, select-  ing fewer tokens per frame tends to achieve better performance than selecting more. For example, the R@1 performance decreases from 47.0 to 45.8 while the number of selected tokens increases from 2 to 50. We consider that fewer informative tokens are sufficient to preserve the salient spatial information, while adding more tokens may bring redundancy problem. Although random selection also improves the performance slightly, it can not beat the proposed learnable token selection module. In <ref type="figure">Fig.5(b)</ref>, we show a retrieval case from the baseline model and the model with token selection transformer. With token selection transformer, the model is able to capture the small object 'bag' in video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-art Models</head><p>MSR-VTT-1kA. We compare our proposed TS2-Net with other state-of-theart methods on five benchmarks. Tab.4 presents the results on MSR-VTT-1kA test set. Our model outperforms previous methods across different evaluation metrics. With token shift transformer and token selection transformer, our model is able to capture subtle motion and salient objects, and thus our final video representation contains rich semantics. Compared with video-to-text retrieval, the gain on text-to-video retrieval is more significant. We consider it is because the proposed token shift and token selection modules enhance the video encoder, while a relative simple text encoder is adopted. Other Benchmarks. Tab.5 presents text-to-video retrieval results on VATEX, LSMDC, ActivityNet-Caption and DiDeMo. Results on these datasets demonstrate the generalization and robustness of our proposed model. We can observe that our model achieves consistent improvements across different datasets, which demonstrates that it is beneficial to encode spatial and temporal features simultaneously by our token shift and token selection. Note that our performance surpasses QB-Norm <ref type="bibr" target="#b6">[7]</ref> on LSMDC and VATEX even without inverted softmax, as shown in Tab.5. More detailed analysis will be provided in supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Results</head><p>We visualize some retrieval examples from the MSR-VTT testing set for text-tovideo retrieval in <ref type="figure">Fig.6</ref>. In the top left example, our model is able to distinguish 'hand rubbing' (in the middle picture) during a guitar-playing scene. The bottom right example shows our model can distinguish 'computer battery' from 'computer'. In the bottom left example, our model retrieves the correct video which contains all actions and objects expressed in the text query, especially <ref type="figure">Fig. 6</ref>. Visualization of text-video retrieval examples. We sorted results based on its similarity scores. Green: ground truth; Red: incorrect the small object 'microphone' and tiny movement 'talking'. In the bottom right example, our model retrievals the correct result although 'rotating' is a periodic movement and is hard to spot. We also select a subset from the MSR-VTT-1kA test set. Queries in this subset are selected based on their corresponding video's visual appearance, where objects mentioned in query are shown in a small part of video and movements mentioned in query is slight. Such as 'little pet shop cat getting a bath and washed with little brush', 'man talks in front of a green bicycle', 'dog is drinking milk with baby nibble bottle', 'a golf player is trying to hit the ball into the pit'. Since such cases account for a small proportion, so the total number of this subset is 103. During inference, we calculate similarity between queries in subset with videos in whole test set. We compare our model with another strong baseline on this subset. Our model achieves 79.6 on R@1 metric, while CLIP4Clip <ref type="bibr" target="#b34">[35]</ref> only achieves 39.8. There is a significant margin and this verifies the effectiveness of TS2-Net in handling local subtle movements and local small entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose Token Shift and Selection Network (TS2-Net), a noval transformer architecture with token shift and selection modules, which aims to further improve the video encoder for better video representation. A token shift transformer is used to capture subtle movements, followed by a token selection transformer to enhance salient object modeling ability. Superior experimental results show our proposed TS2-Net outperforms start-of-the-art methods on five text-video retrieval benchmarks, including MSR-VTT, VATEX, LSMDC, ActivityNet-Caption and DiDeMo. A Inverted Softmax.</p><p>The hubness phenomenon <ref type="bibr" target="#b29">[30]</ref> is that a data point occurs among the k nearest neighbors of other data points. Dual softmax loss (DSL) was mentioned in CAMoE <ref type="bibr" target="#b11">[12]</ref>, which adopts a inverted softmax <ref type="bibr" target="#b41">[42]</ref>. QB-Norm <ref type="bibr" target="#b6">[7]</ref> proposes a querybank normalization with dynamic inverted softmax (DIS) to deal with hubness problem. CLIP2TV <ref type="bibr" target="#b21">[22]</ref> also reports its results with inverted softmax. We compare their results with basic inverted softmax during inference in Tab.6. Our results again surpass all other methods with significant improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Evaluation Summary on Different Benchmarks</head><p>We compared our model to other state-of-the-art methods on different video retrieval benchmark datasets in the main paper. <ref type="table">Table 7</ref> summarizes the performance comparison between our proposed model TS2-Net and the previous best model on five different benchmark datasets. Among all these datasets, VATEX <ref type="bibr" target="#b45">[46]</ref> and MSR-VTT <ref type="bibr" target="#b47">[48]</ref> contain standard captions with average length of 15 words and 8 words respectively. LSMDC <ref type="bibr" target="#b40">[41]</ref> contains videos in the movie domain. There is no movie overlap between the training and test set. So it can verify the generalization ability of a model. ActivityNet-Caption <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> and DiDeMo <ref type="bibr" target="#b1">[2]</ref> offer paragraph-video retrieval, which means that the query text involves multi sentences and the video duration is long. Therefore, the query text in these two datasets contains more semantic information. We show some examples of these datasets in <ref type="figure" target="#fig_4">Fig.7</ref>. Our model TS2-Net consistently maintains the state-of-the-art performance on all benchmark datasets with very different characteristics, which demonstrates that our model TS2-Net has decent generalization ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C More Qualitative Results</head><p>We provide more qualitative results in <ref type="figure" target="#fig_5">Fig.8</ref>. In the top left example, our model is able to differentiate the horses that are 'having fun' from the horses that are stationary. In the top right example, our model can capture the small object and correctly retrieve the video with 'thought bubbles'. Surprisingly, with token shift and token selection module, our model is able to distinguish some adjective <ref type="table">Table 7</ref>. Text-to-Video retrieval results on five benchmarks. We select the previous best performance on each dataset for comparison. We show some failure cases as well in <ref type="figure" target="#fig_6">Fig.9</ref>, where our model fails to rank the groundtruth video at the top. However, we could argue for these failure cases and consider that our model may actually retrieve the more relevant video. For example, in the left case, the video retrieved by our model (in the red box) seems to be more relevant to the query text, since both 'cup' and 'talking' can be seen in our results, while the 'talking' can not be seen in the ground truth.</p><p>Based on further analysis, we consider that there are also many vague and general annotations in the datasets, such as the example shown in the right case in <ref type="figure" target="#fig_6">Fig.9</ref>. Such query annotations account for 1-2% of the dataset. We believe our model has a potential to gain in all metrics if such cases get fixed with more discriminative annotations.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of the proposed TS2-Net model for text-video retrieval, which consists of three key components: the text encoder, the video encoder, and the text-video matching. The video encoder is composed of the Token Shift Transformer and Token Selection Transformer. ('Repre' is short for 'Representation')</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Illustration of different types of Shift operation and our proposed Token Temporal Shift. 'T, P, C' refer to video temporal dimension, video token, and feature channel respectively. Each vertical cube group represents a spatial-temporal video token. Cubes with dash line represent tensor truncated, and white cubes represent tensor padding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Illustration of Token Selection Module. Top-K informative tokens are selected per frame from original spatial-temporal tokens for following feature aggregation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(ViT16) 49.4 75.6 85.3 2.0 13.5 46.6 75.9 84.9 2.0 8.9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of Text-Video retrieval pairs from different benchmark datasets</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of more text-video retrieval examples. We rank the retrieval results based on their similarity scores. Green boxed: the correctly retrieved groundtruth video; Red boxed: incorrectly retrieved videos</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Visualization of some failure text-video retrieval examples. We rank the retrieval results based on their similarity scores. Green boxed: the correctly retrieved groundtruth video; Red boxed: the incorrectly retrieved video by our model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>LSMDC<ref type="bibr" target="#b40">[41]</ref> contains 118,081 video clips, which are extracted from 202 movies. Each video clip has one caption. There are about 100k videos in the training set, 7,408 videos in the validation set and 1,000 videos in the test set. Especially, videos in the test set are from movies disjoint with the training and validation set.</figDesc><table /><note>contains 10,000 video clips with 20 captions per video. Our experiments follow 1k-A split protocol used in [21,32,36], where the training set has 9,000 videos with its corresponding captions and test set has 1,000 text-video pairs.? VATEX[46] contains 34,991 video clips with several captions per video. We follow HGR[11] split protocol. There are 25,991 videos in the training set, 1,500 videos in the validation set and 1,500 videos in the test set. ?? ActivityNet-Caption[18,25] contains 20,000 YouTube videos. Following the same setting as in [35,51,21], we regard it as a paragraph-video retrieval by concatenate all descriptions of a video. We train our model on train split and test our model on val1 split.? DiDeMo[2] contains over 10k videos. There are 8,395 videos in the training set, 1,065 videos in the validation set and 1,004 videos in the test set. Fol- lowing the same setting as in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Performance comparison with different parameter settings of the Token Shift Transformer on MSR-VTT-1k-A test split Performance comparison between other shift operation variants and our proposed token shift module on MSR-VTT-1k-A test split Channel Shift[52] 45.1 73.8 83.5 13.9 44.7 73.3 82.2 9.8 402.6 [CLS] Channel Shift[52] 45.8 74.3 83.0 13.6 44.7 72.9 82.5 9.8 403.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Text =? Video</cell><cell>Video =? Text</cell></row><row><cell>Method</cell><cell cols="4">Layers Ratio R@1 R@5 R@10 MnR R@1 R@5 R@10 MnR rsum</cell></row><row><cell>Baseline</cell><cell>-</cell><cell>-</cell><cell cols="2">45.4 74.3 82.7 13.6 44.5 72.3 82.3 9.8 401.5</cell></row><row><cell cols="5">w/ Token Shift 1-12 25% 42.8 71.2 80.9 14.4 43.2 70.3 80.4 11.3 388.8</cell></row><row><cell cols="5">w/ Token Shift 3-12 25% 44.1 71.0 81.8 14.5 43.5 71.2 81.8 10.8 393.4</cell></row><row><cell cols="5">w/ Token Shift 5-12 25% 44.4 71.9 81.6 14.6 44.8 72.0 80.6 11.3 395.3</cell></row><row><cell cols="5">w/ Token Shift 7-12 25% 44.1 72.3 82.9 13.6 43.8 72.3 82.1 10.3 397.5</cell></row><row><cell cols="5">w/ Token Shift 9-12 25% 45.2 73.8 83.1 13.4 45.3 72.1 82.5 9.5 402</cell></row><row><cell cols="5">w/ Token Shift 11-12 12.5% 46.0 73.3 82.2 13.8 45.8 72.9 83.0 9.5 403.2</cell></row><row><cell cols="5">w/ Token Shift 11-12 50% 46.1 74.5 83.3 13.3 45.6 72.9 82.2 9.5 404.6</cell></row><row><cell cols="5">w/ Token Shift 11-12 25% 46.2 73.9 83.8 13.0 45.6 73.5 83.2 9.3 406.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Text =? Video</cell><cell>Video =? Text</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">R@1 R@5 R@10 MnR R@1 R@5 R@10 MnR rsum</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell cols="2">45.4 74.3 82.7 13.6 44.5 72.3 82.3 9.8 401.5</cell></row><row><cell cols="2">Channel Shift[52]</cell><cell></cell><cell cols="2">45.6 73.6 83.1 13.7 45.0 73.2 82.7 9.7 403.2</cell></row><row><cell>[VIS] Token Shift</cell><cell></cell><cell></cell><cell cols="2">46.2 73.9 83.8 13.0 45.6 73.5 83.2 9.3 406.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison results with different settings of Token Selection Transformer</figDesc><table><row><cell></cell><cell></cell><cell>Text =? Video</cell><cell>Video =? Text</cell></row><row><cell>Method</cell><cell cols="3">top-K R@1 R@5 R@10 MnR R@1 R@5 R@10 MnR rsum</cell></row><row><cell>Token Shift</cell><cell>1</cell><cell cols="2">46.2 73.9 83.8 13.0 45.6 73.5 83.2 9.3 406.2</cell></row><row><cell>w/ all token</cell><cell cols="3">50 45.8 73.5 83.4 13.5 44.7 73.1 82.4 9.4 402.9</cell></row><row><cell cols="2">w/ Random select 4</cell><cell cols="2">46.4 73.9 83.5 13.1 45.1 73.5 82.1 9.5 404.5</cell></row><row><cell>w/ Select token</cell><cell>2</cell><cell cols="2">47.0 74.2 83.6 13.1 45.6 74.0 83.5 9.3 407.9</cell></row><row><cell>w/ Select token</cell><cell>6</cell><cell cols="2">46.6 74.4 84.3 13.2 44.5 73.8 83.2 9.2 406.8</cell></row><row><cell>w/ Select token</cell><cell>8</cell><cell cols="2">46.4 73.9 83.5 13.2 45.0 74.1 83.9 9.2 406.8</cell></row><row><cell>TS2-Net</cell><cell cols="3">4 47.0 74.5 83.8 13.0 45.3 74.1 83.7 9.2 408.4</cell></row><row><cell cols="4">Table 4. Retrieval results on MSR-VTT-1kA. Other SOTA methods are adopted as</cell></row><row><cell cols="4">comparisons. Note that CLIP2TV uses patch size of 16?16, so we use TS2-Net(ViT16)</cell></row><row><cell cols="4">for fair comparison. All results in this table do not use inverted softmax</cell></row><row><cell></cell><cell></cell><cell>Text =? Video</cell><cell>Video =? Text</cell></row><row><cell>Method</cell><cell cols="3">R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR</cell></row><row><cell>CE[32]</cell><cell cols="3">20.9 48.8 62.4 6.0 28.2 20.6 50.3 64.0 5.3 25.1</cell></row><row><cell>TACo[49]</cell><cell cols="2">26.7 54.5 68.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Text-to-Video retrieval results on VATEX, LSMDC, ActivityNet and DiDeMo. QB-Norm uses dynamic inverted softmax during inference, while other methods report results without inverted softmax</figDesc><table><row><cell></cell><cell>VATEX</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSMDC</cell></row><row><cell>Method</cell><cell cols="4">R@1 R@5 R@10 MdR MeanR</cell><cell>Method</cell><cell>R@1 R@5 R@10 MdR MeanR</cell></row><row><cell>Dual Enc.[15]</cell><cell cols="3">31.1 67.5 78.9 3.0</cell><cell>-</cell><cell cols="2">JSFusion[50] 9.1 21.2 34.1 36.0</cell><cell>-</cell></row><row><cell>HGR[11]</cell><cell cols="3">35.1 73.5 83.5 2.0</cell><cell>-</cell><cell>CE[32]</cell><cell>11.2 26.9 34.9 25.3</cell><cell>-</cell></row><row><cell>CLIP[38]</cell><cell cols="4">39.7 72.3 82.2 2.0 12.8</cell><cell>Frozen[4]</cell><cell>15.0 30.8 39.8 20.0</cell><cell>-</cell></row><row><cell>CLIP4Clip[35]</cell><cell cols="3">55.9 89.2 95.0 1.0</cell><cell>3.9</cell><cell cols="2">CLIP4Clip[35] 22.6 41.0 49.1 11.0 61.0</cell></row><row><cell>QB-Norm*[7]</cell><cell cols="3">58.8 88.3 93.8 1.0</cell><cell>-</cell><cell cols="2">QB-Norm*[7] 22.4 40.1 49.5 11.0</cell><cell>-</cell></row><row><cell>CLIP2Video[19]</cell><cell cols="3">57.3 90.0 95.5 1.0</cell><cell>3.6</cell><cell cols="2">CAMoE[12] 22.5 42.6 50.9</cell><cell>-</cell><cell>56.5</cell></row><row><cell>TS2-Net</cell><cell cols="3">59.1 90.0 95.2 1.0</cell><cell>3.5</cell><cell cols="2">TS2-Net 23.4 42.3 50.9 9.0 56.9</cell></row><row><cell></cell><cell>ActivityNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DiDeMo</cell></row><row><cell>Method</cell><cell cols="4">R@1 R@5 R@10 MdR MeanR</cell><cell>Method</cell><cell>R@1 R@5 R@10 MdR MeanR</cell></row><row><cell>CE[32]</cell><cell cols="4">20.5 47.7 63.9 6.0 23.1</cell><cell cols="2">ClipBERT[26] 20.4 48.0 60.8 6.0</cell><cell>-</cell></row><row><cell>ClipBERT[26]</cell><cell cols="3">21.3 49.0 63.5 6.0</cell><cell>-</cell><cell cols="2">TT-CE[14] 21.1 47.3 61.1 6.3</cell><cell>-</cell></row><row><cell cols="2">MMT-Pretrained[21] 28.7 61.4</cell><cell>-</cell><cell cols="2">3.3 16.0</cell><cell>Frozen[4]</cell><cell>31.0 59.8 72.4 3.0</cell><cell>-</cell></row><row><cell>CLIP4Clip[35]</cell><cell>40.5 73.4</cell><cell>-</cell><cell>2.0</cell><cell>7.5</cell><cell cols="2">CLIP4Clip[35] 42.5 70.2 80.6 2.0 17.5</cell></row><row><cell>TS2-Net</cell><cell cols="3">41.0 73.6 84.5 2.0</cell><cell>8.4</cell><cell cols="2">TS2-Net 41.8 71.6 82.0 2.0 14.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Text-to-Video retrieval results with Inverted Softmax</figDesc><table><row><cell></cell><cell>MSRVTT-1kA</cell><cell></cell><cell>DiDeMo</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">R@1 R@5 R@10 MdR MeanR</cell><cell cols="2">Method R@1 R@5 R@10 MdR MeanR</cell></row><row><cell>QB-Norm[7]</cell><cell>47.2 73.0 83.0 2.0</cell><cell>-</cell><cell>QB-Norm[7] 43.5 71.4 80.9 2.0</cell><cell>-</cell></row><row><cell>CAMoE[12]</cell><cell cols="2">47.3 74.2 84.5 2.0 11.9</cell><cell cols="2">CAMoE[12] 43.8 71.4 79.9 2.0 16.3</cell></row><row><cell>TS2-Net</cell><cell cols="2">51.1 76.9 85.6 1.0 11.7</cell><cell cols="2">TS2-Net 47.4 74.1 82.4 2.0 12.9</cell></row><row><cell>CLIP2TV[22]</cell><cell cols="2">52.9 78.5 86.5 1.0 12.8</cell><cell></cell><cell></cell></row><row><cell cols="3">TS2-Net(ViT16) 54.0 79.3 87.4 1.0 11.7</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>words. For example, our model correctly retrieves the video with 'mental bowl' rather than 'glass bowl' (and 'overweight people' rather than normal people) in the bottom examples.</figDesc><table><row><cell>Dataset</cell><cell>Method</cell><cell cols="2">R@1 R@5 R@10 MdR rsum</cell></row><row><cell>MSR-VTT[48]</cell><cell cols="2">CLIP2Video[19] 45.6 72.6 81.7 TS2-Net(Ours) 47.0 74.5 83.8</cell><cell>2.0 2.0 205.3 199.9</cell></row><row><cell>VATEX[46]</cell><cell cols="2">CLIP2Video[19] 57.3 90.0 95.5 TS2-Net(Ours) 59.1 90.0 95.2</cell><cell>1.0 1.0 244.3 242.8</cell></row><row><cell>LSMDC[41]</cell><cell cols="2">CAMoE[12] TS2-Net(Ours) 23.4 42.3 50.9 22.5 42.6 50.9</cell><cell>-9.0 116.6 116.0</cell></row><row><cell>DiDeMo[2]</cell><cell cols="2">CLIP4Clip[35] TS2-Net(Ours) 41.8 71.6 82.0 42.5 70.2 80.6</cell><cell>2.0 2.0 195.4 193.3</cell></row><row><cell>ActivityNet[18,25]</cell><cell cols="2">CLIP4Clip[35] TS2-Net(Ours) 41.0 73.6 84.5 40.5 73.4 -</cell><cell>2.0 2.0 199.1 -</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Perturbation techniques in online learning and optimization. Perturbations, Optimization, and Statistics p</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tewari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">223</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6836" to="6846" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning with differentiable pertubed optimizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Berthet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Teboul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Vert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9508" to="9519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12777</idno>
		<title level="m">Cross modal retrieval with querybank normalisation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Space-time mixing attention for video transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Perez Rua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fine-grained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improving video-text retrieval by multi-stream corpus alignment and dual softmax loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04290</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Differentiable patch selection for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2351" to="2360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. ICLR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Activitynet: A largescale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.11097</idno>
		<title level="m">Clip2video: Mastering video-text retrieval via image clip</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05610</idno>
		<title level="m">Clip2tv: An empirical study on transformer-based methods for video-text retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization 3rd int</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. for Learning Representations</title>
		<meeting><address><addrLine>San</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dense-captioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11336" to="11344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hero: Hierarchical encoder for video+ language omni-representation pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A strong and robust baseline for text-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hit: Hierarchical transformer with momentum contrast for video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11915" to="11925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13230</idno>
		<title level="m">Video swin transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pre-training model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<title level="m">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamicvit: Efficient vision transformers with dynamic token sparsification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="94" to="120" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Y</forename><surname>Hammerla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<title level="m">Vl-bert: Pre-training of generic visual-linguistic representations. ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Efficient video transformers with spatial-temporal token selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">G</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11591</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">T2vlad: global-local sequence alignment for textvideo retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Vatex: A large-scale, high-quality multilingual dataset for video-and-language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4581" to="4591" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Taco: Token-aware cascade contrastive learning for video-text alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11562" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Token shift transformer for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="917" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
