<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Frustratingly Simple Domain Generalization via Image Stylization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Somavarapu</surname></persName>
							<email>nsomavarapu3@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<postCode>30332</postCode>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
							<email>cyma@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<postCode>30332</postCode>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<email>zkira@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology Atlanta</orgName>
								<address>
									<postCode>30332</postCode>
									<region>GA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Frustratingly Simple Domain Generalization via Image Stylization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNNs) show impressive performance in the standard classification setting where training and testing data are drawn i.i.d. from a given domain. However, CNNs do not readily generalize to new domains with different statistics, a setting that is simple for humans. In this work, we address the Domain Generalization problem, where the classifier must generalize to an unknown target domain. Inspired by recent works that have shown a difference in biases between CNNs and humans, we demonstrate an extremely simple yet effective method, namely correcting this bias by augmenting the dataset with stylized images. In contrast with existing stylization works, which use external data sources such as art, we further introduce a method that is entirely in-domain using no such extra sources of data. We provide a detailed analysis as to the mechanism by which the method works, verifying our claim that it changes the shape/texture bias, and demonstrate results surpassing or comparable to the state of the arts that utilize much more complex methods 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Humans demonstrate an amazing ability to respond robustly to a number of different settings. For example, humans can generally deal with moderate changes in illumination, background or color when identifying an object. One of the central goals of vision research is to build methods which share this ability of humans. On the other hand, a significant amount of research in machine learning has gone into fully supervised methods under the i.i.d setting. Although this has led to remarkable results, supervised methods still demonstrate frailty. Recht et al. <ref type="bibr" target="#b31">[32]</ref> show that computer vision models trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> and CIFAR10 <ref type="bibr" target="#b21">[22]</ref> do not generalize to data collected through a nearly identical process. Similarly, Hendrycks and Dietterich <ref type="bibr" target="#b14">[15]</ref> show that modern convolutional neural network (CNN) architectures display poor robustness to common perturbations that humans have no problem dealing with. Partially accounting for this, the biases of CNNs are significantly different than that of humans, which leads to a discrepancy in how decisions are made <ref type="bibr" target="#b13">[14]</ref>.</p><p>Domain Adaptation (DA) is an area of work that attempts to deal with this issue by making use of unlabeled target data along with data from one or more source domains. A number of distribution alignment methods have been proposed with good empirical performance <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28]</ref>. A more general version of this problem is Domain Generalization (DG) where no (even unlabeled) target data is available. This setting is scientifically interesting and very practically motivated, since ideally a model should not need to be retrained in new domains when deployed in the world. Due to the lack of target domain knowledge, it is key to consider the biases of the model to ensure features are robust across domains are used. A line of previous work has approached this through the use of distribution matching techniques across the source domains via adversarial distribution alignment with MMD <ref type="bibr" target="#b25">[26]</ref> or by minimizing domain dissimilarity <ref type="bibr" target="#b30">[31]</ref>.</p><p>In this paper, in contrast to distribution matching, we explicitly consider model biases which are more generalizable by decreasing reliance on irrelevant features. Specifically, we hypothesize that implicit variability within the source domains themselves can be leveraged to train models that focus more on robust features (e.g. shape), as opposed to less relevant features (e.g., textural information). Such models would likely be more robust when generalizing out of domain <ref type="bibr" target="#b29">[30]</ref>. To this end, we first introduce a simple yet effective probabilistic data augmentation method which varies the texture of images by stylizing them in the style of paintings. Intuitively, by introducing a variance in the local information, while maintain the same class label, we hope that the model relies more heavily on the more general feature of shape instead of the less general feature of texture. This shift in bias, as discussed previously, is crucial in the DG setting. Combining this augmentation with batched source training, we show that this comparatively simple method surpasses state of the arts or achieves comparable performance on the common DG datasets of PACS <ref type="bibr" target="#b22">[23]</ref>, VLCS <ref type="bibr" target="#b8">[9]</ref> and Office-Home <ref type="bibr" target="#b37">[38]</ref>. We additionally introduce a (pairwise) Inter-Source stylization method, which uses only in-domain information and eliminates the requirement of using an external source for stylization. We also empirically show that this method consistently produces improvement across different backbone networks. Finally, we provide several quantitative analyses to show how the proposed method impacts the model's shape and texture bias. The analysis leads to several interesting findings, for example, (1) shape accuracy is increased but texture accuracy is largely unchanged, (2) surprisingly, Inter-Source stylization achieves similar performance to stylization with external painting dataset, indicating that there is enough cross-source variability to leverage through stylization.</p><p>Our contributions are summarized below:</p><p>? We introduce a simple transformation based on stylization which uses only in-domain data and surpasses or is competitive with current state of the arts for domain generalization. ? We provide insights into the inner workings of our method by generating a cue-conflict dataset and comparing the shape bias, shape accuracy, and texture accuracy of models trained in different ways. ? We observed that a model trained with stylization increases its predictiveness of shape but interestingly still maintains predictiveness of texture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Domain Generalization</head><p>The goal of domain generalization (DG) is to learn a representation which generalizes over multiple unknown data distributions. DG captures real world scenarios by providing a number of source domains that can be used for training and an unseen target domain that the model is tested on. A large amount of previous work in this area relies on methods that design models which reduce reliance on domain specific artifacts. Some of these methods include low-rank parameterized models <ref type="bibr" target="#b22">[23]</ref>, domain specific classifiers which are fused by an aggregation module <ref type="bibr" target="#b6">[7]</ref>, unsupervised latent discovery of domains <ref type="bibr" target="#b28">[29]</ref> and meta-learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b40">41]</ref>. Other methods rely on augmentation of source data with the hope of capturing the target domain in either image space <ref type="bibr" target="#b35">[36]</ref> or in an embedding space <ref type="bibr" target="#b38">[39]</ref>. Still other methods employ self-supervised losses which are paired with the standard classification loss, such as <ref type="bibr" target="#b1">[2]</ref> which uses the jigsaw puzzle task. Our method just relies on a simple augmentation during source training, but focuses on a specific form of augmentation (stylization) that can use inter-source information in order to implicitly improve invariance to irrelevant features. We note that this method is extremely simple, compared to more sophisticated methods utilizing meta-learning, classifiers for specific domains, unsupervised domain discovery, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Network Bias</head><p>Since in DG there is no target data to rely on, we propose that the implicit biases in the model are important since they must tend towards those that are more generalizable to unknown domains, even though only source domains are available. A large body of work has shown that CNNs likely rely on significantly different features than humans for classification. Early on Gatys et al. <ref type="bibr" target="#b11">[12]</ref>, while designing a method for texture synthesis, showed that a linear classifier on top of the texture representations of VGG19 <ref type="bibr" target="#b34">[35]</ref> achieves nearly the same classification accuracy as a VGG19 network directly trained on the task. Later Jo and Bengio <ref type="bibr" target="#b19">[20]</ref> showed that networks latch on to statistical regularities in the Fourier statistics which are independent of the actual content of the image.</p><p>A line of recent work has started to consider the biases of CNNs and effects that this has, Wang et al. <ref type="bibr" target="#b39">[40]</ref> seek to penalize the predictive power of the early layers of the network via an adversarial loss. Geirhos et al. <ref type="bibr" target="#b13">[14]</ref> showed that, while humans mostly rely on shape for classification, common CNNs rely far more on texture and low-level information. They introduce stylization to study the comparative reliance of humans and CNNs on shape and texture. Specifically, stylization (or style transfer) methods seek to match the content of a particular image to the style of a separate image. The pioneering work of Gatys et al. <ref type="bibr" target="#b12">[13]</ref> uses the same Gram Matrix based texture representation from Gatys et al. <ref type="bibr" target="#b11">[12]</ref> paired with activations from different layers of a CNN to match the style and content of two images through an iterative optimization process. Using this idea, Geirhos et al. <ref type="bibr" target="#b13">[14]</ref> introduce stylized ImageNet, a dataset with ImageNet <ref type="bibr" target="#b4">[5]</ref> images in the style of common paintings. A model is then trained on this dataset and fine-tuned on ImageNet to achieve improved ImageNet validation performance. In addition it is shown that this process increases the shape bias of the model.</p><p>In this paper, we extend these works by first introducing stylization as a probabilistic augmentation. Existing works have also used stylization for augmentation <ref type="bibr" target="#b18">[19]</ref> but have not focused on domain generalization, and the biases that are introduced as a result, and have also used external sources as the style sources hence using extra data. Importantly, we show that the same results can be attained by only using inter-source stylization in the domain generalization setting, i.e., we do not rely on the addition of the extra painting dataset for a source of style (see Sec. 4). We hypothesize that variability in the sources allows the model to learn representations that are more robust to irrelevant features, specifically increasing its shape bias and reducing its texture bias, leading to improved generalization. We further provide extensive analysis showing this, specifically in the context of shape/texture bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Image Stylization for Domain Generalization</head><p>In this paper, we tackle the Domain Generalization (DG) problem, where there are multiple source datasets and the goal is to learn models generalizable to unknown target datasets, where there is some domain shift. We denote the source distributions {D S1 , ..., D Sn } and the target distribution D T . Note that in most of the DG datasets n = 3. With a slight abuse of notation we denote samples drawn randomly from one of the source domains as x, y ? {D S1 , ..., D Sn }. We also use f (x; ?) : X ? R c to denote a CNN with softmax output, where c denotes the number of classes. Finally we denote the standard cross entropy classification loss as L c (f (x; ?), y).</p><p>The key intuition for our method stems from the perspective of increasing the model bias towards features that are more generalizable, by decreasing reliance on irrelevant features even if they perform well on the (validation set of) source domains. Specifically, we hypothesize that having access to multiple source domains or even sufficient variation within a single domain can be leveraged in order to infer types of variability irrelevant to generalization (e.g., texture bias) to further encourage relevant features (e.g., shape bias). Similar to data augmentation methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17]</ref>, we can use this to learn models robust to such variations, but in a manner that does not requiring specifying all of the possible transformations. In summary, we seek to expose the network to a variety of irrelevant features such as varying textures, which is implicitly available in the multi-source data, so as to make the network more robust when applied out of domain. A key question is how to leverage such inter-source variability implicitly available in the data. We propose that stylization is one such method to do this; since each domain contains its own set of irrelevant features, transforming the content of one source domain using the style of other source domains presents a simple method for utilizing multiple variations implicitly available in the data and which do not destroy the semantic meaning (since content is preserved).</p><p>In the next section, we introduce probabilistic stylization in order to implement this idea, and show that unlike traditional stylization (which uses external data sources such as art) we can apply inter-source stylization to the same effect as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Probabilistic Stylization</head><p>Here we define a simple transformation which we denote S Q,p (x) : R N xN ? R N xN . This transformation replaces the original image x with a stylized version of itself with probability p, where the style is drawn uniformly at random from the style set Q. The transformation is implemented using AdaIN <ref type="bibr" target="#b17">[18]</ref> which employs a stylization network to achieve fast stylization to arbitrary styles.</p><p>In the initial setting, we follow many of the stylization works and select the set Q to be the painterby-numbers dataset <ref type="bibr" target="#b1">2</ref> . Some examples of this stylization for content images from the PACS dataset can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>. We subsequently relax this to not include external sources.</p><p>Our entire training process can be described with the following equation (depicted in <ref type="figure" target="#fig_0">Figure 1</ref>). Intuitively, this shows the standard classification training process with random application of stylization.</p><formula xml:id="formula_0">min ? E x,y?D S L c (f (S Q,p (x); ?), y)<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Inter-Source Stylization</head><p>Leveraging the painter-by-numbers dataset as the source of stylization introduces an additional source of information into the training process even if does not contain labels, making it difficult to compare under the standard DG setting. To overcome this, we propose a method which simply uses the other available sources for stylization. Surprisingly, as described in Section 5, this achieves similar performance to stylization with the painting dataset, indicating that there is enough cross-source variability to leverage through stylization. This amounts to modifying the set Q in our stylization transformation, Q = {D S1 , ..., D Sn }/D c , where D c denotes the domain of the image currently under stylization. Taking this to the extreme we also experiment with intra-source stylization where stylization happens only within a single domain, i.e. Q = D c . For example in this setting a image from the photo domain could only be stylized as other photo images. Some examples of PACS content images generated through the inter-source stylization process can be seen in <ref type="figure" target="#fig_1">Figure 2</ref>. Images stylized through inter-source stylization have more visual variability than those stylized solely to paintings, however, both retain important semantic attributes. The training process for inter-source stylization then becomes optimizing the following objective:</p><formula xml:id="formula_1">min ? E x,y?{D S 1 ,...,D Sn }/Dc [L c (f (S D S ,p (x); ?), y)]<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We benchmark our method over a number of common DG datasets: PACS <ref type="bibr" target="#b22">[23]</ref>, VLCS <ref type="bibr" target="#b8">[9]</ref>, and Office-Home <ref type="bibr" target="#b37">[38]</ref> datasets. First, we validate the proposed method on the PACS dataset which spans four distinct categories: Photo, Art, Cartoon and Sketch. We then evaluate our method on the VLCS <ref type="bibr" target="#b8">[9]</ref> dataset which contains only photos, but from different datasets. The PACS dataset contains 9991 images while the VLCS dataset contains 10,729 images. Both the PACS and VLCS datasets contain a small number of classes, with 7 and 5 respectively. Thus, to demonstrate the effectiveness of our method on relatively larger datasets, we test on the Office-Home dataset which contains 65 classes and 15,500 images. In each setting, we compare with the state of the arts, and our method significantly surpasses or is comparable to much more complex methods. We further provide some analysis that indicates that the network became more shape accurate, slightly less texture accurate and became more biased to shape through this process; see Sec. 5 for more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">PACS Dataset</head><p>We adopt the testing procedure detailed by <ref type="bibr">Li</ref>  Training detail. Optimization is performed via Stochastic Gradient Descent (SGD) with a learning rate of 0.001 and momentum of 0.9, a weight decay of 0.0005, and batch size of 128. The model is trained for 80 epochs with a reduction in the learning rate at epoch 60. The stylization probability and stylizing strength is set to 10% and 100%, respectively. Similar to prior work we employ common data augmentations of horizontal flipping with 50% probability, random cropping retaining 80% ? 100% of the image and random color jitter. The hyper-parameters and the model evaluated on the test domains are chosen via the source validation set accuracy of all sources batched together averaged over all domains. We employ Resnet18 and AlexNet models pre-trained on ImageNet for all experiments, following the methodology introduced in Li et al. <ref type="bibr" target="#b22">[23]</ref> and used in contemporary work.</p><p>We report average accuracy over three independent runs of the method in <ref type="table" target="#tab_1">Table 1</ref>  <ref type="bibr" target="#b2">3</ref> . We can see that our method achieves state of the art results in all four domains and it is interesting to note that our method has the largest performance gain in the Sketch setting. Here shape information is the most important, as features based on local textural information will likely not generalize. In Sec. 5, we provide further analysis as to the relationship of model invariances and performance in the Sketch domain supporting this view of generalization. We can also see that the differences between stylization within domain and stylization with the painter-by-numbers dataset is negligible. On the other hand, lack of a large amount of inter-source variability affects the performance of intra-source stylization, although it still contains enough variability to significantly benefit generalization. The method is applicable across different models as can be seen from <ref type="table" target="#tab_1">Table 1</ref>. Both AlexNet and ResNet18 benefit from the introduction of stylized augmentation indicating that benefits of proper model bias are not model specific.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">VLCS Dataset</head><p>VLCS is another widely used DG dataset which is collected from four classic datasets, PASCAL (VOC) <ref type="bibr" target="#b7">[8]</ref>, LabelME <ref type="bibr" target="#b32">[33]</ref>, Caltech <ref type="bibr" target="#b9">[10]</ref>, and SUN <ref type="bibr" target="#b3">[4]</ref>. Each domain in this setting contains the five classes bird, chair, car, dog, person and as before three domains are chosen for training with the held out domain used for testing. In this setting, we use AlexNet <ref type="bibr" target="#b20">[21]</ref> for comparison purposes to prior work. The same training parameters are used as in the PACS setting except a learning rate of 0.0005 and a weight decay of 0.00005, each of which was tuned on the source validation set. In <ref type="table" target="#tab_2">Table 2</ref> 3 , we show the performance of the stylization training method and compare with a number of different methods. Our method performs comparably or exceeds the state of arts showing that shape information is helpful in this setting as well. We note that although our method results in similar performance, methods like MMLD are significantly more complicated requiring a careful mix of clustering, adversarial loss, an entropy loss and a standard classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Office-Home Dataset</head><p>The Office-Home Dataset <ref type="bibr" target="#b37">[38]</ref> also contains four domains, Art, Clipart, Product, and Real-World. This dataset contains significantly more classes than either the PACS or VLCS datasets with 65 common items from the office and home. As employed in previous work, we adopt the testing procedure employed in <ref type="bibr" target="#b6">[7]</ref> which is very similar to the PACS testing setting. As before, three domains are used for training and the remaining domain is the testing domain, 90% of the data is used for training with the remaining 10% used for validation. In this setting we employ the same hyper-parameters used in the PACS setting with the ResNet18 model. We can see from <ref type="table" target="#tab_3">Table 3</ref> 3 that,  even with a significant increase in classes, stylization is able to help the model generalize, validating the efficacy of shape bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We now analyze various aspects of our approach, including an analysis of shape/texture predictiveness and bias, as well as analysis of which sources are most useful for use in stylization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">How does our method affect shape bias, shape accuracy, and texture accuracy?</head><p>Here we provide some experimentation which sheds some light on how our method works. Inspired by <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b15">[16]</ref>, we seek to measure the difference in shape bias, shape accuracy and texture accuracy of the model in order to test our hypothesis that our method decreases reliance on less generalizable features (texture) and increases reliance on more generalizable features (shape).</p><p>Dataset preparation. In order to compute each of the above metrics, we first generate a cue-conflict dataset, in which the shape of the object in the image conflicts with the texture of the images (see <ref type="figure" target="#fig_3">Figure 3</ref>). These images are generated by applying the stylization method of <ref type="bibr" target="#b11">[12]</ref> to a texture and content image. A set of textures were sourced from online in a manner identical to <ref type="bibr" target="#b13">[14]</ref> and for content images we used images drawn from the Sketch dataset in PACS. Each texture and content image is chosen so that all models under comparison predict the correct class. For example, a dog texture image is only taken if every model correctly predicts it as a dog, and a sketch of a house is only chosen as a content image if every model correctly predicts it as a house. This limitation results in a maximum of 45 images per a class in the dataset as this is the maximal number of sketch images classified correctly by the least performant model we seek to analyze. Finally, to actually generate the dataset, each content image was stylized with a randomly chosen texture image, excluding the setting where the content class and texture class match. We chose this setting since the Sketch dataset is the most shape reliant dataset and since this presents the largest increase in accuracy in the PACS dataset <ref type="table" target="#tab_1">(Table 1</ref>) Also silhouettes were employed in <ref type="bibr" target="#b13">[14]</ref>, as disambiguation of shape and texture effects requires a silhouette, with no added texture, for the shape class. In addition, the selection of arbitrary texture and shape not seen in the dataset is important analysis for the DG setting since we wish to see how a model performs when it is not trained on any of the target data.</p><p>Shape bias. Shape bias is defined for a specific class as the proportion of class predictions which the model makes based on shape, in the cue-conflict dataset. We can define a sample from the cue-conflict dataset as (x, y s , y t ), where x is the cue-conflict image and y s , y t represent the shape label and texture label respectively. The shape bias for a particular class c is then the percentage of the time that the model predict the shape class for a cue conflict image when it predicts class c. Formally, it can be defined as follows:</p><formula xml:id="formula_2">i 1[f(x i ; ?) = y i s ] and 1[f(x i ; ?) = c] i (1[f (x i ; ?) = y i s ] and 1[f(x i ; ?) = c]) + i 1[f(x i ; ?) = y i t ] and 1[f(x i ; ?) = c]<label>(3)</label></formula><p>Shape accuracy. To calculate shape accuracy, we freeze the weights of the model and attach a linear classifier to the penultimate layer of the ResNet18 model. We then train the classifier to predict the shape class of each image. Intuitively this metric measures the amount of shape information that the model contains. This is different than the shape bias since the bias measures the preference of the    model, but does not preclude the existence of information in the model which it does not make use of. Due to the small size of the cue-conflict dataset, following the procedure of <ref type="bibr" target="#b15">[16]</ref>, we perform five-fold cross validation and report the average of the max validation accuracy over the training period over all five folds.</p><p>Texture accuracy. Similar to shape accuracy, texture accuracy is computed by adding a linear classifier onto the penultimate layer of the ResNet18 model. Instead of training the model to predict the shape class, however, we train the model to predict the texture class. Here we hope to understand how much texture information is available in the model.</p><p>We compare three models, a model trained with no augmentation, a model trained with augmentation from prior work consisting of horizontal flipping, color jitter and random crops and a model trained with our stylization method combined with basic augmentation. We perform this experiment on each of these models so that the progression of shape importance is clear. It is clear from <ref type="table" target="#tab_5">Table 4</ref> and <ref type="table" target="#tab_6">Table 5</ref> that the stylized training results in a model that has significantly higher shape bias and shape accuracy. Interestingly, however, the texture accuracy decreases only marginally. This is surprising since one might expect the introduction of different textures to encourage the network to become invariant to texture. One possible explanation for this could be that the classification layer of the model learns to ignore the textural information even though it may be present. An interesting finding is that as shape bias increases, the resulting domain generalization performance seems to increase as well; this validates the hypothesis that encouraging the correct biases can improve generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Are all sources equally useful?</head><p>Since we utilize inter-source variability through stylization, one can ask whether all sources are necessary or whether they equally benefit performance. In <ref type="table" target="#tab_7">Table 6</ref>, we show the effects of using different domains for inter-source stylization so as to ascertain which source domains are most important for each target dataset. The results show that not all source datasets are necessary, with one or two sources being important for each target. Further, different sources are important for different targets but in most cases using more than one source increases performance. For all cases, the performance of using all three sources is close the best condition implying that there is not a need to select among the sources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we introduced a simple technique, stylization, for improving out-of-domain performance by encouraging the model to learn more generalizable biases, e.g., shape. Interestingly, we show that different domains contain variable information, which can be used to construct a model with favorable invariances, showing that this can even be achieved with the variances present within even a single domain. Central to our paper is a consideration of the biases which enables a model to generalize. We show by correcting this common bias in existing CNNs, models can be made to improve performance in general settings without the need for domain specific data. B Ablation of Style Probability <ref type="table" target="#tab_1">Table 11</ref> includes an ablation of style probabilities and the effect it has on PACS accuracy. It is clear that there is an amount of stylization that is optimal and stylization beyond this results in reduced performance. However, the results very stable across large changes in probabilities (e.g. 0.25 and 0.50) showing that the method is not overly sensitive to this hyper-parameter.  <ref type="table">Table 7</ref>: Prediction accuracy of Stylized training on the PACS Dataset with ResNet-18 as compared to recent state-of-the-art methods. Each reported accuracy is the average of three independent runs of the method with standard deviations added. The runs with an asterisk(*) denote a different training/testing procedure which computes max target dataset accuracy over the training period in lieu of using in source validation data for model selection.   <ref type="table">Table 9</ref>: Prediction Accuracy of Stylized training on the VLCS Dataset as compared to recent state of the art methods. We report average accuracy and standard deviation over 3 independent runs of the method. The runs with an asterisk(*) denote a different training/testing procedure which computes max target dataset accuracy over the training period in lieu of using in source validation data for model selection.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Stylization Training Comparison</head><p>We explore different methods of training the AdaIN stylization network in our two stylization methods. The original AdaIN network is trained with the COCO dataset for content images and the Painter By Numbers dataset as style. First, we explore alternatively training the network with ImageNet images as content and Painter By Numbers as the style data for the painting stylization setting. Second, we explore re-training the network in the inter-source setting by again using ImageNet images as content images, but using the relevant DG dataset (with target removed) as the style data. This results in one stylization model for stylizing images as paintings and 12 different stylization models for inter-source stylization, one for each target domain in each DG dataset. For example, an AdaIN model trained to be used for inter-source training on the PACS dataset with Sketch as the target domain would be trained with data from the Photo, Art and Cartoon datasets as style data. <ref type="figure" target="#fig_4">Figure 4</ref> depicts this visually. Note that we did not perform any tuning for this training, which may result in further performance improvements. This is due to the fact that tuning this method would require runs of stylization and model training for just a single data point, making it prohibitively expensive. The results of this experiment are in <ref type="table" target="#tab_1">Tables 12 and 13</ref>. In each setting retraining AdaIN results in similar performance indicating that the stylization method is robust the specific choice of datasets used.    <ref type="table" target="#tab_1">Table 13</ref>: Results of retraining the AdaIN stylization model in the inter-source stylization setting compared to stylization via the pretrained AdaIN model. In each case the mean and standard deviation is reported over three independent runs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of training pipeline of the Proposed Method. Each image from the source domain is replaced with a stylized version of itself with a certain probability. Paintings are used to stylize in the standard setting; however, to avoid the introduction of extra datasets, the source domains themselves can be used as sources of style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Samples from painting stylized PACS images and Inter-Source Stylized PACS images. On the right, the inter-source stylized images depict when one domain is used as the content with another used as the style.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Bias</head><label></label><figDesc>Shape Bias: { !"#$ = Dog} Texture Bias: { !"#$ = Giraffe} CNN { !"#$ = Dog} Texture Acc: { !"#$ = Giraffe}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Visual depiction of shape bias, shape accuracy and texture accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>!Figure 4 :</head><label>4</label><figDesc>! , ! " , ! # , ! $ ! %&amp;'()* Illustration of training pipeline for retraining AdaIN in both the painting stylization and inter-source setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al.<ref type="bibr" target="#b22">[23]</ref>, for the four domains Photo, Art, Cartoon and Sketch. In this setting, three domains are used for training and the remaining domain is the testing domain. Results are provided for each testing domain and averaged over all testing domains. Each dataset contains the same seven classes: dog, elephant, giraffe, guitar, horse, house, and person.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state of the arts on the PACS<ref type="bibr" target="#b22">[23]</ref> Dataset. Asterisk(*) denotes a different training/testing procedure, which computes max target dataset accuracy over the training period in lieu of using in source validation data for model selection.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ResNet-18</cell><cell></cell><cell></cell><cell>AlexNet</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">Photo Art</cell><cell cols="3">Cartoon Sketch Avg.</cell><cell>Photo Art</cell><cell cols="2">Cartoon Sketch Avg.</cell></row><row><cell>D-SAM [7]</cell><cell></cell><cell cols="3">95.30 77.33 72.43</cell><cell>77.83</cell><cell cols="3">80.72 85.55 63.87 70.70</cell><cell>66.66</cell><cell>71.20</cell></row><row><cell>MMLD [29]</cell><cell></cell><cell cols="3">96.09 81.28 77.16</cell><cell>72.29</cell><cell cols="3">81.83 89.00 69.27 72.83</cell><cell>66.44</cell><cell>73.38</cell></row><row><cell>MLDG [24]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">88.00 66.23 66.88</cell><cell>58.96</cell><cell>70.01</cell></row><row><cell>Meta-Reg [1]</cell><cell></cell><cell cols="3">95.50 83.70 77.20</cell><cell>70.30</cell><cell cols="3">81.70 91.07 69.82 70.35</cell><cell>59.26</cell><cell>72.62</cell></row><row><cell>Epi-FCR [25]</cell><cell></cell><cell cols="3">93.90 82.10 77.00</cell><cell>73.00</cell><cell cols="3">81.50 86.10 64.70 72.30</cell><cell>65.00</cell><cell>72.00</cell></row><row><cell>JiGen [2]</cell><cell></cell><cell cols="3">96.03 79.42 75.25</cell><cell>71.35</cell><cell cols="3">80.51 88.98 67.63 71.71</cell><cell>65.18</cell><cell>74.38</cell></row><row><cell>PAR [40]</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">90.40 68.70 70.50</cell><cell>64.60</cell><cell>73.54</cell></row><row><cell>Ours (Painting)</cell><cell></cell><cell cols="3">96.29 84.60 78.81</cell><cell>78.90</cell><cell cols="3">84.64 90.00 73.71 72.37</cell><cell>70.40</cell><cell>76.62</cell></row><row><cell cols="2">Ours (Intra-Source)</cell><cell cols="3">96.45 80.81 78.85</cell><cell>77.83</cell><cell cols="3">83.49 90.72 71.09 73.14</cell><cell>68.67</cell><cell>75.90</cell></row><row><cell cols="2">Ours (Inter-Source)</cell><cell cols="3">96.27 83.17 78.90</cell><cell>79.87</cell><cell cols="3">84.55 89.76 72.52 71.72</cell><cell>71.21</cell><cell>76.30</cell></row><row><cell>MASF [6]*</cell><cell></cell><cell cols="3">94.99 80.29 77.17</cell><cell>71.69</cell><cell cols="3">81.79 90.68 70.35 72.46</cell><cell>67.33</cell><cell>75.21</cell></row><row><cell>Ours (Painting)*</cell><cell></cell><cell cols="3">96.85 85.45 79.56</cell><cell>79.32</cell><cell cols="3">85.30 91.60 76.01 73.82</cell><cell>73.07</cell><cell>78.62</cell></row><row><cell cols="2">Ours (Intra-Source)</cell><cell cols="3">97.05 82.73 79.64</cell><cell>80.10</cell><cell cols="3">84.88 91.49 73.06 73.74</cell><cell>72.33</cell><cell>77.66</cell></row><row><cell cols="5">Ours (Inter-Source)* 96.89 84.33 80.15</cell><cell>81.53</cell><cell cols="3">85.72 91.52 74.59 73.15</cell><cell>74.71</cell><cell>78.49</cell></row><row><cell></cell><cell></cell><cell>AlexNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Pascal LabelMe Caltech Sun</cell><cell>Avg.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D-SAM [7]</cell><cell cols="2">58. 59 56.95</cell><cell>91.75</cell><cell cols="2">60.84 67.03</cell><cell></cell><cell></cell><cell></cell></row><row><cell>JiGen [2]</cell><cell>70.62</cell><cell>60.90</cell><cell>96.93</cell><cell cols="2">64.30 73.19</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMLD [29]</cell><cell>71.96</cell><cell>58.77</cell><cell>96.66</cell><cell cols="2">68.13 73.88</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Epi-FCR [25]</cell><cell>67.10</cell><cell>64.30</cell><cell>94.10</cell><cell cols="2">65.90 72.90</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MMD-AAE [26]</cell><cell>67.70</cell><cell>62.60</cell><cell>94.90</cell><cell cols="2">64.90 72.28</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (Painting)</cell><cell>69.81</cell><cell>58.90</cell><cell>96.57</cell><cell cols="2">69.82 73.77</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (Intra-Source)</cell><cell>70.98</cell><cell>59.98</cell><cell>96.10</cell><cell cols="2">69.38 74.11</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (Inter-Source)</cell><cell>71.17</cell><cell>60.19</cell><cell>96.27</cell><cell cols="2">67.68 73.83</cell><cell></cell><cell></cell><cell></cell></row><row><cell>MASF* [6]</cell><cell>69.14</cell><cell>64.90</cell><cell>94.78</cell><cell cols="2">67.64 74.11</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours (Painting)*</cell><cell>72.04</cell><cell>61.97</cell><cell>97.81</cell><cell cols="2">70.67 75.62</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours (Intra-Source)* 72.28</cell><cell>61.57</cell><cell>97.51</cell><cell cols="2">70.76 75.53</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Ours (Inter-Source)* 72.30</cell><cell>61.95</cell><cell>97.51</cell><cell cols="2">70.93 75.67</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>ResNet-18</cell><cell></cell></row><row><cell></cell><cell>Art</cell><cell cols="3">Clipart Product Real</cell><cell>Avg.</cell></row><row><cell>D-SAM [7]</cell><cell cols="2">58.03 44.37</cell><cell>69.22</cell><cell>71.45 60.77</cell></row><row><cell>JiGen [2]</cell><cell cols="2">53.04 47.51</cell><cell>71.47</cell><cell>72.79 61.20</cell></row><row><cell>Ours (Painting)</cell><cell cols="2">59.87 49.91</cell><cell>72.82</cell><cell>75.20 64.45</cell></row><row><cell cols="3">Ours (Intra-Source) 59.34 50.31</cell><cell>72.67</cell><cell>74.93 64.31</cell></row><row><cell cols="3">Ours (Inter-Source) 59.52 50.86</cell><cell>72.15</cell><cell>75.13 64.42</cell></row><row><cell>Comparison with state of the arts on</cell><cell></cell><cell></cell><cell></cell></row><row><cell>VLCS [9]. Asterisk(*) denotes a different train-</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ing/testing procedure, same as in Table 1.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison with state of the arts on the OH [38] Dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Shape Bias of models trained with style and color jitter, color jitter, no augmentation.</figDesc><table><row><cell></cell><cell cols="2">Shape Texture PACS Sketch</cell></row><row><cell cols="2">Random Untrained 26.35 63.81</cell><cell>2.04</cell></row><row><cell>Basic</cell><cell cols="2">46.99 100.00 65.66</cell></row><row><cell>Color Jitter</cell><cell>52.70 99.36</cell><cell>71.91</cell></row><row><cell cols="2">Style + Color Jitter 59.05 99.05</cell><cell>78.90</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Shape Accuracy and Texture</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Accuracy of models trained with style</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">and color jitter, color jitter, no augmen-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">tation and a random untrained model</cell></row><row><cell></cell><cell></cell><cell cols="3">Source Stylization Datasets</cell><cell></cell></row><row><cell></cell><cell>A,C,S A,S</cell><cell>A,C</cell><cell>C,S</cell><cell>A</cell><cell>C</cell><cell>S</cell></row><row><cell>Photo (P)</cell><cell cols="6">96.27 96.33 96.13 96.41 96.35 96.49 96.19</cell></row><row><cell></cell><cell>P,C,S C,S</cell><cell>C,P</cell><cell>P,S</cell><cell>C</cell><cell>P</cell><cell>S</cell></row><row><cell>Art (A)</cell><cell cols="6">83.17 78.81 82.30 82.96 78.22 83.32 79.75</cell></row><row><cell></cell><cell>A,P,S A,S</cell><cell>A,P</cell><cell>P,S</cell><cell>A</cell><cell>P</cell><cell>S</cell></row><row><cell cols="7">Cartoon (C) 78.90 78.56 78.92 78.54 78.50 78.84 77.59</cell></row><row><cell></cell><cell>A,P,C A,C</cell><cell>A,P</cell><cell>P,C</cell><cell>A</cell><cell>P</cell><cell>C</cell></row><row><cell>Sketch (S)</cell><cell cols="6">79.87 80.27 75.40 80.67 74.47 73.99 79.63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Test Accuracy when only limited sources are available for stylization. The columns represent the sources that are used for inter-source stylization. The rows are the test dataset being tested on. Each result shown is the average over 3 independent runs.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Tables 7,<ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10</ref> contain the same results reported in Section 4 with standard deviations added on. All experiments are reported as 3 run averages with standard deviations, run on a mix of machines which typically have a 28-core CPU, 384GB RAM with Nvidia RTX 2080Ti, Titan X and Titan Xp gpus. On such machines a results for a 3 run average for the PACS or VLCS dataset takes 4-6 hours while the OH dataset takes 6-8 hours.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>? 00.03 84.60 ? 1.22 78.81 ? 00.17 78.90 ? 00.84 84.64 ? 00.14 Ours (Inter-Source) 96.27 ? 00.19 83.17 ? 00.61 78.90 ? 00.15 79.87 ? 00.60 84.55 ? 00.15 ? 00.19 85.45 ? 00.29 79.56 ? 00.46 79.32 ? 00.17 85.30 ? 00.08 Ours (Inter-Source)* 96.89 ? 00.45 84.33 ? 00.55 80.15 ? 00.30 81.53 ? 00.47 85.72 ? 00.40</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ResNet-18 -PACS</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Avg.</cell></row><row><cell>D-SAM [7]</cell><cell>95.30</cell><cell>77.33</cell><cell>72.43</cell><cell>77.83</cell><cell>80.72</cell></row><row><cell>MMLD [29]</cell><cell>96.09</cell><cell>81.28</cell><cell>77.16</cell><cell>72.29</cell><cell>81.83</cell></row><row><cell>Meta-Reg [1]</cell><cell cols="5">95.50 ? 00.24 83.70 ? 00.19 77.20 ? 00.31 70.30 ? 00.28 81.70</cell></row><row><cell>Epi-FCR [25]</cell><cell>93.90</cell><cell>82.10</cell><cell>77.00</cell><cell>73.00</cell><cell>81.50</cell></row><row><cell>JiGen [2]</cell><cell>96.03</cell><cell>79.42</cell><cell>75.25</cell><cell>71.35</cell><cell>80.51</cell></row><row><cell cols="2">Ours (Painting) 96.29 MASF [6]* 94.99 ? 0.09</cell><cell>80.29 ? 0.18</cell><cell>77.17 ? 0.08</cell><cell>71.69 ? 0.22</cell><cell>81.79</cell></row><row><cell>Ours (Painting)*</cell><cell>96.85</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>00.75 73.71 ? 01.68 72.37 ? 00.44 70.40 ? 02.28 76.62 ? 01.19 Ours (Inter-Source) 89.76 ? 00.36 72.52 ? 00.84 71.72 ? 02.31 71.21 ? 01.56 76.30 ? 01.08 ? 00.58 76.01 ? 00.35 73.82 ? 00.54 73.07 ? 01.13 78.62 ? 00.19 Ours (Inter-Source)* 91.52 ? 00.24 74.59 ? 00.83 73.15 ? 00.15 74.71 ? 00.94 78.49 ? 00.26</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AlexNet -PACS</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Avg.</cell></row><row><cell>D-SAM [7]</cell><cell>85.55</cell><cell>63.87</cell><cell>70.70</cell><cell>66.66</cell><cell>71.20</cell></row><row><cell>MMLD [29]</cell><cell>89.00</cell><cell>69.27</cell><cell>72.83</cell><cell>66.44</cell><cell>73.38</cell></row><row><cell>MLDG [24]</cell><cell>88.00</cell><cell>66.23</cell><cell>66.88</cell><cell>58.96</cell><cell>70.01</cell></row><row><cell>Meta-Reg [1]</cell><cell cols="5">91.07 ? 00.41 69.82 ? 00.76 70.35 ? 00.63 59.26 ? 00.31 72.62</cell></row><row><cell>Epi-FCR [25]</cell><cell>86.10</cell><cell>64.70</cell><cell>72.30</cell><cell>65.00</cell><cell>72.00</cell></row><row><cell>JiGen [2]</cell><cell>88.98</cell><cell>67.63</cell><cell>71.71</cell><cell>65.18</cell><cell>74.38</cell></row><row><cell>PAR [40]</cell><cell>90.40</cell><cell>68.70</cell><cell>70.50</cell><cell>64.60</cell><cell>73.54</cell></row><row><cell cols="2">Ours (Painting) 90.00 ? MASF [6]* 90.68 ? 0.12</cell><cell>70.35 ? 0.33</cell><cell>72.46 ? 0.19</cell><cell>67.33 ? 0.12</cell><cell>75.21</cell></row><row><cell>Ours (Painting)*</cell><cell>91.60</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Prediction accuracy of Stylized training on the PACS Dataset with AlexNet as compared to recent state-of-the-art methods. Each reported accuracy is the average of three independent runs of the method with standard deviations added. The runs with an asterisk(*) denote a different training/testing procedure which computes max target dataset accuracy over the training period in lieu of using in source validation data for model selection.? 00.88 58.90 ? 01.16 96.57 ? 00.46 69.82 ? 00.48 73.77 ? 00.24 Ours (Inter-Source) 71.17 ? 01.49 60.19 ? 00.71 96.27 ? 00.87 67.68 ? 01.68 73.83 ? 00.77 MASF [6]* 69.14 ? 00.19 64.90 ? 00.08 94.78 ? 00.16 67.64 ? 00.12 74.11 Ours (Painting)* 72.04 ? 00.90 61.97 ? 00.65 97.81 ? 00.12 70.67 ? 00.26 75.62 ? 00.26 Ours (Inter-Source)* 72.30 ? 00.42 61.95 ? 00.08 97.51 ? 00.41 70.93 ? 00.37 75.67 ? 00.24</figDesc><table><row><cell></cell><cell></cell><cell cols="2">AlexNet -VLCS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Pascal(VOC)</cell><cell>LabelMe</cell><cell>Caltech</cell><cell>Sun</cell><cell>Avg.</cell></row><row><cell>MMD-AAE [24]</cell><cell>67.70</cell><cell>62.60</cell><cell>94.90</cell><cell>64.90</cell><cell>72.28</cell></row><row><cell>D-SAM [7]</cell><cell>58.59</cell><cell>56.95</cell><cell>91.75</cell><cell>60.84</cell><cell>67.03</cell></row><row><cell>MMLD [29]</cell><cell>71.96</cell><cell>58.77</cell><cell>96.66</cell><cell>68.13</cell><cell>73.88</cell></row><row><cell>Epi-FCR [25]</cell><cell>67.10</cell><cell>64.30</cell><cell>94.10</cell><cell>65.90</cell><cell>72.90</cell></row><row><cell>JiGen [2]</cell><cell>70.62</cell><cell>60.90</cell><cell>96.93</cell><cell>64.30</cell><cell>73.19</cell></row><row><cell>Ours (Painting)</cell><cell>69.81</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>? 00.50 49.91 ? 00.31 72.82 ? 00.26 75.20 ? 00.15 64.45 ? 00.13 Ours (Inter-Source) 59.52 ? 00.45 50.86 ? 00.20 72.15 ? 00.12 75.13 ? 00.23 64.42 ? 00.21</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ResNet-18 -OfficeHome</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Art</cell><cell>Clipart</cell><cell>Product</cell><cell>Real</cell><cell>Avg.</cell></row><row><cell>D-SAM [7]</cell><cell>58.03</cell><cell>44.37</cell><cell>69.22</cell><cell>71.45</cell><cell>60.77</cell></row><row><cell>JiGen [2]</cell><cell>53.04</cell><cell>47.51</cell><cell>71.47</cell><cell>72.79</cell><cell>61.20</cell></row><row><cell>Ours (Painting)</cell><cell>59.87</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Prediction Accuracy of Stylized training on the OH Dataset as compared to recent state of the art methods. We report average accuracy and standard deviation over 3 independent runs of the method.</figDesc><table><row><cell></cell><cell cols="2">ResNet18 -PACS</cell><cell></cell></row><row><cell cols="2">Style Prob. Photo Art</cell><cell cols="3">Cartoon Sketch Avg</cell></row><row><cell>0.25</cell><cell cols="2">96.07 84.03 77.57</cell><cell>79.68</cell><cell>84.34</cell></row><row><cell>0.50</cell><cell cols="2">95.57 83.41 77.05</cell><cell>80.40</cell><cell>84.11</cell></row><row><cell>0.75</cell><cell cols="2">95.19 83.06 75.23</cell><cell>79.90</cell><cell>83.37</cell></row><row><cell>1.00</cell><cell cols="2">94.71 82.79 72.13</cell><cell>75.36</cell><cell>81.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 11 :</head><label>11</label><figDesc>Style Probability vs. PACS Accuracy</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>Trained) 96.29 ? 0.03 84.60 ? 1.22 78.81 ? 0.17 78.90 ? 0.84 84.64 ? 0.14 PACS (Re-Trained) 96.23 ? 0.31 83.35 ? 0.81 79.74 ? 0.38 77.46 ? 0.13 84.19 ? 0.22 ? 0.50 49.91 ? 0.31 72.82 ? 0.26 75.20 ? 0.15 64.45 ? 0.13 OH (Re-Trained) 59.62 ? 0.22 49.65 ? 0.39 72.25 ? 0.06 75.08 ? 0.23 64.15 ? 0.10 Trained) 90.00 ? 0.75 73.71 ? 1.68 72.37 ? 0.44 70.40 ? 2.28 76.62 ? 1.19 PACS (Re-Trained) 90.04 ? 0.09 72.25 ? 1.78 70.80 ? 0.28 69.19 ? 0.79 75.57 ? 0.58 Trained) 69.81 ? 0.88 58.90 ? 1.16 96.57 ? 0.46 69.82 ? 0.48 73.77 ? 0.24 VLCS (Re-Trained) 69.91 ? 0.34 59.48 ? 1.74 96.54 ? 0.59 66.87 ? 0.89 73.20 ? 0.58</figDesc><table><row><cell></cell><cell></cell><cell>ResNet18</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Avg.</cell></row><row><cell>PACS (Pre-Method</cell><cell>Art</cell><cell>Clipart</cell><cell>Product</cell><cell>Real</cell><cell>Avg.</cell></row><row><cell>OH (Pre-Trained)</cell><cell cols="2">59.87 AlexNet</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Photo</cell><cell>Art</cell><cell>Cartoon</cell><cell>Sketch</cell><cell>Avg.</cell></row><row><cell>PACS (Pre-Method</cell><cell cols="2">Pascal (VOC) LabelMe</cell><cell>Caltech</cell><cell>Sun</cell><cell>Avg.</cell></row><row><cell>VLCS (Pre-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 12 :</head><label>12</label><figDesc>Results of retraining the AdaIN stylization model in the painting stylization setting compared to stylization via the pretrained AdaIN model. In each case the mean and standard deviation is reported over three independent runs. Trained) 96.27 ? 0.19 83.17 ? 0.61 78.90 ? 0.15 79.87 ? 0.60 84.55 ? 0.15 PACS (Re-Trained) 96.43 ? 0.15 80.31 ? 0.23 79.74 ? 0.45 77.55 ? 1.04 83.51 ? 0.31 59.52 ? 0.45 50.86 ? 0.20 72.15 ? 0.12 75.13 ? 0.23 64.42 ? 0.21 OH (Re-Trained) 60.06 ? 0.19 51.35 ? 0.51 72.29 ? 0.18 74.68 ? 0.20 64.60 ? 0.17 Trained) 89.76 ? 0.36 72.52 ? 0.84 71.72 ? 2.31 71.21 ? 1.56 76.30 ? 1.08 PACS (Re-Trained) 90.08 ? 0.33 71.84 ? 1.92 70.45 ? 0.21 70.37 ? 0.75 75.68 ? 0.45 Trained) 71.17 ? 1.49 60.19 ? 0.71 96.27 ? 0.87 67.68 ? 1.68 73.83 ? 0.77 VLCS (Re-Trained) 71.18 ? 0.66 58.06 ? 1.24 96.27 ? 1.05 68.02 ? 1.19 73.38 ? 0.66</figDesc><table><row><cell>ResNet18</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code will be released at https://github.com/GT-RIPL/DomainGeneralization-Stylization.Preprint. Under review.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.kaggle.com/c/painter-by-numbers</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">For visual clarity an identical table with standard deviation can be found in the supplementary materials.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was funded by DARPA's Learning with Less Labels (LwLL) program under agreement HR0011-18-S-0044.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-ensembling with gan-based data augmentation for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6830" to="6840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting hierarchical context on a large database of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain generalization via modelagnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>De Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domain generalization with domain-specific aggregation modules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition: 40th German Conference</title>
		<meeting><address><addrLine>Stuttgart, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">11269</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Rockmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 conference on computer vision and pattern recognition workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="178" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<title level="m">Unsupervised domain adaptation by backpropagation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenettrained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<title level="m">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Exploring the origins and prevalence of texture bias in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.09071</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Auggan: Cross domain adaptation with gan-based data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="718" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1501" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bonner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breckon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Obara</surname></persName>
		</author>
		<title level="m">Style augmentation: Data augmentation via style randomization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Measuring the tendency of cnns to learn surface statistical regularities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.11561</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The cifar-10 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/kriz/cifar.html" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">55</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Episodic training for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<title level="m">Learning transferable features with deep adaptation networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07661</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The need for biases in learning generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1980" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, Laboratory for Computer Science Research . . .</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10811</idno>
		<title level="m">Do imagenet classifiers generalize to imagenet</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Labelme: a database and webbased tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A dirt-t approach to unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Narui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE) Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">One-shot imitation from observing humans via domain-adaptive meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01557</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
