<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sparse Activity and Sparse Connectivity in Supervised Learning G?nther Palm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2013">2013</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Thom</surname></persName>
							<email>markus.thom@uni-ulm.de</email>
							<affiliation key="aff0">
								<orgName type="department">driveU / Institute of Measurement, Control and Microtechnology</orgName>
								<orgName type="institution">Ulm University</orgName>
								<address>
									<postCode>89081</postCode>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guenther</forename><surname>Palm@uni-Ulm</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Neural Information Processing</orgName>
								<orgName type="institution">Ulm University</orgName>
								<address>
									<postCode>89081</postCode>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>De</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Neural Information Processing</orgName>
								<orgName type="institution">Ulm University</orgName>
								<address>
									<postCode>89081</postCode>
									<settlement>Ulm</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Sparse Activity and Sparse Connectivity in Supervised Learning G?nther Palm</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Journal of Machine Learning Research</title>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="page" from="1091" to="1143"/>
							<date type="published" when="2013">2013</date>
						</imprint>
					</monogr>
					<note>This article has been first published in Editor: Aapo Hyv?rinen</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>supervised learning</term>
					<term>sparseness projection</term>
					<term>sparse activity</term>
					<term>sparse connectivity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sparseness is a useful regularizer for learning in a wide range of applications, in particular in neural networks. This paper proposes a model targeted at classification tasks, where sparse activity and sparse connectivity are used to enhance classification capabilities. The tool for achieving this is a sparseness-enforcing projection operator which finds the closest vector with a pre-defined sparseness for any given vector. In the theoretical part of this paper, a comprehensive theory for such a projection is developed. In conclusion, it is shown that the projection is differentiable almost everywhere and can thus be implemented as a smooth neuronal transfer function. The entire model can hence be tuned end-to-end using gradient-based methods. Experiments on the MNIST database of handwritten digits show that classification performance can be boosted by sparse activity or sparse connectivity. With a combination of both, performance can be significantly better compared to classical non-sparse approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sparseness is a concept of efficiency in neural networks, and exists in two variants in that context <ref type="bibr" target="#b40">(Laughlin and Sejnowski, 2003)</ref>. The sparse activity property means that only a small fraction of neurons is active at any time. The sparse connectivity property means that each neuron is connected to only a limited number of other neurons. Both properties have been observed in mammalian brains <ref type="bibr" target="#b33">(Hubel and Wiesel, 1959;</ref><ref type="bibr" target="#b57">Olshausen and Field, 2004;</ref><ref type="bibr" target="#b51">Mason et al., 1991;</ref><ref type="bibr" target="#b50">Markram et al., 1997)</ref> and have inspired a variety of machine learning algorithms. A notable result was achieved through the sparse coding model of <ref type="bibr" target="#b56">Olshausen and Field (1996)</ref>. Given small patches from images of natural scenes, the model is able to produce Gabor-like filters, resembling properties of simple cells found in mammalian primary visual cortex <ref type="bibr" target="#b33">(Hubel and Wiesel, 1959;</ref><ref type="bibr" target="#b76">Vinje and Gallant, 2000)</ref>. Another example is the optimal brain damage method of <ref type="bibr" target="#b42">LeCun et al. (1990)</ref>, which can be used to prune synaptic connections in a neural network, making connectivity sparse. Although only a small fraction of possible connections remains after pruning, this is sufficient to achieve equivalent classification results. Since then, numerous approaches on how to measure sparseness have been proposed, see <ref type="bibr" target="#b34">Hurley and Rickard (2009)</ref> for an overview, and how to achieve sparse solutions of classical machine learning problems. c 2013 Markus <ref type="bibr">Thom and G?nther Palm.</ref> arXiv:1603.08367v1 <ref type="bibr">[cs.</ref>LG] 28 Mar 2016</p><p>The L 0 pseudo-norm is a natural sparseness measure. Its computation consists of counting the number of non-vanishing entries in a vector. Using it rather than other sparseness measures has been shown to induce biologically more plausible properties <ref type="bibr" target="#b63">(Rehn and Sommer, 2007)</ref>. However, finding of optimal solutions subject to the L 0 pseudo-norm turns out to be NP-hard <ref type="bibr" target="#b54">(Natarajan, 1995;</ref><ref type="bibr" target="#b78">Weston et al., 2003)</ref>. Analytical properties of this counting measure are very poor, for it is non-continuous, rendering the localization of approximate solutions difficult. The Manhattan norm of a vector is a convex relaxation of the L 0 pseudo-norm <ref type="bibr" target="#b14">(Donoho, 2006)</ref>, and has been employed in a vast range of applications. This sparseness measure has the significant disadvantage of not being scale-invariant, so that an intuitive notion of sparseness cannot be derived from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Hoyer's Normalized Sparseness Measure</head><p>A normalized sparseness measure ? based on the ratio of the L 1 or Manhattan norm and the L 2 or Euclidean norm of a vector has been proposed by <ref type="bibr" target="#b32">Hoyer (2004)</ref>,</p><formula xml:id="formula_0">? : R n \ { 0 } ? [0, 1] , x ? ? n ? x 1/ x 2 ? n ? 1 ,</formula><p>where higher values indicate more sparse vectors. ? is well-defined because x 2 ? x 1 ? ? n x 2 holds for all x ? R n <ref type="bibr" target="#b39">(Laub, 2004)</ref>. As ?(?x) = ?(x) for all ? 0 and all x ? R n \ { 0 }, ? is also scale-invariant. As composition of differentiable functions, ? is differentiable on its entire domain.</p><p>This sparseness measure fulfills all criteria of <ref type="bibr" target="#b34">Hurley and Rickard (2009)</ref> except for Dalton's fourth law, which states that the sparseness of a vector should be identical to the sparseness of the vector resulting from multiple concatenation of the original vector. This property, however, is not crucial for a proper sparseness measure. For example, sparseness of connectivity in a biological brain increases quickly with its volume, so that connectivity in a human brain is about 170 times more sparse than in a rat brain <ref type="bibr" target="#b36">(Karbowski, 2003)</ref>. It follows that ? features all desirable properties of a proper sparseness measure.</p><p>A sparseness-enforcing projection operator, suitable for projected gradient descent algorithms, was proposed by <ref type="bibr" target="#b32">Hoyer (2004)</ref> for optimization with respect to ?. For a pre-defined target degree of sparseness ? * ? (0, 1), the operator finds the closest vector of a given scale that has sparseness ? * given an arbitrary vector. This can be expressed formally as Euclidean projection onto parameterizations of the sets S (? 1 ,? 2 ) := { s ? R n | s 1 = ? 1 and s 2 = ? 2 } and S (? 1 ,? 2 ) ?0 := S (? 1 ,? 2 ) ? R n ?0 .</p><p>The first set is for achieving unrestricted projections, whereas the latter set is useful in situations where only non-negative solutions are feasible, for example in non-negative matrix factorization problems. The constants ? 1 , ? 2 &gt; 0 are target norms and can be chosen such that all points in these sets achieve a sparseness of ? * . For example, if ? 2 was set to unity for yielding normalized projections, then ? 1 can be easily derived from the definition of ?.</p><p>Hoyer's original algorithm for computation of such a projection is an alternating projection onto a hyperplane representing the L 1 norm constraint, a hypersphere representing the L 2 norm constraint, and the non-negative orthant. A slightly modified version of this algorithm has been proved to be correct by <ref type="bibr" target="#b71">Theis et al. (2005)</ref> in the special case when exactly one negative entry emerges that is zeroed out in the orthant projection. However, there is still no mathematically satisfactory proof for the general case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Contributions of this Paper</head><p>This paper improves upon previous work in the following ways. Section 2 proposes a simple algorithm for carrying out sparseness-enforcing projections with respect to Hoyer's sparseness measure. Further, an improved algorithm is proposed and compared with Hoyer's original algorithm. Because the projection itself is differentiable, it is the ideal tool for achieving sparseness in gradient-based learning. This is exploited in Section 3, where the sparseness projection is used to obtain a classifier that features both sparse activity and sparse connectivity in a natural way. The benefit of these two key properties is demonstrated on a real-world classification problem, proving that sparseness acts as regularizer and improves classification results. The final sections give an overview of related concepts and conclude this paper.</p><p>On the theoretical side, a first rigorous and mathematically satisfactory analysis of the properties of the sparseness-enforcing projection is provided. This is lengthy and technical and therefore deferred into several appendixes. Appendix A fixes the notation and gives an introduction to general projections. In Appendix B, certain symmetries of subsets of the Euclidean space and their effect on projections onto such sets is studied. The problem of finding projections onto sets where Hoyer's sparseness measure attains a constant value is addressed in Appendix C. Ultimately, the algorithms proposed in Section 2 are proved to be correct. Appendix D investigates analytical properties of the sparseness projection and concludes with an efficient algorithm that computes its gradient. The gradients for optimization of the parameters of the architecture proposed in Section 3 are collected in the final Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Algorithms for the Sparseness-Enforcing Projection Operator</head><p>The projection onto a set is a fundamental concept, for example see <ref type="bibr" target="#b13">Deutsch (2001)</ref>: Because R n is finite-dimensional, proj M (x) is nonempty for all x ? R n if and only if M is closed, and proj M (x) is a singleton for all x ? R n if and only if M is closed and convex <ref type="bibr" target="#b13">(Deutsch, 2001)</ref>. In the literature, the elements from proj M (x) are also called best approximations to x from M.</p><p>Projections onto sets that fulfill certain symmetries are of special interest in this paper and are formalized and discussed in Appendix B in greater detail. It is notable that projections onto a permutation-invariant set M, that is a set where membership is stable upon coordinate permutation, are order-preserving. This is proved in Lemma 9(a). As a consequence, when a vector is sorted in ascending or descending order, then its projection onto M is sorted accordingly. If M is reflection-invariant, that is when the signs of arbitrary coordinates can be swapped without violating membership in M, then the projection onto M is orthant-preserving, as shown in Lemma 9(b). This means that a point and its projection onto M are located in the same orthant. By exploiting this property, projections onto M can be yielded by recording and discarding the signs of the coordinates of the argument, projecting onto M ? R n ?0 , and finally restoring the signs of the coordinates of the result using the signs of the argument. This is formalized in Lemma 11.</p><p>As an example for these concepts, consider the set Z := { x ? R n | x 0 = ? } of all vectors with exactly ? ? N non-vanishing entries. Z is clearly both permutation-invariant and reflectioninvariant. Therefore, the projection with respect to an L 0 pseudo-norm constraint must be both order-preserving and orthant-preserving. In fact, the projection onto Z consists simply of zeroing out all entries but the ? that are greatest in absolute value <ref type="bibr" target="#b4">(Blumensath and Davies, 2009</ref>). This trivially fulfills the aforementioned properties of order-preservation and orthant-preservation.</p><p>Permutation-invariance and reflection-invariance are closed under intersection and union operations. Therefore, the unrestricted target set S (? 1 ,? 2 ) for the ? projection is permutation-invariant and reflection-invariant. It is hence enough to handle projections onto S (? 1 ,? 2 ) ?0 in the first place, as projections onto the unrestricted target set can easily be recovered.</p><p>In the remainder of this section, let n ? N be the problem dimensionality and let ? 1 , ? 2 &gt; 0 be the fixed target norms, which must fulfill ? 2 ? ? 1 ? ? n? 2 to avoid the existence of only trivial solutions. In the applications of the sparseness projection in this paper, ? 2 is always set to unity to achieve normalized projections, and ? 1 is adjusted as explained in Section 1.1 to achieve the target degree of sparseness ? * . The related problem of finding the best approximation to a point x regardless of the concrete scaling, that is computing projections onto { s ? R n \ { 0 } | ?(s) = ? * }, can be solved by projecting x onto S (? 1 ,? 2 ) and rescaling the result p such as to minimize x ? ?p 2 under variation of ? ? R, which yields ? = x, p / p 2 2 . This method is justified theoretically by Remark 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Alternating Projections</head><p>First note that the target set can be written as an intersection of simpler sets. Let e 1 , . . . , e n ? R n be the canonical basis of the n-dimensional Euclidean space R n . Further, let e := ? n i=1 e i ? R n be the vector where all entries are identical to unity. Then H := { a ? R n | e T a = ? 1 } denotes the target hyperplane where the coordinates of all points sum up to ? 1 . In the non-negative orthant R n ?0 , this is equivalent to the L 1 norm constraint. Further, define K := { q ? R n | q 2 = ? 2 } as the target hypersphere of all points satisfying the L 2 norm constraint. This yields the following factorization:</p><formula xml:id="formula_1">S (? 1 ,? 2 ) ?0 = R n ?0 ? H ? K =: D.</formula><p>For computation of projections onto an intersection of a finite number of closed and convex sets, it is enough to perform alternating projections onto the members of the intersection <ref type="bibr" target="#b13">(Deutsch, 2001)</ref>. As K is clearly non-convex, this general approach has to be altered to work in this specific setup. First, consider L := H ? K, which denotes the intersection of the L 1 norm target hyperplane and the L 2 norm target hypersphere. L essentially possesses the structure of a hypercircle, that is, all points in L lie also in H and there is a central point m ? H and a real number ? ? 0 such that all points in L have squared distance ? from m. It will be shown in Appendix C that m = ? 1/n ? e ? R n and ? = ? 2 2 ? ? 2 1/n. The intersection of the non-negative orthant with the L 1 norm hyperplane, C := R n ?0 ? H, is a scaled canonical simplex. Its barycenter coincides with the barycenter m of L. Finally, for an index set I ? {1, . . . , n} let L I := { a ? L | a i = 0 for all i I } denote the subset of points from L, where all coordinates with index not in I vanish. Its barycenter is given by m I = ? 1/d ? ? i?I e i ? R n . With these preparations, a simple algorithm can be proposed; it computes the sparseness-enforcing projection with respect to a constraint induced by Hoyer's sparseness measure ?.</p><p>Theorem 2 For every x ? R n , Algorithm 1 computes an element from proj D (x). If r m after line 1 and r m I after line 4 in all iterations, then proj D (x) is a singleton.</p><p>Algorithm 1: Proposed algorithm for computing the sparseness-enforcing projection operator for Hoyer's sparseness measure ?.</p><p>Input: x ? R n and ? 1 ,</p><formula xml:id="formula_2">? 2 ? R &gt;0 with ? 2 ? ? 1 ? ? n? 2 . Output: s ? proj D (x) where D = S (? 1 ,? 2 ) ?0</formula><p>. // Project onto target hyperplane H and target hypercircle L. 1 r := proj H (x); 2 s ? proj L (r); // Perform alternating projections until feasible solution is found. 3 while s R n ?0 do // Project onto scaled canonical simplex C. 4 r := proj C (s); // Project onto L keeping already vanished coordinates at zero.</p><formula xml:id="formula_3">5 s ? proj L I (r) where I := { i ? {1, . . . , n} | r i 0 }; 6 end</formula><p>As already pointed out, the idea of Algorithm 1 is that projections onto D can be computed by alternating projections onto the geometric structures just defined. The rigorous proof of correctness from Appendix C proceeds by showing that the set of solutions is not tampered by projection onto the intermediate structures H, C, L and L I . Because of the non-convexity of L and L I , the relation between these sets and the simplex C is non-trivial and needs long arguments to be described further, see especially Lemma 26 and Corollary 27.</p><p>The projection onto the hyperplane H is straightforward and discussed in Section C.1.1. As L is essentially a hypersphere embedded in a subspace H of R n , projections of points from H onto L are achieved by shifting and scaling, see Section C.1.2. The alternating projection onto H and L in the beginning of Algorithm 1 make the result of the projection onto D invariant to positive scaling and arbitrary shifting of the argument, as shown in Corollary 19. This is especially useful in practice, alleviating the need for certain pre-processing methods. The formula for projections onto L can be generalized for projections onto L I for an index set I ? {1, . . . , n}, by keeping already vanished coordinates at zero, see Section C.3.</p><p>Projections onto the simplex C are more involved and discussed at length in Section C.2. The most relevant result is that if x ? R n \C, then there exists a separatort ? R such that p := proj C (x) = max (x ?t ? e, 0), where the maximum is taken element-wise <ref type="bibr" target="#b7">(Chen and Ye, 2011)</ref>. In the cases considered in this paper it is alwayst ? 0 as shown in Lemma 28. This implies that all entries in x that are less thant do not survive the projection, and hence the L 0 pseudo-norm of x is strictly greater than that of p. The simplex projection therefore enhances sparseness.</p><p>The separatort and the number of nonzero entries in the projection onto C can be computed with Algorithm 2, which is an adapted version of the algorithm of <ref type="bibr" target="#b7">Chen and Ye (2011)</ref>. In line 1, S n denotes the symmetric group and P ? denotes the permutation matrix associated with a permutation ? ? S n . The algorithm works by sorting its argument x and then determiningt as the mean value of the largest entries of x minus the target L 1 norm ? 1 . The number of relevant entries for computation oft is equal to the L 0 pseudo-norm of the projection and is found by trying all feasible values, starting with the largest ones. The computational complexity of Algorithm 2 is dominated by sorting the input vector and is thus quasilinear.</p><p>Algorithm 2: Computation of information for performing projections onto C, which is a scaled canonical simplex. This is an adapted version of the algorithm of <ref type="bibr" target="#b7">Chen and Ye (2011)</ref>.</p><p>Input:</p><formula xml:id="formula_4">x ? R n \C and ? 1 ? R &gt;0 . Output: (t, d) ? R ? N such that proj C (x) = max (x ?t ? e, 0) and proj C (x) 0 = d.</formula><p>// Sort the input vector in descending order. 1 Let ? ? S n such that x ?(1) ? ? ? ? ? x ?(n) and y := P ? x ? R n ; // Find the only feasible separatort. 2 s := 0; 3 for i := 1 to n ? 1 do 4 s := s + y i ; t := s?? 1 i ; 5 if t ? y i+1 then return (t, i); 6 end 7 s := s + y n ; t := s?? 1 n ; return (t, n);</p><p>Algorithm 3: Explicit and optimized variant of Algorithm 1.</p><p>Input: x ? R n and ? 1 ,</p><formula xml:id="formula_5">? 2 ? R &gt;0 with ? 2 ? ? 1 ? ? n? 2 . Output: s ? proj D (x) where D = S (? 1 ,? 2 ) ?0 . 1 procedure proj_L(y ? R d ) 2 ? := ? 2 2 ? ? 2 1/d;</formula><p>// Compute squared radius of L I (Lemma 15). // pick a sorted projection (Remark 18). 7 else y := ? 1/d ? e + ? /? ? (y ? ? 1/d ? e); // Pick unique projection (Lemma 17). 8 end // Beginning of main body. 9 Let ? ? S n such that x ?(1) ? ? ? ? ? x ?(n) and y := P ? x ? R n ; // Sort the input vector. 10 y := y + 1 /n ? (? 1 ? ? n i=1 y i ) e; // Project onto H (Lemma 13). 11 proj_L (y 1 , . . . , y n ); // Project in-place onto L.</p><p>// Perform alternating projections until feasible solution is found. 12 d := n; // Store current number of relevant entries of y.</p><formula xml:id="formula_6">13 while (y 1 , . . . , y d ) T R d ?0 do</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Optimized Variant</head><p>Because of the permutation-invariance of the sets involved in the projections, it is enough to sort the vector that is to be projected onto D once. This guarantees that the working vector that emerges from subsequent projections is sorted also. No additional sorting has then to be carried out when using Algorithm 2 for projections onto C. This additionally has the side effect that the non-vanishing entries of the working vector are always concentrated in its first entries. Hence all relevant information can always be stored in a small unit-stride array, to which access is more efficient than to a large sparse array. Further, the index set I of non-vanishing entries in the working vector is always of the form I = {1, . . . , d}, where d is the number of nonzero entries. Algorithm 3 is a variant of Algorithm 1 where these optimizations were applied, and where the explicit formulas for the intermediate projections were used. The following result, which is proved in Appendix C, states that both algorithms always compute the same result:</p><p>Theorem 3 Algorithm 1 is equivalent to Algorithm 3.</p><p>Projections onto C increase the amount of vanishing entries in the working vector, which is of finite dimension n. Hence, at most n alternating projections are carried out, and the algorithm terminates in finite time. Further, the complexity of each iteration is at most linear in the L 0 pseudo-norm of the working vector. The theoretic overall computational complexity is thus at most quadratic in problem dimensionality n.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Comparison with Hoyer's Original Algorithm</head><p>The original algorithm for the sparseness-enforcing projection operator proposed by <ref type="bibr" target="#b32">Hoyer (2004)</ref> is hard to understand, and correctness has been proved by <ref type="bibr" target="#b71">Theis et al. (2005)</ref> in a special case only. A simple alternative has been proposed with Algorithm 1 in this paper. Based on the symmetries induced by Hoyer's sparseness measure ? and by exploiting the projection onto a simplex, an improved method was given in Algorithm 3.</p><p>The improved algorithm proposed in this paper always requires at most the same number of iterations of alternating projections as the original algorithm. The original algorithm uses a projection onto the non-negative orthant R n ?0 to achieve vanishing coordinates in the working vector. This operation can be written as proj R n ?0 (x) = max(x, 0). In the improved algorithm, a simplex projection is used for this purpose, expressed formally as proj C (x) = max (x ?t ? e, 0) witht ? R chosen accordingly. Due to the theoretical results on simplex geometry from Section C.2 and their application in Lemma 28 in Section C.3, the numbert is always non-negative. Therefore, at least the same amount of entries is set to zero in the simplex projection compared to the projection onto the non-negative orthant, see also Corollary 29. Hence with induction for the number of non-vanishing entries in the working vector, the number of iterations the proposed algorithm needs to terminate is bounded by the number of iterations the original method needs to terminate given the same input.</p><p>The experimental determination of an estimate of the number of iterations required was carried out as follows. Random vectors with sparseness 0.15 were sampled and their sparse projections were computed using the respective algorithms, to gain the best normalized approximations with a target sparseness degree of ? * := 0.90. For both algorithms the very same vectors were used as input. During the run-time of the algorithms, the number of iterations that were necessary to compute the result were counted. Additionally, the number of nonzero entries in the working vector was Figure 1: Comparison of the number of iterations of the original algorithm for the projection onto D with the improved version as proposed in this paper. The sparseness-enforcing projection with target sparseness 0.90 was carried out for input vectors of sparseness 0.15. The thick lines indicate the mean number of iterations required for the projection, and the thin lines indicate the minimum and maximum number of iterations, respectively. Even for input vectors with a million entries, less than 14 iterations are required to find the projection. With the improved algorithm, this reduces to at most 10 iterations.</p><p>recorded in each iteration. This was done for different dimensionalities, and for each dimensionality 1000 vectors were sampled. <ref type="figure" target="#fig_0">Figure 1</ref> shows statistics on the number of iterations the algorithms needed to terminate. As was already observed by <ref type="bibr" target="#b32">Hoyer (2004)</ref>, the number of required iterations grows very slowly with problem dimensionality. For n = 10 6 , only between 12 and 14 iterations were needed with the original algorithm to compute the result. With Algorithm 3, this can be improved to requiring 9 to 10 iterations, which amounts to roughly 30% less iterations. Due to the small slope in the number of required iterations, it can be conjectured that this quantity is at most logarithmic in problem dimensionality n. If this applies, the complexity of Algorithm 3 is at most quasilinear. Because the input vector is sorted in the beginning, it is also not possible to fall below this complexity class.</p><p>The progress of working dimensionality reduction for problem dimensionality n = 1000 is depicted in <ref type="figure" target="#fig_3">Figure 2</ref>, averaged over the 1000 input vectors from the experiment. After the first iteration, that is after projecting onto H and L, the working dimensionality still matches the input dimensionality. Starting with the second iteration, dimensions are discarded by projecting onto R n ?0 in the original algorithm and onto C in the improved variant, which yields vanishing entries in the working vectors. With the original algorithm, in the mean 54% of all entries are nonzero after the second iteration, while with the improved algorithm only 27% of the original 1000 dimensions remain in the mean. This trend continues in subsequent iterations such that the final working dimensionality is reached more quickly with the algorithm proposed in this paper. Although using Algorithm 2 to perform the simplex projection is more expensive than just setting negative entries to zero in the orthant projection, the overhead quickly amortizes because of the boost in dimensionality reduction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Algorithm</head><p>Improved Algorithm The algorithms were run with input vectors of dimensionality 1000 and initial sparseness 0.15 to compute projections with sparseness 0.90. Standard deviations were always less than 1%; they were omitted in the plot to avoid clutter. The algorithm proposed in this paper reduces dimensionality more quickly and terminates earlier than the original algorithm. For determination of the relative speedup incorporated with both the simplex projection and the access to unit-stride arrays due to the permutation-invariance, both algorithms were implemented as C++ programs using an optimized implementation of the BLAS library for carrying out the vector operations. The employed processor was an Intel Core i7-990X. For a range of different dimensionalities, a set of vectors with varying initial sparseness were sampled. The number of the vectors for every pair of dimensionality and initial sparseness was chosen such that the processing time of the algorithms was several orders of magnitudes greater than the latency time of the operation system. Then the absolute time needed for the algorithms to compute the projections with a target sparseness of 0.90 were measured, and their ratio was taken to compute the relative speedup. The results of this experiment are depicted in <ref type="figure" target="#fig_4">Figure 3</ref>. It is evident that the maximum speedup is achieved for vectors with a dimensionality between 2 9 and 2 15 , and an initial sparseness greater than 0.40. For low initial sparseness, as is achieved by randomly sampled vectors, a speedup of about 2.5 can be achieved for a broad spectrum of dimensionality between 2 4 and 2 13 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Iteration</head><p>The improvements to the original algorithm are thus not only theoretical, but also noticeable in practice. The speedup is especially useful when the projection is used as a neuronal transfer function in a classifier as proposed in Section 3, because then the computational complexity of the prediction of class membership of unknown samples can be reduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Function Definition and Differentiability</head><p>It is clear from Theorem 2 that the projection onto D is unique almost everywhere. Therefore the set R := { x ? R n | |proj D (x)| 1 } is a null set. However, R ? as for example the projection is not unique for vectors where all entries are identical. In other words, for x := ?e ? R n for some ? ? R follows proj H (x) = m and proj L (m) = L. If n = 2 a possible solution is given by (?, ?) T ? proj D (x) with ? and ? given as stated in Remark 18, as in this case ? and ? are positive. Additionally, another solution is given by (?, ?) T ? proj D (x) which is unequal to the other solution because of ? ?. A similar argument can be used to show non-uniqueness for all n ? 2. As R is merely a small set, non-uniqueness is not an issue in practical applications.</p><p>The sparseness-enforcing projection operator that is restricted to non-negative solutions can thus be cast almost everywhere as a function</p><formula xml:id="formula_7">? ?0 : R n \ R ? D, x ? proj D (x).</formula><p>Exploiting reflection-invariance implies that the unrestricted variant of the projection</p><formula xml:id="formula_8">? : R n \ R ? S (? 1 ,? 2 ) , x ? s ? ? ?0 (|x|) ,</formula><p>is well-defined, where s ? { ?1 } n is given as described in Lemma 11. Note that computation of ? ?0 is a crucial prerequisite to computation of the unrestricted variant ?. It will be used exclusively in Section 3 because non-negativity is not necessary in the application proposed there. If ? or ? ?0 is employed in an objective function that is to be optimized, the information whether these functions are differentiable is crucial for selecting an optimization strategy. As an example, consider once more projections onto Z := { x ? R n | x 0 = ? } where ? ? N is a constant. It was already mentioned in Section 2 that the projection onto Z consists simply of zeroing out the elements that are smallest in absolute value. Let x ? R n be a point and let ? ? S n be a permutation such that x ?(1) ? ? ? ? ? x ?(n) . Clearly, if x ?(?)</p><p>x ?(?+1) then proj Z (x) = y where y i = x i for i ? {?(1), . . . , ?(?)} and y i = 0 for i ? {?(? + 1), . . . , ?(n)}. Moreover, when x ?(?)</p><p>x ?(?+1) then there exists a neighborhood U of x such that proj</p><formula xml:id="formula_9">Z (s) = ? ? i=1 s ?(i) e ?(i) for all s ? U. With this closed- form expression, s ? proj Z (s) is differentiable in x with gradient ? proj Z (x) /?x = diag ? ? i=1 e ?(i)</formula><p>, that is the identity matrix where the entries on the diagonal belonging to small absolute values of x have been zeroed out. If the requirement on x is not fulfilled, then a small distortion of x is sufficient to find a point in which the projection onto Z is differentiable.</p><p>In contrast to the L 0 projection, differentiability of ? and ? ?0 is non-trivial. A full-length discussion is given in Appendix D, and concludes that both ? and ? ?0 are differentiable almost everywhere. It is more efficient when only the product of the gradient with an arbitrary vector needs to be computed, see Corollary 36. Such an expression emerges in a natural way by application of the chain rule to an objective function where the sparseness-enforcing projection is used. In practice this weaker form is thus mostly no restriction and preferable for efficiency reasons over the more general complete gradient as given in Theorem 35.</p><p>The derivative of ? ?0 is obtained by exploiting the structure of Algorithm 1. Because the projection onto D is essentially a composition of projections onto H, C, L and L I , the overall gradient can be computed using the chain rule. The gradients of the intermediate projections are simple expressions and can be combined to yield one matrix for each iteration of alternating projections. Since these iteration gradients are basically sums of dyadic products, their product with an arbitrary vector can be computed by primitive vector operations. With matrix product associativity, this process can be repeated to efficiently compute the product of the gradient of ? ?0 with an arbitrary vector. For this, it is sufficient to record some intermediate quantities during execution of Algorithm 3, which does not add any major overhead to the algorithm itself. The gradient of the unrestricted variant ? can be deduced in a straightforward way from the gradient of ? ?0 because of their close relationship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sparse Activity and Sparse Connectivity in Supervised Learning</head><p>The sparseness-enforcing projection operator can be cast almost everywhere as vector-valued function ?, which is differentiable almost everywhere, see Section 2.4. This section proposes a hybrid of an auto-encoder network and a two-layer neural network, where the sparseness projection is employed as a neuronal transfer function. The proposed model is called supervised online autoencoder (SOAE) and is intended for classification by means of a neural network that features sparse activity and sparse connectivity. Because of the analytical properties of the sparseness-enforcing projection operator, the model can be optimized end-to-end using gradient-based methods. <ref type="figure">Figure 4</ref> depicts the data flow in the proposed model. There is one module for reconstruction capabilities and one module for classification capabilities. The reconstruction module, depicted on the left of <ref type="figure">Figure 4</ref>, operates by converting an input sample x ? R d into an internal representation h ? R n , and then computing an approximationx ? R d to the original input sample. In doing so, the product u ? R n of the input sample with a matrix of bases W ? R d?n is computed, and a transfer function f : R n ? R n is applied. For sparse activity, f can be chosen to be the sparseness-enforcing projection operator ? or the projection with respect to the L 0 pseudo-norm. This guarantees that the internal representation is sparsely populated and close to u. The reconstruction is achieved like in a linear generative model, by multiplication of the matrix of bases with the internal representation. Hence the same matrix W is used for both encoding and decoding, rendering the reconstruction module symmetric, or in other words with tied weights. This approach is similar to principal com- <ref type="figure">Figure 4</ref>: Architecture and data flow of supervised online auto-encoder (SOAE). The circle on the left, mapping from an input sample x to its approximationx comprises the reconstruction module. The classification module consists of the mapping from x to the classification decision y. The matrix of bases W shall be sparsely populated to account for the sparse connectivity property. If the transfer function f is set to the sparseness projection, the internal representation h will be sparsely populated, fulfilling the sparse activity property. ponent analysis <ref type="bibr" target="#b31">(Hotelling, 1933)</ref>, restricted Boltzmann machines for deep auto-encoder networks <ref type="bibr" target="#b27">(Hinton et al., 2006)</ref> and to sparse encoding symmetric machine <ref type="bibr" target="#b62">(Ranzato et al., 2008)</ref>. By enforcing W to be sparsely populated, the sparse connectivity property holds as well. More formally, the aim is that ?(We i ) = ? W holds for all i ? {1, . . . , n}, where ? W ? (0, 1) is the target degree of connectivity sparseness and We i is the i-th column of W . This condition was adopted from non-negative matrix factorization with sparseness constraints <ref type="bibr" target="#b32">(Hoyer, 2004)</ref>. In the context of neural networks, the synaptic weights of individual neurons are stored in the columns of the weight matrix W . The interpretation of this formal sparseness constraint is then that each neuron is only allowed to be sparsely connected with the input layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><formula xml:id="formula_10">W ? R d?n ? out ? R c W out ? R n?c f : R n ? R n g : R c ? R c W ? R d?n W h =:x ? x ? R d u := W T x ? R n v := W T out h + ? out ? R c h := f (u) ? R n g(v) =: y ? t ? R c</formula><p>The classification module is shown on the right-hand side of <ref type="figure">Figure 4</ref>. It computes a classification decision y ? R c by feeding h through a one-layer neural network. The network output y is yielded through computation of the product with a matrix of weights W out ? R n?c , addition of a threshold vector ? out ? R c and application of a transfer function g : R c ? R c . This module shares the inference of the internal representation with the reconstruction module, which can also be considered a one-layer neural network. Therefore the entire processing path from x to y forms a two-layer neural network <ref type="bibr" target="#b65">(Rumelhart et al., 1986)</ref>, where W stores the synaptic weights of the hidden layer, and W out and ? out are the parameters of the output layer.</p><p>The input sample x shall be approximated byx, and the target vector for classification t ? R c shall be approximated by y. This is achieved by optimization of the parameters of SOAE, that is the quantities W , W out and ? out . The goodness of the approximation x ?x is estimated using a differentiable similarity measure s R : R d ? R d ? R, and the approximation y ? t is assessed by another similarity measure s C : R c ? R c ? R. For minimizing the deviation in both approximations, the objective function</p><formula xml:id="formula_11">E SOAE (W, W out , ? out ) := (1 ? ?) ? s R (x, x) + ? ? s C (y, t)</formula><p>shall be optimized, where ? ? [0, 1] controls the trade-off between reconstruction and classification capabilities. To incorporate sparse connectivity, feasible solutions are restricted to fulfill ?(We i ) = ? W for all i ? {1, . . . , n}. If ? = 0, then SOAE is identical to a symmetric auto-encoder network with sparse activity and sparse connectivity. In the case of ? = 1, SOAE forms a two-layer neural network for classification with a sparsely connected hidden layer and where the activity in the hidden layer is sparse. The parameter ? can also be used to blend continuously between these two extremes. Note thatx only depends on W but not on W out or ? out , but y depends on W , W out and ? out . Hence W out and ? out are only relevant when ? &gt; 0, whereas W is essential for all choices of ?.</p><p>An appropriate choice for s R is the correlation coefficient (see for example <ref type="bibr" target="#b64">Rodgers and Nicewander, 1988)</ref>, because it is normed to values in the interval [?1, 1], invariant to affine-linear transformations, and differentiable. If f is set to ?, then a model that is invariant to the concrete scaling and shifting of the occurring quantities can be yielded. This follows because ? is also invariant to such transformations, see Corollary 19. The similarity measure for classification capabilities s C is chosen to be the cross-entropy error function <ref type="bibr" target="#b3">(Bishop, 1995)</ref>, which was shown empirically by <ref type="bibr" target="#b68">Simard et al. (2003)</ref> to induce better classification capabilities than the mean squared error function. The softmax transfer function <ref type="bibr" target="#b3">(Bishop, 1995)</ref> is used as transfer function g of the output layer. It provides a natural pairing together with the cross-entropy error function <ref type="bibr" target="#b17">(Dunne and Campbell, 1997)</ref> and supports multi-class classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Algorithm</head><p>The proposed optimization algorithm for minimization of the objective function E SOAE is projected gradient descent <ref type="bibr" target="#b2">(Bertsekas, 1999)</ref>. Here, each update to the degrees of freedom is followed by application of the sparseness projection to the columns of W to enforce sparse connectivity. There are theoretical results on the convergence of projected gradient methods when projections are carried out onto convex sets <ref type="bibr" target="#b2">(Bertsekas, 1999)</ref>, but here the target set for projection is non-convex. Nevertheless, the experiments described below show that projected gradient descent is an adequate heuristic in the situation of the SOAE framework to tune the network parameters. For completeness, the gradients of E SOAE with respect to the network parameters are given in Appendix E. Update steps are carried out after every presentation of a pair of an input sample and associated target vector. This online learning procedure results in faster learning and improves generalization capabilities over batch learning <ref type="bibr" target="#b79">(Wilson and Martinez, 2003;</ref><ref type="bibr" target="#b5">Bottou and LeCun, 2004)</ref>.</p><p>A learning set with samples from R d and associated target vectors from { 0, 1 } c as one-of-ccodes is input to the algorithm. The dimensionality of the internal representation n and the target degree of sparseness with respect to the connectivity ? W ? (0, 1) are parameters of the algorithm. Sparseness of connectivity increases for larger ? W , as Hoyer's sparseness measure is employed in the definition of the set of feasible solutions.</p><p>Two possible choices for the hidden layer's transfer function f to achieve sparse activity were discussed in this paper. One possibility is to carry out the projection with respect to the L 0 pseudonorm. The more sophisticated method is to use the unrestricted sparseness-enforcing projection operator ? with respect to Hoyer's sparseness measure ?, which can be carried out by Algorithm 3.</p><p>In both cases, a target degree for sparse activity is a parameter of the learning algorithm. In case of the L 0 projection, this sparseness degree is denoted by ? ? {1, . . . , n}, and sparseness increases with smaller values of it. For the ? projection, ? H ? (0, 1) is used, where larger values indicate more sparse activity.</p><p>Initialization of the columns of W is achieved by selecting a random subset of the learning set, similar to the initialization of radial basis function networks <ref type="bibr" target="#b3">(Bishop, 1995)</ref>. This ensures significant activity of the hidden layer from the very start, resulting in strong gradients and therefore reducing training time. The parameters of the output layer, that is W out and ? out , are initialized by sampling from a zero-mean Gaussian distribution with a standard deviation of 1 /100.</p><p>In every epoch, a randomly selected subset of samples and associated target vectors from the learning set is used for stochastic gradient descent to update W , W out and ? out . The results from Appendix E can be used to efficiently compute the gradient of the objective function. There, the gradient for the transfer function f only emerges as a product with a vector. The gradient for the L 0 projection is trivial and was given as an example in Section 2.4. If f is Hoyer's sparseness-enforcing projection operator, it is possible to exploit that only the product of the gradient with a vector is needed. In this case, it is more efficient to compute the result of the multiplication implicitly using Corollary 36 and thus avoid the computation of the entire gradient of ?.</p><p>After every epoch, a sparseness projection is applied to the columns of W . This guarantees that ?(We i ) = ? W holds for all i ? {1, . . . , n}, and therefore the sparse connectivity property is fulfilled. The trade-off variable ? which controls the weight of the reconstruction and the classification term is adjusted according to ?(?) := 1 ? exp (? ? /100), where ? ? N denotes the number of the current epoch. Thus ? starts at zero, increases slowly and asymptotically reaches one. The emphasis at the beginning of the optimization is thus on reconstruction capabilities. Subsequently, classification capabilities are incorporated slowly, and in the final phase of training classification capabilities exclusively are optimized. This continuous variant of unsupervised pre-training <ref type="bibr" target="#b27">(Hinton et al., 2006)</ref> leads to parameters in the vicinity of a good minimizer for classification capabilities before classification is preferred over reconstruction through the trade-off parameter ?. Compared to the choice ? ? 1 this strategy helps to stabilize the trajectory in parameter space and makes the objective function values settle down more quickly, such that the termination criterion is satisfied earlier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Description of Experiments</head><p>To assess the classification capabilities and the impact of sparse activity and sparse connectivity, the MNIST database of handwritten digits (LeCun and Cortes, 1998) was employed. It is a popular benchmark data set for classification algorithms, and numerous results with respect to this data set are reported in the literature. The database consists of 70 000 samples, divided into a learning set of 60 000 samples and an evaluation set of 10 000 samples. Each sample represents a digit of size 28?28 pixels and has a class label from {0, . . . , 9} associated with it. Therefore the input and output dimensionalities are d := 28 2 = 784 and c := 10, respectively. The classification error is given in percent of all 10 000 evaluation samples, hence 0.01% corresponds to a single misclassified digit.</p><p>For generation of the original data set, the placement of the digits has been achieved based on their barycenter <ref type="bibr" target="#b41">(LeCun and Cortes, 1998)</ref>. Because of sampling and rounding errors, the localization uncertainty can hence be assumed to be less than one pixel in both directions. To account for this uncertainty, the learning set was augmented by jittering each sample in each of eight possible directions by one pixel, yielding 540 000 samples for learning in total. The evaluation set was left unchanged to yield results that can be compared to the literature. As noted by <ref type="bibr" target="#b27">Hinton et al. (2006)</ref>, the learning problem is no more permutation-invariant due to the jittering, as information on the neighborhood of the pixels is implicitly incorporated in the learning set.</p><p>However, classification results improve dramatically when such prior knowledge is used. This was demonstrated by <ref type="bibr" target="#b66">Sch?lkopf (1997)</ref> using the virtual support vector method, which improved a support vector machine with polynomial kernel of degree five from an error of 1.4% to 1.0% by jittering the support vectors by one pixel in four principal directions. This result was extended by DeCoste and <ref type="bibr" target="#b11">Sch?lkopf (2002)</ref>, where a support vector machine with a polynomial kernel of degree nine was improved from an error of 1.22% to 0.68% by jittering in all possible eight directions. Further improvements can be achieved by generating artificial training samples using elastic distortions <ref type="bibr" target="#b68">(Simard et al., 2003)</ref>. This reduced the error of a two-layer neural network with 800 hidden units to 0.7%, compared to the 1.1% error yielded when training on samples created by affine distortions. Very big and very deep neural networks possess a large number of adaptable weights. In conjunction with elastic and affine distortions such neural networks can yield errors as low as 0.35% <ref type="bibr" target="#b8">(Cire?an et al., 2010)</ref>. The current record error of 0.23% is held by an approach that combines distorted samples with a committee of convolutional neural networks <ref type="bibr" target="#b9">(Cire?an et al., 2012)</ref>. This is an architecture that has been optimized exclusively for input data that represents images, that is where the neighborhood of the pixels is hard-wired in the classifier. To allow for a plain evaluation that does not depend on additional parameters for creating artificial samples, the jittered learning set with 540 000 samples is used throughout this paper.</p><p>The experimental methodology was as follows. The number of hidden units was chosen to be n := 1000 in all experiments that are described below. This is an increased number compared to the 800 hidden units employed by <ref type="bibr" target="#b68">Simard et al. (2003)</ref>, but promises to yield better results when an adequate number of learning samples is used. As all tested learning algorithms are essentially gradient descent methods, an initial step size had to be chosen. For each candidate step size, five runs of a two-fold cross validation were carried out on the learning set. Then, for each step size the median of the ten resulting classification errors was computed. The winning step size was then determined to be the one that achieved a minimum median of classification errors.</p><p>In every epoch, 21 600 samples were randomly chosen from the learning set and presented to the network. This number of samples was chosen as it is 1 /25-th of the jittered learning set. The step size was multiplicatively annealed using a factor of 0.999 after every epoch. Optimization was terminated once the relative change in the objective function became very small and no more significant progress on the learning set could be observed. The resulting classifiers were then applied to the evaluation set, and misclassifications were counted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Experimental Results</head><p>Two variants of the supervised online auto-encoder architecture as proposed in this section were trained on the augmented learning set. In both variants, the target degree of sparse connectivity was set to ? W := 0.75. This choice was made because 96% of all samples in the learning set possess a sparseness which is less than 0.75. Therefore, the resulting bases are forced to be truly sparsely connected compared to the sparseness of the digits.</p><p>The first variant is denoted by SOAE-?. Here, the sparseness-enforcing projection operator ? was used as transfer function f in the hidden layer. Target degrees of sparse activity ? H with respect to Hoyer's sparseness measure ? were chosen from the interval [0.20, 0.95] in steps of size 0.05.</p><p>This variant was then trained on the jittered learning set using the method described in Section 3.2. For every value of ? H , the resulting sparseness of activity was measured after training using the L 0 pseudo-norm. For this, each sample of the learning set was presented to the networks, and the number of active units in the hidden layer was counted. <ref type="figure">Figure 5</ref> shows the resulting mean value and standard deviation of sparse activity. If ? H = 0.20 is chosen, then in the mean about 800 of the total 1000 hidden units are active upon presentation of a sample from the learning set. For ? H = 0.80 only one hundred units are active at any one time, and for ? H = 0.95 there are only eleven active units. The standard deviation of the activity decreases when sparseness increases, hence the mapping from ? H to the resulting number of active units becomes more accurate.</p><p>The second variant, denoted SOAE-L 0 , differs from SOAE-? in that the projection with respect to the L 0 pseudo-norm as transfer function f was used. The target sparseness of activity is given by a parameter ? ? {1, . . . , n}, which controls the exact number of units that are allowed to be active at any one time. For the experiments, the values for ? were chosen to match the mean activities from the SOAE-? experiments. This way the results of both variants can be compared based on a unified value of activity sparseness. The results are depicted in <ref type="figure" target="#fig_7">Figure 6</ref>. Usage of the ? projection consequently outperforms the L 0 projection for all sparseness degrees. Even for high sparseness of activity, that is when only about ten percent of the units are allowed to be active at any one time, good classification capabilities can be obtained with SOAE-?. For ? ? [242, 558], the classification results of SOAE-L 0 reach an optimum. SOAE-? is more robust, as classification capabilities first begin to collapse when sparseness is below 5%, whereas SOAE-L 0 starts to degenerate when sparseness falls below 20%. For ? H ? [0.45, 0.85], roughly translating to between 5% and 50% activity, about equal classification performance is achieved using SOAE-?.</p><p>It can thus be concluded that using the sparseness-enforcing projection operator as described in this paper yields better results than when the simple L 0 projection is used to achieve sparse activity. To assess the benefit more precisely and to investigate the effect of individual factors, several comparative experiments have been carried out. A summary of these experiments and their outcome is given in <ref type="table">Table 1</ref>. The variants SOAE-? and SOAE-L 0 denote the entirety of the respective experiments where sparseness of activity lies in the intervals described above, that is ? H ? [0.45, 0.85] and ? ? [242, 558], respectively. Using these intervals, SOAE-? and SOAE-L 0 achieved a median error of 0.75% and 0.82% on the evaluation set, respectively. Variant SOAE-?-conn is essentially equal to SOAE-?, except for sparse connectivity not being incorporated. Sparseness of activity here was also chosen to be ? H ? [0.45, 0.85], which resulted in about equal classification results over the entire range. Dropping of sparse connectivity increases misclassifications, for the median error of SOAE-?-conn is 0.81% and thereby greater than the median error of SOAE-?.</p><p>The other five approaches included in the comparison are multi-layer perceptrons (MLPs) with the same topology and dynamics as the classification module of supervised online auto-encoder, with two exceptions. First, the transfer function of the hidden layer f was set to a hyperbolic tangent, thus not including explicit sparse activity. Second, in all but one experiment sparse connectivity was either not incorporated, or achieved through other means than by performing a ? projection after each learning epoch. Besides the variation in sparseness of connectivity, the experiments differ in the initialization of the network parameters.</p><p>For each variant, 55 runs were carried out and the resulting classifiers were applied to the evaluation set to compute the classification error. Then, the best four and the worst four results were discarded and not included in further analysis. Hence a random sample of size 47 was achieved, where 15% of the original data were trimmed away. This procedure was also applied to the results  of SOAE-?, SOAE-?-conn, and SOAE-L 0 , to obtain a total of eight random samples of equal size for comparison with another. The most basic variant, denoted the baseline in this discussion, is MLP-random, where all network parameters were initialized randomly. This achieved a median error of 0.88% on the evaluation set, being considerably worse than SOAE-?. For variant MLP-samples, the hidden layer was initialized by replication of n randomly chosen samples from the learning set. This did decrease the overall learning time. However, the median classification error was slightly worse with 0.91% compared to MLP-random.</p><p>For variant MLP-SCFC, the network parameters were initialized in an unsupervised manner using the sparse coding for fast classification (SCFC) algorithm <ref type="bibr" target="#b72">(Thom et al., 2011a)</ref>. This method is a precursor to the SOAE proposed in this paper. It also features sparse connectivity and sparse activity but differs in some essential parts. First, sparseness of activity is achieved through a latent variable that stores the optimal sparse code words of all samples simultaneously. Using this matrix of code words, the activity of individual units was enforced to be sparse over time on the entire learning set. SOAE achieves sparseness over space, as for each sample only a pre-defined fraction of units is allowed to be active at any one time. A second difference is that sparse activity is achieved only indirectly by approximation of the latent matrix of code words with a feed-forward representation. With SOAE, sparseness of activity is guaranteed by construction. MLP-SCFC achieved a median classification error of 0.91% on the MNIST evaluation set, rendering it slightly worse than MLP-random and equivalent to MLP-samples.</p><p>The first experiment that incorporates only sparse connectivity is SMLP-SCFC. Initialization was done as for MLP-SCFC, but during training sparseness of connectivity was yielded by application of the sparseness-enforcing projection operator to the weights of the hidden layer after every learning epoch. Hence the sparseness gained from unsupervised initialization was retained.</p><p>MLP-SCFC features sparse connectivity only after initialization, but loses this property when training proceeds. With this slight modification, the median error of SMLP-SCFC decreases to 0.81%, which is significantly better than the baseline result.</p><p>The effect of better generalization due to sparse connectivity has also been observed by <ref type="bibr" target="#b42">LeCun et al. (1990)</ref> in the context of convolutional neural networks. It can be explained by the bias-variance decomposition of the generalization error <ref type="bibr" target="#b22">(Geman et al., 1992)</ref>. When the effective number of the degrees of freedom is constrained, overfitting will be less likely and hence classifiers produce better results on average. The same argument can be applied to SOAE-?, where additional sparse activity further improves classification results.</p><p>The last variant is called MLP-OBD. Here, the optimal brain damage (OBD) algorithm <ref type="bibr" target="#b42">(LeCun et al., 1990</ref>) was used to prune synaptic connections in the hidden layer that are irrelevant for the computation of the classification decision of the network. The parameters of the network were first initialized randomly and then optimized on the learning set. Then the impact for each synaptic connection on the objective function was estimated using the Taylor series of the objective function, where a diagonal approximation of the Hessian was employed and terms of cubic or higher order were neglected. Using this information, the number of connections was halved by setting the weight of connections with low impact to zero. The network was then retrained with weights of removed connections kept at zero. This procedure was repeated until a target percentage ? of active synaptic connections in the hidden layer was achieved. For the results reported here, ? = 12.5% was chosen as this reflects the sparse connectivity ? W = 0.75 of the other approaches best. MLP-OBD achieved a median classification error of 0.89%, which is comparable to the baseline result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Statistical Analysis and Conclusions</head><p>A statistical analysis was carried out to assess the significance of the differences in the performance of the eight algorithms. The procedure follows the proposals of <ref type="bibr" target="#b60">Pizarro et al. (2002)</ref> and <ref type="bibr" target="#b12">Dem?ar (2006)</ref> for hypothesis testing, and is concluded by effect size estimation as proposed by <ref type="bibr" target="#b25">Grissom (1994)</ref> and <ref type="bibr" target="#b0">Acion et al. (2006)</ref>. For each algorithm, a sample of size 47 was available, allowing for robust analysis results.</p><p>First, all results were tested for normality using the test developed by <ref type="bibr" target="#b67">Shapiro and Wilk (1965)</ref>. The resulting test statistics W and p-values are given in <ref type="table">Table 1</ref>. As all p-values are large, it cannot be rejected that the samples came from normally distributed populations. Thus normality is assumed in the remainder of this discussion. Next, the test proposed by <ref type="bibr" target="#b44">Levene (1960)</ref> was applied to determine whether equality of variances of the groups holds. This resulted in a test statistic F = 2.7979 with 7 and 368 degrees of freedom, and therefore a p-value of 0.0075. Hence the hypothesis that all group variances are equal can be rejected with very high significance. Consequently, parametric omnibus and post-hoc tests cannot be applied, as they require the groups to have equal variance.</p><p>As an alternative, the nonparametric test by <ref type="bibr" target="#b38">Kruskal and Wallis (1952)</ref> which is based on rank information was employed to test whether all algorithms produced classifiers with equal classification errors in the mean. The test statistic was H = 214.44 with 7 degrees of freedom, and the p-value was less than 10 ?15 . There is hence a statistically significant difference in the mean classification results. To locate this deviation, a critical difference for comparing the mean ranks of the algorithms was computed. A Tukey-Kramer type modification applied to Dunn's procedure yields this critical difference, which is less conservative than Nemenyi's procedure for the Kruskal-Wallis test <ref type="bibr" target="#b29">(Hochberg and Tamhane, 1987)</ref>. Note that this approach is nevertheless similar to the post-hoc 0 50 100 150 200 250 300</p><formula xml:id="formula_12">CD (? = 0.01) SOAE-? MLP-samples SOAE-?-conn MLP-SCFC SMLP-SCFC MLP-OBD SOAE-L 0 MLP-random Figure 7</formula><p>: Diagram for multiple comparison of algorithms following <ref type="bibr" target="#b12">Dem?ar (2006)</ref>. For each algorithm, the mean rank was computed during the Kruskal-Wallis test. Then, a critical difference (CD) was computed at the ? = 0.01 significance level. Two algorithms produce classification results that are statistically not equal if the difference between their mean ranks is greater than the critical difference. This induced three groups of algorithms that produced statistically equivalent results, which are marked with black bars.</p><p>procedure proposed by <ref type="bibr" target="#b12">Dem?ar (2006)</ref> for paired observations, such that the diagrams proposed there can be adapted to the case for unpaired observations. The result is depicted in <ref type="figure">Figure 7</ref>, where the critical difference for statistical significance at the ? = 0.01 level is given. This test induces a highly significant partitioning of the eight algorithms, namely three groups A, B and C given by</p><formula xml:id="formula_13">A := { SOAE-? } , B := { SOAE-?-conn, SOAE-L 0 , SMLP-SCFC } , and C := { MLP-OBD, MLP-random, MLP-samples, MLP-SCFC } .</formula><p>This partition in turn induces an equivalence relation. Statistical equivalence is hence unambiguous and well-defined at ? = 0.01. Moreover, the p-value for this partition is 0.007. If the significance level ? would have been set lower than this, then groups A and B would blend together. To assess the benefit when an algorithm from one group is chosen over an algorithm from another group, the probability of superior experiment outcome was estimated <ref type="bibr" target="#b25">(Grissom, 1994;</ref><ref type="bibr" target="#b0">Acion et al., 2006)</ref>. For this, the classification errors were pooled with respect to membership in the three groups. It was then tested whether these pooled results still come from normal distributions. As group A is a singleton, this is trivially fulfilled with the result from <ref type="table">Table 1</ref>. For group B, the Shapiro-Wilk test statistic was W = 0.9845 and the p-value was 0.11. Group C achieved a test statistic of W = 0.9882 and a p-value of 0.12. If a standard significance level of ? = 0.01 is chosen, then B and C can be assumed to be normally distributed also.</p><p>Let E G be the random variable modeling the classification results of the algorithms from group G ? { A, B,C }. It is assumed that E G is normally distributed with unknown mean and unknown variance for all G. Then E G ? EG is clearly normally distributed also for two groups G,G ? { A, B,C }. Therefore, the probability P(E G &lt; EG) that one algorithm produces a better classifier than another could be computed from the Gaussian error function if the group means and variances were known. However, using Rao-Blackwell theory a minimum variance unbiased estimatorR 2 of this probability can be computed easily <ref type="bibr" target="#b15">(Downton, 1973)</ref>. Evaluation of the expression forR 2 shows that P(E A &lt; E B ) can be estimated by 0.87, P(E B &lt; E C ) can be estimated by 0.88, and P(E A &lt; E C ) can be estimated by 0.99. Therefore, the effect of choosing SOAE-? over any of the seven other algorithms is dramatic <ref type="bibr" target="#b25">(Grissom, 1994)</ref>.</p><p>These results can be interpreted as follows. When neither sparse activity nor sparse connectivity is incorporated, then the worst classification results are obtained regardless of the initialization of the network parameters. The exception is MLP-OBD which incorporates sparse connectivity, although, as its name says, in a destructive way. Once a synaptic connection has been removed, it cannot be recovered, as the measure for relevance of <ref type="bibr" target="#b42">LeCun et al. (1990)</ref> vanishes for synaptic connections of zero strength. The statistics for SMLP-SCFC shows that when sparse connectivity is obtained using the sparseness-enforcing projection operator, then superior results can be achieved. Because of the nature of projected gradient descent, it is possible here to restore deleted connections if it helps to decrease the classification error during learning. For SOAE-?-conn only sparse activity was used, and classification results were statistically equivalent to SMLP-SCFC.</p><p>Therefore, using either sparse activity or sparse connectivity improves classification capabilities. When both are used, then results improve even more as variant SOAE-? shows. This does not hold for SOAE-L 0 however, where the L 0 projection was used as transfer function. As Hoyer's sparseness measure ? and the according projection possess desirable analytical properties, they can be considered smooth approximations to the L 0 pseudo-norm. It is this smoothness which seems to produce this benefit in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>This section reviews work related with the contents of this paper. First, the theoretical foundations of the sparseness-enforcing projection operator are discussed. Next, its application as neuronal transfer function to achieve sparse activity in a classification scenario is put in context with alternative approaches, and possible advantages of sparse connectivity are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Sparseness-Enforcing Projection Operator</head><p>The first major part of this paper dealt with improvements to the work of <ref type="bibr" target="#b32">Hoyer (2004)</ref> and <ref type="bibr" target="#b71">Theis et al. (2005)</ref>. Here, an algorithm for the sparseness-enforcing projection with respect to Hoyer's sparseness measure ? was proposed. The technical proof of correctness is given in Appendix C. The set that should be projected onto is an intersection of a simplex C and a hypercircle L, which is a hypersphere lying in a hyperplane. The overall procedure can be described as performing alternating projections onto C and certain subsets of L. This approach is common for handling projections onto intersections of individual sets. For example, von <ref type="bibr" target="#b77">Neumann (1950)</ref> proposed essentially the same idea when the investigated sets are closed subspaces, and has shown that this converges to a solution. A similar approach can be carried out for intersections of closed, convex cones <ref type="bibr" target="#b18">(Dykstra, 1983)</ref>, which can be generalized to translated cones that can be used to approximate any convex set <ref type="bibr" target="#b19">(Dykstra and Boyle, 1987)</ref>. For these alternating methods, it is only necessary to know how projections onto individual members of the intersection can be achieved.</p><p>Although these methods exhibit great generality, they have two severe drawbacks in the scenario of this paper. First, the target set for projection must be an intersection of convex sets. The scaled canonical simplex C is clearly convex, but the hypercircle L is non-convex if it contains more than one point. The condition that generates L cannot easily be weakened to achieve convexity. If the original hypersphere were replaced with a closed ball, then L would be convex. But this changes the meaning of the problem dramatically, as now virtually any sparseness below the original target degree of sparseness can be obtained. This is because when the target L 1 norm ? 1 is fixed, the sparseness measure ? decreases whenever the target L 2 norm decreases. In geometric terms, the method proposed in this paper performs a projection from within a circle onto its boundary to increase the sparseness of the working vector. This argument is given in more detail in <ref type="figure" target="#fig_0">Figure 11</ref> and the proof of Lemma 28(f).</p><p>The second drawback of the general methods for projecting onto intersections is that a solution is only achieved asymptotically, even when the convexity requirements are fulfilled. Due to the special structure of C and L, the number of alternating projections that have to be carried out to find a solution using Algorithm 3 is bounded from above by the problem dimensionality. Thus an exact projection is always found in finite time. Furthermore, the solution is guaranteed to be found in time that is at most quadratic in problem dimensionality.</p><p>A crucial point is the computation of the projection onto C and certain subsets of L. Due to the nature of the L 2 norm, the latter is straightforward. For the former, efficient algorithms have been proposed recently <ref type="bibr" target="#b16">(Duchi et al., 2008;</ref><ref type="bibr" target="#b7">Chen and Ye, 2011)</ref>. When only independent solutions are required, the projection of a point x onto a scaled canonical simplex of L 1 norm ? 1 can also be carried out in linear time <ref type="bibr" target="#b46">(Liu and Ye, 2009)</ref>, without having to sort the vector that is to be projected. This can be achieved by showing that the separatort for performing the simplex projection is the unique zero of the monotonically decreasing function t ? max (|x| ? t ? e, 0) 1 ? ? 1 . The zero of this function can be found efficiently using the bisection method, and exploiting the special structure of the occurring expressions <ref type="bibr" target="#b46">(Liu and Ye, 2009</ref>).</p><p>In the context of this paper an explicit closed-form expression fort is preferable as it permits additional insight into the properties of the projected point. The major part in proving the correctness of Algorithm 1 is the interconnection between C and L, that is that the final solution has zero entries at the according positions in the working vector and thus a chain monotonically decreasing in L 0 pseudo-norm is achieved. This result is established through Lemma 26, which characterizes projections onto certain faces of a simplex, Corollary 27 and their application in Lemma 28.</p><p>Analysis of the theoretical properties of the sparseness-enforcing projection is concluded with its differentiability in Appendix D. The idea is to exploit the finiteness of the projection sequence and to apply the chain rule of differential calculus. It is necessary to show that the projection chain is robust in a neighborhood of the argument. This reduces analysis to individual projection steps which have already been studied in the literature. For example, the projection onto a closed, convex set is guaranteed to be differentiable almost everywhere <ref type="bibr" target="#b28">(Hiriart-Urruty, 1982)</ref>. Here non-convexity of L is not an issue, as the only critical point is its barycenter. For the simplex C, a characterization of critical points is given with Lemma 32 and Lemma 33, and it is shown that the expression for the projection onto C is invariant to local changes. An explicit expression for construction of the gradient of the sparseness-enforcing projection operator is given in Theorem 35. In Corollary 36 it is shown that the computation of the product of the gradient with an arbitrary vector can be achieved efficiently by exploiting sparseness and the special structure of the gradient.</p><p>Similar approaches for sparseness projections are discussed in the following. The iterative hard thresholding algorithm is a gradient descent algorithm, where a projection onto an L 0 pseudo-norm constraint is performed <ref type="bibr" target="#b4">(Blumensath and Davies, 2009</ref>). Its application lies in compressed sensing, where a linear generative model is used to infer a sparse representation for a given observation. Sparseness here acts as regularizer which is necessary because observations are sampled below the Nyquist rate. In spite of the simplicity of the method, it can be shown that it achieves a good approximation to the optimal solution of this NP-hard problem <ref type="bibr" target="#b4">(Blumensath and Davies, 2009)</ref>.</p><p>Closely related with the work of this paper is the generalization of Hoyer's sparseness measure by <ref type="bibr" target="#b70">Theis and Tanaka (2006)</ref>. Here, the L 1 norm constraint is replaced with a generalized L p pseudo-norm constraint, such that the sparseness measure becomes ? p (x) := x p/ x 2 . For p = 1, Hoyer's sparseness measure up to a constant normalization is obtained. When p converges decreasingly to zero, then ? p (x) p converges point-wise to the L 0 pseudo-norm. Hence for small values of p a more natural sparseness measure is obtained. <ref type="bibr" target="#b70">Theis and Tanaka (2006)</ref> also proposed an extension of Hoyer's projection algorithm. It is essentially von Neumann's alternating projection method, where closed subspaces have been replaced by "spheres" that are induced by L p pseudo-norms. Note that these sets are non-convex when p &lt; 1, such that convergence is not guaranteed. Further, no closedform solution for the projection onto an "L p -sphere" is known for p { 1, 2, ? }, such that numerical methods have to be employed.</p><p>A problem where similar projections are employed is to minimize a convex function subject to group sparseness (see for example <ref type="bibr" target="#b20">Friedman et al., 2010)</ref>. In this context, mixed norm balls are of particular interest <ref type="bibr" target="#b69">(Sra, 2012)</ref>. For a matrix X ? R n?g , the mixed L p,q norm is defined as the L p norm of the L q norms of the columns of X, that is X p,q := Xe 1 q , . . . , Xe g q T p . Here, X can be interpreted to be a data point with entries partitioned into g groups. When p = 1, then the projection onto a simplex can be generalized directly for q = 2 (van den <ref type="bibr" target="#b74">Berg et al., 2008)</ref> and for q = ? <ref type="bibr" target="#b61">(Quattoni et al., 2009</ref>). The case when p = 1 and q ? 1 is more difficult, but can be solved as well <ref type="bibr" target="#b47">(Liu and Ye, 2010;</ref><ref type="bibr" target="#b69">Sra, 2012)</ref>.</p><p>The last problem discussed here is the elastic net criterion <ref type="bibr" target="#b80">(Zou and Hastie, 2005)</ref>, which is a constraint on the sum of an L 1 norm and an L 2 norm. The feasible set can be written as the convex set N := { s ? R n | ? 1 s 1 + ? 2 s 2 2 ? 1 }, where ? 1 , ? 2 ? 0 control the shape of N. Note that in N only the sum of two norms is considered, whereas the non-convex set S (? 1 ,? 2 ) consists of the intersection of two different constraints. Therefore, the elastic net induces a different notion of sparseness than Hoyer's sparseness measure ? does. As is the case for mixed norm balls, the projection onto a simplex can be generalized to achieve projections onto N <ref type="bibr" target="#b49">(Mairal et al., 2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Supervised Online Auto-Encoder</head><p>The sparseness-enforcing projection operator ? with respect to Hoyer's sparseness measure ? and the projection onto an L 0 pseudo-norm constraint are differentiable almost everywhere. Thus they are suitable for gradient-based optimization algorithms. In Section 3, they were used as transfer functions in a hybrid of an auto-encoder network and a two-layer neural network to infer a sparse internal representation. This representation was subsequently employed to approximate the input sample and to compute a classification decision. In addition, the matrix of bases which was used to compute the internal representation was enforced to be sparsely populated by application of the sparseness projection after each learning epoch. Hence the supervised online auto-encoder proposed in this paper features both sparse activity and sparse connectivity.</p><p>These two key properties have also been investigated and exploited in the context of autoassociative memories for binary inputs. If the entries of the training patterns are sparsely populated, the weight matrix of the memory will be sparsely populated as well after training if Hebbian-like learning rules are used <ref type="bibr" target="#b37">(Kohonen, 1972)</ref>. The assumption of sparsely coded inputs also results in increased completion capacity and noise resistance of the associative memory <ref type="bibr" target="#b59">(Palm, 1980)</ref>. If the input data is not sparse inherently, feature detectors can perform a sparsification prior to the actual processing through the memory <ref type="bibr" target="#b1">(Baum et al., 1988)</ref>.</p><p>A purely generative model that also possesses these two key properties is non-negative matrix factorization with sparseness constraints <ref type="bibr" target="#b32">(Hoyer, 2004)</ref>. This is an extension to plain non-negative matrix factorization <ref type="bibr" target="#b58">(Paatero and Tapper, 1994)</ref> which was shown to achieve sparse connectivity on certain data sets <ref type="bibr" target="#b43">(Lee and Seung, 1999)</ref>. However, there are data sets on which this does not work <ref type="bibr" target="#b45">(Li et al., 2001;</ref><ref type="bibr" target="#b32">Hoyer, 2004)</ref>. Although Hoyer's model makes sparseness easily controllable by explicit constraints, it is not inherently suited to classification tasks. An extension intended to incorporate class membership information to increase discriminative capabilities was proposed by <ref type="bibr" target="#b26">Heiler and Schn?rr (2006)</ref>. In their approach, an additional constraint was added ensuring that every internal representation is close to the mean of all internal representations that belong to the same class. In other words, the method can be interpreted as supervised clustering, with the number of clusters equal to the number of classes. However, there is no guarantee that a distribution of internal representations exists such that both the reproduction error is minimized and the internal representations can be arranged in such a pattern. Unfortunately, <ref type="bibr" target="#b26">Heiler and Schn?rr (2006)</ref> used only a subset of a small data set for handwritten digit recognition to evaluate their approach.</p><p>A precursor to the supervised online auto-encoder was proposed by <ref type="bibr" target="#b72">Thom et al. (2011a)</ref>. There, inference of sparse internal representations was achieved by fitting a one-layer neural network to approximate a latent variable of optimal sparse representations. The transfer function used for this approximation was a hyperbolic tangent raised to an odd power greater or equal to three. This resulted in a depression of activities with small magnitude, favoring sparseness of the result. Similar techniques to achieve a shrinkage-like effect for increasing sparseness of activity in a neural network were used by <ref type="bibr" target="#b24">Gregor and LeCun (2010)</ref> and <ref type="bibr" target="#b23">Glorot et al. (2011)</ref>. Information processing is here purely local, that is a scalar function is evaluated entrywise on a vector, and thus no information is interchanged among individual entries.</p><p>The use of non-local shrinkage to reduce Gaussian noise in sparse coding has already been described by <ref type="bibr" target="#b35">Hyv?rinen et al. (1999)</ref>. Here, a maximum likelihood estimate with only weak assumptions yields a shrinkage operation, which can be conceived as projection onto a scaled canonical simplex. In the use case of object recognition, a hard shrinkage was also employed to de-noise filter responses <ref type="bibr" target="#b53">(Mutch and Lowe, 2006)</ref>. Whenever a best approximation from a permutationinvariant set is used, a shrinkage-like operation must be employed. Using a projection operator as neural transfer function is hence a natural extension of these ideas. When the projection is sufficiently smooth, the entire model can be tuned end-to-end using gradient methods to achieve an auto-encoder or a classifier.</p><p>The second building block from <ref type="bibr" target="#b72">Thom et al. (2011a)</ref> that was incorporated into supervised online auto-encoder is the architectural concept for classification. It is well-known that two layers in a neural network are sufficient to approximate any continuous function on a compactum with arbitrary precision <ref type="bibr" target="#b10">(Cybenko, 1989;</ref><ref type="bibr" target="#b21">Funahashi, 1989;</ref><ref type="bibr" target="#b30">Hornik et al., 1989)</ref>. Similar architectures have also been proposed for classification in combination with sparse coding of the inputs. However, sparse connectivity was not considered in this context. <ref type="bibr" target="#b6">Bradley and Bagnell (2009)</ref> used the Kullback-Leibler divergence as implicit sparseness penalty term and combined this with the backpropagation algorithm to yield a classifier that achieved a 1.30% error rate on the MNIST evaluation set. The Kullback-Leibler divergence was chosen to replace the usual L 1 norm penalty term, as it is smoother than the latter and therefore sparsely coded internal representations are more stable subject to subtle changes of the input. A related technique is supervised dictionary learning by <ref type="bibr" target="#b48">Mairal et al. (2009)</ref>, where the objective function is an additive combination of a classification error term, a term for the reproduction error, and an L 1 norm constraint. Inference of sparse internal representations is achieved through solving an optimization problem. Such procedures are time-consuming and greatly increase the computational complexity of classification. With this approach, a classifi-cation error of 1.05% on the MNIST evaluation set was achieved. These two approaches used the original MNIST learning set without jittering the digits and can thus be considered permutationinvariant. Augmentation of the learning set with virtual samples would have contributed to improve classification performance, as demonstrated by <ref type="bibr" target="#b66">Sch?lkopf (1997)</ref>.</p><p>Finally consider once more the sparse connectivity property, which is mostly neglected in the literature in favor of sparse activity. It was shown in this paper that sparse connectivity helps to improve generalization capabilities. In practice, this property can also be used to reduce the computational complexity of classification by one order of magnitude <ref type="bibr" target="#b73">(Thom et al., 2011b)</ref>. This results from exploiting sparseness and using sparse matrix-vector multiplication algorithms to infer the internal representation, which is the major computational burden in class membership prediction. It was shown in this paper and by <ref type="bibr" target="#b73">Thom et al. (2011b)</ref> that a small number of nonzero entries in the weight matrix of the hidden layer is sufficient for achieving good classification results. Furthermore, the additional savings in required storage capacity and bandwidth allow using platforms with modest computational power for practical implementations. Sparseness is therefore an elementary concept of efficiency in artificial processing systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Without sparseness in their brains, higher mammals probably would not have developed to viable life-forms. This important concept of efficiency was discovered by neuroscientists, and practical benefit was obtained by the engineers of artificial information processing systems. This paper studied Hoyer's sparseness measure ?, and in particular the projection of arbitrary vectors onto sets where ? attains a constant value. A simple yet efficient algorithm for computing this sparsenessenforcing projection operator was proposed in this paper, and its correctness was proved. In addition, it was demonstrated that the proposed algorithm is superior in run-time to Hoyer's original algorithm. The analysis of the theoretical properties of this projection was concluded by showing it is differentiable almost everywhere.</p><p>As projections onto ? constraints are well-understood, they constitute the ideal tool for building systems that can benefit from sparseness constraints. An original use case was introduced in this paper. Here, the ? projection was implemented as neuronal transfer function, yielding a differentiable closed-form expression for inference of sparse code words. Besides this sparse activity, the connectivity in this system was also forced to be sparse by performing the ? projection after the presentation of learning examples. Because of its smoothness, the entire system can be optimized end-to-end by gradient-based methods, yielding a classification architecture exhibiting true sparse information processing.</p><p>This supervised online auto-encoder was applied on a benchmark data set for pattern recognition. Because sparseness constraints reduce the amount of feasible solutions, it is not clear in the first place whether the same performance can be achieved at all. However, when the target degree of sparseness of the activity is in a reasonable range, classification results are not only equivalent but superior to classical non-sparse approaches. This result is supported by statistical evaluation showing that this performance increase is not merely coincidental, but statistically significant. Therefore, sparseness can be seen as regularizer that offers the potential to improve artificial systems in the same way it seems to improve biological systems. This appendix fixes the notation and provides prerequisites for the following appendices. N denotes the natural numbers including zero, R the real numbers and R ?0 the non-negative real numbers. R n is the n-dimensional Euclidean space with canonical basis e 1 , . . . , e n ? R n , and e := ? n i=1 e i ? R n denotes the vector where all entries are identical to unity. For all other vectors, a subscript denotes the corresponding entry of the vector, that is x i = e T i x for x ? R n . The amount of nonzero entries in a vector is given by the L 0 pseudo-norm, ? 0 . ? 1 and ? 2 denote the Manhattan norm and Euclidean norm, respectively. ?, ? denotes the canonical dot product in the Euclidean space. Given a vector x, diag(x) denotes the square matrix with x on its main diagonal and zero entries at all other positions, and a ? b = diag(a)b denotes the Hadamard product or entrywise product for vectors a and b. When A and B are square matrices, then diag(A, B) denotes the block diagonal matrix with the blocks given by A and B. S n is the symmetric group, and P ? denotes the permutation matrix for ? ? S n . For a set M ? U, M C denotes its complement in the universal set U, where U ? { R n , {1, . . . , n} } is clear from the context. The power set of M is denoted by ?(M). If M ? R n , then ?M denotes its boundary in the topological sense. The sign function is denoted by sgn(?). A list of symbols that are frequently used throughout the paper is given in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The important concept of the projection onto a set was given in Definition 1. The following basic statement will be used extensively in this paper and follows from x, x = x 2 2 for all x ? R n and the fact that the scalar product is a symmetric bilinear form <ref type="bibr" target="#b39">(Laub, 2004)</ref>:</p><formula xml:id="formula_14">Proposition 4 Let a, b ? R n . Then a ? b 2 2 = a 2 2 + b 2 2 ? 2 a, b . Further it is a ? b 2 2 = a ? p 2 2 + p ? b 2 2 + 2 a ? p, p ? b for all p ? R n .</formula><p>As an example, note that the outcome of the sparseness-enforcing projection operator depends only on the target sparseness degree up to scaling:</p><p>Remark 5 Let ? 1 , ? 2 &gt; 0 and? 1 ,? 2 &gt; 0 be pairs of target norms such that ? 1/? 2 =?1/? 2 . Then</p><formula xml:id="formula_15">proj S (? 1 ,? 2 ) (x) =?2/? 2 ? proj S (? 1 ,? 2 ) (x) for all x ? R n .</formula><p>Proof It is sufficient to show only one inclusion. Let x ? R n be arbitrary, p ? proj S (? 1 ,? 2 ) (x) and r ? S (? 1 ,? 2 ) . Definep :=?1/? 1 ? p =?2/? 2 ? p ? R n , then p 1 = ? 1/? 1 ? p 1 =? 1 and analogously p 2 =? 2 , hencep ? S (? 1 ,? 2 ) . For the claim to hold it has now to be shown that p ? x 2 ? r ? x 2 . Write r := ? 2/? 2 ?r ? R n , which in fact lies in S (? 1 ,? 2 ) . So p ? x 2 ? r ? x 2 by definition of p, and with Proposition 4 follows r ? x 2 2 ? p ? x 2 2 = r 2 2 + x 2 2 ? 2 r, x ? p 2 2 ? x 2 2 + 2 p, x = 2 p ?r, x =?2/? 2 ? 2 p ? r, x =?2/? 2 ? r ? x 2 2 ? p ? x 2 2 ? 0. Hence only the ratio of the target L 1 norm to the target L 2 norm is important and not their actual scale. This argument can be generalized to projections onto any scale-invariant set and therefore naturally holds also for S (? 1 ,? 2 ) ?0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symbol and Definition Meaning</head><p>? (see Section 1.1) Sparseness measure by <ref type="bibr" target="#b32">Hoyer (2004)</ref> ? and ? ?0 (see Section 2.4) Sparseness projection cast as function n ? N Problem dimensionality e 1 , . . . , e n ? R n Canonical basis of R n e := ? n i=1 e i ? R n Vector where all entries are one</p><formula xml:id="formula_16">? 1 ? R &gt;0 Target L 1 or Manhattan norm ? 2 ? R &gt;0 Target L 2 or Euclidean norm S (? 1 ,? 2 ) ? R n (see Section 1.1)</formula><p>Target set for sparseness projection</p><formula xml:id="formula_17">S (? 1 ,? 2 ) ?0 := S (? 1 ,? 2 ) ? R n ?0</formula><p>Target set for non-negative sparseness projection</p><formula xml:id="formula_18">D := S (? 1 ,? 2 ) ?0</formula><p>Short for the non-negative target set  </p><formula xml:id="formula_19">H := { a ? R n | e T a = ? 1 } Target hyperplane K := { q ? R n | q 2 = ? 2 } Target hypersphere L := H ? K Target hypercircle C := R n ?0 ? H Scaled canonical simplex m := ? 1/n ? e ? R</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Projections onto Symmetric Sets</head><p>This appendix investigates certain symmetries of sets and their effect on projections onto such sets. A great variety of sparseness measures fulfills certain symmetries as vector entries are equally weighted, see <ref type="bibr" target="#b34">Hurley and Rickard (2009)</ref>. This means that no entry is preferred over another, and for negative entries usually the absolute value or the squared value is taken, such that the signs of the entries are ignored. Consider the following definition of symmetries that are to be analyzed: In other words, a subset M of the Euclidean space is permutation-invariant if set membership is invariant to permutation of individual coordinates. M is reflection-invariant if single entries can be negated without violating set membership. This is equivalent to x ? 2 ? i?I x i e i ? M for all x ? M and all index sets I ? {1, . . . , n}, which is a condition that is technically easier to handle. The following observation states that these symmetries are closed under common set operations:</p><p>Remark <ref type="formula">7</ref>   Hence, a function f is order-preserving if the relative order of entries of its arguments does not change upon function evaluation. Thus if the entries of x are sorted in ascending or descending order, then so are the entries of every vector in f (x). Orthant-preservation denotes the fact that x and every vector from f (x) are located in the same orthant. The link between set symmetries and projection properties is established by the following result. A weaker form of its statements has been described by <ref type="bibr" target="#b16">Duchi et al. (2008)</ref> in the special case of a projection onto a simplex.</p><p>Lemma 9 Let ? M ? R n and p : R n ? ?(M), x ? proj M (x). Then the following holds:</p><p>(a) When M is permutation-invariant, then p is order-preserving.</p><p>(b) When M is reflection-invariant, then p is orthant-preserving.</p><p>Proof (a) Let x ? R n and p ? proj M (x). Let i, j ? {1, . . . , n} with x i &gt; x j . Assume that p i &lt; p j . Let ? := (i, j) ? S n and q := P ? p, then q ? M because of M being permutation-invariant. Consider d := p ? x 2 2 ? q ? x 2 2 . Because ? is a single transposition, application of Proposition 4 yields d = 2(p j ? p i )(x i ? x j ). By requirement d &gt; 0, which contradicts the minimality of p as being a projection of x onto M. Hence p i ? p j must hold.</p><formula xml:id="formula_20">(b) Let x ? R n and p ? proj M (x). Define I := { i ? {1, . . . , n} | sgn(x i ) sgn(p i ) }.</formula><p>The claim holds trivially if I = ?. Assume I ? and define q := p ? 2 ? i?I p i e i . It follows q ? M because M is reflection-invariant. Proposition 4 implies q 2 2 = p 2 2 , and clearly q, x = p, x ? 2 ? i?I p i x i . Therefore application of Proposition 4 yields d := p ? x 2 2 ? q ? x 2 2 = ?4 ? i?I p i x i . By the definition of I one obtains p i x i ? { ?1, 0 }. Hence would there be an index i ? I with p i 0 and x i 0, then d &gt; 0, but p ? x 2 2 &gt; q ? x 2 2 would contradict the minimality of p. Therefore I = { i ? {1, . . . , n} | p i = 0 or x i = 0 }, and the claim follows.</p><p>When the projection onto a permutation-invariant set is unique, then equal entries of the argument cause equal entries in the projection:</p><p>Remark 10 Let ? M ? R n be permutation-invariant and x ? R n . When p = proj M (x) is unique, then p i = p j follows for all i, j ? {1, . . . , n} with x i = x j .</p><p>Proof Let x ? R n , p = proj M (x) and i, j ? {1, . . . , n} with x i = x j . Assume p i p j would hold and let ? := (i, j) ? S n and q := P ? p p. With the permutation-invariance of M follows q ? M, and q ? x 2 = p ? x 2 with x i = x j . Hence q ? proj M (x), so q = p with the uniqueness of the projection, which contradicts q p. Therefore, p i = p j . The next result shows how solutions to a projection onto reflection-invariant sets can be turned into non-negative solutions and vice-versa. Its second part was already observed by <ref type="bibr" target="#b32">Hoyer (2004)</ref>, in the special case of the sparseness-enforcing projection operator, and by <ref type="bibr" target="#b16">Duchi et al. (2008)</ref>, when the connection between projections onto a simplex and onto an L 1 ball was studied. Both did not provide a proof, but in the latter work a hint to a possible proof was given. With Lemma 11 it suffices to consider non-negative solutions for projections onto reflection-invariant sets.</p><p>Lemma 11 Let ? A ? R n be reflection-invariant, B := A ? R n ?0 and p, x ? R n . Then: (a) If p ? proj A (x), then |p| ? proj B (|x|). Proof First note that if q ? proj A (x), then sgn(x i ) = sgn(q i ) or x i = 0 or q i = 0 with Lemma 9(b). Hence for all i ? {1, . . . , n} follows</p><formula xml:id="formula_21">(|q i | ? |x i |) 2 = (q i ? sgn(q i ) ? x i ? sgn(x i )) 2 = (q i ? x i ) 2 , and there- fore q ? x 2 2 = |q| ? |x| 2 2 . Furthermore, |q| ? A because of A reflection-invariant and |q| ? R n ?0 , so |q| ? B.</formula><p>(a) Let p ? proj A (x) and q ? B, then |p| ? B and it has to be shown that |p| ? |x| 2 ? q ? |x| 2 . Define I := { i ? {1, . . . , n} | x i &lt; 0 } andq := q ? 2 ? i?I q i e i , that is the signs of entries in I are flipped. Clearlyq ? A, so in conjunction with the remark at the beginning of the proof follows |p| ? |x| 2 2 = p ? x 2 2 ? q ? x 2 2 . For i I one obtains</p><formula xml:id="formula_22">x i ? 0 andq i = q i , henceq i ? x i = q i ? |x i |. For i ? I follows x i &lt; 0 andq i = ?q i , henceq i ?x i = ? (q i ? |x i |). This yields (q i ? x i ) 2 = (q i ? |x i |) 2 for all i ? {1, .</formula><p>. . , n}, thus q ? x 2 2 = q ? |x| 2 2 , and the claim follows. (b) Let p ? proj B (|x|). If i ? {1, . . . , n} with x i ? 0, then clearly s i p i ? x i = p i ? |x i |. For i ? {1, . . . , n} with x i &lt; 0 follows s i p i ? x i = ? (p i ? |x i |). Therefore, s ? p ? x 2 2 = p ? |x| 2 2 . Let q ? proj A (x), then the remark at the beginning of the proof yields q ? x 2 2 = |q| ? |x| 2 2 and |q| ? B. p ? proj B (|x|) yields p ? |x| 2 2 ? |q| ? |x| 2 2 , and the claim follows. Using this result immediately yields a condition for projections to be absolutely order-preserving:</p><p>Lemma 12 Let ? M ? R n be both permutation-invariant and reflection-invariant. Then the function p : R n ? ?(M), x ? proj M (x), is absolutely order-preserving.</p><p>Proof Let x ? R n , p ? proj M (x), and i, j ? {1, . . . , n} with |x i | &gt; x j . Define L := M ? R n ?0 , which is permutation-invariant with Remark 7. Lemma 11 implies that |p| ? proj L (|x|), and with Lemma 9 follows |p i | ? p j .</p><p>The application of these elementary results to projections onto sets on which functions achieve constant values is straightforward. Examples were given in Section 2 with the sets Z and S (? 1 ,? 2 ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C. Proof of Correctness of Algorithm 1 and Algorithm 3</head><p>The purpose of this appendix is to rigorously prove correctness of Algorithm 1 and Algorithm 3, that is that they compute projections onto S (? 1 ,? 2 ) ?0</p><p>. Projections onto S (? 1 ,? 2 ) can then be inferred easily as explained in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Geometric Structures and First Considerations</head><p>The aim is to compute projections onto D, which is the intersection of the non-negative orthant R n ?0 , the target hyperplane H and the target hypersphere K, see Section 2.1. Further, the intersection of H and K yields a hypercircle L, and the intersection of R n ?0 and H yields a scaled canonical simplex C. The structure of H and L will be analyzed in Section C.1.1 and Section C.1.2, respectively. The properties of C are discussed in Section C.2 and Section C.3. These results will then be used in Section C.4 to prove Theorem 2 and Theorem 3.</p><p>For the analysis of subsets where certain coordinates vanish, it is useful to define the following quantities for an index set I ? {1, . . . , n} with cardinality d := |I|. The corresponding face of C is denoted by C I := { c ? C | c i = 0 for all i I } and has barycenter m I := ? 1/d ? ? i?I e i ? C I . Further, L I := { a ? L | a i = 0 for all i I } denotes the hypercircle with according vanishing entries, and ? I := ? 2 2 ? ? 2 1/d is the squared radius of L I . Note that m I is also the barycenter of L I . With these definitions the intermediate goal is now to prove that projections onto D can be computed by alternating projections onto the geometric structures defined earlier. The idea is to show that the set of solutions is not tampered by alternating projections onto H, C, L and L I .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 L 1 NORM CONSTRAINT-TARGET HYPERPLANE</head><p>First, the projection onto the target hyperplane H is considered. Lemma 13 is an elaborated version of a result from <ref type="bibr" target="#b71">Theis et al. (2005)</ref>, which is included here for completeness. Using its statements, it can be assumed that the considered point lies on H without modification of the solution set of the projection onto the target set D.</p><p>Lemma 13 Let x ? R n . Then the following holds: Proof (a) This is essentially a projection onto a hyperplane, yielding a unique result.</p><p>(b) With (a) follows r ? x = 1 /n ? ? 1 ? e T x e. Hence h, r ? x = ? 1/n ? ? 1 ? e T x holds for arbitrary h ? H. This expression is independent of the entries of h, which yields a ? b, r ? x = 0 for every a, b ? H. Now let p ? proj D (x), that is p ? x 2 ? q ? x 2 for all q ? D. Let q ? D be arbitrary. With D ? H follows q ? p, r ? x = 0, and thus Proposition 4 yields q ? r 2 2 ? p ? r 2</p><formula xml:id="formula_23">2 = q ? x 2 2 ? p ? x 2 2 + 2 q ? p, x ? r = q ? x 2 2 ? p ? x 2 2 ? 0, hence p ? r 2 2 ? q ? r 2 2 , so p ? proj D (r). For the converse let p ? proj D (r). Analogously q ? x 2 2 ? p ? x 2 2 = q ? r 2 2 ? p ? r 2 2 ? 0, hence p ? proj D (x).</formula><p>Therefore, the barycenter m is the projection of the origin onto H. The next remark gathers additional information on the norm of m and dot products with this point.</p><p>Remark 14 It is m 2 2 = ? 2 1/n. Further, m, h = ? 2 1/n for all h ? H, and thus h ? m 2 2 = h 2 2 ? ? 2 1/n with Proposition 4.</p><p>C.1.2 L 1 AND L 2 NORM CONSTRAINT-TARGET HYPERSPHERE After the projection onto the target hyperplane H has been carried out, consider now the joint constraint of H and the target hypersphere K. First note that L = H ? K is a hypercircle, that is a hypersphere in the subspace H, with intrinsic dimensionality reduced by one:</p><p>Lemma 15 Consider L = H ? K and ? = ? 2 2 ? ? 2 1/n. Then the following holds:</p><formula xml:id="formula_24">(a) L =L := { q ? H | q ? m 2 2 = ? }. (b) L ? if and only if ? 2 ? ? 1/ ? n.</formula><p>Proof (a) Follows immediately from Remark 14.</p><p>(b) L is nonempty if and only if ? ? 0 using (a), and ? = (</p><formula xml:id="formula_25">? 2 + ? 1/ ? n) (? 2 ? ? 1/ ? n). Hence, with ? 1 , ? 2 &gt; 0 one obtains ? ? 0 if and only if ? 2 ? ? 1/ ? n ? 0.</formula><p>Hence L ? by the requirement that ? 2 ? ? 1 ? ? n? 2 . Further, the following observation follows immediately from Proposition 4 and a 2 = b 2 = ? 2 for all a, b ? L:</p><formula xml:id="formula_26">Remark 16 For all a, b ? L it is a ? b 2 2 = 2 ? 2 2 ? a, b , hence a, b = ? 2 2 ? 1 /2 ? a ? b 2 2 .</formula><p>Therefore, on L the dot product is equal to the Euclidean norm up to an additive constant. Next consider projections onto L and note that the solution set with respect to D is not changed by this operation. The major arguments for this result have been taken over from <ref type="bibr" target="#b71">Theis et al. (2005)</ref>. Here, the statements from Lemma 15 have been incorporated and the resulting quadratic equation was solved explicitly, simplifying the original version of <ref type="bibr" target="#b71">Theis et al. (2005)</ref>.  Let q ? L be arbitrary. One obtains q 2 = s 2 with q, s ? K and therefore application of Proposition 4 yields q ? r 2 2 ? s ? r 2 2 = 2 s ? q, r . With Remark 14 follows m, r = ? 2 1/n and r 2 2 = r ? m 2 2 + ? 2 1/n. Hence, s, r = (1 ? ?) m, r + ? r, r = ? 2 1/n + ? r ? m 2 2 . On the other hand, from s ? m = ?(r ? m) and hence r = (1 ? 1 /?) m + 1 /? ? s, and using Remark 16 it follows that q, r = (1 ? 1 /?) ? 2 1/n + 1 /? ? ? 2 2 ? 1 /2 ? q ? s 2 2 . Therefore with ? r ? m 2 2 = ? /? one obtains</p><formula xml:id="formula_27">s ? q, r = ? r ? m 2 2 + 1 /? ? ? 2 1/n ? ? 2 2 + 1 /2 ? q ? s 2 2 = 1 2? q ? s 2 2 ,</formula><p>and the claim follows directly by substitution. (b) Let q ? L, then (a) implies q ? r 2 2 ? s ? r 2 2 = 1 /? ? q ? s 2 2 ? 0 with equality if and only if q = s because of ? 2 being positive definite. Thus s is the unique projection of r onto L.</p><p>(c) With (a) follows q ? s 2 2 ? p ? s 2 2 = ? q ? r 2 2 ? p ? r 2 2 for all p, q ? D because of D ? L. For proj D (r) ? proj D (s), let p ? proj D (r) and q ? D. By definition p ? r 2 2 ? q ? r 2 2 , and thus q ? s 2 2 ? p ? s 2 2 ? 0 with ? &gt; 0, hence p ? proj D (s). For the converse, let p ? proj D (s) and q ? D. Similarly, q ? r 2 2 ? p ? r 2 2 = 1 /? ? q ? s 2 2 ? p ? s 2 2 ? 0, thus p ? proj D (r). Lemma 17 does not hold when r = m, which forms a null set. In practice, however, this can occur when the input vector x for Algorithm 1 is poorly chosen, for example if all entries are equal. In this case, proj L (r) = L, hence any point from L can be chosen for further processing.</p><p>Remark 18 One possibility in the case r = m would be to choose the point s := ? ? n?1 i=1 e i + ?e n where ?, ? ? R for s ? proj L (r), that is forcing the last entry to be unequal to the other ones. For satisfying s ? L, set ? := ? 1/n + ? ? / ? n(n?1) and ? := ? 1 ? ?(n ? 1) = ? 1/n ? ? ?(n?1) / ? n. This yields</p><formula xml:id="formula_28">? ? ? = ? ? 1 / ? n(n?1) + ? n?1 / ? n &gt; 0, hence ? ?.</formula><p>This choice has the convenient side effect of s being sorted in descending order.</p><p>Combining these properties of H and L, it can now be shown that projections onto D are invariant to affine-linear transformations with positive scaling:</p><p>Corollary 19 Let ? &gt; 0, ? ? R and x ? R n . Then proj D (?x + ?e) = proj D (x).</p><p>Proof Let ? &gt; 0, ? ? R and x ? R n . With Lemma 13 and Lemma 17 it is enough to show that proj L (proj H (?x + ?e)) = proj L (proj H (x)). Letx := ?x + ?e,r := proj H (x) ands := proj L (r). Lemma 13 and e T e = n yieldr = (?x + ?e) + 1 /n ? ? 1 ? ?e T x ? ?e T e e = ?x + 1 /n ? ? 1 ? ?e T x e. Hencer is independent of ?. Lemma 17 yieldss = m +? (r ? m), where? := ? ? / r?m 2 . Application of Proposition 4 yields</p><formula xml:id="formula_29">r 2 2 = ?x 2 2 + 1 /n ? ? 1 ? ?e T x e 2 2 + 2 ?x, 1 /n ? ? 1 ? ?e T x e = ? 2 x 2 2 + 1 /n ? ? 1 ? ?e T x ? 1 + ?e T x = ? 2 x 2 2 + 1 /n ? ? 2 1 ? ? 2 (e T x) 2 ,</formula><p>and with Remark 14 follows r ? m 2 2 = r 2 2 ? ? 2 1/n = ? 2 x 2 2 ? 1 /n?(e T x) 2 . Let r := proj H (x) and s := proj L (r), then Lemma 13 and Lemma 17 imply r = x + 1 /n ? ? 1 ? e T x e and s = m + ? (r ? m), where ? := ? ? / r?m 2 . Likewise r ? m 2 2 = x 2 2 ? 1 /n ? (e T x) 2 , and hence ? /? = r?m 2/ r?m 2 = ?, where ? &gt; 0 must hold. This yields s = m +? (r ? m) = m + ? /? ? ?x + ? 1/n ? e ? ? /n ? e T xe ? ? 1/n ? e = m + ? x ? 1 /n ? e T xe = s, which shows that the projection is invariant.</p><p>Therefore, shifting and positive scaling of the argument of Algorithm 1 do not change the outcome. An overview of the steps carried out this far is given in <ref type="figure" target="#fig_14">Figure 8</ref>. Consider a point x ? R n and s := proj L (proj H (x)). When s ? R n ?0 , then already s ? D and hence s ? proj D (x). Therefore only situations in which s R n ?0 holds are relevant in the remainder of this discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Simplex Geometry</head><p>The joint constraint of the target hyperplane H with non-negativity yields simplex C. The following definition is likewise to definitions from Chen and Ye (2011) and <ref type="bibr" target="#b52">Michelot (1986)</ref>:</p><p>Definition 20 For n ? N, n ? 1, the set n := { ? ? R n ?0 | e T ? = 1 } is called canonical n-simplex.</p><p>It is clear that C = R n ?0 ? H = { ? 1 ? | ? ? n } is a scaled canonical simplex. Further, for an index set I ? {1, . . . , n} the set C I = { c ? C | c i = 0 for all i I } is a face of the simplex, which intrinsically possesses the structure of a simplex itself-although of reduced intrinsic dimensionality. Consider the following observation on the topology of C embedded in the subspace H:</p><p>Proposition 21 Let c = ? 1 ? ? C with ? ? n . Then c ? ?C in the metric space (H, ? 2 ) if and only if there is a j ? {1, . . . , n} with ? j = 0. The proof is simple and omitted as it does not contribute to deeper insight. Hence the faces C I are subsets of ?C, which is the topological border of C in (H, ? 2 ). Using Proposition 21 a statement on the inradius of C can be made, which in turn can be used to show that for n = 2 no simplex projection has to be carried out at all:</p><p>Proposition 22 The squared inradius of C is ? in := ? 2 1 n(n?1) . It is L ? C for n = 2. Proof Because C is closed and convex, it is enough to consider the distance between interior points and boundary points. Hence the insphere radius of a point p ? C can be computed as being the minimum distance to any of the boundary points. With Proposition 21 these points can be characterized as points where at least one entry vanishes. Using Lagrange multipliers it can be shown that min c??C m ? c 2 2 = ? in . Further, it can be shown that no point other than m is center of a larger insphere. This is achieved by constructing projections on certain faces of C, as is discussed in detail in the forthcoming Lemma 26. When n = 2 and ? 2 ? ? 1 , which is fulfilled by requirement on ? 1 and ? 2 , then ? = ? 2 2 ? ? 2 1/n ? ? 2 1/2 = ? in , and L ? C follows with Lemma 15. The projection within H from outside a simplex is unique and must be located on its boundary:</p><p>Remark 23 Let s ? H \C. Then proj C (s) ? ?C in (H, ? 2 ).</p><p>The proof is obvious because C is closed and convex. By combination of Proposition 21 and Remark 23, it is now evident that the projection within H onto C yields vanishing entries. After the first projection onto H, this subspace is never left throughout the arguments presented here, such that Remark 23 always applies. It has yet to be shown that the projection onto D possesses zero entries in the same coordinates. This way, a reduction of problem dimensionality can be achieved, and an iterative algorithm can be constructed to compute the projection onto D. The algorithm is guaranteed to terminate at the latest when the problem dimensionality equals two with Proposition 22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 PROJECTION ONTO A SIMPLEX</head><p>Quite a few methods have been proposed for carrying out projections onto canonical simplexes. An iterative algorithm was developed by <ref type="bibr" target="#b52">Michelot (1986)</ref> which is very similar to Hoyer's original method for computation of the projection onto D. A simpler and more effective algorithm has been developed by <ref type="bibr" target="#b16">Duchi et al. (2008)</ref>. Building upon this work, <ref type="bibr" target="#b7">Chen and Ye (2011)</ref> have proposed and rigorously proved correctness of a very similar algorithm, which is more explicit than that of <ref type="bibr" target="#b16">Duchi et al. (2008)</ref>. Their algorithm can be adapted to better suit the needs for the sparseness-enforcing projection. This adapted version was given by Algorithm 2 in Section 2. The following note makes the adaptations explicit.</p><p>Proposition 24 Let x ? R n \C and p := proj C (x). Then the following holds:</p><p>(a) There existst ? R such that p = max (x ?t ? e, 0), where the maximum is taken element-wise.</p><p>(b) Algorithm 2 computest such that (a) holds and the number of nonzero entries in p.</p><p>Proof The arguments from Chen and Ye (2011) hold for projections onto n . The case of the scaled canonical simplex can be recovered using p = ? 1 ? proj n ( x /? 1 ). Therefore lines 4 and 7 of Algorithm 2 can be adapted from t := s?1 i and t := s?1 n to t := s?? 1 i and t := s?? 1 n , respectively. The correct number of nonzero entries in p follows immediately from its expression from (a), the fact that y is sorted in descending order and the termination criterion of Algorithm 2.</p><p>As already described in Section 2.2, symmetries can be exploited for projections onto C:</p><p>Remark 25 When x is already sorted in descending order, then no sorting is needed at the beginning of Algorithm 2. The projection p is then sorted also, because C is permutation-invariant. In this case, the nonzero entries of p are located in the first d := p 0 entries, while p d+1 = ? ? ? = p n = 0.</p><p>This fact is useful for optimizing access to the relevant entries of the working vector, which can then be stored contiguously in memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 PROJECTION ONTO A FACE OF A SIMPLEX</head><p>The projection within H onto C yields zero entries in the working vector. It still remains to be shown that the projection onto D possesses zero entries at the same coordinates as the projection onto C. If this holds true, then the dimensionality of the original problem can be reduced, and iterative arguments can be applied. The main building block in the proof is the explicit construction of projections from within the simplex onto a certain face. The next Lemma is fundamental for proving correctness of Algorithm 1. It describes the construction of the result of the projection onto a simplex face and poses a statement on its norm, which in turn is used to prove that the position of vanishing entries does not change upon projection.</p><p>Lemma 26 Let q ? C and let ? I ? {1, . . . , n} be an arbitrary index set. Then there exists an s ? C I with q ? v 2 2 = q ? s 2 2 + s ? v 2 2 for all v ? C I . If additionally max j?J q j ? min i?I q i holds for J := I C , then s 2 ? q 2 with equality if and only if q j = 0 for all j ? J.</p><p>More precisely, let h := |J| and let J = { j 1 , . . . , j h } such that q j 1 ? ? ? ? ? q j h . Consider the sequence s (0) , . . . , s (h) ? R n defined iteratively by s (0) := q and</p><formula xml:id="formula_30">s (k) := s (k?1) ? s (k?1) j k e j k + 1 n?k s (k?1) j k e ? ? k i=1 e j i</formula><p>for k ? {1, . . . , h}. Write s := s (h) . Then the following holds:</p><formula xml:id="formula_31">(a) s (k) ? C { j 1 ,..., j k } C for all k ? {1, . . . , h}. (b) s (0) ? s (k) , s (k) ? s (k+1) = 0 for all k ? {0, . . . , h ? 1}. (c) s (k) ? q 2 2 = ? k i=1 s (i) ? s (i?1) 2 2 for all k ? {0, . . . , h}. (d) s (k?1) j k = q j k + 1 n?k+1 ? k?1 i=1 q j i for all k ? {1, . . . , h}. (e) s (0) ? s (k) , s (k) ? v = 0 for all k ? {0, .</formula><p>. . , h} and for all v ? C I . If max j?J q j ? min i?I q i , then the following holds as well:</p><formula xml:id="formula_32">(f) q ? v 2 2 = q ? s (k) 2 2 + s (k) ? v</formula><formula xml:id="formula_33">(h) s (k) j 1 ? ? ? ? ? s (k) j h ? min i?I s (k)</formula><p>i for all k ? {0, . . . , h}.</p><formula xml:id="formula_34">(i) s (k?1) j k ? ? 1</formula><p>n?k+1 for all k ? {1, . . . , h}.</p><p>(j) s (k?1) 2 ? s (k) 2 for all k ? {1, . . . , h}, and hence s 2 ? q 2 . (k) s 2 = q 2 if and only if q j = 0 for all j ? J.</p><p>Proof In other words, s (k) is constructed from s (k?1) by setting entry j k to zero, and adjusting all remaining entries, but the ones previously set to zero, such that the L 1 norm is preserved. This generates a finite series of points progressively approaching C I , see (a), where the final point is from C I . As all relevant dot products vanish, see (b) and (e), this is a process of orthogonal projections. Hence the distance between points can be computed using the Pythagorean theorem, see (c) and (f). In (g) it is then shown that s is the unique projection of q onto C I .</p><p>If the entry j k in s (k?1) does not vanish, then the L 2 norm of the newly constructed point is greater than that of the original point, see (j) and (k). The entries with indices from J must be sufficiently small for this non-decreasing norm property to hold, see (h) and (i). The magnitude of these entries, however, is strongly connected with the magnitudes of respective entries from the original point q, that is, the rank is preserved from one point to its successor. <ref type="figure" target="#fig_17">Figure 9</ref> gives an example for n = 3 in which cases the non-decreasing norm property holds.</p><p>Let a k := 1 n?k e ? ? k i=1 e j i ? e j k ? R n for k ? {1, . . . , h}. Then s (k) = s (k?1) + s (a) First note that e T a k = 1 n?k (n ? k) ? 1 = 0 and that a k ? R n ?0 for all k ? {1, . . . , h}. It is now shown by induction that s (k) lies on the claimed face of C. For k = 1, one obtains s (1) j 1 = 0 and s (1) i = q i + 1 n?1 q j 1 for i j 1 . Thus s</p><p>(1) i ? 0 for all i ? {1, . . . , n} because of q i ? 0 for all i ? {1, . . . , n}. Further e T s (1) = e T q + q j 1 e T a 1 = e T q = ? 1 , hence s (1) ? C {1,...,n}\{ j 1 } .</p><formula xml:id="formula_35">For k ? 1 ? k, assume s (k?1) i = 0 for all i ? { j 1 , . . . , j k?1 }, s (k?1) i ? 0 for all i ? {1, .</formula><p>. . , n} and e T s (k?1) = ? 1 . Clearly, s</p><formula xml:id="formula_36">(k) i = s (k?1) i = 0 holds for i ? { j 1 , . . . , j k?1 }. Furthermore, one obtains s (k) j k = s (k?1) j k ? s (k?1) j k = 0. With s (k?1) ? R n ?0 and a k ? R n ?0 follows that s (k) ? R n ?0 . Finally it is e T s (k) = e T s (k?1) + s (k?1) j k e T a k = e T s (k?1) = ? 1 . Hence s (k) ? C {1,...,n}\{ j 1 ,..., j k } . (b) For i ? {1, . . . , k} follows e ? ? i ?=1 e j ? , e ? ? k+1 ?=1 e j ? = e, e ? e, ? k+1 ?=1 e j ? ? ? i ?=1 e j ? , e + ? i ?=1 e j ? , ? k+1 ?=1 e j ? = n ? (k + 1) ? i + i = n ? k ? 1, and therefore a i , a k+1 = 1 (n?i)(n?k?1) e ? ? i ?=1 e j ? , e ? ? k+1 ?=1 e j ? + e j i , e j k+1 ? 1 n?i e ? ? i ?=1 e j ? , e j k+1 ? 1 n?k?1 e j i , e ? ? k+1 ?=1 e j ? = n?k?1 (n?i)(n?k?1) + 0 ? 1 n?i ? 0 n?k?1 = 0.</formula><p>Thus</p><formula xml:id="formula_37">s (0) ? s (k) , s (k) ? s (k+1) = ? k i=1 s (i?1) j i a i , s (k) j k+1 a k+1 = ? k i=1 s (i?1) j i s (k)</formula><p>j k+1 a i , a k+1 = 0.</p><p>(c) Follows by induction using Proposition 4 and (b). (d) Clearly, e j k , a i = 1 n?i for i ? {1, . . . , k ? 1}, hence with induction follows</p><formula xml:id="formula_38">s (k?1) j k = e j k , s (k?1) = e j k , s (0) + ? k?1 i=1 s (i?1) j i e j k , a i = q j k + ? k?1 i=1 1 n?i s (i?1) j i IH = q j k + ? k?1 i=1 1 n?i q j i + ? k?1 i=1 ? i?1 ?=1 1 n?i 1 n?i+1 q j ? = q j k + ? k?1 i=1 q j i 1 n?i + ? k?1 ?=i+1 1 n?? 1 n??+1 . Using ? k?1 i=1 ? i?1 ?=1 = ? 1??&lt;i?k?1 = ? k?1 ?=1 ? k?1 i=?+1</formula><p>the order of summation was changed after the induction step, and then the variables i and ? were swapped. For the claim to hold it is enough to show that 1 n?i + ? k?1 ?=i+1 1 n?? 1 n??+1 = 1 n?k+1 for all i ? {1, . . . , k ? 1}, which follows by reverse induction. (e) Proof by induction. Let v ? C I , that is e T v = ? 1 and v j = 0 for all j ? J. For k = 0, s (0) ?s (k) = 0 and the claim follows. For k ?1 ? k, first note that a k , q = 1 Therefore,</p><formula xml:id="formula_39">n?k ? 1 ? ? k i=1 q j i ?q j k , a k , s (k?1) = a k , s (0) +? k?1 i=1 s (i?1) j i a k , a i = a k ,</formula><formula xml:id="formula_40">s (0) ? s (k) , s (k) ? v = s (0) ? s (k?1) ? s (k?1) j k a k , s (k?1) + s (k?1) j k a k ? v = s (0) ? s (k?1) , s (k?1) ? v + s (k?1) j k a k , s (0) ? 2s (k?1) ? s (k?1) j k a k + v IH = s (k?1) j k ? a k , q ? s (k?1) j k a k , a k + a k , v = s (k?1) j k ? ? 1 n?k + 1 n?k ? k i=1 q j i + q j k ? s (k?1) j k 1 + 1 n?k + ? 1 n?k = s (k?1) j k q j k 1 + 1 n?k + 1 n?k ? k?1 i=1 q j i ? s (k?1) j k 1 + 1 n?k = 0,</formula><p>where the final equality was yielded using the statement from (d) and 1 + 1 n?k ? 1 n?k+1 = 1 n?k . (f) Follows using Proposition 4 and (e). (g) A unique Euclidean projection exists because C I is closed and convex. s ? C I with (a), and q ? s 2 2 ? q ? v 2 2 for all v ? C I with (f). Therefore s = proj C I (q). (h) In the remainder of the proof assume that max j?J q j ? min i?I q i holds. It is first shown by induction that s (k)</p><formula xml:id="formula_41">j 1 ? ? ? ? ? s (k)</formula><p>j h for all k ? {0, . . . , h}. For k = 0 this is fulfilled as requirement on q and by definition of J. For k ? 1 ? k, let ?, ? ? { j 1 , . . . , j h } with ? &lt; ?. Then with ? denoting the indicator function and with A :</p><formula xml:id="formula_42">= ? { j k } (?) ? ? { j k } (?) + 1 n?k ? { j 1 ,..., j k } C (?) ? ? { j 1 ,..., j k } C (?) follows s (k) ? ? s (k) ? = s (k?1) ? ? s (k?1) ? + s (k?1) j k A.</formula><p>Clearly, when A ? 0 then the claim follows with the induction hypothesis and with s (k?1) j k ? 0 due to (a). First consider the case of ? ? { j 1 , . . . , j k?1 }. If ? ? { j 1 , . . . , j k?1 } also, then A = 0. If ? = j k , then A = ?1, and hence s</p><formula xml:id="formula_43">(k) ? ? s (k) ? = s (k?1) ? ? s (k?1) ? ? s (k?1) ? = ?s (k?1) ?</formula><p>which however vanishes with (a). If ? ? { j k+1 , . . . , j h }, then A = 1 n?k ? 0. If ? = j k , then ? ? { j k+1 , . . . , j h }, and then A = 1 + 1 n?k ? 0. If ? ? { j k+1 , . . . , j h }, then ? ? { j ?+1 , . . . , j h }, thus A = 0. Hence the first claim is always fulfilled.</p><p>Next, it is shown that max j?J s</p><formula xml:id="formula_44">(k) j ? min i?I s (k) i for all k ? {0, . . . , h}. For k = 0 this is the requirement on q. For k ? 1 ? k, let i ? I and j ? J. It is then ? { j k } (i) = 0, ? { j k } ( j) ? { 0, 1 }, ? { j 1 ,..., j k } C (i) = 1 and ? { j 1 ,..., j k } C ( j) = 0, therefore s (k) i ? s (k) j = s (k?1) i ? s (k?1) j k ? { j k } (i) + 1 n?k s (k?1) j k ? { j 1 ,..., j k } C (i) ? s (k?1) j + s (k?1) j k ? { j k } ( j) ? 1 n?k s (k?1) j k ? { j 1 ,..., j k } C ( j) = s (k?1) i ? s (k?1) j + s (k?1) j k 1 n?k + ? { j k } ( j) ? 0, where s (k?1) i ? s (k?1) j</formula><p>? 0 holds by induction hypothesis.</p><p>(i) With (a) follows ? 1 = e T s (k?1) and s</p><formula xml:id="formula_45">(k?1) j i = 0 for all i ? {1, . . . , k ? 1}. s (k?1) i ? s (k?1) j k holds for all i ? I with (h), and s (k?1) j i ? s (k?1) j k for all i ? {k + 1, . . . , h}. Therefore, ? 1 = ? n i=1 s (k?1) i = ? i?I s (k?1) i + ? k?1 i=1 s (k?1) j i + s (k?1) j k + ? h i=k+1 s (k?1) j i ? (n ? h) + 1 + (h ? k) s (k?1) j k = (n ? k + 1) s (k?1) j k</formula><p>, and the claim follows because n ? k + 1 &gt; 0.</p><p>(j) In (e) it was shown that a k 2 2 = 1 + 1 n?k . Furthermore,</p><formula xml:id="formula_46">s (k?1) , a k = 1 n?k ? 1 ? ? k?1 i=1 s (k?1) j i ? s (k?1) j k ? s (k?1) j k = 1 n?k ? 1 ? s (k?1) j k ? s (k?1) j k , because all s (k?1) j i vanish for i ? {1, .</formula><p>. . , k ? 1} using (a). Application of Proposition 4 yields</p><formula xml:id="formula_47">s (k) 2 2 ? s (k?1) 2 2 = s (k?1) j k a k 2 2 + 2s (k?1) j k s (k?1) , a k = s (k?1) j k s (k?1) j k 1 + 1 n?k + 2 n?k ? 1 ? s (k?1) j k ? 2s (k?1) j k = s (k?1) j k 2? 1 n?k ? s (k?1) j k 1 + 1 n?k , which is non-negative when s (k?1) j k ? 1 + 1 n?k ?1 ? 2? 1 n?k = 2? 1 n?k+1 . With (i) this is always fulfilled, hence s (k?1)</formula><p>2 ? s (k) 2 , and s 2 ? q 2 follows immediately using a telescoping sum argument. (k) When q j = 0 for all j ? J, then s = q and the claim follows. When there is a j k ? { j 1 , . . . , j h } with q j k 0, let k be minimal such that either k = 1 or q j k?1 = 0, hence s</p><formula xml:id="formula_48">(k?1) j k = q j k . With (i) follows 0 &lt; s (k?1) j k ? ? 1 n?k+1 &lt; 2? 1 n?k+1 , and hence s (k) 2 2 ? s (k?1) 2 2 &gt; 0 with (j), thus s 2 &gt; q 2 .</formula><p>The application of Lemma 26 then shows that the projection of a point from a face C I of C onto D must reside on the same face C I , given the original point is located within a sphere with squared radius ? I around m I . As will be shown in Lemma 28, this is automatically fulfilled for projections from L onto C. Proof Let J := {1, . . . , n} \ I, and let q ? proj D (v). Assume there is at least one j ? J with q j 0.</p><p>For showing max j?J q j ? min i?I q i , assume there are i ? I and j ? J with q j &gt; q i . Then v i &gt; 0 and v j = 0 because of v ? C I . D is permutation-invariant using Remark 7 as intersection of permutationinvariant sets. Hence let ? := (i, j) ? S n be the transposition swapping i and j, and consider</p><formula xml:id="formula_49">d := q ? v 2 2 ? P ? q ? v 2 2 = 2 (q j ? q i ) (v i ? v j ) . It is d &gt; 0 because of q j ? q i &gt; 0 and v i ? v j = v i &gt; 0. Hence P ? q ? v 2 &lt; q ? v 2 and P ? q ? D,</formula><p>which violates the minimality of q. Therefore, max j?J q j ? min i?I q i must hold. A drawing for the next arguments is given in <ref type="figure" target="#fig_0">Figure 10</ref>. With Lemma 26 there is an s ? C I such that q ? v 2 2 = q ? s 2 2 + s ? v 2 2 and s 2 &gt; ? 2 . Consider f :</p><formula xml:id="formula_50">[0, 1] ? R, ? ? v + ? (s ? v) 2 .</formula><p>Clearly f (0) = v 2 &lt; ? 2 and f (1) = s 2 &gt; ? 2 , hence with the intermediate value theorem there exists a ? * ? (0, 1) with f (? * ) = ? 2 . Let t := v + ? * (s ? v) ? R n , which lies in C I because of v, s ? C I and C I is convex. By construction t 2 = ? 2 , hence t ?D := { a ? D | a i = 0 for all i I }. </p><formula xml:id="formula_51">Clearly, v ? t 2 + t ? s 2 = |? * | ? s ? v 2 + |1 ? ? * | ? v ? s 2 = s ? v 2 . Let p ? projD(v), then v ? p 2 ? v ? t 2 . Therefore, q ? v 2 2 = q ? s 2 2 + s ? v 2 2 = q ? s 2 2 + ( v ? t 2 + t ? s 2 ) 2 &gt; v ? t 2 2 ? v ? p 2 2 .</formula><formula xml:id="formula_52">Because ofD ? D, p ? D also, hence q ? v 2 ? p ? v 2 , which contradicts q ? v 2 &gt; v ? p 2 .</formula><p>Hence, q j = 0 for all j ? J must hold, and thus q ? C I .</p><p>Because C I is isomorphic to a simplex, but with lower dimensionality than C, an algorithm can be constructed to compute the projection onto D, as discussed in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Self-Similarity of the Feasible Set</head><p>The next Lemma summarizes previous results and analyzes projections from L onto C in greater detail. It shows that the solution set with respect to the projection onto D is not tampered, and that all solutions have zeros at the same positions as the projection onto C. <ref type="figure" target="#fig_0">Figure 11</ref> provides orientation on the quantities discussed in Lemma 28.</p><p>Lemma <ref type="formula">28</ref>  (f) proj D (r) ? C I , hence from q ? proj D (r) follows q i = 0 for all i I.</p><p>(g) Let q ? proj D (r). Then q ? r, s ? r = 0, and thus q ? s 2 2 = q ? r 2 2 + r ? s 2 2 .</p><p>(h) proj D (r) = proj D (s) ? C I .</p><p>Proof (a) The existence oft ? R such that r = max (s ?t ? e, 0) is guaranteed by Proposition 24. It remains to be shown thatt ? 0. Consider the index set J := { j ? {1, . . . , n} | s j ? 0 } of nonnegative entries of s. Because of s ? L \C one obtains e T s = ? 1 and there is an index i with s i &lt; 0, hence J {1, . . . , n}. Therefore, ? 1 = ? j?J s j + ? j J s j &lt; ? j?J s j . Now assumet &lt; 0, then s j ?t &gt; 0 for all j ? J, and hence r j = s j ?t for all j ? J. Using r ? C yields</p><formula xml:id="formula_53">? 1 = ? n j=1 r j ? ? j?J r j = ? j?J (s j ?t) = ? j?J s j ? |J| ?t &gt; ? j?J s j ,</formula><p>which contradicts ? j?J s j &gt; ? 1 . Hencet ? 0 must hold. (b) Would I = ? hold, then r = 0, which is impossible because of r ? C. I = {1, . . . , n} would violate the existence of vanishing entries in r, as is guaranteed by Remark 23.</p><p>(c) Using (a): When i ? I, then 0 &lt; r i = s i ?t and the claim follows. When i I, then r i = 0, hence s i ?t ? 0 and the claim follows.</p><p>(d) e T r = e T s = ? 1 because r, s ? H, hence using (c) yields (h) Let p ? proj D (s) and q ? proj D (r). Then using (g) and Proposition 4 one obtains that q ? s 2 2 ? p ? s 2 2 = q ? r 2 2 ? p ? r 2 2 + 2 p ? r, s ? r . With (c) and (d) follows</p><formula xml:id="formula_54">0 = e, s ? r = ? i?I (s i ? r i ) + ? i I (s i ? r i ) = ? i?It + ? i I s i = dt + ? 1 ? ? i?I s i , thus r, s ? r = ? i?I r i (s i ? r i ) = ? i?I (s i ?t)t =t ? i?I s i ? dt = ? 1t . (e) m I , s ? r = ? 1/d ? ? i?I (s i ? r i ) = ? 1/d ? ? i?It = ?</formula><formula xml:id="formula_55">p ? r, s ? r = ? i?I p i (s i ? r i ) + ? i I p i (s i ? r i ) ? r, s ? r = ? i?I p it + ? i I p i s i ? ? 1t =t ? 1 ? ? i I p i + ? i I p i s i ? ? 1t = ? i I p i (s i ?t) ? 0. Now q ? proj D (r) yields q ? r 2 2 ? p ? r 2 2 , and hence q ? s 2 2 ? p ? s 2 2 ? 0, thus q ? proj D (s). Similarly, q ? s 2 2 ? p ? s 2 ? 0 with p ? proj D (s), so q ? r 2 2 ? p ? r 2 2 ? ?2 p ? r, s ? r ? 0, therefore p ? proj D (r).</formula><p>The following corollary states a similar result as in <ref type="bibr" target="#b71">Theis et al. (2005)</ref>. However, the proof here uses the notion of simplex projections instead of relying on pure analytical statements. The result presented here is stronger, as multiple entries of the vector can be set to zero simultaneously, while in <ref type="bibr" target="#b71">Theis et al. (2005)</ref> at most one entry can be zeroed out in a single iteration.</p><p>Corollary 29 Let s ? L \C and p ? proj D (s). Then p i = 0 for all i ? {1, . . . , n} with s i ? 0.</p><p>Proof Let i ? {1, . . . , n} with s i ? 0. Let r ? proj C (s). With Lemma 28(a) follows r i = 0 becaus? t ? 0, and the claim follows with Lemma 28(h).</p><p>The final step is to meet the hypersphere constraint again. For this, the simplex projection r is projected onto the target hypersphere, simultaneously keeping already vanished entries at zero, yielding a point u. Lemma 30 gives an explicit formulation of this projection and shows that the solution set with respect to the projection onto D stays the same. Refer to <ref type="figure" target="#fig_0">Figure 11</ref> for a sketch of the construction of u.  Application of Lemma 17 to ?(r) and ?(u) implies that ?(u) = projL(?(r)). Let q ? L I , then ?(q) ?L, hence ?(u) ? ?(r) 2 ? ?(q) ? ?(r) 2 . From i I follows r i = u i = q i = 0, hence u ? r 2 = ?(u) ? ?(r) 2 and q ? r 2 = ?(q) ? ?(r) 2 , and the claim follows.</p><p>(d) For the converse of ?, let ? : R d ? R n ,x ? x where x i = 0 for all i I and x i =x j when there is a j ? {1, . . . , d} with i = i j . Analogous to the above, membership of? in one ofH,K,L or D implies membership of ?(?) in H, K, L or D, respectively.</p><p>With Lemma 28(f) and Lemma 28(h) it is enough to show proj D (u) = proj D (r). Like in (c), from Lemma 17 follows as well that projD(?(r)) = projD(?(u)). Let p ? proj D (u) and q ? proj D (r), then p ? C I with (b) and q ? C I with Lemma 28(f), and thus ?(p), ?(q) ?D. Assume ?(p) projD(?(u)), then there exists an a ?D with ?(a) ? u 2 = a ? ?(u) 2 &lt; ?(p) ? ?(u) 2 = p ? u 2 , violating the minimality of p. Hence ?(p) ? projD(?(u)), and analogously follows ?(q) ? projD(?(r)). Now projD(?(r)) = projD(?(u)) implies that ?(p) ? projD(?(r)) and ?(q) ? projD(?(u)). Thus, p ? r 2 = ?(p) ? ?(r) 2 = ?(q) ? ?(r) 2 = q ? r 2 , so p ? proj D (r), and one obtains analogously that q ? proj D (u). Therefore proj D (u) = proj D (r).</p><p>With Lemma 30 a point u is constructed. If u ? C, then it is already the solution for the projection onto D. Otherwise, Lemma 28 and Lemma 30 can be applied once more, gaining a new point u.</p><p>Lemma 28(b) states that the amount of nonzero entries of u must decrease, hence this process can be repeated for at most n iterations. If a point with only two non-vanishing entries results, it is guaranteed to be a solution by Proposition 22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proof of Theorem 2 and Theorem 3</head><p>Using the previous results it can now be shown that the proposed Algorithm 1 actually computes a correct solution, and that the algorithm always terminates in finite time.</p><p>Proof of Theorem 2 For proving partial correctness, let x ? R n be arbitrary. Lemma 13 yields proj D (x) = proj D (r) after line 1, and with Lemma 17 follows proj D (x) = proj D (s) after line 2. There is a pre-test loop in line 3, and it has to be shown that the loop-invariant is proj D (x) = proj D (s). At the beginning of the loop, s R n ?0 must hold, thus s ? L \ C. After line 4, proj D (x) = proj D (r) holds with Lemma 28. Then with Lemma 30, proj D (x) = proj D (s) is ensured after line 5, hence the loop-invariant holds. Thus, after the loop it is proj D (x) = proj D (s) and s ? D, so proj D (x) = s. If r = m in line 2 or r = m I in line 5, s can be chosen to be any point from L or L I , respectively, for example the point given in Remark 18. In this case, the projection is not unique, but a valid representative is found.</p><p>To prove total correctness, it has to be shown that the loop in line 3 terminates. Remark 23 applied to C I guarantees that the number of nonzero entries in s is strictly less at the end of the loop than the number of nonzero entries upon entering the loop. Hence, at most n iterations of the loop can be carried out, and when |I| = 2 the solution is already in D with Proposition 22. Thus the algorithm terminates in finite time.</p><p>It remains to be shown that the optimized variant is also correct.</p><p>Proof of Theorem 3 First note that Algorithm 3 consists of a procedure proj_L carrying out projections onto L and L I in-place, and a main body. A function proj_C is called to obtain the information on how to perform projections onto C. This is carried out by Algorithm 2. Upon entry of the main body, the input vector x is sorted in descending order, yielding a vector y. The algorithm then operates on the sorted vector y, and undoes the sorting permutation at the end. Because H, L and C are permutation-invariant, the projections onto the respective sets are guaranteed to remain sorted with Lemma 9.</p><p>Therefore, y has not to be sorted again for the simplex projection, as Algorithm 2 would require. Also note from Lemma 28 that in the simplex projection the smallest elements are set to zero, and the original Algorithm 1 continues working on the d non-vanishing entries. Because of the order-preservation, entries d + 1, . . . , n of y are zero, and all relevant information is concentrated in y 1 , . . . , y d . Therefore, Algorithm 3 can continue working on these first d entries only, and the index set of non-vanishing entries is always I = {1, . . . , d}. As the nonzero elements are stored contiguously in memory, access to y can be realized as a small unit-stride array. This is more efficient than working on a large and sparsely populated vector. Therefore, the loop starting at line 13 corresponds to the loop starting at line 3 in Algorithm 1. At the end of the main body, the sorting permutation ? is inverted and the entries from the sorted result vector y are stored in a new vector s. Because y d+1 , . . . , y n = 0, these entries can be ignored by setting the entire vector s to zero before-hand.</p><p>The proposed optimizations hence lead to the same solution which the original algorithm computes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Analytical Properties of the Sparseness-Enforcing Projection Operator</head><p>In this appendix, it is studied in which situations ? ?0 and ? as defined in Section 2.4 are differentiable, and hence continuous. Further, an explicit expression for their gradient is sought. It is clear by Theorem 2 that the projection of any point onto D can be written as finite composition of projections onto H, L, C and L I , respectively. In other words, for all points x ? R n \ R there exists a finite sequence of index sets I 1 , . . . , I h ? {1, . . . , n} with I j I j+1 for j ? {1, . . . , h ? 1} such that</p><formula xml:id="formula_56">? ?0 (x) = j=h 1 proj L I j ? proj C ? proj L ? proj H (x),</formula><p>where j=h 1 denotes iterated composition of functions, starting with j = h and decreasing until j = 1, that is ? ?0 (x) = proj L I h (proj C (? ? ? proj L I 1 (proj C (proj L (proj H (x)))) ? ? ? )). The sequence I 1 , . . . , I h here depends on x. The intermediate goal is to show that this sequence remains fixed in a neighborhood of x, and that each projection in the chain is differentiable almost everywhere. This then implies differentiability of ? ?0 except for a null set. Because of the close relationship of ? with ? ?0 , ? is then also differentiable almost everywhere as shown in the end of this appendix.</p><p>The projection onto H is differentiable everywhere, as is clear from its explicit formula given in Lemma 13. Considering L and L I for I ? {1, . . . , n}, the projection is unique and can be cast as function R n ? R n unless the point to be projected is equal to the barycenters m and m I , respectively. By considering the explicit formulas given in Lemma 17 and Lemma 30, it is clear that these functions are differentiable as composition of differentiable functions. Thus only the projection onto the simplex C demands attention. Note that the numbert from Proposition 24 is equal to the mean value of the entries of the argument, that survive the projection, modulo an additive constant:</p><p>Proposition 31 Let x ? R n \ C and p := proj C (x). Then there is a set I ? {1, . . . , n} such that p = max(x ?t ? e, 0) wheret = 1 /|I| ? (? i?I x i ? ? 1 ).</p><p>Proof Follows directly from Proposition 24 and Algorithm 2 by undoing the permutation ?.</p><p>Note that when I = {1, . . . , n}, this is very similar to the projection onto H, see Lemma 13. The next result states a condition under which I is locally constant, and hence identifies points where the projection onto C is differentiable with a closed form expression:</p><p>Lemma 32 Let x ? R n \ C, and let p := proj C (x), I ? {1, . . . , n} andt ? R be given as in Proposition 31. When x i t for all i ? {1, . . . , n}, then the following holds where u := ? i?I e i ? R n and v := e ? u ? R n are the indicator vectors of I and I C , respectively: (a) p i &gt; 0 if and only if i ? I.  (e) s ? proj C (s) is differentiable in x.</p><formula xml:id="formula_57">(b) p = x + 1 /d ? ? 1 ? u T x u ? v ? x,</formula><p>Proof Only a sketch of a proof is presented here. (a) Follows from the characterization oft given in Proposition 31. (b) The identity can be validated directly using (a) and Proposition 31. (c) Follows by choosing ? := 1 /2 ? min i?{1,...,n} |x i ?t|, which is positive by requirement on x.</p><p>(d) Validation follows like in (b) using (c).</p><p>(e) The projection onto C can be written locally in closed form using (d). In the same neighborhood, the index set of vanishing entries of the projected points does not change. Hence, the projection is differentiable as a composition of differentiable functions.</p><p>It is clear that there are points in which s ? proj C (s) is continuous but not differentiable, for example points that are projected onto one of the vertices of C. The structure in this situation is locally equivalent to that of the absolute value function. However, for every point where the projection onto C is not differentiable, a subtle change is sufficient to find a point where the projection is differentiable:</p><p>Lemma 33 Consider the function p : R n \C ? C, s ? proj C (s) and let x ? R n \C be a point such that p is not differentiable in x. Then for all ? &gt; 0 there exists a point y ? R n with x ? y 2 &lt; ? such that p is differentiable in y.</p><p>Proof Lett ? R be the separator from Proposition 31 for the projection onto C. Let the index set of all collisions witht be denoted by J := { j ? {1, . . . , n} | x j =t }, which is nonempty with Lemma 32 because p is not differentiable in x. Define ? := ? / ? 4|J| &gt; 0 and consider y := x ? ? ? j?J e j ? R n . Clearly, x ? y 2 = ? /2. From Proposition 31 follows that the separatingt for the projection onto C is independent of the entries of x with indices in J, as long as they are less than or equal tot. Because ? &gt; 0, these entries in y are strictly smaller thant, hence p is differentiable in y with Lemma 32.</p><p>Therefore the set on which s ? proj C (s) is not differentiable forms a null set. The next result gathers the gradients of the individual projections involved in the computation of the sparseness-enforcing projection operator with respect to ?. Using the chain rule, the gradient of ? ?0 can be derived afterwards as multiplication of the individual gradients.</p><p>Lemma 34 The individual projections for ? ?0 are differentiable almost everywhere. Their gradients are given as follows:</p><p>(a) ? proj H (x) ?x = E n ? 1 /n ? ee T , where E n ? R n?n is the identity matrix.</p><formula xml:id="formula_58">(b) ? proj L (x) ?x = ? ? x?m 2 E n ? 1 / x?m 2 2 ? (x ? m)(x ? m) T . (c) ? proj C (x) ?x = E n ? 1 /d ? uu T ? diag(v).</formula><p>Here, I := { i ? {1, . . . , n} | e T i proj C (x) 0 } is the index set of nonzero entries of the projection onto C, d := |I|, u := ? i?I e i ? R n and v := e ? u ? R n . Clearly, the gradients for proj H and for proj L are special cases of the gradients of proj C and proj L I , respectively. Therefore, they need no separate handling in the computation of the overall gradient. Exploiting the special structure of the matrices involved and the readily sorted input as in Algorithm 3, the gradient computation can be further optimized. For the remainder of this appendix, let O a?b ? { 0 } a?b and J a?b ? { 1 } a?b denote the matrices with a rows and b columns where all entries equal zero and unity, respectively.</p><p>Theorem 35 Let x ? R n be sorted in descending order and ? ?0 be differentiable in x. Let h ? N denote the number of iterations Algorithm 3 needs to terminate. In every iteration of the algorithm, store the following values for i ? {1, . . . , h}, where line numbers reference Algorithm 3:</p><p>? d i ? N denoting the current dimensionality as determined by lines 12 and 14.</p><p>? ? i := ? /? ? R where ? and ? are determined in lines 2 and 3, respectively.</p><p>? r(i) := y ? m I ? R d i as computed in line 7.</p><p>Let N := d h = ? ?0 (x) 0 denote the number of nonzero entries in the projection onto D, and define s(i) := e T 1 r(i), . . . , e T N r(i) T ? R N as the first N entries of each r(i). For i ? {1, . . . , h} let A i := ? i E N ? ? i/d i ? J N?N ? ? i s(i)s(i) T + ? i/d i ? s(i)s(i) T J N?N ? R N?N where ? i := ? i/ r(i) 2 2 , and let A := ? i=h 1 A i = A h ? ? ? A 1 ? R N?N . Then the gradient of ? ?0 in x is diag A, O (n?N)?(n?N) , that is a block diagonal matrix where the quadratic submatrix with row and column indices from 1 to N is given by A, and where all other entries vanish.</p><p>Proof The gradient of projections onto H is merely a special case of projections onto C, which also applies to the respective projections onto L and L I , see Lemma 34. Hence, the very first iteration is a special case of iterations with i &gt; 1. Consider one single iteration i ? {1, . . . , h} of Algorithm 3, that is the computation of proj L I ? proj C for some I ? {1, . . . , n}. Write d := d i , ? := ? i , ? := ? i and r := r(i) for short. Because the input vector x is sorted by requirement, all intermediate vectors that are projected are sorted as well using Lemma 9. Thus I = {1, . . . , d} holds. With Lemma 34, the gradient G C ? R n?n of the projection onto both H and C is of the form G C := E n ? 1 /d ? uu T ? diag(v), where u := ? d i=1 e i and v := e ? u. Let q := (r 1 , . . . , r d , 0, . . . , 0) T ? R n be a copy of r padded with zeros to achieve full dimensionality n. The gradient of the projection onto L, and in general L I , is given by G L := ?E n ? ?qq ? R n?n using Lemma 34. The gradient of the whole iteration is then given by the chain rule, yielding</p><formula xml:id="formula_59">G := G L G C = ?E n ? ? /d ? uu T ? ? diag(v) ? ?qq T + ? /d ? qq T uu T + ?qq T diag(v) ? R n?n .</formula><p>Write O := O (n?d)?(n?d) , then G is a block diagonal matrix of a matrix from R d?d and O: Note that E n ?diag(v) = diag(E d , O), uu T = diag(J d?d , O), and qq T = diag(rr T , O). Therefore, qq T diag(v) = diag(rr T , O) ? 0 0 0 E n?d = 0, and qq T uu T = diag(rr T J d?d , O). Thus</p><formula xml:id="formula_60">G = ? diag(E d , O) ? ? /d ? diag(J d?d , O) ? ? diag(rr T , O) + ? /d ? diag(rr T J d?d , O) = diag ?E d ? ? /d ? J d?d ? ?rr T + ? /d ? rr T J d?d , O .</formula><p>By denoting the gradient of iteration i by matrix G i ? R n?n for i ? {1, . . . , h} and by application of the chain rule follows that the gradient of all iterations is given by ? i=h 1 G i . In this matrix, all entries but the top left submatrix of dimensionality N ? N are vanishing, where N = d h . This is because the according statement applies to G i for the top left submatrix of dimensionality d i ? d i , and d 1 &gt; ? ? ? &gt; d h holds, and only the according entries survive the matrix multiplication. Therefore it is sufficient to compute only the top left N ? N entries of the gradients of the individual iterations, as the remaining entries are not relevant for the final gradient. This is reflected by the definition of the matrices A i for i ? {1, . . . , h} from the claim.</p><p>The gradient can thus be computed using matrix-matrix multiplications, where the matrices are square and the edge length is the number of nonzero entries in the result of the projection. This computation is more efficient than using the n ? n matrices of the individual projections. However, when the target degree of sparseness is low, and thus the amount of nonzero entries N in the result of the projection is large, gradient computation can become very inefficient. In practice, often only the product of the gradient with an arbitrary vector is required. In this case, the procedure can be sped up by exploiting the special structure of the gradient of ? ?0 :</p><p>Corollary 36 Let x ? R n be sorted in descending order and ? ?0 be differentiable in x. The product of the gradient of ? ?0 in x with an arbitrary vector can be computed using vector operations only.</p><p>Proof Note that because of the associativity of the matrix product it is enough to consider the product of the gradient G ? R n?n of one iteration of Algorithm 3 with one vector y ? R n . Because of the statements of Theorem 35, it suffices to consider the top left N ? N entries of G and the first N entries of y, as all other entries vanish. Therefore let A := ?E N ? ? /d ? J N?N ? ?ss T + ? /d ? ss T J N?N ? R N?N be the non-vanishing block of G as given by Theorem 35, let u := J N?1 ? R N be the vector of ones such that uu T = J N?N , and let z := (y 1 , . . . , y N ) T ? R N denote the vector with the first entries of y. Using matrix product associativity and distributivity over multiplication with a scalar yields Az = ? (z ? 1 /d ? z, u ? u) + ? ( 1 /d ? s, u z, u ? s, z ) s, where z, u = ? N i=1 z i and s, u = ? N i=1 s i . Hence Az can be computed in-place from z by subtraction of a scalar value from all entries, rescaling by ?, and adding a scaled version of vector s.</p><p>Although in Theorem 35 and Corollary 36 it was necessary that the input vector is sorted, the general case can easily be recovered:</p><p>Proposition 37 Let x ? R n be a point, ? ? S n such that y := P ? x ? R n is sorted in descending order and ? ?0 be differentiable in y with gradient G ? R n?n . Then ? ?0 is also differentiable in x, and the gradient is P T ? GP ? . Proof Follows with P T ? = P ? ?1 = P ?1 ? , ? ?0 (x) = P T ? ? ?0 (P ? x) = P ? ? ?0 (y) and the chain rule. Likewise, the gradient for the unrestricted projection ? can be computed from the gradient for ? ?0 :</p><p>Proposition 38 Let x ? R n be a point such that ? ?0 is differentiable in |x| with gradient G ? R n?n . Let s ? { ?1 } n be given such that ?(x) = s ? ? ?0 (|x|). Then ? is differentiable in x, and the gradient is diag(s)G diag(s).</p><p>Proof Follows analogously to Proposition 37, using |x| = s ? x = diag(s)x.</p><p>Summing up, the gradient of the projection onto S (? 1 ,? 2 ) ?0 and S (? 1 ,? 2 ) can be computed efficiently by bookkeeping a few values as discussed in Theorem 35, and applying simple operations to recover the general case. When only the product of the gradient with a vector is required, the computation can be made more efficient as stated in Corollary 36. Direct application of Theorem 35 should be avoided in this situation because of the high computational complexity.</p><p>The objective function E SOAE is a convex combination of two similarity measures s R and s C . The degrees of freedom W , W out and ? out of the SOAE architecture should be tuned by gradient-based methods to minimize these functions. This appendix reports the gradient information needed for reproduction of the experiments. The first statement addresses the reconstruction module.</p><p>Proposition 39 It is ( ?s R (x, x) /?W) T = xgW f (u) + g T h T ? R d?n where g := ?s R (x,x) /?x ? R 1?d is the gradient of the similarity measure with respect to its first argument. Additionally, ( ?s R (x, x) /?W out ) T = 0 ? R n?c and ?s R (x, x) /?? out = 0 ? R 1?c .</p><p>Proof As s R does not depend on W out or ? out , the respective gradients vanish. The symmetry between encoding and decoding yieldsx := W ? f W T x . The gradient for W follows using the chain rule and the product rule for matrix calculus, see <ref type="bibr" target="#b55">Neudecker (1969)</ref> and <ref type="bibr" target="#b75">Vetter (1970)</ref>.</p><p>The correlation coefficient is the recommended choice for the similarity measure of the reconstruction module because it is normed and invariant to affine-linear transformations. It is also differentiable almost everywhere:</p><p>Proposition 40 If s R is the correlation coefficient and x,x ? R d \ { 0 }, then</p><formula xml:id="formula_61">?s R (x,x) ?x T = 1 ? ?? x ? e, x d e ? s R (x,x) ? x ? e,x d e ? R d ,</formula><p>where all entries of e ? R d are unity, ? := x 2 2 ? 1 /d ? e,x 2 ? R and ? := x 2 2 ? 1 /d ? e, x 2 ? R.</p><p>Proof One obtains ?? ? s R (x, x) = x,x ? 1 /d ? e,x e, x because s R is the correlation coefficient. The claim then follows with the quotient rule.</p><p>The gradients of the similarity measure for classification capabilities are essentially equal to those of an ordinary two-layer neural network, and can be computed using the back-propagation algorithm <ref type="bibr" target="#b65">(Rumelhart et al., 1986)</ref>. However, the pairing of the softmax transfer function with the crossentropy error function provides a particularly simple structure of the gradient <ref type="bibr" target="#b17">(Dunne and Campbell, 1997)</ref>. For completeness, the gradients of the classification module of SOAE are summarized:</p><p>Proposition 41 If s C is the cross-entropy error function, g is the softmax transfer function and the target vector for classification t is a one-of-c code, then ( ?s C (y, t) /?W out ) T = h ? (y ? t) T ? R n?c , ?s C (y, t) /?? out = (y ? t) T ? R 1?c and ( ?s C (y, t) /?W) T = x ? (y ? t) T W T out f (u) ? R d?n .</p><p>Proof Basic matrix calculus <ref type="bibr" target="#b55">(Neudecker, 1969;</ref><ref type="bibr" target="#b75">Vetter, 1970)</ref> yields ?s C (y, t) /?? out = ( ?s C (y, t) /?y)?g (y), ( ?s C (y, t) /?W out ) T = h ? ( ?s C (y, t) /?? out ) and ( ?s C (y, t) /?W) T = x ? ( ?s C (y, t) /?? out ) ?W T out f (u) . By requirement ?s C (y, t) /?y = ?(t y) T , where denotes the element-wise quotient, g (y) = diag(y) ? yy T and ? c i=1 t i = 1. Therefore ( ?s C (y, t) /?? out ) T = yy T ? diag(y) ? (t y) = y ? y, t y ? y ?t y = y ?t using y, t y = ? c i=1 y i ? t i/y i = 1, and the claim follows. As E SOAE is a convex combination of the reconstruction error and the classification error, its overall gradient follows immediately from Proposition 39 and Proposition 41. Proposition 40, the results from Appendix D, and the gradient of the L 0 projection as described in Section 2.4 can then be used to compute the explicit gradients for the procedure proposed in this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 1</head><label>1</label><figDesc>Let x ? R n and ? M ? R n . Then every point in proj M (x) := { y ? M | y ? x 2 ? z ? x 2 for all z ? M } is called Euclidean projection of x onto M. When there is exactly one point y in proj M (x), then y = proj M (x) is used as an abbreviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of the number of non-vanishing entries in the working vectors of the original algorithm and the improved algorithm during run-time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Ratio of the computation time of the original algorithm and the improved algorithm for a variety of input dimensionality and initial vector sparseness. Numbers greater than one indicate parameterizations where the proposed algorithm is more efficient than the original one. There is a large region where the speedup is decent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Sparseness Degree with Respect to Sparseness Measure ? L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>, . . . , n} Index set of nonzero entries d := |I| ? N Working dimensionality L I := { a ? L | a i = 0 for all i I } Points in L where certain coordinates vanish C I := { c ? C | c i = 0 for all i I } Face of simplex C m I := ? 1/d ? ? i?I e i ? R n Barycenter of L I and C I ? I := ? 2 2 ? ? 2 1/d ? R Squared radius of L I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Definition 6</head><label>6</label><figDesc>Let ? M ? R n . Then M is called permutation-invariant if and only if P ? x ? M for all x ? M and all permutations ? ? S n . Further, M is called reflection-invariant if and only if b ? x ? M for all x ? M and all b ? { ?1 } n .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Let ? A, B ? R n . When A and B are permutation-invariant or reflection-invariant, then so are A ? B, A ? B and A C .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The proof is obvious by elementary set algebra. Now consider the following general properties of functions mapping to power sets:Definition 8 Let ? M ? R n , let ?(M) be its power set and let f : R n ? ?(M) be a function. f is called order-preserving if and only if x i &gt; x j implies p i ? p j for all x ? R n , for all p ? f (x) and for all i, j ? {1, . . . , n}. f is called absolutely order-preserving if and only if from |x i | &gt; x j follows |p i | ? p j for all x ? R n , for all p ? f (x) and for all i, j ? {1, . . . , n}. f is called orthant-preserving if and only if sgn(x i ) = sgn(p i ) or x i = 0 or p i = 0 for all x ? M and all p ? f (x).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(b) If p ? proj B (|x|), then s ? p ? proj A (x) where s ? { ?1 }n is given by s i := 1 if x i ? 0 and s i := ?1 otherwise for all i ? {1, . . . , n}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) proj H (x) = x + 1 /n ? ? 1 ? e T x e. (b) Let r := proj H (x). Then proj D (x) = proj D (r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Lemma 17</head><label>17</label><figDesc>Let r ? H with r m. Let s := m + ?(r ? m) where ? := ? ? / r?m 2 . Then: (a) ? &gt; 0, s ? L, and q ? r 2 2 ? s ? r 2 2 = 1 /? ? q ? s 2 2 for all q ? L. (b) s = proj L (r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>(c) proj D (r) = proj D (s). Proof (a) First note that ? &gt; 0 because of r m. Clearly s = (1 ? ?)m + ?r. Further, s ? H because of e T s = ? 1 , and s ? L because of s ? m 2 2 = ? and Lemma 15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 8 :</head><label>8</label><figDesc>Sketch of the situation in Section C.1, projected onto target hyperplane H. r is the projection of the input point x onto H. s is the projection of r onto the hypercircle L, which has squared radius ?. The intersection of H with the non-negative orthant is a simplex and denoted by C. The feasible set D is the intersection of C and L, and is marked with solid black lines. With Lemma 13 and Lemma 17 follows that proj D (x) = proj D (r) = proj D (s), hence the next steps consist of projecting s onto C for finding the projection of x onto D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>22</head><label></label><figDesc>for all k ? {0, . . . , h} and for all v ? C I . (g) s = proj C I (q).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>and with induction follows s (k) = s (0) + ? k i=1 s (i?1) j i a i for k ? {1, . . . , h}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 9 :</head><label>9</label><figDesc>q because a k , a i = 0 for all i ? {1, . . . , k ?1} as shown in (b), thus a k , s (0) ? 2s (k?1) = ? a k , q . Furthermore a k , v = ? 1 n?k because e j i , v = v j i = 0 for all i ? {1, . . . , h}, and a k , a k = a k 2 2 = 1 (n?k) 2 e ? ? k i=1 e Situation of Lemma 26: The projection of a point q from within the simplex C onto one of its faces C I yields point s. When q is sufficiently close to C I , then s 2 ? q 2 . This does not hold when the point projected onto C I is too far away. For example, the pointq which is located outside the dashed square with edge length 2 m I ? m 2 would yield a projection with a smaller Euclidean norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 10 :</head><label>10</label><figDesc>Sketch of the proof of Corollary 27: The projection of v onto D must be located on simplex face C I . Assume there is a projection q C I , then it must be sufficiently close to C I for application of Lemma 26. The projection s of q onto C I is located outsideL I = { a ? L | a i = 0 for all i I }.Hence the intersection t of the line between v and s and L I is in C I due to its convexity, and it is farther than the projection p of v ont? D = { a ? D | a i = 0 for all i I }. Therefore q cannot be the projection of v onto D.Corollary 27 Let I ? {1, . . . , n}, let v ? C I with v 2 &lt; ? 2 , and let q ? proj D (v). Then q ? C I .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 11 :</head><label>11</label><figDesc>Situation of Lemma 28 and Lemma 30: The point s is projected onto simplex C yielding r, which resides on one of its faces C I . From there point u can be constructed by projecting within C I onto the target hypersphere.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Let s ? L \C and r ? proj C (s). Let I := { i ? {1, . . . , n} | r i 0 } and d := |I|. Then: (a) There existst ? R ?0 such that r = max (s ?t ? e, 0), with the maximum taken element-wise. (b) I ? and I {1, . . . , n}. (c) s i &gt;t and s i ? r i =t for all i ? I. s i ?t and s i ? r i = s i for all i I. (d) r, s ? r = ? 1t . (e) m I , s ? r = ? 1t , thus m I ? r, s ? r = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>1t with (c), and the claim follows with (d). (f) r ? C I by definition of I. Using this, (c) andt ? 0 from (a) yields? 2 2 = s 2 2 = ? i?I s 2 i + ? i I s 2 i = ? i?I (r i +t) 2 + ? i I s 2 i &gt; ? i?I r 2 i + dt 2 + 2t? i?I r i = r 2 2 + dt 2 + 2? 1t ? r 2 2 ,thus the claim holds using Corollary 27. (g) With (f) follows q ? C I . Hence q, s ? r = ? i?I q i (s i ? r i ) =t ? i?I q i = ? 1t with (c), and the claims follow with (d) and Proposition 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Lemma 30</head><label>30</label><figDesc>Let s ? L \ C, r := proj C (s) = max (s ?t ? e, 0) witht ? R ?0 using Lemma 28. Let I := { i ? {1, . . . , n} | r i 0 } and d := |I|. Let u := m I + ? (r ? m I ) where ? := ? ? I/ r?m I 2 . Then: (a) u ? L and u i = 0 for all i I, hence u ? L I . (b) proj D (u) ? C I . (c) u = proj L I (r).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>(d) proj D (u) = proj D (r) = proj D (s) ? C I . Proof (a) Clearly u ? H. With m I , r ? H and Remark 14 follows that m, m I = m, r = ? 2 1/n. Moreover, m I , m I = ? 2 1/d and r, m I= ? i?I r i ? ? 1/d = ? 2 1/d, therefore r, m I ? m = m I , m I ? m . With u = (1 ? ?) m I + ?r it is u, m I ? m = m I , m I ? m = ? 2 1 ( 1 /d ? 1 /n). Hence u ? m I , m I ? m = 0. Further, m ? m m, m I = ? 2 1 ( 1 /d ? 1 /n). Thus with Proposition 4, u ? m 2 2 = u ? m I 2 2 + m I ? m 2 2 = ? I + ? 2 1 ( 1 /d ? 1 /n) = ?, and with Lemma 15 follows u ? L. For i I, one obtains u i = (1 ? ?) e T i m I + ?r i = 0, hence u ? L I . (b) If u ? C, then u ? D because of u ? L with (a), and hence u = proj D (u). The claim then follows with u i = 0 for all i I.If u C, then let q ? proj D (u). With Corollary 29 applied to u follows that q i = 0 for all i with u i ? 0, especially for all i I. Hence the claim follows.(c) Write I = {i 1 , . . . , i d } and consider ? : R n ? R d , (x 1 , . . . , x n ) T ? (x i 1 , . . . , x i d ) T . Further, letH := { a ? R d | e T a = ? 1 },K := { q ? R d | q 2 = ? 2 },L :=H ?K andD := R d ?0 ?H ?K. Clearly, when x i = 0 for all i I, then e T x = e T ?(x) and x 2 = ?(x) 2 . Thus in this case membership of x in one of H, K, L or D implies membership of ?(x) inH,K,L orD, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>where ? denotes the Hadamard product and d := |I|.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>(c) There exists a constant ? &gt; 0 and a neighborhood U := { s ? R n | x ? s 2 &lt; ? } of x, suchthat sgn(proj C (s)) = sgn(p) for all s ? U.(d) proj C (s) = s + 1 /d ? ? 1 ? u T s u ? v ? s for all s ? U.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>1 / x?m I 2 2 ? (x ? m I )(x ? m I ) T . Proof (a) Follows from the closed form expression in Lemma 13. (b) Lemma 17 yields proj L (x) = m + ?(x) ? (x ? m) with ?(x) = ? ? / x?m 2 . With the quotient rule follows ??(x) /?x = ? ? ? / x?m 3 2 ? (x ? m) T , as ? does not depend on x. The claim then follows by application of the product rule. (c) Follows from Lemma 32, similar to (a), using v ? x = diag(v)x. (d) Follows exactly as in (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Figure 5: Resulting amount of nonzero entries in an internal representation h with 1000 entries, depending on the target degree of sparseness for activity ? H with respect to Hoyer's sparseness measure ?. For low values of ? H , about 80% of the entries are nonzero, whereas for very high sparseness degrees only 1% of the entries do not vanish. The error bars indicate ? one standard deviation distance from the mean value. Standard deviation shrinks with increasing sparseness degree, making the mapping more accurate.Figure 6: Resulting classification error on the MNIST evaluation set for the supervised online autoencoder network, in dependence of sparseness of activity in the hidden layer. The projection onto an L 0 pseudo-norm constraint for variant SOAE-L 0 and the projection onto a constraint determined by sparseness measure ? for variant SOAE-? were used as transfer functions. The error bars indicate ? one standard deviation difference from the mean. Overview of comparative experiments. The second and third columns indicate whether sparse connectivity or sparse activity was incorporated, respectively. The fourth column reports the result of a statistical test for normality, which is interpreted in Section 3.5. The final column gives the median ? one standard deviation of the achieved classification error on the MNIST evaluation set. The results for each experiment were trimmed to gain a sample of size 47, allowing for statistical robust estimates.</figDesc><table><row><cell></cell><cell>Approach</cell><cell></cell><cell>Sparse</cell><cell>Sparse</cell><cell></cell><cell cols="2">Result (W, p) of</cell><cell cols="2">Evaluation</cell></row><row><cell></cell><cell></cell><cell cols="2">Connectivity</cell><cell cols="2">Activity</cell><cell cols="4">Shapiro-Wilk Test Error [%]</cell></row><row><cell></cell><cell>SOAE-?</cell><cell cols="2">? W = 0.75</cell><cell cols="2">? H ? [0.45, 0.85]</cell><cell cols="2">(0.9802, 0.60)</cell><cell cols="2">0.75 ? 0.04</cell></row><row><cell></cell><cell>SOAE-L 0</cell><cell cols="2">? W = 0.75</cell><cell cols="2">? ? [242, 558]</cell><cell cols="2">(0.9786, 0.53)</cell><cell cols="2">0.82 ? 0.05</cell></row><row><cell cols="2">SOAE-?-conn</cell><cell></cell><cell>none</cell><cell cols="2">? H ? [0.45, 0.85]</cell><cell cols="2">(0.9747, 0.40)</cell><cell cols="2">0.81 ? 0.04</cell></row><row><cell cols="2">SMLP-SCFC</cell><cell cols="2">? W = 0.75</cell><cell>none</cell><cell></cell><cell cols="2">(0.9770, 0.47)</cell><cell cols="2">0.81 ? 0.05</cell></row><row><cell></cell><cell>MLP-OBD</cell><cell cols="2">? = 12.5%</cell><cell>none</cell><cell></cell><cell cols="2">(0.9807, 0.62)</cell><cell cols="2">0.89 ? 0.04</cell></row><row><cell cols="2">MLP-random</cell><cell></cell><cell>none</cell><cell>none</cell><cell></cell><cell cols="2">(0.9798, 0.58)</cell><cell cols="2">0.88 ? 0.03</cell></row><row><cell cols="2">MLP-samples</cell><cell></cell><cell>none</cell><cell>none</cell><cell></cell><cell cols="2">(0.9773, 0.49)</cell><cell cols="2">0.91 ? 0.05</cell></row><row><cell cols="2">MLP-SCFC</cell><cell></cell><cell>none</cell><cell>none</cell><cell></cell><cell cols="2">(0.9794, 0.57)</cell><cell cols="2">0.91 ? 0.06</cell></row><row><cell>Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.6</cell><cell></cell><cell cols="2">SOAE-L 0</cell><cell cols="2">SOAE-?</cell><cell></cell><cell></cell></row><row><cell>Classification Error [%]</cell><cell>0.6 0.8 1.0 1.2 1.4</cell><cell></cell><cell cols="3">? ? [242, 558] ? H ? [0.45, 0.85]</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>30</cell><cell>40</cell><cell>50</cell><cell>60</cell><cell>70</cell><cell>80</cell></row><row><cell></cell><cell></cell><cell cols="7">Sparseness of Activity Measured in L 0 Pseudo-Norm [%]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>A list of symbols used frequently in this paper and their meaning.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors wish to thank Patrik O. Hoyer and Xiaojing Ye for sharing the source code of their algorithms. The authors are also grateful to the anonymous reviewers for their valuable comments and feedback. This work was supported by Daimler AG, Germany.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Notation and Prerequisites</head><p>Appendix E. Gradients for SOAE Learning</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic index: An intuitive non-parametric approach to measuring the size of treatment effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Acion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Temple</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arndt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in Medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="591" to="602" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Internal representations for associative memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wilczek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="217" to="228" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Nonlinear Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Neural Networks for Pattern Recognition</title>
		<imprint>
			<publisher>Clarendon Press</publisher>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A simple, efficient and near optimal algorithm for compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blumensath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Davies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3357" to="3360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="217" to="224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Differentiable sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Projection onto a simplex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.6081v2</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>University of Florida</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep, big, simple neural nets for handwritten digit recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3207" to="3220" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Control, Signals, and Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Training invariant support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="161" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Statistical comparisons of classifiers over multiple data sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dem?ar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Best Approximation in Inner Product Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Deutsch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">For most large underdetermined systems of linear equations the minimal 1 -norm solution is also the sparsest solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on Pure and Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="797" to="829" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The estimation of Pr(Y &lt; X) in the normal case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Downton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="551" to="558" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient projections onto the 1 -ball for learning in high dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="272" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Dunne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Australasian Conference on Neural Networks</title>
		<meeting>the Australasian Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="181" to="185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An algorithm for restricted least squares regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Dykstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">384</biblScope>
			<biblScope unit="page" from="837" to="842" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">An algorithm for least squares projections onto the intersection of translated, convex cones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Dykstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Boyle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="391" to="399" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A note on the group lasso and a sparse group lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1001.0736v1</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Stanford University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">On the approximate realization of continuous mappings by neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="192" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural networks and the bias/variance dilemma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bienenstock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doursat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning fast approximations of sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Probability of the superior outcome of one treatment over another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Grissom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Psychology</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="314" to="316" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning sparse representations by non-negative matrix factorization and sequential cone programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schn?rr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1385" to="1407" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">At what points is the projection mapping differentiable? The American Mathematical Monthly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Hiriart-Urruty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1982" />
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="456" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tamhane</surname></persName>
		</author>
		<title level="m">Multiple Comparison Procedures</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Analysis of a complex of statistical variables into principal components</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="417" to="441" />
			<date type="published" when="1933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-negative matrix factorization with sparseness constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1457" to="1469" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Receptive fields of single neurones in the cat&apos;s striate cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hubel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Wiesel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="574" to="591" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Comparing measures of sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rickard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4723" to="4741" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Sparse code shrinkage: Denoising by nonlinear maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 11</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="473" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">How does connectivity between cortical areas depend on brain size? Implications for efficient computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karbowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="356" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Correlation matrix memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computers, C</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="353" to="359" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Use of ranks in one-criterion variance analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Wallis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="583" to="621" />
			<date type="published" when="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Matrix Analysis for Scientists and Engineers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Laub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Society for Industrial and Applied Mathematics</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Communication in neuronal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Laughlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">301</biblScope>
			<biblScope unit="issue">5641</biblScope>
			<biblScope unit="page" from="1870" to="1874" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning the parts of objects by nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Seung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">401</biblScope>
			<biblScope unit="issue">6755</biblScope>
			<biblScope unit="page" from="788" to="791" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Robust tests for equality of variances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Levene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to Probability and Statistics: Essays in Honor of Harold Hotelling</title>
		<imprint>
			<publisher>Stanford University Press</publisher>
			<date type="published" when="1960" />
			<biblScope unit="page" from="278" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning spatially localized, parts-based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="207" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Efficient euclidean projections in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="657" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Efficient 1 / q norm regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1009.4766v1</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>Arizona State University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Supervised dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 21</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Online learning for matrix factorization and sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="19" to="60" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Physiology and anatomy of synaptic connections between thick tufted pyramidal neurones in the developing rat neocortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Markram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>L?bke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frotscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sakmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Physiology</title>
		<imprint>
			<biblScope unit="volume">500</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="409" to="440" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Synaptic transmission between individual pyramidal neurons of the rat visual cortex in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stratford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="84" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">A finite algorithm for finding the projection of a point onto the canonical simplex of R n</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michelot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="195" to="200" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multiclass object recognition with sparse, localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Sparse approximate solutions to linear systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="234" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Some theorems on matrix differentiation with special reference to Kronecker matrix products</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Neudecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">327</biblScope>
			<biblScope unit="page" from="953" to="963" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Emergence of simple-cell receptive field properties by learning a sparse code for natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="issue">6583</biblScope>
			<biblScope unit="page" from="607" to="609" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sparse coding of sensory inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Opinion in Neurobiology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="481" to="487" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Paatero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Tapper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environmetrics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="111" to="126" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On associative memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="31" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Multiple comparison procedures applied to model selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Galindo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1-4</biblScope>
			<biblScope unit="page" from="155" to="173" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">An efficient projection for l 1,? regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="857" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sparse feature learning for deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1185" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A network that uses few active neurones to code visual input predicts the diverse shapes of cortical receptive fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>Sommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Thirteen ways to look at the correlation coefficient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Nicewander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="66" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="issue">6088</biblScope>
			<biblScope unit="page" from="533" to="536" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Support Vector Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t Berlin</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">An analysis of variance test for normality (complete samples)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Wilk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="591" to="611" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Document Analysis and Recognition</title>
		<meeting>the International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Fast projections onto mixed-norm balls with applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="358" to="377" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Sparseness by iterative projections onto spheres</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<meeting>the IEEE International Conference on Acoustics, Speech and Signal Processing</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="709" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">First results on uniqueness of sparse non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stadlthanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Signal Processing Conference</title>
		<meeting>the European Signal Processing Conference</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1672" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Supervised matrix factorization with sparseness constraints and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schweiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Joint Conference on Neural Networks</title>
		<meeting>the International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="973" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Training of sparsely connected MLPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schweiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">6835</biblScope>
			<biblScope unit="page" from="356" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Group sparsity via linear-time projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Friedlander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
		<idno>TR-2008-09</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of British Columbia</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Derivative operations on matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="244" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Sparse coding and decorrelation in primary visual cortex during natural vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Vinje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Gallant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">287</biblScope>
			<biblScope unit="issue">5456</biblScope>
			<biblScope unit="page" from="1273" to="1276" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">The Geometry of Orthogonal Spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Functional Operators</title>
		<imprint>
			<biblScope unit="volume">II</biblScope>
			<date type="published" when="1950" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Use of the zero-norm with linear models and kernel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elisseeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tipping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1439" to="1461" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The general inefficiency of batch training for gradient descent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1429" to="1451" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="301" to="320" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
