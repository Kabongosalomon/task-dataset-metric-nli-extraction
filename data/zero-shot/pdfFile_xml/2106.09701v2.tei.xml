<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Smith</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Balloch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilin</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Samsung Research America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Always Be Dreaming: A New Approach for Data-Free Class-Incremental Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern computer vision applications suffer from catastrophic forgetting when incrementally learning new concepts over time. The most successful approaches to alleviate this forgetting require extensive replay of previously seen data, which is problematic when memory constraints or data legality concerns exist. In this work, we consider the high-impact problem of Data-Free Class-Incremental Learning (DFCIL), where an incremental learning agent must learn new concepts over time without storing generators or training data from past tasks. One approach for DFCIL is to replay synthetic images produced by inverting a frozen copy of the learner's classification model, but we show this approach fails for common class-incremental benchmarks when using standard distillation strategies. We diagnose the cause of this failure and propose a novel incremental distillation strategy for DFCIL, contributing a modified crossentropy training and importance-weighted feature distillation, and show that our method results in up to a 25.1% increase in final task accuracy (absolute difference) compared to SOTA DFCIL methods for common class-incremental benchmarks. Our method even outperforms several standard replay based methods which store a coreset of images. Our code is available at https://github.com/ GT-RIPL/AlwaysBeDreaming-DFCIL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A shortcoming of modern computer vision settings is that they often assume offline training with a large dataset encompassing all objects to be encountered during deployment. In practice, many applications require a model be continuously updated after new environments/situations are encountered. This is the class-incremental learning paradigm (also known as a subset of continual or lifelong learning), with the loss of knowledge over sequences of learning tasks referred to as catastrophic forgetting. Suc-* Correspondence to: James Smith jamessealesmith@gatech.edu cessful incremental learning approaches have an unfortunate commonality: they require extensive memory for replay of previously seen or modeled data to avoid the catastrophic forgetting problem. This is concerning for many computer vision applications because 1) Many computer vision applications are on-device and therefore memory constrained <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b53">54]</ref>, and 2) Many computer vision applications learn from data which cannot be legally stored <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b63">64]</ref>. This leads us to ask: How can computer vision systems incrementally incorporate new information without storing data? We refer to this setting as Data-Free Class-Incremental Learning (DFCIL) (also known as Data-Free Continual Learning <ref type="bibr" target="#b61">[62]</ref>).</p><p>An intuitive approach for DFCIL is to simultaneously train a generative model to be sampled for replay <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref>. Unfortunately, training a generative model is much more computationally and memory intensive compared to a classification model. Additionally, it is not clear whether generating images from the data distribution will violate data legality concerns because using a generative model increases the chance of memorizing potentially sensitive data <ref type="bibr" target="#b43">[44]</ref>. Instead, we explore the concept of modelinversion image synthesis, where we can invert the already provided inference network to obtain images with similar activations in the network to the training data. This idea is inviting because it requires no additional networks to be trained (it only requires the existing inference network) and is less susceptible to data privacy concerns.</p><p>The closest existing work for the DFCIL problem is DeepInversion <ref type="bibr" target="#b61">[62]</ref>, which optimizes random noise into images for knowledge distillation using a frozen teacher network. DeepInversion is designed for standard studentteacher knowledge distillation and achieves state-of-the-art performance for this task. Unfortunately, the authors report that when trying class-incremental learning for tasks where the old images and new images are similar (such as tasks from the same dataset, a standard benchmarking practice for class-incremental learning), their method performs "statistically equivalent or slightly worse compared to Learning without Forgetting (LwF)", with LwF <ref type="bibr" target="#b35">[36]</ref> being their most competitive existing baseline.  <ref type="figure">Figure 1</ref>: The distribution of feature embeddings when using synthetic replay data for class-incremental learning. (a) A straight application of synthetic data makes the model learn features more distinguishable between real and fake instead of task 1 and 2. This is the main problem analyzed and addressed in this work. (b) Modifying classification loss and adding regularization mitigates the feature drifting between real and fake. (c) This is the desired feature distributions. Our method makes task 1 and 2 more separable.</p><p>The goal of this paper (summarized in <ref type="figure">Figure 1</ref>) is to dissect the cause of this failure and propose a solution for DFCIL. Specifically, we reason that when training a model with real images from the current task and synthetic images representing the past tasks, the feature extraction model causes the feature distributions of real images from the past tasks (which are not available during training) to be close in the feature space to the real images from the current task and far in the feature space from the synthetic images. This causes a bias for the model to falsely predict real images from the previous tasks with current task labels. This phenomena indicates that when training a network with two distributions of data, containing both a semantic shift (past tasks versus current task) and a distribution shift (synthetic data versus real data), the distribution shift has a higher effect on the feature embeddings. Thus, validation/test images from the previous classes will be identified as new classes due to the model fixating on their domain (i.e., realistic versus synthetic pixel distribution) rather than their semantic content (i.e., past versus current task).</p><p>To address this issue, we propose a novel classincremental learning method which learns features for the new task with a local classification loss which excludes the synthetic data and past-task linear heads, instead relying on importance-weighted feature distillation and linear head fine-tuning to separate feature embeddings of the new and past tasks. We show that our method represents the new state of the art for the DFCIL setting, resulting in up to a 25.1% increase in final task accuracy (absolute difference) compared to DeepInversion for common class-incremental benchmarks, and even outperforms popular replay baselines Naive Rehearsal and LwF with a coreset. In summary, we make the following contributions: 1. We use a classic class-incremental learning benchmark to diagnose and analyze why standard distillation approaches for class-incremental learning (such as Deep-Inversion) fail when using synthetic replay data.</p><p>2. We directly address this failure with a modified crossentropy minimization, importance-weighted feature distillation, and linear head fine-tuning.</p><p>3. We achieve a new state of the art performance for the DFCIL setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background and Related Work</head><p>Catastrophic Forgetting: Approaches to mitigate catastrophic forgetting can be organized into a few broad categories and are all useful depending on which constraints are present. For example, methods which expand a model's architecture as new tasks are encountered are highly effective for applications where a model growing with tasks is practical <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53]</ref>. We do not consider these methods because the model parameters grow linearly with the number of tasks. Experience replay with stored data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b58">59]</ref> or samples from a generative model <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref> is highly effective when storing training data or training/saving a generative model is possible, unlike the DFCIL setting. Another approach is to regularize the model with respect to past task knowledge while training the new task. This can either be done by regularizing the model in the weight space (i.e., penalize changes to model parameters) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">13,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b62">63]</ref> or the prediction space (i.e., penalize changes to model predictions) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b51">52]</ref>. Prediction space regularization (accomplished using knowledge distillation) has been found to perform better than model regularization based methods for class-incremental learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b56">57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation in Class-Incremental Learning:</head><p>Based on the original work proposing knowledge distillation from a large model to a smaller model <ref type="bibr" target="#b21">[22]</ref>, methods such as Learning Without Forgetting <ref type="bibr" target="#b35">[36]</ref>, Distillation and Retrospection <ref type="bibr" target="#b22">[23]</ref>, End-To-End Incremental Learning <ref type="bibr" target="#b6">[7]</ref>, Global Distillation <ref type="bibr" target="#b32">[33]</ref>, and Bias Correction <ref type="bibr" target="#b60">[61]</ref> have effectively leveraged knowledge distillation as a prediction regularization technique for incremental learning. The high level idea of knowledge distillation is to periodically save a frozen copy of the model (here we use a ConvNet) and to ensure the new model makes similar predictions to the frozen model over a set of distillation images (while simultaneously learning the new task). Knowledge distillation does not require that the frozen model be replaced at the task sequence boundaries, but this is typically done when evaluating competing methods. This regularization can also take place in the feature space rather than the prediction space <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref>, which we refer to as feature distillation. These knowledge distillation methods require stored data over which to enforce similar predictions, but the next section describes a form of knowledge distillation which does not require training data.</p><p>Data-Free Knowledge Distillation: Knowledge from a neural network can be transferred in the absence of training data. We refer to the line of work which synthesizes distillation images using the trained inference network itself and resulting activation statistics as data-free knowledge distillation. This approach is very important for applications where training data is sensitive and not easily available for legality issues. The first such work we are aware of, DeepDream <ref type="bibr" target="#b42">[43]</ref>, optimizes randomly generated noise into images which minimize classification loss and an image prior. Another early method <ref type="bibr" target="#b38">[39]</ref> matches stored layer statistics from a trained "teacher" model while leaving a small memory footprint using frequency-based compression techniques. The Data-Free Learning method <ref type="bibr" target="#b9">[10]</ref> exploits a GAN architecture to synthesize images which match the trained teacher's statistics while balancing content losses which maximize both temperature scaled linear heads (to drive class-specific content high) and class prediction entropy (to encourage high diversity of classes sampled). Three recent methods leverage layer content stored in batch-normalization layers to synthesize realistic looking images for knowledge distillation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref>.</p><p>To the best of our knowledge, only two class-incremental learning methods are designed for the data-free perspective. Automatic-Recall Machines (ARM) <ref type="bibr" target="#b25">[26]</ref> perturb training data from the current task into images which maximize "forgetting" from the past tasks. However, this method is designed for a "single pass" setting where data is only trained for one epoch, which is a different setting than ours. Deep-Inversion <ref type="bibr" target="#b61">[62]</ref> also evaluates data-free knowledge distillation in the class-incremental learning paradigm, but only found success in small task sequences (max of three) using tasks which are very distinct in image content. Our paper dissects why the DeepInversion method fails at difficult class-incremental learning problems and proposes a solution for successful data-free class-incremental learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Class-Incremental Learning: In class-incremental learning, a model is shown labeled data corresponding to M semantic object classes c 1 , c 2 , . . . , c M over a series of N tasks corresponding to non-overlapping subsets of classes. We use the notation T n to denote the set of classes introduced in task n, with |T n | denoting the number of object classes in task n. Each class appears in only a single task, and the goal is to incrementally learn to classify new object classes as they are introduced while retaining performance on previously learned classes. The class-incremental learning setting is challenging because no task indexes are provided to the learner during inference and the learner must support classification across all classes seen up to task n <ref type="bibr" target="#b24">[25]</ref>. This is more difficult than task-incremental learning, where the task indexes are given during both training and inference. While our setting does not necessitate known task boundaries during training, we follow prior works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b61">62]</ref> for fair comparison and create model copies at the task boundaries for each method.</p><p>To describe our inference model, we denote ? i,n as the model ? at time i that has been trained with the classes from task n. For example, ? n,1:n refers to the model trained during task n and its logits associated with all tasks up to and including class n. We drop the second index when describing the model trained during task n with all logits (for example, ? n ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Baseline Approach for Data-Free Class Incremental Learning</head><p>In this section, we propose a general baseline for datafree class incremental learning based on efforts in prior work. We start by summarizing the data synthesis (i.e. generation of images from the discriminative model itself) approach we found most successful for class-incremental learning. We then review pertinent knowledge distillation losses, ultimately focusing on the loss functions used by DeepInversion <ref type="bibr" target="#b61">[62]</ref> for class-incremental learning. Model-Inversion Image Synthesis: Most model-inversion image synthesis approaches seek to synthesize images by directly optimizing them with respect to a prior discriminative model ? n?1 . For B synthetic images, a tensor X ? R B?H?W ?C = {x 1 ? ? ?x b }, where H, W , and C correspond to the training data image dimensions, is initialized from Gaussian noise. However, it is computationally inefficient to optimize one batch of images at a time. Especially considering that class-incremental learning is expected to be computationally efficient, we choose to approximate this optimization from noise to synthesized images using a ConvNet-parameterized function F ? . This allows the framework to train F ? once per task (using only ? n?1 , i.e., no data), only store it temporarily during that given task, sample synthetic images as needed during training for task T n , and then discarded it at the end of the task.</p><p>F ? can struggle with synthetic class diversity; rather than condition F ? on class labels Y , we follow <ref type="bibr" target="#b9">[10]</ref> and optimize the diversity of class predictions of synthetic images to match a uniform distribution. Denoting p ? (x) as the predicted class distribution produced by model ? for some input x, we want to maximize the entropy of the mean class prediction vector for synthetic samplesX. Formally, we minimize a label diversity loss:</p><formula xml:id="formula_0">L div (? ) = ?H inf o 1 B b p ? (x b )<label>(1)</label></formula><p>where H inf o is the information entropy. Notice when the loss is taken at the minimum, every element in the mean class prediction vector would equal 1 |T1...Tn?1| , meaning that classes are generated with at roughly the same rate.</p><p>In addition to diversity, to consistently synthesize useful images in the DFCIL setting, the images must enforce calibrated class confidences, consistency of feature statistics, and a locally smooth latent space, described below.</p><p>Content loss, L con , maximizes class prediction confidence with respect to the image tensorX such that ? n?1 should make confident predictions on all inputs. Formally, L con is the cross entropy classification loss between the class predictions ofX and the maximum class prediction? Y :</p><formula xml:id="formula_1">L con (X,? ) = L CE p ?temp ?n?1,1:n?1 (x),?<label>(2)</label></formula><formula xml:id="formula_2">y = argmax y?T1...Tn?1 p ?n?1,1:n?1 (x)<label>(3)</label></formula><p>where L CE is standard cross entropy loss and the logit output of ? is scaled by a temperature constant ? temp . By combining L con with L div , we ensure that the synthesized images will represent the distribution of all past task classes Prior work has found that the complexity of modelinversion can cause the distribution of ? n?1 features to deviate greatly from distributions over batches of synthetic images. Intuitively, the batch statistics of synthesized images should match those of the batch normalization layers in ? n?1 . To enforce this, stat alignment loss, L stat , penalizes the deviation between intermediate layer batchnormalization statistics (BNS) stored in ? n?1 and features at those layers for synthetic images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b61">62]</ref>:</p><formula xml:id="formula_3">L stat (X) = 1 L L l=1 BN S(?X ,l ,?X ,l , ? l , ? l ) (4) BN S(?,?, ?, ?) = KL(N (?, ? 2 ))||N (?,? 2 ) = log? ? ? 1 2 1 ? ? 2 + (? ??) 2 ? 2<label>(5)</label></formula><p>where KL denotes the Kullback-Leibler (KL) divergence, ?X ,l ,?X ,l are the mean and standard deviation of the features at layer l for a given mini-batch of synthesized image, and ? l , ? l are the batch-norm statistics of said layer l. Because batch statistics of ? n?1 are stored in batch normalization layers, this loss does not require any additional storage. Additionally, prior knowledge tells us natural images are more locally smooth in pixel space than the initial noise. As such, we can stabilize the optimization by minimizing the smoothness prior loss L prior . Formally, L prior is the L2 distance between each synthesized image (x) and a version blurred with Gaussian kernel (x blur ):</p><formula xml:id="formula_4">L prior (X) = ||x ?x blur || 2 2 (6)</formula><p>All together, assuming the use of F ? for efficiency, the final loss for the baseline is therefore:</p><formula xml:id="formula_5">min F ? ? con L con (X,? ) + ? div L div (? ) + ? stat L stat (X) + ? prior L prior (X)<label>(7)</label></formula><p>Importantly: although we optimize F ? rather thanX as done in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b61">62]</ref>, this method can use the latter with a sacrifice to computational efficiency.</p><p>Distilling Synthetic Data for Class-Incremental Learning: In the class-incremental learning setting, where the classes of task T n are modeled without unlearning the representation of classes of tasks T 1 ? ? ? T n?1 , knowledge distillation over the synthesized images is most often used to regularizes ? n , forcing it to learn T n with minimal degradation to T 1 ? ? ? T n?1 knowledge. For task T n , we synthesize images from a frozen copy of (? n?1 ) trained during task T n?1 . These synthetic images then help us distill knowledge learned in tasks T 1 ? ? ? T n?1 into our current model (? n ) as it learns from T n data.</p><p>In our baseline, we adopt the distillation approach used in DeepInversion <ref type="bibr" target="#b61">[62]</ref>, which generalizes the original Learning without Forgetting (LwF) <ref type="bibr" target="#b35">[36]</ref> distillation approach. Formally, given current task data, x, and synthesized distillation data,x, we minimize:</p><formula xml:id="formula_6">min ?n L CE p ?n,1:n (x), y + L DI KD (x, ? n , ? n?1 ) + L DI KD (x, ? n , ? n?1 )<label>(8)</label></formula><p>where L DI KD is a knowledge distillation regularization as: L DI KD (x, ? n , ? n?1 ) = KL(p ?n?1,1:n (x)||p ?n,1:n (x)) (9) Here, p ?n?1,1:n (x) is simply p ?n?1,1:n?1 (x) appended with zeros to represent zero class-probability for the new classes (which are not available for ? n?1,1:n?1 ). The key idea of L DI KD is that, as the logits of the teacher and student will always be different in the class incremental learning setting, appending zeros to the class-probability vectors aligns the student and teacher logit dimensions for a better transfer of knowledge.</p><p>(a) DeepInversion <ref type="bibr" target="#b61">[62]</ref> (b) Our Method <ref type="figure">Figure 2</ref>: Representational distance scores (MID) between feature embeddings of real task 1 data and synthetic task 1 data (blue), real task 2 data (red). Task 1 corresponds to ten classes of CIFAR-100 while task 2 corresponds to a different ten classes of CIFAR-100; the results are generated after training on task 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Diagnosis: Feature Embedding Prioritizes Domains Over Semantics</head><p>To understand why the baseline approach for DFCIL fails, we analyze representational distance between embedded features with a metric that captures the distance between mean embedded images of two distribution samples. Specifically, we assign a Mean Image Distance (MID) score between a reference sample of images x a and another sample of images x b , where a high score indicates dissimilar features and a low score indicates similar features. We calculate this score as:</p><formula xml:id="formula_7">M ID(z a , z b ) = ? a ? ? b ? a 2<label>(10)</label></formula><p>where z a , z b is the penultimate feature embedding of x a , x b ; ? a , ? b are the mean image feature embeddings of x a , x b ; and ? 2 a is the feature variance of x a . We normalize the distance between mean embedded images by the standard deviation of the reference distribution sample x a to minimize the impact of highly deviating features. Additional analysis using Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b15">[16]</ref> is available in our Appendix (B).</p><p>For our analysis, we start by training our model for the first two tasks in the ten-task CIFAR-100 benchmark described in Section 7. We calculate MID between feature embeddings of real task 1 data and real task 2 data, and then we calculate MID between feature embeddings of real task 1 data and synthetic task 1 data. The results are reported in <ref type="figure">Figure 2</ref>. For (a) DeepInversion, the MID score between real task 1 data and synthetic task 1 data is significantly higher than the MID score between real task 1 data and real task 2 data. This indicates that the embedding space prioritizes domain over semantics, which is detrimental because the classifier will learn the decision boundary between synthetic task 1 and real task 2, introducing great classification error with real task 1 images. This diagnosis motivates our approach, which is proposed in the following section. For (b) our method, the MID score between real task 1 data and synthetic task 1 data is much lower, indicating that our feature embedding prioritizes semantics over domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">A New Distillation Strategy for DFCIL</head><p>We take the perspective that continual learning should balance: (i) learning features for the new task, (ii) minimizing feature drift over the previous task, and (iii) separating class overlap between new and previous classes in the embedding space (this is similarly discussed in another work <ref type="bibr" target="#b23">[24]</ref> under a different setting). Generally, (i) and (iii) are simultaneously achieved with L CE , but we argue that by separating this into two different losses, features for the new task are learned which do not discriminate between real and synthetic images (i.e. avoid the feature domain bias problem). Following this idea, we propose a new classincremental learning approach designed for DFCIL which addresses each of these goals independently, as described in the rest of this section. Learning current task features: The intuition behind our method is to learn features for our current task while circumventing the feature embedding for real data becoming highly biased towards the most recent task. That is, we form L CE so that the likelihood of x being real versus synthetic, p ?n,n (x ? X real ), is not helpful in its minimization. We do this by computing cross entropy classification loss locally across the new class linear heads without including the past class linear heads. With this formation, we prevent the model from learning to separate the new and past class data via domain (i.e. synthetic vs. real). Formally, we minimize:</p><formula xml:id="formula_8">L Tn CE p ?n,n (x), y = L CE p ?n,n (x|y ? T n ), y<label>(11)</label></formula><p>Minimizing feature drift over previous task data: Because our distillation images are of another domain than the real current task images (causing the feature domain bias problem), we seek a loss function which directly alleviates forgetting in the feature space. An alternative to standard knowledge distillation over softmax predictions (L DI KD ) is feature distillation, which instead distils the feature content  <ref type="formula" target="#formula_0">14)</ref>. We use blue arrows to designate the compute path of the synthetic previous tasks data, green arrows to designate the compute path of the real current task data, and yellow arrows to designated the compute path of both real and synthetic data. We separate out the task Tn head to show that the local CE loss Eq. (11) uses only this head. from the penultimate layer. This is formally given as:</p><formula xml:id="formula_9">L f eat KD (?) = ||? L?1 n,1:n?1 (x) ? ? L?1 n?1,1:n?1 (x)|| 2 2<label>(12)</label></formula><p>where L ? 1 denotes the penultimate layer output of the model. Our intuition is that there exists a trade-off between standard knowledge distillation and feature distillation. L f eat KD reinforces important components of past task data, but it is a strong regularization which inhibits the plasticity of the model (and its ability to learn the new task). On the other hand, L DI KD does not inhibit learning the new task but can be minimized with a solution in which feature drift has occurred, resulting in the real vs synthetic bias.</p><p>Instead, we desire an importance-weighted feature distillation which reinforces only the most important components of past task data while allowing less important features to be adapted for the new task. We simply use the linear heads of T 1 ? ? ? T n?1 from the frozen model ? n?1 , or: </p><p>By using this importance-weight matrix, features associated with a high magnitude in W are more important to preserve. In this way, the frozen linear heads from the past tasks indicate approximately how much a change to each feature affects the class distribution. Separating Current and Past Decision Boundaries: Finally, we need to separate the decision boundaries of the current and past classes without allowing the feature space to distinguish between the real and synthetic data. We do this by fine-tuning the linear classification head of ? n,1:n with standard cross-entropy loss. Importantly, this loss does not update any parameters in ? n,1:n besides the final linear classification head. Formally, we minimize:</p><p>L T1:n F T p ?n,1:n (x, y) = L CE p * ?n,1:n (x, y)</p><p>where p * is calculated with ? 1:L?1 n,1:n (i.e., every layer in the model except the classification layer) frozen, updating only ? L n,1:n . As done in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61]</ref>, we add task-balancing loss weighting to balance the contributions from the current task with the past tasks. </p><p>where the ? terms weight the contributions of L wf eat KD and L F T with respect to L Tn</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We evaluate our approach with several image datasets in the class incremental continual learning setting. We implemented baselines which do not store training data for rehearsal: Deep Generative Replay (DGR) <ref type="bibr" target="#b50">[51]</ref>, Learning without Forgetting (LwF) <ref type="bibr" target="#b35">[36]</ref>, and Deep Inversion (Deep-Inversion) <ref type="bibr" target="#b61">[62]</ref>. Additionally, we report the upper bound performance (i.e., trained offline) and performance for a neural network trained only on classification loss using the new task training data (we refer to this as Base). We note that a downside of any generative method is that they (1) require long-term storage of a generative model and (2) may violate data legality concerns.</p><p>For a fair comparison, our implementation of DeepInversion uses the same image synthesis strategy as our method, with the difference being the distillation method. We do not tune hyperparameters on the full task set because tuning hyperparameters with hold out data from all tasks may violate the principal of continual learning that states each task is visited only once <ref type="bibr" target="#b57">[58]</ref>. Importantly, we shuffle the class <ref type="table" target="#tab_4">Table 1</ref>: Results (%) for data-free class-incremental learning on CIFAR-100 for various numbers of tasks <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20</ref>  order before sampling tasks, and do this with a consistent seed (different for each trial) so that results can be directly compared. We include supplementary details and metrics in our Appendix: additional results (A), additional experiment details (C), and hyperparameter selection (D). Evaluation Metrics: Following prior works, we evaluate methods in the class-incremental learning setting using: (I) final performance, or the performance with respect to all past classes after having seen all N tasks (referred to as A N,1:N ); and (II) ?, or the average (over all tasks) normalized task accuracy with respect to an offline oracle method <ref type="bibr" target="#b19">[20]</ref>. We use index i to index tasks through time and index n to index tasks with respect to test/validation data (for example, A i,n describes the accuracy of our model after task i on task n data). Specifically:</p><formula xml:id="formula_13">A i,n = 1 |D test n | (x,y)?D test n 1(?(x, ? i,n ) = y |? ? T n )<label>(16)</label></formula><formula xml:id="formula_14">? = 1 N N i=1 i n=1 |T n | |T 1:i | A i,1:n A of f line,1:n<label>(17)</label></formula><p>where A of f ine is the task accuracy trained in the offline setting (i.e., the upper-bound performance). ? is designed to evaluate the global task and is therefore computed with respect to all previous classes. For the final task accuracy in our results, we will denote A N,1:N as simply A N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-Free Class-Incremental Learning -CIFAR-100</head><p>Benchmark: Our first benchmark is ten-task classincremental learning on the CIFAR-100 dataset <ref type="bibr" target="#b30">[31]</ref> which contains 100 classes of 32x32x3 images. Following prior work <ref type="bibr" target="#b60">[61]</ref>, we train with a 32-layer ResNet <ref type="bibr" target="#b20">[21]</ref> for 250 epochs. The learning rate is set to 0.1 and is reduced by 10 after 100, 150, and 200 epochs. We use a weight decay of 0.0002 and batch size of 128. Using a simple grid search to find hyperparameters for Eq <ref type="formula" target="#formula_5">(7)</ref>, we found {? con , ? div , ? stat , ? prior , ? temp } as {1 , 1, 5e1, 1e-3, 1e3} (these hyperparameters are not introduced by our method). For Eq. (15) in our method, we found {? kd , ? f t } as {1e-1 ,1}, and we use prior reported loss-weighting hyperparameters for our implementations of other methods. We use a temperature scaling of 2 for all softmax knowledge distillation instances. We model F with the same parameters as <ref type="bibr" target="#b40">[41]</ref> and train Eq. (7) after starting each task using 5,000 training steps of Adam optimization (learning rate 0.001).</p><p>The results are given in <ref type="table" target="#tab_4">Table 1</ref>. We see that our method outperforms not only the DFCIL methods (including a 25.1% increase in final task accuracy over DeepInversion), but even the generative approach (despite their use of significant additional memory between tasks). To our surprise, we found DGR <ref type="bibr" target="#b50">[51]</ref> to perform poorly for class-incremental learning on this dataset (and in fact every dataset we experiment with); this finding is repeated in another recent work <ref type="bibr" target="#b55">[56]</ref> which also found DGR to perform worse 1 than Base. We are not surprised to see that LwF <ref type="bibr" target="#b35">[36]</ref> performs worse than naive rehearsal, as this is also common for class-incremental learning <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b55">56]</ref>. Finally, we observe that synthetic data does not improve LwF's performance. This is consistent with our finding that the feature embedding prioritizes domains over semantics when using standard distillation strategies.  Class-Incremental Learning with Replay Data -CIFAR-100 Benchmark: In <ref type="table" target="#tab_1">Table 2</ref>, we compare our method (which does not store replay data) to other methods which do store replay data. We use the same number of coreset images as the <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b60">61]</ref>. We found that our method can perform significantly greater than LwF and Rehearsal, which store replay data. We also compare our method with a SOTA replay-based class-incremental learning method: Bias Correction (BiC) <ref type="bibr" target="#b60">[61]</ref>. Despite not storing any replay data, our method performs roughly in between BiC and LwF, though there is still a considerable (and expected) gap between our data-free approach and BiC. In summary, these results indicate that our method achieves State-of-the-Art performance for Data-Free Class-Incremental Learning, and our method closes much of the performance gap between Data-Free Class-Incremental Learning and Stateof-the-Art replay-based methods.</p><p>Ablation Study -CIFAR-100 Benchmark: We separate the components of our method to independently evaluate their effect on final performance, shown in Tables 3. We first look at the effect of removing task balancing loss weighting. As previously reported <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61]</ref>, this loss weighting has a significant effect on our performance. Next, we show that replacing the modified cross-entropy loss with standard cross-entropy loss cuts performance in half. Similarly, we show that ablating real and synthetic data distillation have the same effect. This indicates that 1) all three of these losses are crucial for our approach, and 2) we still establish SOTA performance despite removing any of these three losses. Finally, we see that removing the fine-tuning cross-entropy loss has the largest effect on performance.</p><p>Conceptually, this makes sense because without this loss there is no way to distinguish new task classes from previous task classes. Class-Incremental Learning with Replay Data -Im-ageNet Benchmark: Finally, we use the ImageNet dataset <ref type="bibr" target="#b48">[49]</ref> to demonstrate how our method performs on large scale 224x224x3 images. Following prior work <ref type="bibr" target="#b60">[61]</ref>, we train with a 18-layer ResNet <ref type="bibr" target="#b20">[21]</ref> for 100 epochs. The learning rate is set to 0.1 and is reduced by 10 after 30, 60, 80, and 90 epochs. We use a weight decay of 0.0001 and batch size of 128. We use the same class-shuffling seed as prior work <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref> and report top-5 accuracy on ten tasks of 100 classes. We scale down to 20k coreset images used in the full ImageNet experiments to 2k, consistent with the relative number of classes. We also double the number of training steps used to train F. Every other experiment detail is the same as the CIFAR-100 experiments.</p><p>The results are given in <ref type="table" target="#tab_3">Table 4</ref>. Importantly, this experiment is significant because the number of parameters stored for replay (2000*224*224*3 = 3e8) far exceeds the number of parameters temporarily stored for synthesizing images <ref type="bibr">(3.3e6)</ref>. Despite requiring only 100 times fewer parameters to store, our method performs reasonably close to replay on this large-scale image experiment. We also far outperform LwF, which is the only DFCIL method to have been previously tried on large-scale ImageNet experiments. Additional experiments on the challenging Tiny-ImageNet dataset <ref type="bibr" target="#b31">[32]</ref>, which demonstrate the scalability of our method, are available in the Appendix (A).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We show that existing class-incremental learning methods perform poorly when learning a new task with real training data and preserving past knowledge with synthetic distillation data. We then contribute a new method which achieves SOTA performance for data-free class-incremental learning, and is comparable to SOTA replay-based approaches. Our research vision is to eliminate the need for storing replay data in class-incremental learning, enabling broad and practical applications of computer vision. An incremental learning solution which does not store data will provide immediate impact to computer vision applications such as reducing memory requirements for autonomous vehicles (which generate an inordinate amount of data), eliminating the need to transfer private medical data for medical imaging research collaboration (which is limited by strict legal protections), or removing the need to track private data for personal device user recommendation systems.</p><p>In this section, we present additional experiments and results. For an alternative view of all results, we show ? plotted by task in <ref type="figure">Figure A</ref>.</p><p>In <ref type="table" target="#tab_4">Tables A/B</ref>, we expand our CIFAR-100 results with two additional methods: (1) LwF.MC <ref type="bibr" target="#b45">[46]</ref>, a more powerful variant of LWF designed for class-incremental learning, and (2) End-to-End Incremental Learning <ref type="bibr" target="#b6">[7]</ref> (E2E). In our implementation of E2E, we use the same data augmentations as our other experiments for a fair comparison. As previously published <ref type="bibr" target="#b60">[61]</ref>, we see that E2E performs slightly worse than BiC and LwF.MC strongly outperforms LWF. Our approach consistently outperforms LwF.MC.</p><p>We also report additional results on the Tiny-ImageNet dataset <ref type="bibr" target="#b31">[32]</ref> in Tables C/D, which contains 200 classes of 64x64 resolution images with 500 training images per class. We use the same experiment settings as CIFAR-100 with 10 classes per task and 20 tasks total. This is a highly challenging dataset with a low upper bound performance (drops from 69.9% to 55.5%), but we arrive at the same conclusions as we did for our CIFAR-100 experiments: our method outperforms all data-free class-incremental learning approaches, and performs slightly worse than state-of-theart approaches which store 2000 images for replay. Importantly, the number of parameters stored for replay in these experiments (2000*64*64*3 = 2.5e7) far exceeds the number of parameters temporarily stored for synthesizing images <ref type="bibr">(8.5e6</ref>). Note that this memory usage in our method can be completely removed at the cost of additional computation. Despite requiring only 10 times fewer parameters to store (and not storing any training data), our method performs reasonably close to state-of-the-art.</p><p>Finally, we expand the main paper results in <ref type="table" target="#tab_4">Table E</ref> to include LWF.MC. Our method and LWF.MC perform similarly, indicating that more work is needed to scale our approach to large 224x224x3 images. This is not surprising because prior work <ref type="bibr" target="#b40">[41]</ref> requires 1 generator per class to scale data-free generative distillation up to ImageNet. We do not have the computational resources to perform this (e.g., full 1000 class ImageNet would require 1000 generators). Instead, our work demonstrates the need for generative data-free knowledge distillation to be efficiently scaled up to the 224x224x3 images of ImageNet. We leave this to future work. We kindly acknowledge that recent works which replay from a generator (close to our setting) also use small variants of ImageNet in their experiments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b44">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Baseline Diagnosis with MMD</head><p>In Section 5, we analyze representational distance between embedded features with a metric that captures the dis-tance between mean embedded images of two distribution samples. This metric is Mean Image Distance (MID) and is calculated with a reference sample of images x a and another sample of images x b , where a high score indicates dissimilar features and a low score indicates similar features. In this section, we repeat the Section 5 experiments with the commonly used unbiased Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b15">[16]</ref>, which gives the distance between embeddings of two distributions in a reproducing kernel Hilbert space.</p><p>As done in Section 5, we start by training our model for the first two tasks in the ten-task CIFAR-100 benchmark. We calculate MMD between feature embeddings of real task 1 data and real task 2 data, and then we calculate MMD between feature embeddings of real task 1 data and synthetic task 1 data. The results are reported in <ref type="figure">Figure B</ref>. For (a) DeepInversion, the MMD score between real task 1 data and synthetic task 1 data is significantly higher than the MMD score between real task 1 data and real task 2 data. As found in Section 5, this indicates that the embedding space prioritizes domain over semantics, which is detrimental because the classifier will learn the decision boundary between synthetic task 1 and real task 2, introducing great classification error with real task 1 images. For (b) our method, the MMD score between real task 1 data and synthetic task 1 data is much lower, indicating that our feature embedding prioritizes semantics over domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Experiment Details</head><p>The majority of experiment details are listed in the main text (Section 7) and are dataset specific. Additionally: (i) we augment training data using standard augmentations such as random horizontal flips and crops, (ii) results were generated using a combination of Titan X and 2080 Ti GPUs, and (iii) synthesized images are sampled from F at each training step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hyperparameter Sweeps</head><p>We tuned hyperparameters using a grid search. The hyperparameters were tuned using k-fold cross validation with three folds of the training data on only half of the tasks. We do not tune hyperparameters on the full task set because tuning hyperparameters with hold out data from all tasks may violate the principal of continual learning that states each task in visited only once <ref type="bibr" target="#b57">[58]</ref>. The results reported outside of this section are on testing splits (defined in the dataset).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion of Class Shuffling Seeds</head><p>Our results are slightly lower than reported in prior work <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b60">61]</ref> because we re-implemented each method in our benchmarking environment. A major difference between our implementation and these works is that, instead of using a fixed seed for a single class-order, we instead (a) ? curve for five task CIFAR-100 (without coreset) (b) ? curve for five task CIFAR-100 (with coreset) (c) ? curve for ten task CIFAR-100 (without coreset) (d) ? curve for ten task CIFAR-100 (with coreset) (e) ? curve for twenty task CIFAR-100 (without coreset) (f) ? curve for twenty task CIFAR-100 (with coreset) (g) ? curve for twenty task Tiny ImageNet (without coreset) (h) ? curve for twenty task Tiny ImageNet (with coreset) <ref type="figure">Figure A</ref>: ? curves showing task number t on the x-axis and ? up to task t on the y-axis.      <ref type="figure">Figure B</ref>: Maximum Mean Discrepancy (MMD) between feature embeddings of real task 1 data and synthetic task 1 data (blue), real task 2 data (red). Task 1 corresponds to ten classes of CIFAR-100 while task 2 corresponds to a different ten classes of CIFAR-100; the results are generated after training on task 2. randomly shuffle the class and task order for each experiment run. The class order has a significant effect on the end results, with our top performing class order resulting in performance similar to results reported in <ref type="bibr" target="#b60">[61]</ref>. We argue that shuffling the class order gives a better representation of method performance while acknowledging both approaches (shuffling and not shuffling) have merit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. t-SNE Visualization</head><p>In <ref type="figure">Figure C</ref>, we show real t-SNE visualizations which reasonably approximate <ref type="figure">Figure 1</ref>.a (DeepInversion) and <ref type="figure">Figure 1</ref>.c (Our Method) from the main text. Results are shown after training the second task in the ten-task CIFAR-100 benchmark. Importantly, the distilling ? 1,1 model and the synthetic data are the same for both methods; only the loss functions are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Training Time</head><p>In <ref type="figure" target="#fig_5">Figure D</ref>, we show the training time (seconds per training batch on a single Titain X Pascal GPU) for the twenty task Tiny-ImageNet benchmark (Tables C/D). Our method is faster than the SOTA replay-based method, BIC, yet slower than the other methods. All of these methods produce a model of the same architecture and therefore have the same inference time (except for BIC which has a very small logit weighting operation).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Our approach combines (i) learning features for the new task with Eq. (11), (ii) minimizing feature drift over the previous task with Eq. (13), and (iii) separating class overlap between new and previous classes in the embedding space with Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>L</head><label></label><figDesc>wf eat KD (?) = ||W ? L?1 n,1:n?1 (x) ? W ? L?1 n?1,1:n?1 (x) || 2 2where W = ? L n?1,1:n?1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Final Objective: Visualized in Figure 3, our final optimization objective is given as: min ?n L Tn CE p ?n,n (x), y + ? kd L wf eat KD ({x,x}, ? n , ? n?1 ) + ? f t L T1:n F T p ?n,1:n ({x,x}), {y,?}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>t-SNE visualizations for (a) Figure 1.a (DeepInversion) and (b) Figure 1.c (Our Method) from the main text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure D :</head><label>D</label><figDesc>Training time for the twenty task Tiny-ImageNet benchmark(Tables C/D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>? 0.9 78.6 ? 1.1 33.7 ? 1.2 69.6 ? 1.6 20.0 ? 1.4 52.5 ? 2.5</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">). Results are reported</cell></row><row><cell>as an average of 3 runs.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Tasks</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell><cell></cell><cell>20</cell><cell></cell></row><row><cell>Method</cell><cell>Replay Data</cell><cell>AN (?)</cell><cell>? (?)</cell><cell>AN (?)</cell><cell>? (?)</cell><cell>AN (?)</cell><cell>? (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell>69.9 ? 0.2</cell><cell>100.0 ? 0.0</cell><cell>69.9 ? 0.2</cell><cell>100.0 ? 0.0</cell><cell>69.9 ? 0.2</cell><cell>100.0 ? 0.0</cell></row><row><cell>Base</cell><cell>None</cell><cell>16.4 ? 0.4</cell><cell>48.9 ? 1.1</cell><cell>8.8 ? 0.1</cell><cell>32.1 ? 1.1</cell><cell>4.4 ? 0.3</cell><cell>19.7 ? 0.7</cell></row><row><cell>DGR [51]</cell><cell>Generator</cell><cell>14.4 ? 0.4</cell><cell>45.5 ? 0.9</cell><cell>8.1 ? 0.1</cell><cell>30.5 ? 0.6</cell><cell>4.1 ? 0.3</cell><cell>19.0 ? 0.3</cell></row><row><cell>LwF [36]</cell><cell>None</cell><cell>17.0 ? 0.1</cell><cell>49.5 ? 0.1</cell><cell>9.2 ? 0.0</cell><cell>33.3 ? 0.9</cell><cell>4.7 ? 0.1</cell><cell>20.1 ? 0.3</cell></row><row><cell>LwF [36]</cell><cell>Synthetic</cell><cell>16.7 ? 0.1</cell><cell>49.8 ? 0.1</cell><cell>8.9 ? 0.0</cell><cell>32.3 ? 0.0</cell><cell>4.7 ? 0.0</cell><cell>19.7 ? 0.0</cell></row><row><cell>DeepInversion [62]</cell><cell>Synthetic</cell><cell>18.8 ? 0.3</cell><cell>53.2 ? 0.9</cell><cell>10.9 ? 0.6</cell><cell>37.9 ? 0.8</cell><cell>5.7 ? 0.3</cell><cell>23.6 ? 0.7</cell></row><row><cell>Ours</cell><cell>Synthetic</cell><cell>43.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results (%) for class-incremental learning with replay data on CIFAR-100 for various numbers of tasks<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20)</ref>. A coreset of 2000 images is leveraged for replay-based methods, and thus these methods do not meet problem the DFCIL constraints (note we report for our method numbers without any coreset). Results are reported as an average of 3 runs.</figDesc><table><row><cell>Tasks</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell><cell></cell><cell>20</cell><cell></cell></row><row><cell>Method</cell><cell>Replay Data</cell><cell>AN (?)</cell><cell>? (?)</cell><cell>AN (?)</cell><cell>? (?)</cell><cell>AN (?)</cell><cell>? (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell>69.9 ? 0.2</cell><cell>100.0 ? 0.0</cell><cell>69.9 ? 0.2</cell><cell>100.0 ? 0.0</cell><cell>69.9 ? 0.2</cell><cell>100.0 ? 0.0</cell></row><row><cell>Naive Rehearsal</cell><cell>Coreset</cell><cell>34.0 ? 0.2</cell><cell>73.4 ? 0.8</cell><cell>24.0 ? 1.0</cell><cell>64.6 ? 2.1</cell><cell>14.9 ? 0.7</cell><cell>51.4 ? 2.9</cell></row><row><cell>LwF [36]</cell><cell>Coreset</cell><cell>39.4 ? 0.3</cell><cell>79.0 ? 0.0</cell><cell>27.4 ? 0.8</cell><cell>69.4 ? 0.4</cell><cell>16.6 ? 0.4</cell><cell>54.2 ? 2.2</cell></row><row><cell>BiC [61]</cell><cell>Coreset</cell><cell cols="6">53.7 ? 0.4 87.5 ? 0.9 45.9 ? 1.8 81.9 ? 2.0 37.5 ? 3.2 71.7 ? 3.4</cell></row><row><cell>Ours</cell><cell>Synthetic</cell><cell>43.9 ? 0.9</cell><cell>78.6 ? 1.1</cell><cell>33.7 ? 1.2</cell><cell>69.6 ? 1.6</cell><cell>20.0 ? 1.4</cell><cell>52.5 ? 2.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation Study Results (%) for ten-task class-incremental learning on CIFAR-100. Results are reported as an average of 3 runs. ? 1.2 69.6 ? 1.6 Ablate Task Balancing Loss Weighting [33, 51, 61] 23.4 ? 1.5 62.2 ? 2.4 Replace Modified CE Loss, Eq. (11), w/ Standard CE Loss 16.5 ? 0.5 46.6 ? 0.9 Ablate Real Data Distillation: Eq. (13) w/ X 15.9 ? 2.1 58.9 ? 3.2</figDesc><table><row><cell>Metric (?)</cell><cell>AN</cell><cell>?</cell></row><row><cell cols="2">Full Method 33.7 Ablate Synthetic Data Distillation: Eq. (13) w/X 12.7 ? 7.4</cell><cell>55.3 ? 8.1</cell></row><row><cell>Ablate FT-CE Loss: Eq. (14)</cell><cell>9.8 ? 0.6</cell><cell>35.9 ? 1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Results (%) for class-incremental learning on five task ImageNet-50. A coreset of 2000 images is leveraged for replaybased methods, and thus these methods do not meet problem the DFCIL constraints. Results are reported as a single run.</figDesc><table><row><cell>Method</cell><cell cols="2">Replay Data AN (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell>89.8</cell></row><row><cell>LwF [36]</cell><cell>None</cell><cell>19.4</cell></row><row><cell>Naive Rehearsal</cell><cell>Coreset</cell><cell>78.9</cell></row><row><cell>LwF [36]</cell><cell>Coreset</cell><cell>84.8</cell></row><row><cell>Ours</cell><cell>Synthetic</cell><cell>71.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A :</head><label>A</label><figDesc>Full Results (%) for data-free class-incremental learning on CIFAR-100 for various numbers of tasks<ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20)</ref>. Results are reported as an average of 3 runs.</figDesc><table><row><cell>Tasks</cell><cell></cell><cell>5</cell><cell></cell><cell>10</cell><cell></cell><cell>20</cell><cell></cell></row><row><cell>Method</cell><cell>Replay Data</cell><cell>AN (?)</cell><cell>? (?)</cell><cell>AN (?)</cell><cell>? (?)</cell><cell>AN (?)</cell><cell>? (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table C :</head><label>C</label><figDesc>Results (%) for data-free class-incremental learning on Tiny ImageNet (20 tasks, 5 classes per task). Results are reported for a single run.</figDesc><table><row><cell>Method</cell><cell cols="3">Replay Data AN (?) ? (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell>55.5</cell><cell>100.0</cell></row><row><cell>Base</cell><cell>None</cell><cell>4.1</cell><cell>21.9</cell></row><row><cell>LwF [36]</cell><cell>None</cell><cell>4.4</cell><cell>22.4</cell></row><row><cell>LwF.MC [46]</cell><cell>None</cell><cell>8.8</cell><cell>37.2</cell></row><row><cell>LwF [36]</cell><cell>Synthetic</cell><cell>4.0</cell><cell>22.0</cell></row><row><cell>DeepInversion [62]</cell><cell>Synthetic</cell><cell>5.1</cell><cell>24.8</cell></row><row><cell>Ours</cell><cell>Synthetic</cell><cell>12.1</cell><cell>49.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table D :</head><label>D</label><figDesc>Results (%) for class-incremental learning with replay data on Tiny ImageNet (20 tasks, 5 classes per task). A coreset of 2000 images is leveraged for replay-based methods, and thus these methods do not meet problem the DFCIL constraints (note we report for our method numbers without any coreset). Results are reported for a single run.</figDesc><table><row><cell>Method</cell><cell cols="3">Replay Data AN (?) ? (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell>55.5</cell><cell>100.0</cell></row><row><cell>Naive Rehearsal</cell><cell>Coreset</cell><cell>6.6</cell><cell>37.7</cell></row><row><cell>LwF [36]</cell><cell>Coreset</cell><cell>6.9</cell><cell>39.7</cell></row><row><cell>E2E [7]</cell><cell>Coreset</cell><cell>16.9</cell><cell>56.3</cell></row><row><cell>BiC [61]</cell><cell>Coreset</cell><cell>17.4</cell><cell>59.8</cell></row><row><cell>Ours</cell><cell>Synthetic</cell><cell>12.1</cell><cell>49.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table E :</head><label>E</label><figDesc>Results (%) for class-incremental learning on five task ImageNet-50. A coreset of 2000 images is leveraged for replaybased methods, and thus these methods do not meet problem the DFCIL constraints. Results are reported as a single run.</figDesc><table><row><cell>Method</cell><cell cols="2">Replay Data AN (?)</cell></row><row><cell>Upper Bound</cell><cell>None</cell><cell>89.8</cell></row><row><cell>LwF [36]</cell><cell>None</cell><cell>19.4</cell></row><row><cell>LwF.MC [46]</cell><cell>None</cell><cell>72.7</cell></row><row><cell>Naive Rehearsal</cell><cell>Coreset</cell><cell>78.9</cell></row><row><cell>LwF [36]</cell><cell>Coreset</cell><cell>84.8</cell></row><row><cell>Ours</cell><cell>Synthetic</cell><cell>71.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table F :</head><label>F</label><figDesc>Range and chosen value of our hyperparameters, chosen with grid search</figDesc><table><row><cell>Hyperparam.</cell><cell>Range</cell><cell>Value</cell></row><row><cell>?con</cell><cell>1e-1, 1, 1e1</cell><cell>1</cell></row><row><cell>? div</cell><cell>1e-1, 1, 1e1</cell><cell>1</cell></row><row><cell>?stat</cell><cell>1, 1e1, 5e1, 1e2</cell><cell>5e1</cell></row><row><cell>?prior</cell><cell>1e-4, 1e-3, 1e-2, 1e-1, 1</cell><cell>1e-3</cell></row><row><cell>?temp</cell><cell>1, 1e1, 1e2, 1e3, 1e4</cell><cell>1e3</cell></row><row><cell>? kd</cell><cell>1e-2, 1e-1, 1</cell><cell>1e-1</cell></row><row><cell>? f t</cell><cell>1e-2, 1e-1, 1</cell><cell>1e-1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We did not implement additional generative-replay results because that is not the focus of our paper. Instead, we compare to 1) other DFCIL methods to show our method performs best in our setting, and 2) SOTA replay-based methods to show our method performs close to SOTA despite not storing replay data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) DeepInversion<ref type="bibr" target="#b61">[62]</ref> (b) Our Method</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work is supported by Samsung Research America.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesca</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Online continual learning with maximal interfered retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Belilovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Caccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Page-Caccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11849" to="11860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Gradient based sample selection for online continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahaf</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Goujaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11816" to="11825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">{EEC}: Learning to encode and regenerate images for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Ayub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The effectiveness of memory replay in large scale continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrdad</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02418,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy-preserving generative deep neural networks support clinical data sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sanjeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">Brian</forename><surname>Bhavnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey S</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Greene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Circulation: Cardiovascular Quality and Outcomes</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">5122</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Francisco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manuel</forename><forename type="middle">J</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicol?s</forename><surname>Mar?n-Jim?nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Guil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Efficient lifelong learning with a-GEM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10486</idno>
		<title level="m">Continual learning with tiny episodic memories</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohui</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanjian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gan memory with no forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulai</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaoyun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Assisting users in a world full of cameras: A privacy-aware infrastructure for computer vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Degeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Norman</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahadev</forename><surname>Satyanarayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1387" to="1396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Uncertainty-guided continual learning with bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02425</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franziska</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09553</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial continual learning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Incremental learning with self-organizing maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Gepperth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Karaoguz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Workshop on Self-Organizing Maps and Learning Vector Quantization, Clustering and Data Visualization (WSOM)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cloud-based or on-device: An empirical study of mobile deep inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Cloud Engineering (IC2E)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="184" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The knowledge within: Methods for data-free model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Haroush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memory efficient experience replay for streaming learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><forename type="middle">D</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Cahill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9769" to="9776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Lifelong machine learning with deep streaming linear discriminant analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01520</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lifelong learning via progressive distillation and retrospection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="437" to="452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a unified classifier incrementally via rebalancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saihui</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Re-evaluating continual learning scenarios: A categorization and case for strong baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anita</forename><surname>Ramasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12488</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Automatic recall machines: Internal replay, continual learning and the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12323,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep generative dual memory network for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitin</forename><surname>Kamra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umang</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10368</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fearnet: Braininspired model for incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Measuring catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Kemker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Mcclure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelina</forename><surname>Abitino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tiny imagenet visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CS 231N</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting with unlabeled data in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A neural dirichlet process mixture model for task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soochan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsoo</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00689</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generative models from the perspective of continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timoth?e</forename><surname>Lesort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Caselles-Dupr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garcia-Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Stoian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Filliat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning without forgetting. IEEE transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuesu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14486,2020.1</idno>
		<imprint/>
	</monogr>
	<note type="report_type">Lifelong navigation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lomonaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maltoni</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03550</idno>
		<title level="m">Core50: a new dataset and benchmark for continuous object recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Data-free knowledge distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Raphael Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Gradient episodic memory for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6470" to="6479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangchen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.05578</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Large-scale generative data-free distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Continuous learning in single-incremental-task scenarios</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Maltoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincenzo</forename><surname>Lomonaco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="56" to="73" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Inceptionism: Going deeper into neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Tyka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Theoretical insights into memorization in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Vaishnavh Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to remember: A synaptic plasticity driven framework for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksiy</forename><surname>Ostapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Puscas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tassilo</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Jahnichen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">H</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR&apos;17</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Catastrophic forgetting, rehearsal and pseudorehearsal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="146" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Experience replay for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Rolnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Wayne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="348" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andrei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Razvan Pascanu, and Raia Hadsell. Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Continual learning with deep generative replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanul</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehong</forename><surname>Jung Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Memory-efficient semi-supervised continual learning: The world is its own replay buffer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Balloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09536</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cameron</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Baer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantine</forename><surname>Dovrolis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02021</idno>
		<title level="m">Unsupervised progressive learning and the STAM architecture</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Fast and memory efficient de-hazing technique for real-time computer vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathap</forename><surname>Soma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Kumar Jatoth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hathiram</forename><surname>Nenavath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SN Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Functional regularisation for continual learning with gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Michalis K Titsias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Brain-inspired replay for continual learning with artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gido M Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Siegelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Gido M Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.10635</idno>
		<title level="m">Generative replay with feedback connections as a general strategy for continual learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Three scenarios for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">S</forename><surname>Gido M Van De Ven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tolias</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07734</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oswald</forename><surname>Johannes Von</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Henning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Sacramento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin F Grewe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00695</idno>
		<title level="m">Continual learning with hypernetworks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Triple memory networks: a brain-inspired method for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03143</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Large scale incremental learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuancheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dreaming to distill: Data-free knowledge transfer via deepinversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Hongxu Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Private-knn: Practical differential privacy for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">A coreset of 2000 images is leveraged for replay-based methods, and thus these methods do not meet problem the DFCIL constraints (note we report for our method numbers without any coreset)</title>
	</analytic>
	<monogr>
		<title level="m">Table B: Results (%) for class-incremental learning with replay data on CIFAR-100 for various numbers of tasks</title>
		<imprint>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
