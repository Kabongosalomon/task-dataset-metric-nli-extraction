<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ylva</forename><surname>Jansson</surname></persName>
							<email>yjansson@kth.se?tony@kth.se</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computational Brain Science Lab</orgName>
								<orgName type="department" key="dep2">Division of Computational Science and Technology</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>SE-100 44 Stock-holm</addrLine>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Lindeberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Computational Brain Science Lab</orgName>
								<orgName type="department" key="dep2">Division of Computational Science and Technology</orgName>
								<orgName type="institution">KTH Royal Institute of Technology</orgName>
								<address>
									<addrLine>SE-100 44 Stock-holm</addrLine>
									<country key="SE">Sweden</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scale-invariant scale-channel networks: Deep networks that generalise to previously unseen scales</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>arXiv preprint</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Deep learning ? Convolutional neural networks ? Invariant neural networks ? Scale covariance ? Scale invariance ? Scale generalisation ? Scale space</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to handle large scale variations is crucial for many real world visual tasks. A straightforward approach for handling scale in a deep network is to process an image at several scales simultaneously in a set of scale channels. Scale invariance can then, in principle, be achieved by using weight sharing between the scale channels together with max or average pooling over the outputs from the scale channels. The ability of such scale-channel networks to generalise to scales not present in the training set over significant scale ranges has, however, not previously been explored.</p><p>In this paper, we present a systematic study of this methodology by implementing different types of scale-channel networks and evaluating their ability to generalise to previously unseen scales. We develop a formalism for analysing the covariance and invariance properties of scale-channel networks, including exploring their relations to scale-space theory, and exploring how different design choices, unique to scaling transformations, affect the overall performance of scale-channel networks. We first show that two previously proposed scale-channel network designs, in one case, generalise no better than a standard CNN to scales not present in the training set, and in the second case, have limited scale generalisation ability. We explain theoretically and demonstrate experimentally why generalisation fails or is limited in these cases. We then propose a new type of foveated scalechannel architecture, where the scale channels process increasingly larger parts of the image with decreasing resolution. This new type of scale-channel network is shown to generalise extremely well, provided sufficient image resolution and the absence of boundary effects. Our proposed FovMax and FovAvg networks perform almost identically The support from the Swedish Research Council (contract 2018-03586) is gratefully acknowledged. over a scale range of 8, also when training on single-scale training data, and do also give improved performance when learning from datasets with large scale variations in the small sample regime.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scaling transformations are as pervasive in natural image data as translations. In any natural scene, the size of the projection of an object on the retina or a digital sensor varies continuously with the distance between the object and the observer. Compared to translations, scale variability is in some sense harder to handle for a biological or artificial agent. It is possible to fixate an object, thus centering it on the retina. The equivalence for scaling, which would be to ensure a constant distance to objects before further processing, is not a viable solution. A human observer can nonetheless recognise an object at a range of scales, from a single observation, and there is, indeed, experimental evidence demonstrating scale-invariant processing in the primate visual cortex <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Convolutional neural networks (CNNs) already encode structural assumptions about translation invariance and locality, which by the successful application of CNNs for computer vision tasks has been demonstrated to constitute useful priors for processing visual data. We propose that structural assumptions about scale could, similarly to translation covariance, be a useful prior in convolutional neural networks.</p><p>Encoding structural priors about a larger group of visual transformations, including scaling transformations and affine transformations, is an integrated part of a range of successful classical computer vision approaches <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref> and in a theory for explaining the computational function of early visual receptive fields <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. There is a growing body of work on invariant CNNs, especially concerning invariance to 2D/3D rotations and flips <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>. There has been some recent work on scale-covariant and scaleinvariant recognition in CNNs, where recent approaches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> have shown improvements compared to standard CNNs for scale variability present both in the training and the testing sets. These approaches have, however, either not been evaluated for the task of generalisation to scales not present in the training set <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> or only across a very limited scale range <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>. Thus, the possibilities for CNNs to generalise to previously unseen scales have so far not been well explored.</p><p>The structure of a standard CNN implies a preferred scale as decided by the fixed size of the filters (often 3 ? 3 or 5 ? 5 kernels) together with the depth and max pooling strategy applied. This determines the resolution at which the image is processed and the size of the receptive fields of individual units at different depths. A vanilla CNN is, therefore, not designed for multi-scale processing. Because of this, state-of-the-art object detection approaches that are exposed to larger scale variability employ different mechanisms, such as branching off classifiers at different depths <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>, learning to transform the input or the filters <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, or by combining the deep network with different types of image pyramids <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>.</p><p>The goal of these approaches has, however, not been to generalise between scales and even though they enable multi-scale processing, they lack the type of structure necessary for true scale invariance. Thus, it is not possible to predict how they will react to objects appearing at new scales in the testing set or a to real world scenario. This can lead to undesirable effects, as shown in the rich literature on adversarial examples, where it has been demonstrated that CNNs suffer from unintuitive failure modes when presented with data outside the training distribution <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60]</ref>. This includes adversarial examples constructed by means of small translations, rotations and scalings <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>, that is transformations that are partially represented in a training set of natural images. Scale-invariant CNNs could enable both multi-scale processing and predictable behaviour when encountering objects at novel scales, without the need to fully span all possible scales in the training set.</p><p>Most likely, a set of different strategies will be needed to handle the full scale variability in the natural world. Full invariance over scale factors of 100 or more, as present in natural images, might not be viable in a network with similar type of processing at fine and coarse scales 1 . We argue, however, that a deep learning based approach that is invariant over a significant scale range could be an important part of the solution to handling also such large scale variations. Note that the term scale invariance has sometimes, in the computer vision literature, been used in a weaker sense of "the ability to process objects of varying sizes" or "learn in the presence of scale variability". We will here use the term in a stricter classical sense of a classifier/feature extractor whose output does not change when the input is transformed.</p><p>One of the simplest CNN architectures used for covariant and invariant image processing is a channel network (also referred to as siamese network) <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b63">64]</ref>. In such an architecture, transformed copies of the input image are processed in parallel by different "channels" (subnetworks) corresponding to a set of image transformations. This approach can together with weight sharing and max or average pooling over the output from the channels enable invariant recognition for finite transformation groups, such as 90 degree rotations and flips. An invariant scale-channel network is a natural extension of invariant channel networks as previously explored for rotations in <ref type="bibr" target="#b25">[26]</ref>. It can equivalently be seen as a way of extending ideas underlying the classical scale-space methodology to deep learning <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, in the sense that the in the absense of further information, the image data is processed at all scales simultaneously, and that the outputs from the scale channels will constitute a non-linear scale-covariant multi-scale representation of the input image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contribution and novelty</head><p>The subject of this paper is to investigate the possibility to construct a scale-invariant CNN based on a scale-channel architecture. The key contributions of our work are to implement different possible types of scale-channel networks and to evaluate the ability of these networks to generalise to previously unseen scales, so that we can train a network at some scale(s) and test it at other scales, without complementary use of data augmentation. It should be noted that previous scale-channel networks exist, but those are explicitly designed for multi-scale processing <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> rather than scale invariance or have not been evaluated with regard to their ability to generalise to unseen scales over any significant scale range <ref type="bibr" target="#b36">[37]</ref>. We here implement and evaluate networks based on principles similar to these previous approaches, but also a new type of foveated scale-channel network, where the individual scale channels process increasingly larger parts of the image with decreasing resolution.</p><p>This implies that fully invariant processing over such wide scale ranges might not be an applicable strategy. Instead different strategies will likely be needed to recognise objects at very low resolution from those needed to recognise objects at very high resolution.</p><p>To enable testing each approach over a large range of scales, we create a new variation of the MNIST dataset, referred to as the MNIST Large Scale dataset, with scale variations up to a factor of 8. This represents a dataset with sufficient resolution and image size to enable invariant recognition over a wide range of scale factors. We also rescale the CIFAR-10 dataset over a scale factor of 4, which is a wider scale range than has previously been evaluated for this dataset. This rescaled CIFAR-10 dataset is used to test if scale-invariant networks can still give significant improvements in generalisation to new scales, in the presence of limited image resolution and for small image sizes. We evaluate the ability to generalise to previously unseen scales for the different types of channel networks, by first training on a single scale or a limited range of scales and then testing recognition for scales not present in the training set. The results are compared to a vanilla CNN baseline.</p><p>Our experiments on the MNIST Large Scale dataset show that two previously used scale-channel network designs or methodologies, in one case, do not generalise any better than a standard CNN to scales not present in the training set or, in the second case, have limited generalisation ability. The first type of method is based on concatenating the outputs from the scale channels and using this as input to a fully connected layer (as opposed to applying max or average pooling over the scale-dimension). We show that such a network does not learn to combine the output from the scale channels in a correct way so as to enable generalisation to previously unseen scales. The reason for this is the absence of a structure to enforce scale invariance. The second type of method is to handle the difference in image size between the rescaled images in the scale channels, by applying the subnetwork corresponding to each channel in a sliding window manner. This methodology, however, implies that the rescaled copies of an image are not processed in the same way, since for an object processed in scale channels corresponding to an upscaled image, a wide range of different, (e.g., non-centered) object views, will be processed, compared to only processing the central view for an object in a downscaled image. This implies that full invariance cannot be achieved, since max (or average) pooling will be performed over different views of the objects for different scales, which implies that the max (or average) over the scale dimension is not guaranteed to be stable when the input is transformed.</p><p>We do, instead, propose a new type of foveated scalechannel architecture, where the scale channels process increasingly larger parts of the image with decreasing resolution. Together with max or average pooling, this leads to our FovMax and FovAvg networks. We show that this approach enables extremely good generalisation, when the image resolution is sufficient and there is an absence of boundary effects. Notably, for rescalings of MNIST, almost identical performance over a scale range of 8 is achieved, when train-ing on single size training data. We further show that, also on the CIFAR-10 dataset, in the presence of severe limitations regarding image resolution and image size, the foveated scalechannel networks still provide considerably better generalisation ability compared to both a standard CNN and an alternative scale-channel approach. We also demonstrate that the FovMax and FovAvg networks give improved performance for datasets with large scale variations in both the training and testing data, in the small sample regime.</p><p>We propose that the presented foveated scale-channel networks will prove useful in situations where a simple approach that can generalise to unseen scales or learning from small datasets with large scale variations is needed. Our study also highlights possibilities and limitations for scale-invariant CNNs and provides a simple baseline to evaluate other approaches against. Finally, we see possibilities to integrate the foveated scale-channel network, or similar types of foveated scale-invariant processing, as subparts in more complex frameworks dealing with large scale variations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Relations to previous contribution</head><p>This paper constitutes a substantially extended version of a conference paper presented at the ICPR 2020 conference <ref type="bibr" target="#b77">[78]</ref> and with substantial additions concerning:</p><p>the motivations underlying this work and the importance of a scale generalisation ability for deep networks (Section 1), a wider overview of related work (Section 1 and Section 2), theoretical relationships between the presented scale-channel networks and the notion of scale-space representation, including theoretical relationships between the presented scale-channel networks and scale-normalised derivatives with associated methods for scale selection (Section 4), more extensive experimental results on the MNIST Large Scale dataset, specifically new experiments that investigate (i) the dependency on the scale range spanned by the scale channels, (ii) the dependency on the sampling density of the scale levels in the scale channels, (iii) the influence of multi-scale learning over different scale intervals, and (iv) an analysis of the scale selection properties over the multiple scale channels for the different types of scale-channel networks (Section 6), experimental results for the CIFAR-10 dataset subject to scaling transformations of the testing data (Section 7), details about the dataset creation for the MNIST Large Scale dataset (Appendix A).</p><p>In relation to the ICPR 2020 paper, this paper therefore (i) gives a more general motivation for scale-channel networks in relation to the topic of scale generalisation, (ii) presents more experimental results for further use cases and an additional dataset, (iii) gives deeper theoretical relationships between scale-channel networks and scale-space theory and (iv) gives overall better descriptions of several of the subjects treated in the paper, including (v) more extensive references to related literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Relations to previous work</head><p>In the area of scale-space theory <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, a multi-scale representation of an input image is created by convolving the image with a set of rescaled Gaussian kernels and Gaussian derivative filters, which are then often combined in non-linear ways. In this way, a powerful methodology has been developed to handle scaling transformations in classical computer vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. The scale-channel networks described in this paper can be seen as an extension of this philosophy of processing an image at all scales simultaneously, as a means of achieving scale invariance, but instead using deep nonlinear feature extractors learned from data, as opposed to hand-crafted image features or image descriptors. CNNs can give impressive performance, but they are sensitive to scale variations. Provided that the architecture of the deep network is sufficiently flexible, moderate increase in the robustness to scaling transformations can be obtained by augmenting the training images with multiple rescaled copies of each training image (scale jittering) <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b79">80]</ref>. The performance does, however, degrade for scales not present in the training set <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b81">82]</ref>, and different network structure may be optimal for small vs. large images <ref type="bibr" target="#b81">[82]</ref>. It is furthermore possible to construct adversarial examples by means of small translations, rotations and scalings <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b61">62]</ref>.</p><p>State-of-the-art CNN based object detection approaches all employ different mechanisms to deal with scale variability, e.g., branching off classifiers at different depths <ref type="bibr" target="#b43">[44]</ref>, learning to transform the input or the filters <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47]</ref>, using different types of image pyramids <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b52">53]</ref>, or other approaches, where the image is rescaled to different resolutions, possibly combined with interactions or pooling between the layers <ref type="bibr" target="#b82">[83,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b81">82]</ref>. There are also deep networks that somehow handle the notion of scale by approaches such as dilated convolutions <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b87">88]</ref>, scale-dependent pooling <ref type="bibr" target="#b88">[89]</ref>, scale-adaptive convolutions <ref type="bibr" target="#b89">[90]</ref>, by spatially warping the image data by a log-polar transformation prior to image filtering <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b41">42]</ref>, or adding additional branches of down-samplings and/or up-samplings in each layer of the network <ref type="bibr" target="#b90">[91,</ref><ref type="bibr" target="#b91">92]</ref>. The goal of these approaches has, however, not been to generalise to previously unseen scales and they lack the structure necessary for true scale invariance.</p><p>Examples of handcrafted scale-invariant hierarchical descriptors are <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b93">94]</ref>. We are, here, interested in combining scale invariance with learning. There exist some previous work aimed explicitly at scale-invariant recognition in CNNs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> These approaches have, however, either not been evaluated for the task of generalisation to scales not present in the training set <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b40">41]</ref> or only across a very limited scale range <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b39">40]</ref>. Previous scalechannel networks exist, but are explicitly designed for multiscale processing <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b76">77]</ref> rather than scale invariance, or have not been evaluated with regard to their ability to generalise to unseen scales over any significant scale range <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b36">37]</ref>. A dual approach to scale-covariant scale-channel networks that, however, allows for scale invariance and scale generalisation, is presented in <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>, based on transforming continuous CNNs expressed in terms of continuous functions for the filter weights with respect to scaling transformations. Other scale-covariant or scale-equivariant approaches to deep networks have also been recently proposed in <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b97">98,</ref><ref type="bibr" target="#b98">99,</ref><ref type="bibr" target="#b99">100]</ref>.</p><p>There is a large literature on approaches to achieve rotationcovariant and rotation-invariant networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> with applications to different domains, including astronomy <ref type="bibr" target="#b63">[64]</ref>, remote sensing <ref type="bibr" target="#b100">[101]</ref>, medical image analysis <ref type="bibr" target="#b101">[102,</ref><ref type="bibr" target="#b102">103,</ref><ref type="bibr" target="#b103">104]</ref> and texture classification <ref type="bibr" target="#b104">[105]</ref>. There are also approaches to invariant networks based on formalism from group theory <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b106">107]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theory of continuous scale-channel networks</head><p>In this section, we will introduce a mathematical framework for modelling and analysing scale-channel networks based on a continuous model of the image space. This model enables straightforward analysis of the covariance and invariance properties of the channel networks, that are later approximated in a discrete implementation. We, here, generalise previous analysis of invariance properties of channel networks <ref type="bibr" target="#b25">[26]</ref> to scale-channel networks. We further analyse covariance properties and additional options for aggregating information across transformation channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Images and image transformations</head><p>We consider images f : R N ? R that are measurable functions in L ? (R N ) and denote this space of images as V . A group of image transformations corresponding to a group G is a family of image transformations T g (g ? G) with a group structure, i.e., fulfilling the group axioms of closure, identity, associativity and inverse. We denote the combination of two group elements g, h ? G by gh and the cardinality of G as |G|. Formally, a group G induces an action on functions by acting on the underlying space on which the function is defined (here the image domain). We are here interested in the group of uniform scalings around x 0 with the group action</p><formula xml:id="formula_0">(S s,x0 f )(x ) = f (x), x = S s (x ? x 0 ) + x 0 ,<label>(1)</label></formula><p>where S s = diag(s). For simplicity, we often assume x 0 = 0 and denote S s,0 as S s corresponding to</p><formula xml:id="formula_1">(S s f )(x) = f (S ?1 s x) = f s (x).<label>(2)</label></formula><p>We will also consider the translation group with the action (where ? ? R N )</p><formula xml:id="formula_2">(D ? f )(x ) = f (x), x = x + ?.<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Invariance and covariance</head><p>Consider a general feature extractor ? : V ? K that maps an image f ? V to a feature representation y ? K. In our continuous model, K will typically correspond to a set of M feature maps (functions) so that ?f ? V M . This is a continuous analogue of a discrete convolutional feature map with M features. A feature extractor 2 ? is covariant 3 to a transformation group G (formally to the group action) if there exists an input independent transformationT g that can align the feature maps of a transformed image with those of the original image</p><formula xml:id="formula_3">?(T g f ) =T g (?f ) ?g ? G, f ? V.<label>(4)</label></formula><p>Thus, for a covariant feature extractor it is possible to predict the feature maps of a transformed image from the feature maps of the original image or, in other words, the order between feature extraction and transformation does not matter, as illustrated in the commutative diagram in <ref type="figure">Figure 1</ref>. <ref type="figure">Fig. 1</ref>: Commutative diagram for a covariant feature extractor ?, showing how the feature map of the transformed image can be matched to the feature map of the original image by a transformation of the feature space. Note thatT g will correspond to the same transformation as T g , but might take a different form in the feature space. <ref type="bibr" target="#b1">2</ref> With regard to the scale-channel networks that we develop later in this paper, note that ? should be seen as representing the entire family of scale channels, not a single-scale channel in isolation. An invariant feature extractor ? will then correspond to the result of max pooling or average pooling over all the scale channels. <ref type="bibr" target="#b2">3</ref> In the deep learning literature, the notion of "equivariance" is also often used for this relationship, which is referred to as "covariance" in scale-space theory. In this paper, we use the terminology "covariance" to maintain consistency with the earlier scale-space literature <ref type="bibr" target="#b107">[108]</ref>.</p><formula xml:id="formula_4">? fT g ? ???? ? ?(Tgf ) =Tg(?f ) ? ?? ? ?? f Tg ? ???? ? Tgf</formula><p>A feature extractor ? is invariant to a transformation group G if the feature representation of a transformed image is equal to the feature representation of the original image</p><formula xml:id="formula_5">?(T g f ) = ?(f ) ?g ? G, f ? V.<label>(5)</label></formula><p>Invariance is thus a special case of covariance, whereT g is the identity transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Continuous model of a CNN</head><p>Let ? : V ? V M k denote a continuous CNN with k layers and M i feature channels in layer i. Let ? (i) represent the transformation between layers i ? 1 and i such that</p><formula xml:id="formula_6">(? (i) f )(x, c) = (? (i) ? (i?1) ? ? ? ? (2) ? (1) f )(x, c),<label>(6)</label></formula><p>where c ? {1, 2, . . . M k } denotes the feature channel and ? = ? (k) . We model the transformation ? (i) between two adjacent layers ? (i?1) f and ? (i) f as a convolution followed by the addition of a bias term b i,c ? R and the application of a pointwise non-linearity ? i : R ? R:</p><formula xml:id="formula_7">(? (i) f )(x, c) = ? i ? ? Mi?1 m=1 ??R N (? (i?1) f )(x ? ?, m) g (i) m,c (?) d? + b i,c ? ?<label>(7)</label></formula><p>where g</p><formula xml:id="formula_8">(i)</formula><p>m,c ? L 1 (R N ) denotes the convolution kernel that propagates information from feature channel m in layer i?1 to output feature channel c in layer i. A final fully connected classification layer with compact support can also be modelled as a convolution combined with a non-linearity ? k that represents a softmax operation over the feature channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scale-channel networks</head><p>The key idea underlying channel networks is to process transformed copies of an input image in parallel, in a set of network "channels" (subnetworks) with shared weights. For finite transformation groups, such as discrete rotations, using one channel corresponding to each group element and applying max pooling over the channel dimension can give an invariant output. For continuous but compact groups, invariance can instead be achieved for a discrete subgroup.</p><p>The scaling group is, however, neither finite nor compact. The key question that we address here is whether a scale-channel network can still support invariant recognition.</p><p>We define a multi-column scale-channel network ? : V ? V M k for the group of scaling transformations S by Each scale channel has a fixed size receptive field/support region in relation to its rescaled image copy, but they will together process input regions corresponding to varying sizes in the original image (circles of corresponding colors). (b) This corresponds to a type of foveated processing, where the center of the image is processed with high resolution, which works well to detect small objects, while larger regions are processed using gradually reduced resolution, which enables detection of larger objects. (c) There is a close similarity between this model and the foveal scale space model <ref type="bibr" target="#b108">[109]</ref>, which was motivated by a combination of regular scale space axioms with a complementary assumption of a uniform limited processing capacity at all scales.</p><p>using a single base network ? :</p><formula xml:id="formula_9">V ? V M k to define a set of scale channels {? s } s?S (? s f )(x, c) = (? S s f )(x, c) = (?f s )(x, c),<label>(8)</label></formula><p>where each channel thus applies exactly the same operation to a scaled copy of the input image (see <ref type="figure" target="#fig_0">Figure 2</ref>(a)). We denote the mapping from the input image to the scale-channel feature maps at depth i as</p><formula xml:id="formula_10">? (i) : V ? V Mi|S| (? (i) f )(x, c, s) = (? (i) s f )(x, c) = (? (i) S s f )(x, c).<label>(9)</label></formula><p>A scale-channel network that is invariant to the continuous group of uniform scaling transformations S = {s ? R + } can be constructed using an infinite set of scale channels {? s } s?S . The following analysis also holds for a set of scale channels corresponding to a discrete subgroup of the group of uniform scaling transformations, such that S = {? i |i ? Z} for some ? &gt; 1.</p><p>The final output ?f from the scale-channel network is an aggregation across the scale dimension of the last layer scale-channel feature maps. In our theoretical treatment, we combine the output of the scale channels by the supremum</p><formula xml:id="formula_11">(? sup f )(x, c) = sup s?S [(? s f )(x, c)] .<label>(10)</label></formula><p>Other permutation invariant operators, such as averaging operations, could also be used. For this construction, the network output will be invariant to rescalings around x 0 = 0 (global scale invariance). This architecture is appropriate when characterising a single centered object that might vary in scale and it is the main architecture that we explore in this paper. Alternatively, one may instead pool over corresponding image points in the original image by operations of the form</p><formula xml:id="formula_12">(? local sup f )(x, c) = sup s?S {(? s f )(S s x, c)}.<label>(11)</label></formula><p>This descriptor instead has the invariance property</p><formula xml:id="formula_13">(? local sup f )(x 0 , c) = (? local sup S s,x0 f )(x 0 , c)</formula><p>for all x 0 , (12) i.e., when scaling around an arbitrary image point, the output at that specific point does not change (local scale invariance). This property makes it more suitable to describe scenes with multiple objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Scale covariance</head><p>Consider a scale-channel network ? (10) that expands the input over the group of uniform scaling transformations S. We can relate the feature map representation ? (i) for a scaled image copy S t f for t ? S and its original f in terms of operator notation as</p><formula xml:id="formula_14">(? (i) S t f )(x, c, s) = (? (i) s S t f )(x, c) = (? (i) S s S t f )(x, c) = (? (i) S st f )(x, c) = (? (i) st f )(x, c) = (? (i) f )(x, c, st),<label>(13)</label></formula><p>where we have used the definitions (8) and <ref type="bibr" target="#b8">(9)</ref> together with the fact that S is a group. A scaling of an image thus only results in a multiplicative shift in the scale dimension of the feature maps. A more general and more rigorous proof using an integral representation of the scale-channel network is given in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Scale invariance</head><p>Consider a scale-channel network ? sup (10) that selects the supremum over scales. We will show that ? sup is scale invariant, i.e., that</p><formula xml:id="formula_15">(? sup S t f )(x, c) = (? sup f )(x, c).<label>(14)</label></formula><p>First, <ref type="bibr" target="#b12">(13)</ref> gives {?</p><formula xml:id="formula_16">(i) s (S t f )} s?S = {? (i) st (f )} s?S .</formula><p>Then, we note that {st} s?S = St = S. This holds both in the case when S = R + and in the case when S = {? i |i ? Z}. Thus, we have</p><formula xml:id="formula_17">{(? (i) s S t f )(x, c)} s?S = {(? (i) st f )(x, c)} s?S = {(? (i) s f )(x, c)} s?S ,<label>(15)</label></formula><p>i.e., the set of outputs from the scale channels for a transformed image is equal to the set of outputs from the scale channels for its original image. For any permutation invariant aggregation operator, such as the supremum, we have that</p><formula xml:id="formula_18">(? sup S t f )(x, c) = sup s?S {(? (k) st f )(x, c)} = sup s?S {(? (k) s f )(x, c)} = (? sup f )(x, c),<label>(16)</label></formula><p>and, thus, ? is invariant to uniform rescalings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Proof of scale and translation covariance using an integral representation of a scale-channel network</head><p>We, here, prove the transformation property</p><formula xml:id="formula_19">(? (i) h)(x, s, c) = (? (i) f )(x + S s S t x 1 ? S t x 2 , st, c) (17)</formula><p>of the scale-channel feature maps under a more general combined scaling transformation and translation of the form</p><formula xml:id="formula_20">h(x ) = f (x) for x = S t (x ? x 1 ) + x 2<label>(18)</label></formula><p>corresponding to</p><formula xml:id="formula_21">h(x) = f (S ?1 t (x ? x 2 ) + x 1 )<label>(19)</label></formula><p>using an integral representation of the deep network. In the special case when x 1 = x 2 = x 0 , this corresponds to a uniform scaling transformation around x 0 (i.e., S x0,s ). With x 1 = x 0 and x 2 = x 0 + ?, this corresponds to a scaling transformation around x 0 followed by a translation D ? . Consider a deep network ? (i) (6) and assume the integral representation <ref type="bibr" target="#b6">(7)</ref>, where we for simplicity of notation incorporate the offsets b i,c into the non-linearities ? i,c . By expanding the integral representation of the rescaled image h (19), we have that that the feature representation in the scale-channel network is given by (with M 0 = 1 for a scalar input image):</p><formula xml:id="formula_22">(? (i) h)(x, s, c) = {definition (9)} = (? (i) s h)(x, c) = {definition (8)} = (? (i) h s )(x, c) = {equation (6)} = (? (i) ? (i?1) . . . ? (2) ? (1) h s )(x, c) = {equation (7)} = ? i,c ? ? Mi?1 mi=1 ?i?R N ? i?1,mi ? ? Mi?2 mi?1=1 ?i?1?R N . . . ? 1,m2 M0 m1=1 ?1?R N h s (x ? ? i ? ? i?1 ? ? ? ? ? ? 1 ) ? g (1) m1,m2 (? 1 ) d? 1 ? ? . . . g (i?1) mi?1,mi (? i?1 ) d? i?1 ? ? g (i) mi,c (? i ) d? i ? ? .<label>(20)</label></formula><p>Under the scaling transformation <ref type="bibr" target="#b17">(18)</ref>, the part of the inte-</p><formula xml:id="formula_23">grand h s (x ? ? i ? ? i?1 ? ? ? ? ? ? 1 ) transforms as follows: h s (x ? ? i ? ? i?1 ? ? ? ? ? ? 1 ) = {h s (x) = h(S ?1 s x) according to definition (2)} = h(S ?1 s (x ? ? i ? ? i?1 ? ? ? ? ? ? 1 )) = {h(x) = f (S ?1 t (x ? x 2 ) + x 1 ) according to (19) } = f (S ?1 t S ?1 s ((x ? ? i ? ? i?1 ? ? ? ? ? ? 1 ) ? S s x 2 + S s S t x 1 ) = {S s S t = S st for scaling transformations} = f (S ?1 st ((x + S s S t x 1 ? S s x 2 ? ? i ? ? i?1 ? ? ? ? ? ? 1 )) = {f st (x) = f (S ?1 st x) according to definition (2)} = f st (x + S s S t x 1 ? S s x 2 ? ? i ? ? i?1 ? ? ? ? ? ? 1 ). (21)</formula><p>Inserting this transformed integrand into the integral representation <ref type="bibr" target="#b19">(20)</ref> gives</p><formula xml:id="formula_24">(? (i) h)(x, s, c) = = ? i,c ? ? Mi?1 mi=1 ?i?R N ? i?1,mi ? ? Mi?2 mi?1=1 ?i?1?R N . . . ? 1,m2 M0 m1=1 ?1?R N f st (x + S s S t x 1 ? S s x 2 ? ? i ? ? i?1 ? ? ? ? ? ? 1 )? g (1) m1,m2 (? 1 ) d? 1 ? ? . . . g (i?1) mi?1,mi (? i?1 ) d? i?1 ? ? g (i) mi,c (? i ) d? i ? ? ,<label>(22)</label></formula><p>which we recognise as</p><formula xml:id="formula_25">(? (i) h)(x, s, c) = (? (i) ? (i?1) . . . ? (2) ? (1) f st )(x + S s S t x 1 ? S s x 2 , c) = (? (i) f st )(x + S s S t x 1 ? S s x 2 , c) = (? (i) st f )(x + S s S t x 1 ? S s x 2 , c) = (? (i) f )(x + S s S t x 1 ? S s x 2 , st, c)<label>(23)</label></formula><p>and which proves the result. Note that for a pure translation (S t = I, x 1 = x 0 and x 2 = x 0 + ?) this gives</p><formula xml:id="formula_26">(? (i) D ? f )(x, c, s) = (? (i) f )(x ? S s ?, s, c).<label>(24)</label></formula><p>Thus, translation covariance is preserved in the scale-channel network but the magnitude of the spatial shift in the feature maps will depend on the scale channel. The discrete implementation and some additional design choices for discrete scale-channel networks are discussed in Section 5, but, first, we will consider the relationship between continuous scalechannel networks and scale-space theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relations between scale-channel networks and scale-space theory</head><p>This section describes relations between the presented scalechannel networks and concepts in scale-space theory, specifically (i) a mapping between scaling the input image using multiple scaling factors, as used in scale-channel networks, or instead scaling the filters multiple times, as done in scalespace theory, and (ii) a relationship to the normalisation over scales of scale-normalised derivatives, which holds if the learning algorithm for a scale-channel network would learn filters corresponding to Gaussian derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Preliminaries 1: The Gaussian scale space</head><p>In classical scale-space theory <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75]</ref>, a multi-scale representation of an input image is created by convolving the image with a set of rescaled and normalised Gaussian kernels. The resulting scale-space representation of an input image f : R N ? R is defined as <ref type="bibr" target="#b68">[69]</ref>:</p><formula xml:id="formula_27">L(x; ?) = u?R N f (x ? u) g(u; ?) du,<label>(25)</label></formula><p>where g : R N ? R + ? R denotes the (rotationally symmetric) Gaussian kernel</p><formula xml:id="formula_28">g(x; ?) = 1 ( ? 2??) N e ?x 2 2? 2 ,<label>(26)</label></formula><p>and we use ? as the scale parameter compared to the more commonly used t = ? 2 . The original image/function is thus embedded into a family of functions parameterised by scale. The scale-space representation is scale covariant and the representation of an original image can be matched to that of a rescaled image by a spatial rescaling and a multiplicative shift along the scale dimension. From this representation, a family of Gaussian derivatives can be computed as</p><formula xml:id="formula_29">L x ? (x; ?) = ? x ? L(x; ?) = ((? x ? g(?; ?)) * f (?))(x),<label>(27)</label></formula><p>where n ? Z and we use multi index notation</p><formula xml:id="formula_30">? = (? 1 , ? ? ? ? N ) such that ? x ? = ? x ? 1 ? ? ? ? x ? N .</formula><p>The scale covariance property also transfers to such Gaussian derivatives, and these visual primitives have been widely used within the classical computer vision paradigm to construct scale-covariant and scale-invariant feature detectors and image descriptors <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b107">108,</ref><ref type="bibr" target="#b17">18</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scaling the image vs. scaling the filter</head><p>The scale-channel networks described in this paper are based on a similar philosophy of processing an image at all scales simultaneously, although the input image, as opposed to the filter, is expanded over scales. We, here, consider the relationship between multi-scale representations computed by applying a set of rescaled kernels to a single-scale image and representations computed by applying the same kernel to a set of rescaled images. Since the scale-space representation can be computed using a single convolutional layer, we compare with a single-layer scale-channel network. We consider the relationship between representations computed by:</p><p>(i) Applying a set of rescaled and scale-normalised filters (this corresponds to normalising filters to constant L 1norm over scales) h :</p><formula xml:id="formula_31">R N ? R h s (x) = 1 s N h( x s )<label>(28)</label></formula><p>to a fixed size input image f (x):</p><formula xml:id="formula_32">L h (x; s) = (f * h s )(x) = u?R N f (u) h s (x ? u) du,<label>(29)</label></formula><p>where the subscript indicates that h might not necessarily be a Gaussian kernel. If h is a Gaussian then L h = L. (ii) Applying a fixed size filter h to a set of rescaled input images</p><formula xml:id="formula_33">M h (x; s) = (f s * h)(x) = u?R N f s (u) h(x ? u) du,<label>(30)</label></formula><p>with</p><formula xml:id="formula_34">f s (x) = f ( x s ).<label>(31)</label></formula><p>This is the representation computed by a single layer in a (continuous) scale-channel network.</p><p>It is straightforward to show that these representations are computationally equivalent and related by a family of scale dependent scaling transformations. We compute using the change of variables u = s v, du = s N dv:</p><formula xml:id="formula_35">L h (x; s) = (f * h s )(x) = u?R N f (x ? u) 1 s N h( u s ) du = u?R N f (x ? sv) 1 s N h(v) s N dv = u?R N f (s( x s ? v)) h(v) dv = u?R N f s ?1 ( x s ? v) h(v) dv = (f s ?1 * h)( x s , s ?1 ).<label>(32)</label></formula><p>Comparing this with <ref type="bibr" target="#b29">(30)</ref>, we see that the two representations are related according to</p><formula xml:id="formula_36">L h (x; s) = M h ( x s ; s ?1 ).<label>(33)</label></formula><p>We note that the relation <ref type="bibr" target="#b32">(33)</ref> preserves the relative scale between the filter and the image for each scale and that both representations are scale covariant. Thus, to convolve a set of rescaled images with a single-scale filter, is computationally equivalent to convolving an image with a set of rescaled filters that are L 1 -normalised over scale. The two representations are related through a spatial rescaling and an inverse mapping of the scale parameter s ? s ?1 . Note that it is straightforward to show, using the integral representation of a scale-channel network <ref type="bibr" target="#b6">(7)</ref>, that a corresponding relation between scaling the image and scaling the filters holds for a multi-layer scale-channel network as well. The result <ref type="bibr" target="#b32">(33)</ref> implies that if a scale-channel network learns a feature corresponding to a Gaussian with standard deviation ?, then the representation computed by the scalechannel network is computationally equivalent to applying the family of kernels</p><formula xml:id="formula_37">h s (x) = 1 s N h( x s ) = 1 ( ? 2?s?) N e ?x 2 2(s?) 2<label>(34)</label></formula><p>to the original image, given the complementary scaling transformation (33) with its associated inverse mapping of the scale parameters s ? s ?1 . Since this is a family of rescaled and L 1 -normalised Gaussians, the scale-channel network will compute a representation computationally equivalent to a Gaussian scale-space representation. For discrete image data, a similar relation holds approximately, provided that the discrete rescaling operation is a sufficiently good approximation of the continuous rescaling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Relation between scale-channel networks and scale-normalised derivatives</head><p>One way to achieve scale invariance within the Gaussian scale-space concept is to first perform scale selection, i.e., identify the relevant scale/scales, and then, e.g., extract features at the identified scale/scales. Scale selection can be done by comparing the magnitudes of ?-normalised derivatives <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>:</p><formula xml:id="formula_38">? ? ? = ? x ? ,??norm = t |?|?/2 ? x ? = ? |?|? ? x ?<label>(35)</label></formula><p>over scales with ? ? [0, 1] as a free parameter and |?| = ? 1 + ? ? ? + ? N . Such derivatives are likely to take maxima at scales corresponding to the relevant physical scales of objects in the image. Although a multi-layer scale-channel network will compute more complex non-linear features, it is enlightening to investigate whether the network can learn to express operations similar to scale-normalised derivatives. This would increase our confidence that scale-channel networks could be expected to work well together with, e.g., max pooling over scales. We will, here, consider the maximally scale-invariant case for scale-normalised derivatives with ? = 1</p><formula xml:id="formula_39">? ? ? = ? |?| ? x ? .<label>(36)</label></formula><p>and show that scale-channel networks can indeed learn features equivalent to such scale-normalised derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Preliminaries II: Gaussian derivatives in terms of Hermite polynomials</head><p>As a preparation for the intended result, we will first establish a relationship between Gaussian derivatives and probabilistic Hermite polynomials. The probabilistic Hermite polynomials He n (x) are in 1-D defined by the relationship</p><formula xml:id="formula_40">He n (x) = (?1) n e x 2 /2 ? x n e ?x 2 /2<label>(37)</label></formula><p>implying that</p><formula xml:id="formula_41">? x n e ?x 2 /2 = (?1) n He n (x) e ?x 2 /2<label>(38)</label></formula><p>and</p><formula xml:id="formula_42">? x n e ?x 2 /2? 2 = (?1) n He n ( x ? ) e ?x 2 /2? 2 1 ? n .<label>(39)</label></formula><p>Applied to a Gaussian function in 1-D, this implies that</p><formula xml:id="formula_43">? x n (g(x; ?)) = = 1 ? 2?? ? x n e ?x 2 /2? 2 = 1 ? 2?? (?1) n ? n He n ( x ? ) e ?x 2 /2? 2 = (?1) n ? n He n ( x ? ) g(x; ?).<label>(40)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Scaling relationship for Gaussian derivative kernels</head><p>We, here, describe the relationship between scale-channel networks and scale-normalised derivatives. Let us assume that the scale-channel network at some layer learns a kernel that corresponds to a Gaussian partial derivative at some scale ?:</p><formula xml:id="formula_44">? x ? g(x; ?) = = ? x ? 1 1 x ? 2 2 ...x ? N N g(x; ?) = g x ? 1 1 x ? 2 2 ...x ? N N (x; ?).<label>(41)</label></formula><p>We will show that when this kernel is applied to all the scale channels, this correspond to a normalisation over scales that is equivalent to scale-normalisation of Gaussian derivatives. For later convenience, we write this learned kernel as a scale-normalised derivative at scale ? for ? = 1 multiplied by some constant C:</p><formula xml:id="formula_45">h(x) = C ? ?1+?2+???+? N g x ? 1 1 x ? 2 2 ...x ? N N (x; ?).<label>(42)</label></formula><p>Then, the corresponding family of equivalent kernels h s (x) in the dual representation <ref type="bibr" target="#b28">(29)</ref>, which represents the same effect on the original image as applying the kernel h(x) to a set of rescaled images f s (x) = f (x/s), provided that a complementary scaling transformation and the inverse mapping of the scale parameter s ? s ?1 are performed, is given by</p><formula xml:id="formula_46">h s (x) = 1 s N h( x s ) = C s N ? ?1+?2+???+? N g x ? 1 1 x ? 2 2 ...x ? N N ( x s ; ?).<label>(43)</label></formula><p>Using Equation <ref type="formula" target="#formula_3">(40)</ref> with <ref type="bibr" target="#b43">44)</ref> in N dimensions, we obtain</p><formula xml:id="formula_47">g(x; ?) = 1 ( ? 2??) N e ?(x 2 1 +x 2 2 +???+x 2 N )/2? 2<label>(</label></formula><formula xml:id="formula_48">h s (x) = C s N ? ?1+?2+???+? N (?1) ?1+?2+???+? N He ?1 ( x 1 s? ) He ?2 ( x 2 s? ) . . . He ? N ( x N s? ) 1 ( ? 2??) N e ?(x 2 1 +x 2 2 +???+x 2 N )/2s 2 ? 2 1 ? ?1+?2+???+? N = C (s?) ?1+?2+???+? N (?1) ?1+?2+???+? N He ?1 ( x 1 s? ) He ?2 ( x 2 s? ) . . . He ? N ( x N s? ) 1 ( ? 2?s?) N e ?(x 2 1 +x 2 2 +???+x 2 N )/2s 2 ? 2 1 (s?) ?1+?2+???+? N .<label>(45)</label></formula><p>Comparing with <ref type="formula" target="#formula_3">(40)</ref>, we recognise this expression as the scale-normalised derivative</p><formula xml:id="formula_49">h s (x) = C (s?) ?1+?2+???+? N g x ? 1 1 x ? 2 2 ...x ? N N (x; s?) (46) of order ? = (? 1 , ? 2 , . . . ? N ) at scale s?.</formula><p>This means that if the scale-channel network learns a partial Gaussian derivative of some order, then the application of that filter to all the scale channels is computationally equivalent to applying corresponding scale-normalised Gaussian derivatives to the original image at all scales.</p><p>While this result has been expressed for partial derivatives, a corresponding results holds also for derivative operators that correspond to directional derivatives of Gaussian kernels in arbitrary directions. This result can be easily understood from the expression for a directional derivative operator ? e n of order n = n 1 + n 2 + ? ? ? + n N in direction e = (e 1 , e 2 , . . . , e N ) with |e| = e 2 1 + e 2 2 + ? ? ? + e 2 N = 1:</p><formula xml:id="formula_50">? e n g(x; ?) = (e 1 ? x1 + e 2 ? x2 + ? ? ? + e N ? x N ) n g(x; ?) = ?1+?2+???+? N =n n ? 1 ! ? 2 ! . . . ? N ! e ?1 1 e ?2 2 . . . e ? N N ? ?1 x1 ? ?2 x2 . . . ? ? N x N g(x; ?) = ?1+?2+???+? N =n n ? 1 ! ? 2 ! . . . ? N ! e ?1 1 e ?2 2 . . . e ? N N g x ? 1 1 x ? 2 2 ...x ? N N (x; ?).<label>(47)</label></formula><p>Since the scale normalisation factors ? |?| for all scale-normalised partial derivatives of the same order |?| = ? 1 + ? 2 + ? ? ?+? N = n are the same, it follows that all linear combinations of partial derivatives of the same order are transformed by the same multiplicative scale normalisation factor, which proves the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Relations to classical scale selection methods</head><p>Specifically, the scaling result for Gaussian derivative kernels implies that a scale-channel network that combines the multiple scale channels by supremum, or for a discrete set of scale channels, max pooling (see further Section 5), will be structurally similar to classical methods for scale selection, which detect maxima over scale of scale-normalised filter responses <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b109">110]</ref>. In the scale-channel networks, max pooling is, however, done over more complex feature responses, already adapted to detect specific objects, while classical scale selection is performed in a class-agnostic way based on low-level features. This makes max pooling in the scale-channel networks also closely related to more specialised classical methods that detect maxima from the scales at which a supervised classifier delivers class labels with the highest posterior <ref type="bibr" target="#b110">[111,</ref><ref type="bibr" target="#b111">112]</ref>. Average pooling over the outputs of a discrete set of scale channels (Section 5) is structurally similar to methods for scale selection that are based on weighted averages of filter responses at different scales <ref type="bibr" target="#b112">[113,</ref><ref type="bibr" target="#b17">18]</ref>. Although there is no guarantee that the learned non-linear features will, indeed, take maxima for relevant scales, one might expect training to promote this, since a failure to do so should be detrimental to the classification performance of these networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discrete scale-channel networks</head><p>Discrete scale-channel networks are implemented by using a standard discrete CNN as the base network ?. For practical applications, it is also necessary to restrict the network to include a finite number of scale channel?</p><formula xml:id="formula_51">S = {? i } ?Kmin?i?Kmax .<label>(48)</label></formula><p>The input image f : Z 2 ? R is assumed to be of finite support. The outputs from the scale channels are, here, aggregated using, e.g., max pooling</p><formula xml:id="formula_52">(? max f )(x, c) = max s?? {(? s f )(x, c, s)}<label>(49)</label></formula><p>or average pooling</p><formula xml:id="formula_53">(? avg f )(x, c) = avg s?? {(? s f )(x, c, s)}.<label>(50)</label></formula><p>We will also implement discrete scale-channel networks that concatenate the outputs from the scale channels, followed by an additional transformation ? : R Mi|?| ? R Mi that mixes the information from the different channels</p><formula xml:id="formula_54">(? conc f )(x, c) = ? [(? s1 f )(x, c), (? s2 f )(x, c) ? ? ? (? s |?| f )(x, c)] .<label>(51)</label></formula><p>? conc does not have any theoretical guarantees of invariance, but since scale concatenation of outputs from the scale channels has been previously used with the explicit aim of scaleinvariant recognition <ref type="bibr" target="#b36">[37]</ref>, we will evaluate that approach also here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Foveated processing</head><p>A standard convolutional neural network ? has a finite support region ? in the input. When rescaling an input image of fixed size/finite support in the scale channels, it is necessary to decide how to process the resulting images of varying size using a feature extractor with fixed support. One option is to process regions of constant size in the scale channels, corresponding to regions of different sizes in the input image. This results in foveated image operations, where a smaller region around the center of the input image is processed at high resolution, while gradually larger regions of the input image are processed at gradually reduced resolution (see  <ref type="formula">c)</ref>). Note how this implies that the scale channels will together process a covariant set of regions, so that for any object size there is always a scale channel with a support matching the size of the object. We will refer to the foveated network architectures ? max , ? avg and ? conc as the FovMax network, the FovAvg network and the FovConc network, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Approximation of scale invariance</head><p>Foveated processing combined with max or average pooling will give an approximation of scale invariance in the continuous model (Section 3.4.2) over a limited scale range.</p><p>The numerical scale warpings of the input images in the scale channels approximate continuous scaling transformations. A discrete set of scale channels will approximate the representation for a continuous scale parameter, where the approximation will be better with a denser sampling of the scaling group. A possible source of problems will, however, arise due to boundary effects caused by a finite scale interval. True scale invariance is only guaranteed for an infinite number of scale channels. In the case of max pooling over a finite set of scale channels, there is a risk that the maximum value over the scale channels moves in or out of the finite scale range covered by the scale channels. Correspondingly, for average pooling, there is a risk that a substantial part of mass of the feature responses from the different scale channels may move in or out of a finite scale interval. The risk for such boundary effects would, however, be mitigated if the network learns to suppress responses for both very zoomed in and very zoomed out objects, so that the contributions from such image structures are close to zero. As a design Since the same operation is performed in all the scale channels, when comparing the output for an original image (left) and a rescaled copy of this image (right), we see that the output code is just shifted along the scale dimension. Thus, if the values taken at the edge of the scale range are small enough, then the maximum over scales will still be preserved between an original and a rescaled image. Correspondingly, for average pooling, there will in this case be no significant change of the mass of the feature response within the scale range spanned by the scale channels. Here, we illustrate the idea for a network that produces a scalar output, but the same argument is valid for vector valued output, where the only difference is that the pooling over the scale dimension is performed for each vector element separately. criterion for scale-channel networks, we therefore propose to include at least a small number of scale channels both below and above the effective training scales of the relevant image structures. Further, we suggest training the network from scratch as opposed to using pretrained weights for the scale channels. Then, we propose that it should be likely that the network will learn to suppress responses for image structures that are far off in scale, since the network would otherwise classify based on use of object views that will hardly provide any useful information. An illustration providing the intuition for how invariance can be achieved in the discrete scale-channel networks is presented in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sliding window processing in the scale channels</head><p>An alternative option for dealing with varying image sizes is to, in each scale channel, process the entire rescaled image by applying the base network in a sliding window man-ner. We, here, evaluate this option, but instead of evaluating the full network anew at each image position, we slide the classifier part of the network (i.e., the last layer) across the convolutional feature map. This is considerably less computationally expensive and, in the case of a network without subsampling by means of strided convolutions (or max pooling), the two approaches are equivalent. Since strided convolution is used in the network, it implies that we here trade some resolution in the output for computational efficiency, where it can be noted that a similar choice is made in the OverFeat detector <ref type="bibr" target="#b47">[48]</ref>. <ref type="bibr" target="#b3">4</ref> Concerning max pooling over space vs. over scale, where according to the most original formulation, a sliding window approach in a scale-space setting would mean that the base network that performs integration over scale should be applied and evaluated anew at all the visited image positions, we, again for reasons of computational efficiency, swap the ordering between max pooling over space vs. over scale, and perform the max pooling over space before the max pooling over scale, since we can then avoid the need for incorporating an explicit mechanism for a skewed/non-vertical pooling operation between corresponding image points at different levels of scale according to <ref type="bibr" target="#b10">(11)</ref>.</p><p>The output from the scale channels can then be combined by max (or average) pooling over space followed by max (or average) pooling over scales 5</p><formula xml:id="formula_55">(? sw,max f )(c) = max s?S max x?? {(? s f )(x, c, s)}.<label>(52)</label></formula><p>We will here only evaluate this architecture using max pooling only, which is structurally similar to the popular multiscale OverFeat detector <ref type="bibr" target="#b47">[48]</ref>. This network will be referred to as the SWMax network. For this scale-channel network to support invariance, it is not sufficient that boundary effects resulting from using a finite number of scale channels are mitigated. When processing regions in the scale channels corresponding to only a single region in the input image, new structures can appear (or disappear) in this region for a rescaled version of the original image. With a linear approach, this might be ex-pected to not cause problems, <ref type="bibr" target="#b5">6</ref> since the best matching pattern will be the one corresponding to the template learned during training. For a deep neural network, however, there is no guarantee that there cannot be strong erroneous responses for, e.g., a partial view of a zoomed in object. We are, here, interested in studying the effects that this has on generalisation in the deep learning context.</p><p>6 Experiments on the MNIST Large Scale dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The MNIST Large Scale dataset</head><p>To evaluate the ability of standard CNNs and scale-channel networks to generalise to unseen scales over a wide scale range, we have created a new version of the standard MNIST dataset <ref type="bibr" target="#b113">[114]</ref>. This new dataset, MNIST Large Scale, which is available online <ref type="bibr" target="#b114">[115]</ref>, is composed of images of size 112? 112 with scale variations of a factor 16 for scale factors s ? [0. <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b7">8]</ref> relative to the original MNIST dataset (see <ref type="figure" target="#fig_3">Figure 4)</ref>. The training and testing sets for the different scale factors are created by resampling the original MNIST training and testing sets using bicubic interpolation followed by smoothing and soft thresholding to reduce discretisation effects. Note that for scale factors &gt; 4, the full digit might not be visible in the image. These scale values are nonetheless included to study the limits of generalisation. More details concerning this dataset are given in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Network and training details</head><p>In the experimental evaluation, we will compare five types of network designs: (i) a (deeper) standard CNN (ii) Fov-Max (max-pooling over the outputs from the scale channels), (iii) FovAvg (average pooling over the outputs from the scale channels), (iv) FovConc (concatenating the outputs from the scale channels) and (v) SWMax (sliding window processing in the scale channels combined with maxpooling over both space and scale).</p><p>The standard CNN is composed of 8 conv-batchnorm-ReLU blocks with 3 ? 3 filters followed by a fully connected layer and a final softmax layer. The number of features/filters in each layer is 16-16-16-16-32-32-32-32-100-10. A stride of 2 is used in convolutional layers 2, 4, 6 and 8. Note that this network is deeper and has more parameters than the networks used as base networks for the scalechannel networks. The reason for using a quite deep network <ref type="bibr" target="#b5">6</ref> When using linear template matching, the best matching pattern for a template learned during training will be a very similar image patch. Thus, when sliding a template across a matching object, it will take the maximum response when centered on the object. When using a non-linear method, however, there is no reason there could not be large responses for non centered views of familiar objects or completely novel patterns.  is to avoid a network structure that is heavily biased towards recognising either small or large digits. A more shallow network would simply not have a receptive field large enough to enable recognising very large objects. The need for extra depth is thus a consequence of the scale preference built into a vanilla CNN architecture. Here, we are aware of this more structural problem of CNNs, but specifically aim to test scale generalisation for a network with a structure that would at least in principle enable scale generalisation. The FovMax, FovAvg, FovConc and SWMax scale-channel networks are constructed using base networks for the scale channels with 4 conv-batchnorm-ReLU blocks with 3?3 filters followed by a fully connected layer and a final softmax layer. The number of features/filters in each layer is 16-16-32-32-100-10. A stride of 2 is used in convolutional layers 2 and 4. Rescaling within the scale channels is done with bilinear interpolation and applying border padding or cropping as needed. The batch normalisation layers are shared between the scale channels for the FovMax, FovAvg and FovConc networks. This implies that the same operation is performed for all scales, to preserve scale covariance and enable scale invariance after max or average pooling.</p><p>We do not apply batch normalisation to the SW network, since this was shown to impair the performance. We believe that this is because the sliding window approach implies a change in the feature distribution for this network when processing data of different sizes. For the batch normalisation to function optimally, the data/feature distribution should stay approximately the same, which is not the case for the SWMax network. <ref type="bibr" target="#b6">7</ref> For the FovAvg and FovMax networks, max pooling and average pooling, respectively, are performed across the logits outputs from the scale channels before the final softmax transformation and cross entropy loss. For the FovConc network, there is a fully connected layer that combines the logits outputs from the multiple scale channels before applying a final softmax transformation and cross entropy loss.</p><p>All the scale-channel architectures have around 70k parameters, whereas the baseline CNN has around 90k parameters.</p><p>All the networks are trained with 50 000 training samples from the MNIST Large Scale dataset for 20 epochs using the Adam optimiser with default parameters in PyTorch: ? 1 = 0.9 and ? 2 = 0.999. During training, 15 % dropout is applied to the first fully connected layer. The learning rate starts at 3e ?3 and decays with a factor 1/e every second epoch towards a minimum learning rate of 5e ?5 . For the SWMax network, the learning rate instead starts at 3e ?4 , since this produced better results in the absence of batch normalisation. Results are reported for the MNIST Large Scale testing set (10 000 samples) as the average of training each network using three different random seeds. The remaining 10 000 samples constitute a validation set, which was used for parameter tuning. Parameter tuning was performed for a single-channel network, and the same parameters were used for the multi-channel networks and for the standard CNN.</p><p>Numerical performance scores for the results in some of the figures to be reported are given in <ref type="bibr" target="#b115">[116]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generalisation to unseen scales</head><p>We, first, evaluate the ability of the standard CNN and the different scale-channel networks to generalise to previously unseen scales. We train each network on either of the sizes 1, 2, and 4 from the MNIST Large Scale dataset and evaluate the performance on the testing set for scale factors between 1/2 and 8. The FovMax, FovAvg and SWMax networks have 17 scale channels spanning the scale range [ <ref type="bibr">1 2 , 8]</ref>. The FovConc network has 3 scale channels spanning the scale range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. <ref type="bibr" target="#b7">8</ref> The results are presented in <ref type="figure" target="#fig_4">Figure 5</ref>. We, first, tested two versions of batch normalisation: (i) normalising the feature responses jointly across all feature maps and (ii) normalising each channel separately. Neither of these options is scale invariant, the first because of the change in the feature distribution for the joint set of feature maps between inputs of different sizes and the second because the same operation is not applied for all feature channels. Both impaired the performance. We thus opt for evaluating the SWMax network with the best configuration we found, which corresponds to training the network from scratch without batch normalisation. <ref type="bibr" target="#b7">8</ref> The FovConc network has worse generalisation performance when including too many scale channels or spanning a too wide scale range. Since we are more interested in the best case rather than the worst case scenario, we, here, picked the best network out of a large range of configurations. note that all the networks achieve similar top performance for the scales seen during training. There are, however, large differences in the abilities of the networks to generalise to unseen scales:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Standard CNN</head><p>The standard CNN shows limited generalisation ability to unseen scales with a large drop in accuracy for scale variations larger than a factor ? 2. This illustrates that, while the network can recognise digits of all sizes, a standard CNN includes no structural prior to promote scale invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">The FovConc network</head><p>The scale generalisation ability of the FovConc network is quite similar to that of the standard CNN, sometimes slightly worse. The reason why the scale generalisation is limited is that although the scale channels share their weights and thus produce a scale-covariant output, when simply concatenating these outputs from the scale channels, there is no structural constraint to support scale invariance. This is consistent with our observation that spanning a too wide scale range (Section 6.4) or using too many channels, the scale generalisation degrades for the FovConc network (Section 6.5). For scales not present during training, there is, simply, no useful training signal to learn the correct weights in the fully connected layer that combines the outputs from the different scale channels. Note that our results are not contradictory to those previously reported for a similar network structure <ref type="bibr" target="#b36">[37]</ref>, since they train on data that contain natural scale variations and test over a quite narrow scale range. What we do show, however, is that this network structure, although it enables multi-scale processing, is not scale invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">The FovAvg and FovMax networks</head><p>We note that the FovMax and FovAvg networks generalise very well, independently of what size the network is trained on. The maximum difference in performance in the size range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> between training on size 1, size 2 or size 4 is less than 0.2 percentage points for these network architectures. Importantly, this shows that, if including a large enough number of sufficiently densely distributed scale channels and training the networks from scratch, boundary effects at the scale boundaries do not prohibit invariant recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">The SWMax network</head><p>We note that the SWMax network generalises considerably better than a standard CNN, but there is some drop in performance for sizes not seen during training. We believe that the main reason for this is, here, that since all the scale  channels are processing a fixed sized region in the input image (as opposed to for foveated processing), new structures might leave or enter this region when an image is rescaled. This might lead to erroneous high responses for unfamiliar views (see Section 5.3). We also noted that the SWMax networks are harder to train (more sensitive to learning rate etc) compared to the foveated network architectures as well as more computationally expensive. Thus, while the FovMax and FovAvg networks still are easy to train and the performance is not degraded when spanning a wide scale range, the SWMax network seems to work best for spanning a more limited scale range, where fewer scale channels are needed (as was indeed the use case in <ref type="bibr" target="#b47">[48]</ref>). <ref type="figure" target="#fig_5">Figure 6</ref> shows the result of experiments to investigate the sensitivity of the scale generalisation properties to how wide range of scale values is spanned by the scale channels. For all the experiments, we have used a scale sampling ratio of ? 2 between adjacent scale channels. All the networks were trained on the single size 2 and were tested for all sizes between  range, without any need to include training data for more than a single scale. The scale generalisation property will, however, be limited by the image resolution for small testing sizes and by the fact that the full object is not visible in the image for larger testing sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Dependency on the scale range spanned by the scale channels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">The SWMax network</head><p>For the SWMax network, the scale generalisation property is improved when including more scale channels, but the network does not generalise as well as the FovAvg and the FovMax networks. It is also noticeable that scale generalisation is harder when for large testing sizes compared to small testing sizes. This is probably because of the problem with unfamiliar partial views present for sliding window processing becoming more pronounced for large testing sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.3">The FovConc network</head><p>For the FovConc network, the scale generalisation is actually worse when including more scale channels. This phenomenon can be understood by considering that the weights in the fully connected layer, which combines information from the concatenated scale channels output, are not controlled by any invariance mechanism. Indeed, the weights corresponding to scales not present during training may take arbitrary values without any significant impact on the training error. Incorrect weights for unseen scales will, however, imply very poor generalisation to those scales.   <ref type="bibr" target="#b7">8]</ref> were trained with varying spacing between the scale channels, either 2, 2 1/2 or 2 1/4 . All the networks were trained on size 2. There is a significant increase in the performance when reducing the spacing between the scale channels from 2 to 2 1/2 , while the effect of a further reduction to 2 1/4 is very small. to the sampling density of the scale channels. All the networks were trained on size 2, with the scale channels spanning the scale range [ <ref type="bibr">1 2 , 8]</ref>, and with a varying spacing between the scale channels: either 2, 2 1/2 or 2 1/4 . For the Fov-Conc network, we also included the spacing 2 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Dependency on the scale sampling density</head><p>The number of scale channels for the different sampling densities were for the 2 2 spacing: 3 channels, for the 2 spacing: 5 channels, for the 2 1/2 spacing: 9 channels and for the 2 1/4 spacing: 17 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.1">The FovAvg and FovMax networks</head><p>For both the FovAvg and FovMax networks, the accuracy is considerably improved when decreasing the ratio between adjacent scale levels from a factor 2 to a factor of 2 1/2 , while a further reduction to 2 1/4 provides very low additional benefits. 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.2">The SWMax network</head><p>The SWMax network is more sensitive to how densely the scale levels are sampled compared to the FovAvg and Fov-Max networks. This sensitivity to the scale sampling density is larger, when observing objects of larger size than those seen during training, as compared to when observing objects of smaller size than those seen during training.</p><p>This, again, illustrates the problem due to partial views of objects, which will be present at some scales but not at others, are more severe when observing larger size objects than seen during training. <ref type="bibr" target="#b8">9</ref> This result is consistent with results about scale sampling in classical scale-space theory, where it is known that uniform scale sampling in units of effective scale ? = log ? <ref type="bibr" target="#b116">[117]</ref> is the natural scale sampling strategy, and a scale sampling ratio of ? 2 often leads to substantially better performance than a scale sampling ratio of 2 in classical scalespace algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.3">The FovConc network</head><p>The FovConc network does actually generalise worse with a denser sampling of scales. In fact, none of the network versions generalises better than a standard CNN. The reason for this is probably that for a dense sampling of scales, there is no need for the last fully connected layer, which processes the concatenated outputs from all the scale channels, to include information from scales further away from the training scale. Thus, the weights corresponding to such scales may take arbitrary values without affecting the accuracy during the training process, thereby implying very poor generalisation to previously unseen scales. Training on multi-scale training data does not improve the scale generalisation ability of the CNN for scales outside the scale range the network is trained on. The network can, indeed, learn to recognise digits of different sizes. But just because it might learn that an object of size 1 is the same as the same object of size 2, this does not at all imply that it will recognise the same object if it has size 4. In other words, the scale generalisation ability within a subrange does not transfer to outside that range. <ref type="figure">Figure 10</ref> shows the result of performing multi-scale training over the size range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> for the scale-channel networks FovMax, FovAvg, FovConc and SWMax as well as the standard CNN. Here, the same scale-channel setup with 17 channels spanning the scale range [ <ref type="bibr">1 2 , 8]</ref> is used for all the scalechannel architectures. When multi-scale training data is used, the advantage of using scale channels spanning a larger scale range no longer incurs a penalty for the FovConc network, since the correct weights can be learned in the fully connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.2">Multi-scale training</head><p>We note that the difference between training on multiscale and single-scale data is striking both for the FovConc network and the standard CNN. It can, however, be noted that the FovConc network works well in this scenario, especially for the scale range included in the training set. Outside this scale range, we note somewhat better generalisation compared to the CNN, while the generalisation is still worse than for the FovAvg and FovMax networks. The FovConc network does, after all, include a mechanism for multi-scale processing and when trained on multi-scale training data, the lack of invariance mechanism in the fully connected layer is less of a problem.</p><p>For the SWMax network, including multi-scale data improves the scale generalisation somewhat compared to singlescale training. The SWMax network does, however, have worse performance for spanning larger scale ranges compared to the other networks. The reason behind this is probably that the multiple views produced in the different scale channels indeed makes the problem harder for this network compared to the foveated networks, which only need to process centered digit views.</p><p>The difference in scale generalisation ability between training on a single scale or multi-scale image data is on the other hand almost indiscernible for the FovMax and FovAvg networks (less than 0.1 % difference in accuracy), illustrating the strong scale invariance properties of these networks.       <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. <ref type="table" target="#tab_7">Tables 2-5</ref> gives relative ranking of the different networks on specific subsets of this data, which can be treated as benchmarks regarding scale generalisation for the MNIST Large Scale dataset. As can be seen from these tables, the FovAvg and FovMax networks have the overall best performance scores of these networks, both for the cases of singlescale training and multi-scale training.</p><p>The FovConc, CNN and SWMax networks are very much improved by multi-scale training, whereas the FovAvg and FovMax networks perform almost as well for single-scale training as for multi-scale training.  <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. The Fov-Avg network shows the highest robustness when decreasing the number of training samples followed by the FovMax network. The FovConc network also shows a small improvement over the standard CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.8">Generalisation from fewer training samples</head><p>Another scenario of interest is when the training data does span a relevant range of scales, but there are few training samples. Theory would predict a correlation between the performance in this scenario and the ability to generalise to unseen scales.</p><p>To test this prediction, we trained the standard CNN and the different scale-channel networks on multi-scale training data spanning the size range <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>, while gradually reducing the number of samples in the training set. Here, the same scale-channel setup with 17 channels spanning the scale range [ <ref type="bibr">1 2 , 8]</ref> was used for all the architectures. The results are presented in <ref type="figure" target="#fig_8">Figure 11</ref>. We can note that the FovConc network shows some improvement over the standard CNN. The SWMax network, on the other hand, does not, and we hypothesise that when using fewer samples, the problem with partial views of objects (see Section 5.3) might be more severe. Note that the way the OverFeat detector is used in the original study <ref type="bibr" target="#b47">[48]</ref> is more similar to our single-scale training scenario, since they use base networks pre-trained on ImageNet. The FovAvg and FovMax networks show the highest robustness also in this scenario. This illustrates that these networks can give improvements when multi-scale training data is available, but there are few training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.9">Scale selection properties</head><p>One may ask, how do the scales "selected" by the networks, i.e., the scales that contribute the most to the feature response of the winning digit class, vary with the size of the object in the image? We, here, investigate the relative contributions from the different scale channels to the classification decision and how they vary with the object size. For this purpose, we train the FovAvg, FovMax, FovConc and SWMax networks on the MNIST Large Scale dataset for each one of the different training sizes 1, 2 and 4 and then accumulate histograms that quantify the contribution from the different scale channels over a range of image sizes in the testing data.</p><p>The histograms are constructed as follows:</p><p>-FovMax: We identify the scale channel that provides the maximum value for the winning digit class and increment the histogram bin corresponding to this scale channel with a unit increment. -FovAvg: The FovAvg network aggregates contributions from multiple scale channels for each classification decision. For the winning digit class, we, consider the relative contributions from the different scale channels and increment each histogram bin with the corresponding fraction of unity of this contribution. The contribution is measured as the absolute value of the feature response before average pooling. -FovConc: We compute the relative contribution from each scale channel as the sum of the weights in the fully connected layer corresponding to the winning digit class and the specific scale channel, multiplied by the feature values corresponding to the output from that scale channel. We increment each histogram bin with the fraction of unity corresponding to the absolute value of the relative contribution from each scale channel. -SWMax: We identify the scale channel that provides the maximum value for the winning digit class and increment the histogram bin corresponding to this scale channel with a unit increment.</p><p>The procedure is repeated for all the testing sizes in the MNIST Large Scale dataset, resulting in two-dimensional scale selection histograms, which show what scale channels contribute to the classification output as function of the size of the image structures in the testing data. The histograms are presented in <ref type="figure" target="#fig_0">Figures 12-13</ref>. As can be seen in <ref type="figure" target="#fig_0">Figure 12</ref>, for the FovAvg and FovMax networks, the selected scale levels do very well follow a linear trend in the sense that the selected scale levels are proportional to the size of the image structures in the testing data. <ref type="bibr" target="#b9">10</ref> The scale selection histograms are also largely similar, irrespective of whether the training is performed for size 1, 2 or 4, illustrating that the scale-invariant properties of the FovAvg and FovMax networks in the continuous case transfer very well to the discrete implementation. In this respect, the resulting scale-selection properties of the FovAvg and FovMax networks share similarities to classical methods for scale selection based on local extrema over scale or weighted averaging over scale of scale-normalised derivative responses <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b112">113,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b109">110]</ref>. This makes sense in light of the result that the scaling properties of the filters applied to the scale channels are similar to the scaling properties of scale-normalised Gaussian derivatives (see Section 4.3.2). The approach for the FovMax network is also closely related to the scale selection approach in <ref type="bibr" target="#b111">[112,</ref><ref type="bibr" target="#b117">118]</ref> based on choosing the scales at which a supervised classifier delivers class labels with the highest posterior.</p><p>As can be seen in <ref type="figure" target="#fig_2">Figure 13</ref>, the behaviour is different for the not scale-invariant FovConc and SWMax networks. For the FovConc network, there is a bias in that the selected <ref type="bibr" target="#b9">10</ref> A certain bias that can be observed for the FovMax and SWMax networks, is that there is a stronger peak in the histogram scale channels for scale channel 1 for small testing sizes, than for the neighbouring scale channels. A possible explanation for this effect is that for scale channel 1 there will not be any effective initial interpolation stage as for the other scale channels, which implies that there is no additional interpolation blur for this scale channel as for the other scale channels, in turn implying a stronger response for this scale channel compared to the neighbouring scale channels. A certain bias towards scale channel 1 can also be observed for the FovConc network. For the FovAvg network, which is also the network that performs clearly best out of these four networks, the bias towards scale channel 1 is, however, very minor. In retrospect, the bias towards scale channel 1 for the other networks could point to replacing the initial bilinear interpolation stage by some other interpolation method, and/or to add a small complementary smoothing stage after the interpolation stage, to ensure that the sum of the effective interpolation blur and the added complementary blur remains approximately the same for neighbouring scale channels. scales are more concentrated towards the size of the training data. The contributions from the different scale channels are also much less concentrated around the linear trend compared to the FovAvg and FovMax networks. Without access to multi-scale training, the FovConc network does not learn scale invariance although this would in principle be possible, e.g., by learning to use equal weights for all the scales, which would implement average pooling over scales.</p><p>For the SWMax network, although the resulting scale selection histogram is largely centered around a linear trend, consistent with the relative robustness to scaling transformations that this network shows, the linear trend is not as clean as for the FovAvg and FovMax networks. For the coarsest scale testing structures, the SWMax network largely fails to activate corresponding scale channels beyond a certain value. This is consistent with the previously problems of not being able generalise to larger testing scales, and is likely related to the previously discussed problem of interference from zoomed-in previously unseen partial views that might give stronger feature responses than the zoomed-out overall shape. Furthermore, for finer or coarser scale testing structures, there are some scale channels for the SWMax network that contribute more to the output than others, and thus demonstrate a lack of true scale invariance.</p><p>In the quantitative scale generalisation experiments presented earlier, it was seen that the lack of scale invariance for the SWMax network leads to lower accuracy when generalising to unseen scales and, for the FovConc network, which here shows the worst scale selection properties, no marked improvement at all over a standard CNN. For the truly scale-invariant FovAvg and FovMax networks, on the other hand, the ability of the networks to correctly identify the scale of the object in a scale-covariant way imply excellent scale generalisation properties.</p><p>7 Experiments on rescalings of the CIFAR-10 dataset</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Dataset</head><p>To investigate if a scale-channel network can still provide a clear advantage over a standard CNN in a more challenging scenario, we use the CIFAR-10 dataset <ref type="bibr" target="#b118">[119]</ref>  <ref type="figure" target="#fig_4">Fig. 15</ref>: Generalisation ability to unseen scales for a standard CNN and different scale-channel network architectures for the rescaled CIFAR-10 dataset. The network is trained on the CIFAR-10 training set (corresponding to scale factor 1.0) and tested on rescaled images from the testing set for relative scale factors between 1 2 and 2. The FovConc network has better scale generalisation compared to the standard CNN, but for larger deviations from the scale that the network is trained on, there is a clear advantage for the FovAvg and the FovMax networks.</p><p>Because already the original dataset is at the limit of being undersampled, reducing the image size further for scale factors s &lt; 1 results in additional loss of object details. The images are also tightly cropped, which implies that increasing the image size for scale factors s &gt; 1 implies a loss of information towards the image boundaries, and that sampling artefacts in the original image data will be amplified. Further, when reducing the image size, we extend the image by mirroring at the image boundaries, adding artefacts in the image structures, caused by the image padding operations. What we evaluate here is thus the limits of the scale-channel networks, near or beyond the limits of image resolution, to see if this approach can still provide a clear advantage over a standard CNN. <ref type="figure" target="#fig_3">Figure 14</ref> shows a few images from the rescaled testing set, with examples of two out of the 10 object classes in the dataset: "airplanes", "cars", "birds", "cats", "deer", "dogs", "frogs", "horses", "ships", and "trucks".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Network and training details</head><p>For the CIFAR-10 Scale dataset, we will compare the Fov-Max, FovAvg and FovConc networks to a standard CNN. <ref type="bibr" target="#b10">11</ref> We use the same network for the CNN as for the individual scale channels, a 7-layer network with conv+batchnorm+ReLU layers with 3 ? 3 kernels and zero padding with width 1. We do not use any spatial max pooling, but use a stride of 2 for convolutional layers 3, 5 and 7. After the final convolutional layer, spatial average pooling is performed over the full feature map down to 1 ? 1 resolution, followed by a final fully connected softmax layer. We do not use dropout, since it did not improve the results for this quite simple network with relatively few parameters. The number of feature channels is 32-32-32-64-64-128-128 for the 7 convolutional layers.</p><p>For the FovAvg and FovMax networks, max pooling and average pooling, respectively, is performed across the logits outputs from the scale channels before the final softmax transformation and cross entropy loss. For the FovConc network, there is a fully connected layer that combines the logits outputs from the multiple scale channels before applying a final softmax transformation and cross entropy loss. We use bilinear interpolation and reflection padding at the image boundaries when computing the rescaled images used as input for the scale channels.</p><p>All the CIFAR-10 networks are trained for 20 000 time steps using 50 000 training samples from the CIFAR-10 training set over 103 epochs, using a batch size of 256 and the Adam optimiser with default parameters in PyTorch: ? 1 = 0.9 and ? 2 = 0.999. A cosine learning rate decay is used <ref type="bibr" target="#b10">11</ref> We do not evaluate the SWMax network on the CIFAR-10 Scale dataset, since it is not meaningful to perform a spatial search for objects in this dataset. with starting learning rate 0.001 and floor learning rate 0.00005, where the learning rate decreases to the floor learning rate after 75 epochs. The networks are then tested on the 10 000 images in the testing set, for relative scaling factors in the interval [ 1 2 , 2]. We chose the learning rate and training schedule based on the CNN performance using the last 10 000 samples of the training set as a validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Experimental results</head><p>The results for the standard CNN are shown in <ref type="figure" target="#fig_4">Figure 15(a)</ref>. It can be seen that, already for scale factors slightly off from 1, there is a noticeable drop in generalisation performance.</p><p>The results for the FovConc network, for different number of scale channels, are presented in <ref type="figure" target="#fig_4">Figure 15</ref>(b). The generalisation ability to new scales is markedly better than for the standard CNN, but the scale generalisation is not improved by adding more scale channels. This can be compared with no improvement over a standard CNN when trained on single-scale MNIST data. We believe that the key difference is that for the CIFAR-10 dataset there are indeed some scale variations present in the training set, and as discussed earlier, it is possible for the FovConc network to learn to generalise by assigning appropriate weights to the layer that combines information from the different scale channels. This illustrates that the method does have some structural advantage compared to a standard CNN, but that multi-scale training data is required to realise this advantage.</p><p>The results for the FovMax and FovAvg networks, for different numbers of scale channels, are presented in <ref type="figure" target="#fig_4">Figure 15(c-d)</ref>, and are significantly better than for the standard CNN and the FovConc network. The accuracy for the smallest scale 1/2 is improved from ? 40% for the CNN to above 70% for the FovAvg and FovMax networks, while the accuracy for the largest scale 2 is improved from ? 30% for the CNN to ? 50% for the FovAvg and FovMax networks.</p><p>For the FovMax network, there is a noticeable improvement by going to a finer scale sampling ratio of 2 1/4 compared to 2 1/2 . Then, the generalisation ability for the Fov-Max network is also somewhat better than for the FovAvg network. The FovAvg network does, however, have slightly better peak performance compared to the FovMax network.</p><p>To summarise, the FovMax and FovAvg networks provide the best generalisation ability to new scales, which is in line with theory. This shows that, also for datasets where the conditions regarding image size and resolution are not such that the scale-channel approach can provide full invariance, our foveated scale-channel networks can nevertheless provide benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Summary and discussion</head><p>We have presented a methodology to handle scaling transformations in deep networks by scale-channel networks. Specifically, we have presented a theoretical formalism for modelling scale-channel networks based on continuous models of the both the filters and the image data, and shown that the continuous scale-channel networks are provably scale covariant and translationally covariant. Combined with max pooling or average pooling over the scale channels, our foveated scale-channel networks are additionally provably scale invariant.</p><p>Experimentally, we have demonstrated that discrete approximations to the continuous foveated scale-channel networks FovMax and FovAvg are very robust to scaling transformations, and allow for scale generalisation, with very good performance for classifying image patterns at new scales not spanned by the training data, because of the continuous invariance properties that they approximate. Experimentally, we have also demonstrated the very limited scale generalisation performance of vanilla CNNs and scale concatenation networks when exposed to testing at scales not spanned by the training data, although those approaches may work rather well when training on multi-scale training data. The reason why those approaches fail regarding scale generalisation, when trained at a single scale or a over a narrow scale interval only, is because of the lack of an explicit mechanism to enforce scale invariance.</p><p>We have further demonstrated that a foveated approach shows better generalisation performance compared to a sliding window approach, especially when moving from a smaller training scale to a large testing scale. Note that this should not be seen as an argument against any type of sliding window processing per se. The foveated networks could, indeed, be applied in a sliding window manner to search for objects in a larger image. Instead, it illustrates that for any specific image point, it is important to process a covariant set of image regions that correspond to different sizes in the input image.</p><p>We have also demonstrated that our FovMax and Fov-Avg scale-channel networks lead to improvements when training on data with significant scale variations in the small sample regime. We have further shown that the selected scale levels for these scale-invariant networks increase linearly with the size of the image structures in the testing data, in a similar way as for classical methods for scale selection.</p><p>From the presented experimental results on the MNIST Large Scale dataset, it is clear that our FovMax and Fov-Avg scale-channel networks do provide a considerable improvement in scale generalisation ability compared to a standard CNN as well as in relation to previous scale-channel approaches. Concerning the CIFAR-10 dataset, it should be noted that full invariance is not possible because of the loss in image information between the original and the rescaled images. Our experiments on this dataset show, nonetheless, that also in the presence of undersampling and serious boundary effects, our FovMax and FovAvg scale-channel networks give considerably improved generalisation ability compared to a standard CNN or alternative scale-channel networks.</p><p>We believe that our proposed foveated scale-channel networks could prove useful in situations where a simple approach that can generalise to unseen scales or learn from small datasets with large scale variations is needed. Strong reasons for using such scale-invariant scale-channel networks could either be because there is a limited amount of multiscale training data, where sharing statistical strength between scales is valuable, or because only a single scale or a limited range of scales is present in the training set, which implies that generalisation outside the scales seen during training is crucial for the performance. Thus, we propose that this type of foveated scale-invariant processing could be included as subparts in more complex frameworks dealing with large scale variations.</p><p>Concerning applications towards object recognition, it should, however, be emphasised that in this study, we have not specifically focused on developing an integrated approach for detecting objects, since the main focus has been to develop ways of handling the notion of scale in a theoretically well-founded manner. Beyond the vanilla sliding-window approach studied in this paper, which has such a built-in object detection capability, also the foveated networks could be applied in a sliding-window fashion, thus being able to also handle smaller objects near the image boundaries, which is not possible if the central point in the image is always used as the origin when resizing the image multiple times to form the input for the different scale channels.</p><p>To avoid explicit exhaustive search over multiple such origins for the foveated representations, such an approach could further be naturally extended to a two-stage approach, where detection of points of interest is first performed using a complementary module that detects points of interest (not necessarily of the same kind as the current regular notion of interest points for image-based matching and recognition), followed by more detailed analysis of these points of interest with a foveated representation. Such an approach would then bear similarity to human vision, by foveating on interesting structures to look at them in more detail. It would specifically also bear similarity to two-stage approaches for object recognition, such as R-CNNs <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b120">121]</ref>, with the difference that the initial detection step does not need to return a full window of interest. Instead, only a single initial point is needed, where the scale, corresponding to the size of the window, is then handled by the built-in scale selection step in the foveated scale-channel network.</p><p>To conclude, the overarching aim of this study has instead been to test the limits of CNNs to generalise to unseen scales over wide scale ranges. The key take home message is a proof of concept that such scale generalisation is possible, if including structural assumptions about scale in the network design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The MNIST Large Scale dataset</head><p>We, here, give a more detailed description of the MNIST Large Scale dataset. The original MNIST dataset <ref type="bibr" target="#b113">[114]</ref> contains images of centered handwritten digits of size 28 ? 28.  <ref type="figure" target="#fig_3">Figure 4</ref>.</p><p>To create an image with a certain scale factor s, the original image is first rescaled/resampled using bicubic interpolation. The image range is then clipped to [0, 256] to remove possible over/undershoot resulting from the bicubic interpolation. The resulting image is embedded into an 112 ? 112 resolution image using zero padding or cropping as needed.</p><p>Large amounts of upsampling tend to result in discretisation artefacts. To reduce the severity of such artefacts, the images are post-processed with discrete Gaussian smoothing <ref type="bibr" target="#b121">[122]</ref> followed by non-linear thresholding. The standard deviation of the discrete Gaussian kernel varies with the scale factor as ?(s) = 7 8 s. After smoothing, the image range is rescaled to the range [0, 255].</p><p>As a final step, an arctan non-linearity is applied to sharpen the resulting image, where the final image intensity I out is computed from the output of the smoothing step I in as:</p><formula xml:id="formula_56">I out = 2 ? arctan(a(I in ? b))<label>(53)</label></formula><p>with a = 0.02 and b = 128. Note that for scale factors &gt; 4, the full digit might not be visible in the image. These scale factors are included to enable studying the limits of generalisation when the entire object is no longer visible (typically the digits are fully contained in the image for s &lt; 4 ? 2). All training data sets are created from the first 50 000 images in the original MNIST training set, while the last 10 000 images in the original MNIST training set are used to create validation sets. The testing data sets are created by rescaling the 10 000 images in the original MNIST testing set. For the multi-scale datasets, scale factors for the individual images are sampled uniformly on a logarithmic scale in the range [s min , s max ].</p><p>The specific MNIST Large Scale dataset used for the experiments in this paper is available online <ref type="bibr" target="#b114">[115]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Foveated scale-channel networks. (a) Foveated scale-channel network that processes an image of the digit 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figures 2(b)-(</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>An illustration of how discrete scale-channel networks approximate scale invariance over a finite scale range. Consider a foveated scale-channel network combined with max or average pooling over the output from the scale channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Samples from the MNIST Large Scale dataset: The MNIST Large Scale dataset is derived from the original MNIST dataset [114] and contains 112 ? 112 sized images of handwritten digits with scale variations of a factor of 16. The scale factors relative the original MNIST dataset are in the range 1 2 (top left) to 8 (bottom right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Generalisation ability to unseen scales for a standard CNN and the different scale-channel network architectures for the MNIST Large Scale dataset. The networks are trained on digits of size 1 (tr1), size 2 (tr2) or size 4 (tr4) and evaluated for varying rescalings of the testing set. We note that the CNN (a) and the FovConc network (b) have poor generalisation ability to unseen scales, while the FovMax and FovAvg networks (c) generalise extremely well. The SWMax network (d) generalises considerably better than a standard CNN, but there is some drop in performance for scales not seen during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Dependency of the scale generalisation property on the scale range spanned by the scale channels: (a)-(b) For the FovAvg and FovMax networks, the scale generalisation property is directly proportional to the scale range spanned by the scale channels, and there is no need to include training data for more than a single scale. (c) For the SWMax network, the scale generalisation is improved when including more scale channels, but the network does not generalise as well as the FovAvg and the FovMax networks. (d) For the FovConc network, the scale generalisation does actually become worse when including more scale channels (in the case of single-scale training), because there is no mechanism to support scale invariance when training the weights in the final fully connected layer that combines the different scale channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Dependency of the scale generalisation property on the scale sampling density: (a)-(b) For the FovAvg and FovMax networks, the overall scale generalisation is very good for all the studied scale sampling rates, although it becomes noticeably better for 2 1/2 compared to 2. For a more close up look regarding the FovAvg and FovMax networks, seeFigure 8. (c) The SWMax network is more sensitive to how densely the scales are samples compared to the FovAvg and the FovMax networks, and the sensitivity to the scale sampling density is larger when observing objects that are larger than those seen during training, as compared to when observing objects that are smaller than those seen during training. (d) The FovConc network actually generalises worse with a denser sampling of scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 and</head><label>7</label><figDesc>Figure 8show the result of experiments to investigate the sensitivity of the scale generalisation property</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Training with smaller training sets with large scale variations. All the network architectures are evaluated on their ability to classify data with large scale variations, while reducing the number of training samples. Both the training and the testing sets here span the size range</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>The MNIST Large Scale dataset is derived from the MNIST dataset by rescaling the original MNIST images. The resulting dataset contains images of size 112 ? 112 with scale variations of a factor of 16. The scale factors s relative the original MNIST images are s ? [ 1 2 , 8]. The dataset is illustrated in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Dependency of the scale generalisation property on the scale sampling density for the FovAvg and FovMax networks: FovMax and FovAvg networks spanning the scale range [ 1 4 ,</figDesc><table><row><cell>Accuracy (%)</cell><cell>96 98 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 1/4 2 1/2 2</cell><cell>Accuracy (%)</cell><cell>96 98 100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2 1/4 2 1/2 2</cell></row><row><cell></cell><cell>0.50</cell><cell>0.71</cell><cell>1.00</cell><cell>1.41</cell><cell>2.00 Scale factor</cell><cell>2.83</cell><cell>4.00</cell><cell>5.66</cell><cell>8.00</cell><cell></cell><cell>0.50</cell><cell>0.71</cell><cell>1.00</cell><cell>1.41</cell><cell>2.00 Scale factor</cell><cell>2.83</cell><cell>4.00</cell><cell>5.66</cell><cell>8.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) The FovAvg network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(b) The FovMax network</cell></row><row><cell cols="2">Fig. 8:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Comparing multi-scale vs. single-scale training for a vanilla CNN. Training is here performed over the size ranges<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4]</ref>, respectively. The scale generalisation when trained on single size training data is presented as dashed grey lines for training sizes 1, 2 and 4, respectively. As can be seen from the results, training on multi-scale training data does not improve the scale generalisation ability of the CNN for sizes outside the size range the network is trained on.Fig. 10: Results of multi-scale training for the scale-channel networks with training sizes uniformly distributed on the size range [1, 4] (with the uniform distribution on a logarithmic scale). These two figures show the same experimental results, where the second figure is zoomed in, to make comparisons between the networks more visible. The presence of multi-scale training data substantially improves the performance of the CNN, the FovConc network and the SWMax network. The difference in performance between single-scale training and multi-scale training is almost indiscernable for the FovAvg and FovMax networks. The overall best performance is obtained for the FovAvg network. Fig-ure 9 shows the result of training the standard CNN on training data with multiple sizes uniformly distributed over the scale ranges [1, 2] and [2, 4], respectively, and testing on all sizes over the range [ 1 2 , 8]. (The size distributions are uniform on a logarithmic scale.)</figDesc><table><row><cell></cell><cell>100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy (%)</cell><cell>40 60</cell><cell>FovAvg tr1-4 FovMax tr1-4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>FovConc tr1-4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>20</cell><cell>SWMax tr1-4 CNN tr1-4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">0.50 20 40 60 80 100 Fig. 9: 0.50 Accuracy (%) 0.71 1.00 1.41 2.00 2.83 4.00 5.66 8.00 Scale factor 0.50 0.71 1.00 1.41 2.00 2.83 4.00 5.66 8.00 Scale factor 98.0 98.2 98.4 98.6 98.8 99.0 99.2 99.4 FovAvg tr1-4 FovMax tr1-4 FovConc tr1-4 SWMax tr1-4 CNN tr1-4 6.6 Multi-scale vs. single-scale training Accuracy (%) All the scale-channel architectures support multi-scale pro-cessing although they might not support scale invariance. We, here, test the performance of the different scale-channel networks when training on multi-scale training data. For the</cell><cell>0.71 CNN tr1-2 1.00 CNN tr2-4 single scale training 1.41</cell><cell>2.00 Scale factor</cell><cell>2.83</cell><cell>4.00</cell><cell>5.66</cell><cell>8.00</cell></row><row><cell cols="3">standard CNN, we also explicitly explore how generalisa-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">tion is affected when training on a smaller scale range to</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">see how this affects generalisation outside the scale range</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">trained on.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>6.6.1 Limits of generalisation for a standard CNN If including data multi-scale data within a some range, could a CNN learn to "extrapolate" outside this scale range?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Compact performance measures regarding scale generalisation on the MNIST Large Scale dataset</figDesc><table><row><cell>Scale range</cell><cell>[1/2, 1]</cell><cell>[1, 4]</cell><cell cols="3">[4, 8] [1/2, 4] [1/2, 8]</cell></row><row><cell>FovAvg 17ch tr1</cell><cell cols="3">99.15 99.27 90.82</cell><cell>99.22</cell><cell>96.76</cell></row><row><cell>FovAvg 17ch tr2</cell><cell cols="3">99.14 99.36 96.55</cell><cell>99.27</cell><cell>98.47</cell></row><row><cell>FovAvg 17ch tr4</cell><cell cols="3">98.78 99.31 96.61</cell><cell>99.11</cell><cell>98.36</cell></row><row><cell>FovAvg 17ch mean(tr1, tr2, tr4)</cell><cell cols="3">99.02 99.32 94.66</cell><cell>99.20</cell><cell>97.86</cell></row><row><cell>FovAvg 17ch tr14</cell><cell cols="3">99.20 99.40 96.50</cell><cell>99.32</cell><cell>98.49</cell></row><row><cell>FovMax 17ch tr1</cell><cell cols="3">99.15 99.35 93.70</cell><cell>99.27</cell><cell>97.63</cell></row><row><cell>FovMax 17ch tr2</cell><cell cols="3">99.15 99.31 92.72</cell><cell>99.25</cell><cell>97.32</cell></row><row><cell>FovMax 17ch tr4</cell><cell cols="3">99.03 99.30 93.26</cell><cell>99.20</cell><cell>97.45</cell></row><row><cell>FovMax 17ch mean(tr1, tr2, tr4)</cell><cell cols="3">99.11 99.32 93.23</cell><cell>99.24</cell><cell>97.47</cell></row><row><cell>FovMax 17ch tr14</cell><cell cols="3">99.16 99.32 94.37</cell><cell>99.26</cell><cell>97.82</cell></row><row><cell>FovConc 3ch tr1</cell><cell cols="2">80.76 48.64</cell><cell>4.61</cell><cell>57.10</cell><cell>44.68</cell></row><row><cell>FovConc 3ch tr2</cell><cell cols="3">22.35 78.17 22.71</cell><cell>59.12</cell><cell>49.55</cell></row><row><cell>FovConc 3ch tr4</cell><cell cols="3">2.57 50.20 82.36</cell><cell>35.64</cell><cell>45.63</cell></row><row><cell>FovConc 3ch mean(tr1, tr2, tr4)</cell><cell cols="3">35.23 59.00 36.56</cell><cell>50.62</cell><cell>46.62</cell></row><row><cell>FovConc 17ch tr14</cell><cell cols="3">89.70 99.33 89.54</cell><cell>95.63</cell><cell>93.63</cell></row><row><cell>SWMax 17ch tr1</cell><cell cols="3">95.06 97.60 69.52</cell><cell>96.53</cell><cell>88.77</cell></row><row><cell>SWMax 17ch tr2</cell><cell cols="3">96.87 97.96 69.28</cell><cell>97.48</cell><cell>89.44</cell></row><row><cell>SWMax 17ch tr4</cell><cell cols="3">91.40 97.23 82.21</cell><cell>95.02</cell><cell>91.04</cell></row><row><cell>SWMax 17ch mean(tr1, tr2, tr4)</cell><cell cols="3">94.44 97.60 73.67</cell><cell>96.34</cell><cell>89.75</cell></row><row><cell>SWMax 17ch tr14</cell><cell cols="3">97.05 98.82 79.40</cell><cell>98.13</cell><cell>92.60</cell></row><row><cell>CNN tr1</cell><cell cols="3">88.26 50.78 11.85</cell><cell>61.46</cell><cell>49.64</cell></row><row><cell>CNN tr2</cell><cell cols="3">27.87 79.88 26.08</cell><cell>61.90</cell><cell>52.60</cell></row><row><cell>CNN tr4</cell><cell cols="3">11.45 54.35 82.59</cell><cell>40.99</cell><cell>49.79</cell></row><row><cell>CNN mean(tr1, tr2, tr4)</cell><cell cols="3">42.53 61.67 40.17</cell><cell>54.78</cell><cell>50.68</cell></row><row><cell>CNN tr14</cell><cell cols="3">88.23 99.09 73.98</cell><cell>94.94</cell><cell>88.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 :</head><label>1</label><figDesc>Average classification accuracy (%) over different size ranges of the testing data. For each type of network (FovAvg, FovMax, FovConc, SWMax or CNN), this table shows the average classification accuracy over different ranges of the size of the testing data in the MNIST Large Scale datasets, for networks trained by single-scale training for either of the training</figDesc><table><row><cell cols="2">sizes 1, 2 or 4 (denoted tr1, tr2, tr4) or multi-scale training data spanning the scale range [1, 4] (denoted tr14). The rows</cell></row><row><cell cols="2">labelled "mean(tr1, tr2, tr4)" give the average value for the training sizes 1, 2 and 4. The reported accuracy is the average</cell></row><row><cell cols="2">of the accuracy for multiple test sizes within the size ranges [1/2, 1], [1, 4], [4, 8], [1/2, 4] and [1/2, 8] with spacing 2 1/4</cell></row><row><cell>between consecutive sizes.</cell><cell></cell></row><row><cell cols="2">Single-scale training evaluated over testing sizes in [1, 4]</cell></row><row><cell>FovAvg mean(tr1, tr2, tr4)</cell><cell>99.32 %</cell></row><row><cell>FovMax mean(tr1, tr2, tr4)</cell><cell>99.32 %</cell></row><row><cell>SWMax mean(tr1, tr2, tr4)</cell><cell>97.60 %</cell></row><row><cell>CNN mean(tr1, tr2, tr4)</cell><cell>61.67 %</cell></row><row><cell>FovConc mean(tr1, tr2, tr4)</cell><cell>59.00 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>Relative ranking of the different networks for single-scale training at either of the training sizes 1, 2 or 4 evaluated over the testing size interval<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>.</figDesc><table><row><cell cols="2">Multi-scale training evaluated over testing sizes in [1, 4]</cell></row><row><cell>FovAvg tr14</cell><cell>99.40 %</cell></row><row><cell>FovConc tr14</cell><cell>99.33 %</cell></row><row><cell>FovMax tr14</cell><cell>99.32 %</cell></row><row><cell>CNN tr14</cell><cell>99.09 %</cell></row><row><cell>SWMax tr14</cell><cell>98.82 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Relative ranking of the different networks for multiscale training over the training size interval<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref> evaluated over the testing size interval<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>.</figDesc><table><row><cell cols="2">Single-scale training evaluated over testing sizes in [1/2, 4]</cell></row><row><cell>FovMax mean(tr1, tr2, tr4)</cell><cell>99.24 %</cell></row><row><cell>FovAvg mean(tr1, tr2, tr4)</cell><cell>99.20 %</cell></row><row><cell>SWMax mean(tr1, tr2, tr4)</cell><cell>96.34 %</cell></row><row><cell>CNN mean(tr1, tr2, tr4)</cell><cell>54.78 %</cell></row><row><cell>FovConc mean(tr1, tr2, tr4)</cell><cell>50.62 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Relative ranking of the different networks for single-scale training at either of the training sizes 1, 2 or 4 evaluated over the testing size interval [1/2, 4].</figDesc><table><row><cell cols="2">Multi-scale training evaluated over testing sizes in [1/2, 4]</cell></row><row><cell>FovAvg tr14</cell><cell>99.32 %</cell></row><row><cell>FovMax tr14</cell><cell>99.26 %</cell></row><row><cell>SWMax tr14</cell><cell>98.13 %</cell></row><row><cell>FovConc tr14</cell><cell>95.63 %</cell></row><row><cell>FovConc tr14</cell><cell>96.32 %</cell></row><row><cell>CNN tr14</cell><cell>94.94 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>6.7 Compact benchmarks regarding the scale generalisation</cell></row><row><cell>performance</cell></row><row><cell>Table 1 gives compact performance measures of the gen-</cell></row><row><cell>eralisation performance of the different types of networks</cell></row><row><cell>considered in the experiments on the MNIST Large Scale</cell></row><row><cell>dataset. For each type of network (FovAvg, FovMax, Fov-</cell></row><row><cell>Conc, SW or CNN), the table gives the average classifica-</cell></row><row><cell>tion accuracy over different ranges of the size of the testing</cell></row><row><cell>data, for networks trained by single-scale training, for either</cell></row><row><cell>of the training sizes 1, 2 or 4 or multi-scale training data</cell></row><row><cell>spanning the scale range</cell></row></table><note>Relative ranking of the different networks for multi- scale training over the training size interval [1, 4] evaluated over the testing size interval [1/2, 4].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Selected scale channels: FovAvg network trained for size 1 Selected scale channels: FovMax network trained for size 1Fig. 12: Visualisation of the scale selection properties of the scale-invariant FovAvg and FovMax networks, when training the network for each one of the sizes 1, 2 and 4. For each testing size, shown on the horizontal axis with increasing testing sizes towards the right, the vertical axis displays a histogram of the relative contribution of the scale channels to the winning classification, with the lowest scale at the bottom and the highest scale at the top. As can be seen from the figures, there is a general tendency of the composed classification scheme to select coarser scale levels with increasing size of the image structures, in agreement with the conceptual similarity to classical methods for scale selection based on detecting local extrema over scale or performing weighted averaging over scale of scale-normalised derivative responses.Selected scale channels: FovConc network trained for size 1 Selected scale channels: SWMax network trained for size 1Fig. 13: Visualisation of the scale selection properties of the not scale-invariant FovConc and SWMax networks, when training the network for each one of the sizes 1, 2 and 4. For each testing size, shown on the horizontal axis with increasing testing sizes towards the right, the vertical axis displays a histogram of the relative contribution of the scale channels to the winning classification, with the lowest scale at the bottom and the highest scale at the top. As can be seen from the figures, the relative contributions from the different scale levels do not as well follow a linear dependency on the size of the input structures as for the scale-invariant FovAvg and FovMax networks. Instead, for the FocConc network, there is a bias towards the size of image structures used for training, whereas for the SWMax network some scale levels dominate for fine-scale or</figDesc><table><row><cell>Selected scale channels: FovAvg network trained for size 2 Selected scale channels: FovMax network trained for size 2 Selected scale channels: FovConc network trained for size 2 Selected scale channels: SWMax network trained for size 2</cell></row><row><cell>Selected scale channels: FovAvg network trained for size 4 Selected scale channels: FovMax network trained for size 4 Selected scale channels: FovConc network trained for size 4 Selected scale channels: SWMax network trained for size 4</cell></row></table><note>(In these figures, the resolution parameter on the vertical axis represents the inverse of scale. Note that the grey-levels in the histograms are not directly comparable, since the grey-levels for each histogram are normalised with respect to the maximum and minimum values in that histogram.)coarse-scale sizes in the testing data. (In these figures, the resolution parameter on the vertical axis represents the inverse of scale. Note that the grey-levels in the histograms are not directly comparable, since the grey-levels for each histogram are normalised with respect to the maximum and minimum values in that histogram.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>. We train on the original training set and test on synthetically rescaled copies of the test set with relative scale factors in the range s ? [0.5, 2.0]. CIFAR-10 represents a dataset, where the conditions for invariance using a scale-channel network are not fulfilled, in the sense that the transformations between different training and testing sizes are not well modelled by continuous scaling transformations, as underlie the presented theory for scale-invariant scale channel networks, based on continuous models of both the image data and the image filtering operations.Fig. 14: Sample images from the rescaled CIFAR-10 testing set (of size 32?32 pixels). The images in the original CIFAR-10 testing set are rescaled for scaling factors between 1 2 and 2, with mirror extension at the image boundaries for scaling factors s &lt; 1. Top row: "frog". Bottom row: "truck". 0.50 0.59 0.71 0.84 1.00 1.19 1.41 1.68 2.00</figDesc><table><row><cell></cell><cell>0.50</cell><cell>0.59</cell><cell>0.71</cell><cell>0.84</cell><cell>1.00</cell><cell>1.19</cell><cell>1.41</cell><cell>1.68</cell><cell>2.00</cell></row><row><cell></cell><cell>80 90</cell><cell></cell><cell></cell><cell>CNN</cell><cell></cell><cell>90 80</cell><cell></cell><cell></cell><cell>FovConc 2 1/4 FovConc 2 1/2 CNN</cell></row><row><cell>Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Scale factor</cell><cell></cell><cell></cell><cell cols="4">0.50 0.59 0.71 0.84 1.00 1.19 1.41 1.68 2.00 Scale factor</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a) Standard CNN</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(b) The FovConc network</cell></row><row><cell></cell><cell>80 90</cell><cell></cell><cell></cell><cell>FovAvg 2 1/4 FovAvg 2 1/2 CNN</cell><cell></cell><cell>90 80</cell><cell></cell><cell></cell><cell>FovMax 2 1/4 FovMax 2 1/2 CNN</cell></row><row><cell>Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell><cell></cell><cell>Accuracy (%)</cell><cell>50 60 70</cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell></row><row><cell></cell><cell>30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>30</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">0.50 0.59 0.71 0.84 1.00 1.19 1.41 1.68 2.00 Scale factor</cell><cell></cell><cell cols="4">0.50 0.59 0.71 0.84 1.00 1.19 1.41 1.68 2.00 Scale factor</cell></row><row><cell></cell><cell></cell><cell cols="2">(c) The FovAvg network</cell><cell></cell><cell></cell><cell></cell><cell cols="2">(d) The FovMax network</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When analysing image data with very large scale variations, the finite receptive field of any detector and the difference in image resolution between objects observed at different scales will imply a large difference in appearance between very small and very large objects.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">A main difference between the OverFeat detector<ref type="bibr" target="#b47">[48]</ref> and our approach, however, is that the OverFeat detector uses a total effective stride of 32, whereas our network has a total effective stride of 4 (2 convolutional layers with stride 2 each). Because of the larger effective stride in the OverFeat detector, they apply their subsampling operation for every spatial offset in the last convolutional layer, whereas we with our smaller effective stride do not need to, since the subsampled image representations are still at a satisfactory resolution.<ref type="bibr" target="#b4">5</ref> Concerning images of finite size, we make use of all the available image data for computing the scale-channel representations used for the sliding window approach, implying that more pixels are processed at a fine scale compared to a coarse scale. This is in contrast to the foveated representations, which are based on using the same number pixels in the scale channels for every resolution.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">Note that for the OverFeat detector<ref type="bibr" target="#b47">[48]</ref> networks pretrained on ImageNet use a pretrained base network which precludes the problem with training a sliding window scale-channel network with batch normalisation from scratch. For the larger scale ranges evaluated here, however, using networks with pretrained weights for the scale channels gives considerably worse generalisation performance. We, here,</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Size invariance in visual object priming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">E</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Physiology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="121" to="133" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Shape representation in the inferior temporal cortex of monkeys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pauls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Current Biology</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="552" to="563" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Size and position invariance of neuronal responses in monkey inferotemporal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fujita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="218" to="226" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Perceptual learning in object recognition: Object specificity and size invariance. Vision Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Furmanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Engel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="473" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast readout of object indentity from macaque inferior temporal cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">310</biblScope>
			<biblScope unit="page" from="863" to="866" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The dynamics of invariant object recognition in the human visual system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Isik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Meyers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="page" from="91" to="102" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature detection with automatic scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="77" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Edge detection and ridge detection with automatic scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="117" to="154" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shape-adapted smoothing in estimation of 3-D shape cues from affine distortions of local 2-D structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>G?rding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="415" to="434" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Feature tracking with automatic selection of spatial scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bretzner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="385" to="392" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Local scale selection for Gaussian based description techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chomat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Verdiere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Crowley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision</title>
		<meeting>European Conf. on Computer Vision<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1842</biblScope>
			<biblScope unit="page" from="117" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reliable feature matching across widely separated views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baumberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR&apos;00</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR&apos;00</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="1774" to="1781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Scale and affine invariant interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speeded up robust features (SURF)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page" from="346" to="359" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">) of Foundations and Trends in Computer Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A Survey on Local Invariant Features</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
			<publisher>Now Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">ASIFT: A new framework for fully affine invariant image comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal of Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="438" to="469" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image matching using generalized scale-space interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A computational theory of visual receptive fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page" from="589" to="635" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Normative theory of visual receptive fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Heliyon</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1872" to="1886" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.08754</idno>
		<title level="m">Flip-rotate-pooling convolution and split dropout on convolution neural networks for image classification</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2012" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting cyclic symmetry in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">TIpooling: Transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5028" to="5037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="519" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rotation equivariant vector field networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5048" to="5057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Steerable CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d steerable CNNs: Learning rotationally equivariant features in volumetric data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Boomsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10381" to="10392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning steerable filters for rotation equivariant CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Storath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cubenet: Equivariance to 3D rotation and translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11209</biblScope>
			<biblScope unit="page" from="567" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant and Fisher discriminative convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="265" to="278" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Spherical CNNs</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Smidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kohlhoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08219</idno>
		<title level="m">Tensor field networks: Rotation-and translationequivariant neural networks for 3D point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.6369</idno>
		<title level="m">Scale-invariant convolutional neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.5104</idno>
		<title level="m">Locally scale-invariant convolutional neural networks. NIPS 2014 Deep Learning and Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scale equivariance in CNNs with vector fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lobry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.11783</idno>
	</analytic>
	<monogr>
		<title level="m">ICML/FAIM 2018 Workshop on Towards Learning with Limited Labels: Equivariance, Invariance, and Beyond</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03861</idno>
		<title level="m">Scale steerable filters for locally scaleinvariant convolutional neural networks. ICML Workshop on Theoretical Physics for Deep Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep scale-spaces: Equivariance over scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7366" to="7378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Polar transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Traffic sign recognition with multiscale convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2809" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A unified multiscale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9908</biblScope>
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Inverse compositional spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2568" to="2576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Warped convolutions: Efficient invariance to spatial transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1461" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6229</idno>
		<title level="m">OverFeat: Integrated recognition, localization and detection using convolutional networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision (ICCV</title>
		<meeting>International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Finding tiny faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="951" to="959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition</title>
		<meeting>Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690</idno>
		<title level="m">A boundary tilting persepective on the phenomenon of adversarial examples</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kouichi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08864</idno>
		<title level="m">One pixel attack for fooling deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deep convolutional networks do not classify based on global object shape</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erlikhman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Kellman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Computational Biology</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1006613</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">A rotation and a translation suffice: Fooling CNNs with simple transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02779</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<title level="m">Manitest: Are classifiers really invariant? In: British Machine Vision Conference (BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cire?an</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Rotation-invariant convolutional neural networks for galaxy morphology prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Monthly Notices of the Royal Astronomical Society</title>
		<imprint>
			<biblScope unit="volume">450</biblScope>
			<biblScope unit="page" from="1441" to="1459" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Basic theory on normalization of pattern (in case of typical one-dimensional pattern)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Iijima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bulletin of the Electrotechnical Laboratory</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="368" to="388" />
			<date type="published" when="1962" />
		</imprint>
	</monogr>
	<note>in Japanese</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Scale-space filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Witkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 8th Int. Joint Conf. Art. Intell</title>
		<meeting>8th Int. Joint Conf. Art. Intell<address><addrLine>Karlsruhe, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="page" from="1019" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The structure of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="363" to="370" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Generic neighborhood operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="597" to="605" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Scale-Space Theory in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Scale-space theory: A basic tool for analysing structures at different scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<ptr target="http://www.csc.kth.se/?tony/abstracts/Lin94-SI-abstract.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="225" to="270" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M J</forename><surname>Florack</surname></persName>
		</author>
		<title level="m">Image Structure. Series in Mathematical Imaging and Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Linear scale-space has first been proposed in Japan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weickert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Imiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="237" to="252" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Front-End Vision and Multi-Scale Image Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">On the axioms of scale space theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Florack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>De Graaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ter Haar Romeny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="267" to="298" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Generalized Gaussian scale-space axiomatics comprising linear scale-space, affine scale-space and spatiotemporal scale-space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="36" to="81" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Learning scale-variant and scaleinvariant features for deep image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Noord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Postma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="583" to="592" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Exploring the ability of CNNs to generalise to previously unseen scales over wide scale ranges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Pattern Recognition (ICPR 2020</title>
		<meeting>Int. Conf. on Pattern Recognition (ICPR 2020</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1181" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Invariance and neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Barnard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casasent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="498" to="508" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">An analysis of scale invariance in object detection -SNIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3578" to="3587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Object detection networks on convolutional feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1476" to="1481" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="472" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">ESPNet: Efficient spatial pyramid of dilated convolutions for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="552" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate CNN object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2129" to="2137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Scale-adaptive convolutions for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2031" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">ELASTIC: Improving CNNs with dynamic scaling policies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2258" to="2267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision</title>
		<meeting>International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICCV 2019</note>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Provably scale-covariant continuous hierarchical networks based on scale-normalized differential expressions coupled in cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="120" to="148" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Scale-covariant and scale-invariant Gaussian derivative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Scale Space and Variational Methods in Computer Vision (SSVM 2021</title>
		<meeting>Scale Space and Variational Methods in Computer Vision (SSVM 2021</meeting>
		<imprint>
			<publisher>Springer LNCS</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">12679</biblScope>
			<biblScope unit="page" from="3" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Scale-covariant and scale-invariant Gaussian derivative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10851-021-01057-9</idno>
		<ptr target="https://doi.org/10.1007/s10851-021-01057-9" />
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">B-spline CNNs on Lie groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szmaja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
		<title level="m">Scale-equivariant steerable networks. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11193</idno>
		<title level="m">Scaleequivariant neural networks with decomposed convolutional filters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">DISCO: Accurate discrete scale convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sosnovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moskalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Learning rotation-invariant convolutional neural networks for object detection in VHR optical remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="7405" to="7415" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Multiscale rotation-invariant convolutional neural networks for lung texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="184" to="195" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Roto-translation covariant convolutional networks for medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lafarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A J</forename><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P W</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention MICCAI</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">11070</biblScope>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Roto-translation equivariant convolutional networks: Application to histopathology image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lafarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bekkers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Duits</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page">101849</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Andrearczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Depeursinge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06889</idno>
		<title level="m">Rotational 3D texture classification using group equivariant CNNs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Visual Cortex and Deep Networks: Learning Invariant Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Anselmi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">On the generalization of equivariance and convolution in neural networks to the action of compact groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Generalized axiomatic scale-space theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Imaging and Electron Physics</title>
		<editor>Hawkes, P.</editor>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="1" to="96" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Foveal scale-space and linear increase of receptive field size as a function of eccentricity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Florack</surname></persName>
		</author>
		<idno>KTH/NA/P--94/27--SE</idno>
		<ptr target="http://www.csc.kth.se/?tony/abstracts/CVAP166.html" />
	</analytic>
	<monogr>
		<title level="j">Dept. of Numerical Analysis and Computer Science</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
	<note>KTH</note>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-03243-2242-1</idno>
		<ptr target="https://doi.org/10.1007/978-3-030-03243-2242-1" />
		<editor>Ikeuchi, K., ed.: Computer Vision. Springer</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Supervised scale-invariant segmentation (and detection)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Scale Space and Variational Methods in Computer Vision (SSVM</title>
		<meeting>Scale Space and Variational Methods in Computer Vision (SSVM<address><addrLine>Israel</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">6667</biblScope>
			<biblScope unit="page" from="350" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Maximum membership scale selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiple Classifier Systems</title>
		<imprint>
			<biblScope unit="volume">5519</biblScope>
			<biblScope unit="page" from="468" to="477" />
			<date type="published" when="2009" />
			<publisher>Springer LNCS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Scale selection properties of generalized scalespace interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="177" to="210" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<ptr target="https://www.zenodo.org/record/3820247" />
		<title level="m">MNIST Large Scale dataset. Zenodo (2020) Available at</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">Exploring the ability of CNNs to generalise to previously unseen scales over wide scale ranges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01536</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Effective scale: A natural unit for measuring scalespace lifetime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1068" to="1074" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
		<title level="m" type="main">Scale selection for supervised image segmentation. Image and Vision Computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M J</forename><surname>Tax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="991" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition</title>
		<meeting>Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Scale-space for discrete signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intell</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="234" to="254" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
