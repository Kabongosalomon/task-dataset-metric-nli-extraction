<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DeepDPM: Deep Clustering With an Unknown Number of Clusters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meitar</forename><surname>Ronen</surname></persName>
							<email>meitarr@post.bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Department of Computer Science</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahaf</forename><forename type="middle">E</forename><surname>Finder</surname></persName>
							<email>finders@post.bgu.ac.ilorenfr@cs.bgu.ac.il</email>
							<affiliation key="aff0">
								<orgName type="department">The Department of Computer Science</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Department of Computer Science</orgName>
								<orgName type="institution">Ben-Gurion University of the Negev</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DeepDPM: Deep Clustering With an Unknown Number of Clusters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Learning (DL) has shown great promise in the unsupervised task of clustering. That said, while in classical (i.e., non-deep) clustering the benefits of the nonparametric approach are well known, most deep-clustering methods are parametric: namely, they require a predefined and fixed number of clusters, denoted by K. When K is unknown, however, using model-selection criteria to choose its optimal value might become computationally expensive, especially in DL as the training process would have to be repeated numerous times. In this work, we bridge this gap by introducing an effective deep-clustering method that does not require knowing the value of K as it infers it during the learning. Using a split/merge framework, a dynamic architecture that adapts to the changing K, and a novel loss, our proposed method outperforms existing nonparametric methods (both classical and deep ones). While the very few existing deep nonparametric methods lack scalability, we demonstrate ours by being the first to report the performance of such a method on ImageNet. We also demonstrate the importance of inferring K by showing how methods that fix it deteriorate in performance when their assumed K value gets further from the ground-truth one, especially on imbalanced datasets. Our code is available at https://github.com/BGU-CS-VIL/DeepDPM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Clustering is an important unsupervised-learning task where, unlike in the supervised case of classification, class labels are unavailable. Moreover, in the purely-unsupervised (and more realistic) setting this work focuses on, the number of classes, denoted by K, and their relative sizes (i.e., the class weights) are unknown too.</p><p>Acknowledgements. This work was supported by the Lynn and William Frankel Center at BGU CS, by the Israeli Council for Higher Education via the BGU Data Science Research Center, and by Israel Science Foundation Personal Grant #360/21. M.R. was also funded by the VATAT National excellence scholarship for female Master's students in Hi-Tech-related fields. . Mean clustering accuracy of 3 runs (? std. dev.) on ImageNet50. The Ground Truth K is 50. Parametric methods such as K-means, DCN++ (an improved variant of <ref type="bibr" target="#b70">[71]</ref>) and SCAN <ref type="bibr" target="#b63">[64]</ref>, require knowing K. When given a poor estimate of K, they deteriorate in performance in a balanced dataset (a) and even more so in an imbalanced dataset (b). In contrast, the proposed DeepDPM does not require knowing K (it infers its value; e.g., K = 55.3 ? 1.53 in (a) and 46.3 ? 2.52 in (b)) and yet yields comparable results.</p><p>The emergence of Deep Learning (DL) has not skipped clustering tasks. DL methods usually cluster large and highdimensional datasets better and more efficiently than classical (i.e., non-deep) clustering methods <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b70">71]</ref>. That said, while in classical clustering it is well understood that nonparametric methods (namely, methods that find K) have advantages over parametric ones (namely, methods that re-quire a known K) <ref type="bibr">[8,</ref><ref type="bibr" target="#b56">57]</ref>, there are only a few nonparametric deep clustering methods. Unfortunately, the latter are neither scalable nor effective enough. Our work bridges this gap by proposing an effective deep nonparametric method, called DeepDPM. In fact, even when K is known, DeepDPM still achieves results comparable to leading parametric methods (especially in imbalanced cases) despite their "unfair" advantage; see, e.g., <ref type="figure" target="#fig_3">Figure 1</ref> or ? 5.</p><p>More generally, the ability to infer the latent K has practical benefits, including the following ones. 1) Without a good estimate of K, parametric methods might suffer in performance. <ref type="figure" target="#fig_3">Figure 1</ref> shows that using the wrong K can have a significant negative effect on parametric methods in both balanced and imbalanced datasets. When the value of K becomes more and more inaccurate, even a State-Of-The-Art (SOTA) parametric deep clustering method, SCAN <ref type="bibr" target="#b63">[64]</ref>, deteriorates in performance significantly. 2) Changing K during training has positive optimization-related implications; e.g., by splitting a single cluster into two, multiple data labels are changed simultaneously. This often translates to large moves on the optimization surface which may lead to convergence to better local optima and performance gains <ref type="bibr">[10]</ref>; e.g., in ? 5 we demonstrate cases where nonparametric methods, ours included, outperform parametric ones even when the latter are given the true K. 3) A common workaround to not knowing K is to use model selection: namely, run a parametric method numerous times, using different K values over a wide range, and then choose the "best" K via an unsupervised criterion. That approach, however, besides missing the aforementioned potential gains (not being able to make large moves), does not scale and is usually infeasible for large datasets, especially in DL. Moreover, the negative societal impact of the model-selection approach must be noted as well: training a deep net tens or hundreds of times on a large dataset consumes prohibitively-large amounts of energy. 4) K itself may be a sought-after quantity of importance.</p><p>Bayesian nonparametric (BNP) mixture models, exemplified by the Dirichlet Process Mixture (DPM) model, offer an elegant, data-adaptive, and mathematically-principled solution for clustering when K is unknown. However, the high computational cost typically associated with DPM inference is arguably why only a few works tried to use it in conjunction with deep clustering (e.g., <ref type="bibr">[11,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74]</ref>). Here we propose to combine the benefits of DL and the DPM effectively. The proposed method, DeepDPM, uses splits and merges of clusters to change K together with a dynamic architecture to accommodate for such changes. It also uses a novel amortized inference for Expectation-Maximization (EM) algorithms in mixture models. DeepDPM can be incorporated in deep pipelines that rely on clustering (e.g., for feature learning). Unlike an offline clustering step (e.g., K-means), DeepDPM is differentiable during most of the training (the exception is during the discrete splits/merges) and thus supports gradi-ent propagation through it. DeepDPM outperforms existing nonparametric clustering methods (both classical and deep ones) across several datasets and metrics. It also handles class imbalance gracefully and scales well to large datasets. While we focus on clustering and not feature learning, we also show examples of clustering on pretrained features as well as jointly learning features and clustering in an endto-end fashion. To summarize, our key contributions are: 1) A deep clustering method that infers the number of clusters. 2) A novel loss that enables a new amortized inference in mixture models. 3) A demonstration of the importance, in deep clustering, of inferring K. 4) Our method outperforms existing nonparametric clustering methods and we are the first to report results of a deep nonparametric clustering method on a large dataset such as ImageNet <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Parametric Deep Clustering methods. Recent such works can be divided into two types: two-step approaches and end-to-end ones. In the former, clustering is performed on features extracted in a pretext task. For instance, Mc-Conville et al. <ref type="bibr" target="#b46">[47]</ref> run K-means on the embeddings, transformed by UMAP <ref type="bibr" target="#b47">[48]</ref>, of a pretrained Autoencoder (AE). While not scalable, <ref type="bibr" target="#b46">[47]</ref> achieves competitive results when it is applicable. Another example is SCAN <ref type="bibr" target="#b63">[64]</ref> which uses unsupervised pretrained feature extractors (e.g., MoCo <ref type="bibr">[13]</ref> and SimCLR <ref type="bibr">[12]</ref>). While reaching SOTA results, SCAN, being parametric, depends on having an estimate of K and, as we show, deteriorates in performance when the estimate is too inaccurate. Moreover, SCAN assumes uniform class weights (i.e. a balanced dataset) and that is often unrealistic in purely-unsupervised cases.</p><p>End-to-end deep methods jointly learn features and clustering, possibly by alternation. Several works use an AE, or a Variational AE (VAE), with an additional clustering loss <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref>; e.g., DCN <ref type="bibr" target="#b70">[71]</ref> runs K-means on the embeddings of a pretrained AE, and retrains it with a loss consisting of a reconstruction term and a clustering-based term, to simultaneously update the features, clusters' centers, and assignments. Other works, e.g. <ref type="bibr">[5,</ref><ref type="bibr">6]</ref>, use convolutional neural nets to alternately learn features and clustering.</p><p>While our work focuses on clustering, not feature learning, we demonstrate how it can also be incorporated with the two approaches above. Moreover, all the methods above assume a predefined and fixed K and, at least the more effective ones among them, take substantial time and resources to train (so searching for the "right" K using model selection is costly and/or inapplicable).</p><p>Nonparametric Classical Clustering. Closely related to our work is BNP clustering and, more specifically, the DPM model <ref type="bibr">[1,</ref><ref type="bibr" target="#b23">24]</ref>. While many computer-vision works rely on BNP clustering <ref type="bibr">[4, 9, 14, 25-28, 30, 32, 33, 38, 39, 41, 44-46, 49, 53-59, 62]</ref>, it has yet to become a mainstream choice, partly due to the lack of efficient large-scale inference tools. Fortunately, this is starting to change; see, e.g., the highly-effective DPM sampler from <ref type="bibr" target="#b20">[21]</ref> (a modern and scalable implementation of the DPM sampler from <ref type="bibr">[10]</ref>) or the scalable streaming DPM inference in <ref type="bibr" target="#b19">[20]</ref>. Of note, an important alternative to sampling is variational DPM inference <ref type="bibr">[3,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b41">42]</ref>. A non-Bayesian example of a popular nonparametric method is DBSCAN <ref type="bibr" target="#b22">[23]</ref> which is density-based and groups together closely-packed points. While DBSCAN has efficient implementations, it is highly sensitive to its hyperparameters which are hard to tune.</p><p>Nonparametric Deep Clustering. Among the very few examples of deep methods that also find K are <ref type="bibr">[11,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b73">74]</ref>. Some of them use an offline DPM inference for pseudolabels for fine-tuning a deep belief network <ref type="bibr">[11]</ref>, or an AE <ref type="bibr" target="#b65">[66]</ref> (similarly to the parametric methods in <ref type="bibr">[5,</ref><ref type="bibr">6,</ref><ref type="bibr" target="#b70">71]</ref>). As the methods in <ref type="bibr" target="#b65">[66]</ref> and <ref type="bibr">[11]</ref> rely on slow DPM samplers, they do not scale to large datasets. AdapVAE <ref type="bibr" target="#b73">[74]</ref> uses a DPM prior for a VAE. In DCC <ref type="bibr" target="#b51">[52]</ref>, feature learning and clustering are performed simultaneously like in <ref type="bibr" target="#b73">[74]</ref>; however, instead of ELBO minimization, DCC uses a nearest-neighbor graph to group points that are close in the latent space of an AE. Our method is empirically more effective than <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b73">74]</ref> and also scales much better. While not a clustering method per-se, and similarly to <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b64">[65]</ref> uses an AE and t-SNE <ref type="bibr" target="#b62">[63]</ref> to find K. Like <ref type="bibr" target="#b46">[47]</ref>, however, <ref type="bibr" target="#b64">[65]</ref> does not scale. In <ref type="bibr" target="#b21">[22]</ref>, a deep net is simultaneously trained on a family of losses instead of a single one. At least in theory, that approach may be adapted to nonparametric clustering but this direction has yet to be explored. Both <ref type="bibr" target="#b59">[60]</ref> and <ref type="bibr" target="#b49">[50]</ref> do not assume a known K, where the former focuses on clustering faces and the latter on generating posterior samples of cluster labels for any new dataset. Unlike our method, however, <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b59">60]</ref> are supervised. Similarly, <ref type="bibr">[2]</ref> iteratively forms clusters by sequentially examining each sample against the members of existing clusters. The clustering criterion is based on a supervised evaluation net. Lastly, while <ref type="bibr" target="#b72">[73]</ref> relies on a BNP mixture, their method (and code) still uses a fixed K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries: DPGMM-based Clustering</head><formula xml:id="formula_0">Let X = (x i ) N i=1 denote N data points in R d .</formula><p>The clustering task aims to partition X into K disjoint groups, where z i is the point-to-cluster assignment, known as the cluster label, of x i . Cluster k consists of all the points labeled as k; i.e., (x i ) i:zi=k . The number of clusters, K ? |{k : k ? z}|, is thus the number of unique elements in z = (z i ) N i=1 . The classical Gaussian Mixture Model (GMM) has a BNP extension: the Dirichlet Process GMM (DPGMM) <ref type="bibr">[1,</ref><ref type="bibr" target="#b23">24]</ref>. Informally, the DPGMM (a specific case of the DPM) entertains the notion of a mixture with infinitely-many Gaussians:</p><formula xml:id="formula_1">p(x|(? k , ? k , ? k ) ? k=1 ) = ? k=1 ? k N (x; ? k , ? k ) (1) where N (x; ? k , ? k )</formula><p>is a Gaussian probability density function (pdf) (of mean ? k ? R d and a d-by-d covariance matrix ? k ) evaluated at x ? R d , ? k &gt; 0 ?k, and ? k=1 ? k = 1. For a gentle introduction to the DPGMM with a computervision audience in mind, see <ref type="bibr">[8,</ref><ref type="bibr" target="#b56">57]</ref>. Let ? k = (? k , ? k ) denote the parameters of Gaussian k. Note the distinction between component k (namely, the k-th Gaussian, identified with its parameter, ? k ) and cluster k. The components, ? = (? k ) ? k=1 , and weights, ? = (? k ) ? k=1 , are assumed to be drawn (independently) from their own prior distributions: the weights, ?, are drawn using the Griffiths-Engen-McCloskey stick-breaking process (GEM) <ref type="bibr" target="#b50">[51]</ref> with a concentration parameter ? &gt; 0, while the parameters, (? k ) ? k=1 , are independent and identically-distributed (i.i.d.) draws from their prior p(? k ), typically a Normal-Inverse Wishart (NIW) distribution. While there are infinitely-many components, note that there are still finitely-many clusters as the latent random variable K is bounded above by N . By possibly renaming cluster indices, we may assume without loss of generality that {k : k ? z} = {1, 2, . . . , K}.</p><p>The DPGMM is often used in clustering when K is unknown. DPGMM inference methods typically seek to find z = (z i ) N i=1 (which implies K) and (? k , ? k ) K k=1 . As explained in our supplementary material (Supmat), the inferred value of K is affected by the following factors: X , ?, and the NIW hyperparameters. Our method ( ? 4) is inspired in part by Chang and Fisher III's DPM sampler <ref type="bibr">[10]</ref> which consists of a split/merge framework <ref type="bibr" target="#b36">[37]</ref> (which we adopt) and a restricted sampler (which is less relevant to our work).</p><p>The split/merge framework augments the latent variables, (? k ) ? k=1 , ?, and (z i ) N i=1 , with auxiliary variables. To each z i , an additional subcluster label, z i ? {1, 2}, is added. To each ? k , two subcomponents are added, ? k,1 , ? k,2 , with nonnegative weights ? k = ( ? k,j ) j?{1,2} (where ? k,1 + ? k,2 = 1), forming a 2-component GMM. Next, splits and merges allow changing K via the Metropolis-Hastings framework <ref type="bibr" target="#b28">[29]</ref>. That is, during the inference, every certain amount of iterations the split of cluster k into its subclusters is proposed. That split is accepted with probability min(1, H s ) where</p><formula xml:id="formula_2">H s = ??(N k,1 )f x (X k,1 ; ?)?(N k,2 )f x (X k,2 ; ?) ?(N k )f x (X k ; ?)<label>(2)</label></formula><p>is the Hastings ratio, ? is the Gamma function, X k = (x i ) i:zi=k stands for the points in cluster k, N k = |X k |, X k,j = (x i ) i:(zi, zi)=(k,j) denotes the points in subcluster j (j ? {1, 2}), N k,j = |X k,j |, and f x (?; ?) is the marginal likelihood where ? represents the NIW hyperparameters. See our Supmat for more details. Upon a split proposal acceptance, each of the newly-born clusters is augmented with two subclusters. This ratio, H s , can be interpreted as comparing the marginal likelihood of the data under the two subclusters with its marginal likelihood under the cluster. Merge proposals are handled similarly (see Supmat).</p><p>--- <ref type="figure">Figure 2</ref>. DeepDPM's pipeline: given features X , the clustering net outputs cluster assignments, R, while the subclustering nets generate subcluster assignments, R. Upon the acceptance of split/merge proposals, all those nets are updated during the learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Proposed Method: DeepDPM</head><p>DeepDPM can be viewed as a DPM inference algorithm. Inspired by <ref type="bibr">[10]</ref>, we use splits and merges to change K where for every cluster we maintain a subcluster pair. For a nominal value of K, rather than resorting to sampling as in <ref type="bibr">[10]</ref>, we use a deep net trained by a novel amortized inference for EM <ref type="bibr">[16]</ref> in a mixture model. DeepDPM has two main parts. The first is a clustering net, while the second consists of K subclustering nets (one for each cluster k, k ? {1, . . . , K}). In ? 4.1 we describe how DeepDPM operates given a nominal value of K and in ? 4.2 how K is changed and how our architecture adapts accordingly. We discuss the amortized-inference aspects of our approach in ? 4.3, our weak prior in ? 4.4, and how DeepDPM may be combined with feature learning in ? 4.5. <ref type="figure">Figure 2</ref> depicts the overall pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">DeepDPM Under a Fixed K</head><p>We start by describing DeepDPM's forward pass during the training. Given a current value of K, the data is first passed to the clustering net, f cl , which generates, for each data point x i , K soft cluster assignments:</p><formula xml:id="formula_3">f cl (X ) = R = (r i ) N i=1 r i = (r i,k ) K k=1 (3)</formula><p>where r i,k ? [0, 1] is the soft assignment of x i to cluster k (also called the responsibility of cluster k to data point</p><formula xml:id="formula_4">x i ) and K k=1 r i,k = 1. From (r i ) N i=1 we compute the hard assignments z = (z i ) N i=1 by z i = arg max k r i,k . Next, each subclustering net, f k sub (where k ? {1, . . . , K})</formula><p>, is fed with the data (hard-) assigned to its respective cluster (i.e., f k sub is fed with X k = (x i ) i:zi=k ) and generates soft subcluster assignments:</p><formula xml:id="formula_5">f k sub (X k ) = R k = ( r i ) i:zi=k r i = ( r i,j ) 2 j=1<label>(4)</label></formula><p>where r i,j ? [0, 1] is the soft assignment of x i to subcluster j (j ? {1, 2}), and r i,1 + r i,2 = 1 ?k ? {1, . . . , K}. As detailed in ? 4.2, the subclusters learned by (f k sub ) K k=1 are used in split proposals. Each of the K + 1 nets (f cl and (f k sub ) K k=1 ) is a simple multilayer perceptron with a single hidden layer. The last layer of f cl has K neurons while the last layer of each f k sub has two. We now introduce a new loss motivated by EM in the Bayesian GMM (though the idea, in fact, also holds in the non-Bayesian GMM case, as well as for EM in parametric mixtures with non-Gaussian components). Concretely, in each epoch, our clustering net is optimized to generate soft assignments that would resemble those obtained by an E step of the EM-GMM algorithm (recall that the E steps of the Bayesian and non-Bayesian EM-GMM coincide). Each E step is followed by a standard M step in a Bayesian GMM, except that the soft assignments used in the Maximum-a-Posterior (MAP) estimates are those produced by our clustering net. We now provide the details. For each x i and each k ? {1, . . . , K} we compute the (standard) E-step probabilities,</p><formula xml:id="formula_6">r E i = (r E i,k ) K k=1 , where r E i,k = ? k N (x i ; ? k , ? k ) K k ? =1 ? k ? N (x i ; ? k ? , ? k ? ) k ? {1, . . . , K} (5) is computed using (? k , ? k , ? k ) K k=1 from the previous epoch. Note that K k=1 r E i,k = 1.</formula><p>We then encourage f cl to generate similar soft assignments using the following new loss:</p><formula xml:id="formula_7">L cl = N i=1 KL(r i ?r E i )<label>(6)</label></formula><p>where KL is the Kullback-Leibler divergence. Next, after every epoch we perform a Bayesian M step but with a twist. Recall that in this step, one uses the weighted versions of the MAP estimates of (? k , ? k , ? k ) K k=1 (computed using standard formulas; see Supmat) where the weights are the r E i,k values (Eq. <ref type="formula" target="#formula_16">(5)</ref>). We apply the same formulas but instead of r E i,k we use the r i,k values (i.e., the output of f cl ). Note that unlike methods (e.g. K-means or SCAN) that enforce/assume uniformity of the weights, our inferred cluster weights, (? k ) K k=1 , are allowed to deviate from uniformity. In principle, for (f k sub ) K k=1 we could have used a loss similar to L cl . However, here we prefer an isotropic loss:</p><formula xml:id="formula_8">L sub = K k=1 N k i=1 2 j=1 r i,j ?x i ? ? k,j ? 2 ?2<label>(7)</label></formula><p>where N k = |X k | and ? k,j is the mean of subcluster j of cluster k, computed after every epoch, alongside with subcluster weights and covariances, using weighted MAP estimates similarly to the clusters' case (see Supmat). This loss is more efficient than the KL loss while the latter (only in the subcluster case) did not yield improvement. The iterative process described above needs to be initialized. We do this using K-means (using some initial value of K for the clustering and K = 2 for the subclustering). DeepDPM is fairly robust to the initial K so the latter can be chosen arbitrarily (see, e.g., ? 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Changing K via Splits and Merges</head><p>During training, we use splits and merges to change K (as in <ref type="bibr">[10]</ref>). Every few epochs, we propose either splits or merges. Since K is changing, the architecture, and more specifically, the last layer of the clustering net and the number of subclustering nets, must change too. Of note, the splits/merges facilitate not only changing the value of K but also large moves, escaping many poor local optima <ref type="bibr">[10]</ref>.</p><p>Splits. In every split step, we propose to split each of the clusters into its two subclusters. A split proposal is accepted stochastically (as in <ref type="bibr">[10]</ref>) with probability min(1, H s ); see Eq. (2). To accommodate for the increase in K, if a split proposal is accepted for cluster k, the k-th unit in the last layer of the clustering net, together with the weights connecting it to the previous hidden layer, is duplicated, and we initialize the parameters of the two new clusters using the parameters learned via the subcluster nets:</p><formula xml:id="formula_9">? k1 ? ? k,1 , ? k1 ? ? k,1 , ? k1 ? ? k ? ? k,1 ? k2 ? ? k,2 , ? k2 ? ? k,2 , ? k2 ? ? k ? ? k,2 (8)</formula><p>where k 1 and k 2 denote the indices of the new clusters. We then also add, to each new cluster, a new subclustering net (dynamically allocating the memory).</p><p>Merges. When considering merges we must ensure we never simultaneously accept the proposals of, e.g., merging clusters k 1 and k 2 and the merging of clusters k 2 and k 3 , thereby mistakenly merging three clusters together. Thus, unlike split proposals which are done in parallel, not all possible merges can be considered simultaneously. To avoid sequentially considering all possible merges, we consider (sequentially) the merges of each cluster with only its 3 nearest neighbors. Merge proposals are accepted/rejected using a Hastings ratio, H m = 1/H s (as in <ref type="bibr">[10]</ref>). If a proposal is accepted, the two clusters are merged and a new subclustering network is initialized. Technically, one of the merged clusters' last layer's units, together with the net's weights connecting it to the previous hidden layer, is removed from f cl , and the parameters and the weight of the newly-born cluster are initialized using the weighted MAP estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Amortized EM Inference</head><p>Suppose we turn off splits/merges and use the Ground Truth (GT) K. Seemingly, this reduces each training epoch to mimicking a single EM iteration. Remarkably, however, and as shown in ? 5, even then our method still yields results that are usually better than the standard EM. We hypothesize that this stems from the fact that we amortize the EM inference; by the virtue of the smoothness of the function learned by the deep net, we improve the prediction for the points in not only the current batch but also other batches. Moreover, the smoothness also serves as an inductive bias such that points which are close in the observation space should have similar labels.</p><p>In principle, instead of using our variational loss we could have also used the GMM negative log likelihood (or log posterior). However, empirically that led to unstable optimization and/or poor results. Moreover, basing our loss on matching soft labels rather than likelihood/posterior elegantly makes the method more general: f cl and L cl can be used as they are for any component type, not just Gaussians.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">A Weak Prior: Letting the Data Speak for Itself</head><p>Recall that the inferred K depends on X , ?, and the NIW hyperparameters. We intentionally choose the prior to be very weak. Meaning, we choose ? as well as the socalled pseudocounts (two of the NIW hyperparameters) to be very low numbers, dwarfed by N , the number of points (see Supmat for details). Thus, we let the data, X , to be the most dominant factor in determining K. The weak prior also means that the Bayesian EM-GMM nearly coincides with the non-Bayesian EM-GMM but still helps in the presence of a degenerate sample covariance or very small clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Feature extraction</head><p>To show the effectiveness of our clustering method, we used two types of feature-extraction paradigms: an endto-end approach, where features and clustering are learned jointly (using alternate optimization), and a two-step approach in which features are learned once (before the clustering) and then held fixed. For the two-step approach, we follow SCAN <ref type="bibr" target="#b63">[64]</ref> and use MoCo <ref type="bibr">[13]</ref> for (unsupervised) feature extraction. For more details, as well as the scheme we use for an end-to-end feature extraction, see Supmat.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>In this section we evaluate DeepDPM and compare it with several key methods on popular image and text datasets at varying scales. In our evaluations we use three common metrics: clustering accuracy (ACC); Normalized Mutual Information (NMI); Adjusted Rand Index (ARI). The higher the better in all three, and they can accommodate for differences between the inferred K and its GT value. See Supmat for more details on the experimental setup and the values of the hyperparameters that we used. Due to space limits, we omit here the NMI and ARI values in several comparisons but these appear in the Supmat. We round the results to 2 decimal places so, e.g., a standard deviation (std. dev.) of .00 may still represent a positive (albeit small) number.</p><p>Comparing with Classical Methods. We compared DeepDPM with classical parametric methods <ref type="table">(K-means;   NMI   ARI  ACC  NMI  ARI  ACC  NMI  ARI  ACC</ref> MNIST <ref type="bibr" target="#b17">[18]</ref> USPS <ref type="bibr" target="#b34">[35]</ref> Fashion-MNIST <ref type="bibr">[</ref> GMM) and nonparametric ones (DBSCAN <ref type="bibr" target="#b22">[23]</ref>, moVB <ref type="bibr" target="#b33">[34]</ref>; the SOTA DPM sampler from <ref type="bibr" target="#b20">[21]</ref>). For feature extraction, we performed the process suggested in <ref type="bibr" target="#b46">[47]</ref>. We performed the evaluation on the MNIST, USPS, and Fashion-MNIST datasets, as well their imbalanced versions (the latter are defined in the Supmat). All the methods used the same (and fixed) data embeddings as input, and the parametric ones were given the GT K, given them an unfair advantage. <ref type="table" target="#tab_4">Table 1</ref> shows that DeepDPM almost uniformly dominates across all datasets and metrics, and its performance gain only increases in the imbalanced cases. It is also observable that, compared with the parametric methods, the nonparametric ones (ours included) are less affected by the imbalance. Moreover, <ref type="table">Table 2</ref> shows that among the nonparametric methods, DeepDPM's inferred K is the closest to the GT K (see Supmat for similar results in the imbalanced case).</p><p>Comparing with Deep Nonparametric Methods. As there exist very few deep nonparametric methods, and some of them reported results only on extremely-small toy datasets <ref type="bibr">[11,</ref><ref type="bibr" target="#b65">66]</ref> (e.g., one of them stated they could not pro-cess even MNIST's train dataset as it was too large for them), we compared DeepDPM with DCC <ref type="bibr" target="#b51">[52]</ref> and AdapVAE <ref type="bibr" target="#b73">[74]</ref>, the only unsupervised deep nonparametric methods that can at least handle the MNIST <ref type="bibr" target="#b17">[18]</ref>, USPS <ref type="bibr" target="#b34">[35]</ref>, and STL-10 <ref type="bibr">[15]</ref> datasets. As both those methods jointly learn features and clustering, and to show the flexible nature of DeepDPM, we demonstrate its integration with two feature-extraction techniques (described in ? 4.5): an end-to-end pipeline (for MNIST and REUTERS-10k [43]) and a two-step approach using features pretrained by MoCo <ref type="bibr">[13]</ref> (for STL-10). Unfortunately, we could not run AdapVAE's published code, and thus resort to including the results reported by them. For DCC, using their code we could reproduce their results only on MNIST, so we compare with both the results we managed to obtain using their code and the ones reported by them. Due to these reproducibility issues, we could compare with those methods only on the original (i.e., balanced) datasets. <ref type="table">Table 3</ref> shows that DeepDPM outperforms both DCC and AdapVAE. Note we could not find other unsupervised deep nonparametric methods (let alone with available code) that scale to even these fairly-small datasets.</p><p>Clustering the Entire ImageNet Dataset. On Ima-geNet, we obtained the following results: ACC: 0.25, NMI: 0.65, ARI: 0.14. Our method was initialized with K = 200 and converged into 707 clusters (GT=1000). These are first results on ImageNet reported for deep nonparametric clustering. <ref type="figure">Figure 3</ref> shows examples of images clustered together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">The Value of Deep Nonparametric Methods</head><p>When Parametric Methods Break. We study the effect of not knowing K on parametric methods, with and without MNIST <ref type="bibr" target="#b17">[18]</ref> STL-10 <ref type="bibr">[15]</ref> Reuters10k class imbalance. We evaluate each method with a wide range of different K values on ImageNet-50. The latter, curated in <ref type="bibr" target="#b63">[64]</ref>, consists of 50 randomly-selected classes of ImageNet <ref type="bibr" target="#b16">[17]</ref>. To generate an imbalanced version of it, we sampled a normalized nonuniform histogram from a uniform distribution over the 50-dimensional probability simplex (i.e., all histograms were equally probable) and then sampled examples from the 50 classes in proportions according to that nonuniform histogram. We compared with 3 parametric methods: 1) K-means; 2) the SOTA method SCAN <ref type="bibr" target="#b63">[64]</ref>; 3) an improved version of DCN <ref type="bibr" target="#b70">[71]</ref>, self-coined DCN++, where instead of training an AE on the raw data, we trained it on top of the embeddings SCAN uses (MoCo <ref type="bibr">[13]</ref>) where, following <ref type="bibr" target="#b63">[64]</ref>, we froze those embeddings during training. For DeepDPM, we used the same features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Final Since SCAN requires large amounts of memory (e.g., we could only run it on 2 RTX-3090 GPU cards with 24GB memory each, compared with DeepDPM for which a single RTX-2080 (or even GTX-1080) with 8GB sufficed), and due to resource constraints, we were limited in how many K values we could run SCAN with and in the number of times each experiment could run (this high computational cost is one of the problems with model selection in parametric methods). Thus, we collected the results of the parametric methods with K values ranging from 5 to 350. For both the balanced and imbalanced cases, we initialized DeepDPM with K = 10. <ref type="figure" target="#fig_3">Figure 1</ref> summarizes the ACC results (see Supmat for ARI/NMI). As the K value used by the parametric methods diverges from the GT (i.e., K = 50), their results deteriorate. Unsurprisingly, when using the GT K, or sufficiently close to it, the parametric methods outperform our ACC nonparametric one, confirming our claim that having a good estimate of K is important for good clustering. <ref type="figure" target="#fig_3">Figure 1a</ref>, however, shows that even with fairly-moderate deviates from the GT K, DeepDPM's result (0.66?.01) surpasses the leading parametric method. Moreover, <ref type="figure" target="#fig_3">Figure 1</ref> shows that the parametric SCAN is sensitive to class imbalance; e.g., in <ref type="figure">Figure</ref> 1b, SCAN performs best when K = 30 suggesting it is due to ignoring many small classes. In contrast, DeepDPM (scoring 0.60 ? .01) is fairly robust to these changes and is comparable in results to SCAN when the latter was given the GT K. In addition, we also show in <ref type="table">Table 4</ref> the performance of other nonparametric methods (3 runs on the same features as ours: MoCo+AE). We include DeepDPM's results with alternation (between clustering and feature learning) and without (i.e., holding the features frozen and training Deep-DPM only once). <ref type="table">Table 5</ref> compares the K values found by the nonparametric methods. DeepDPM inferred a K value close to the GT in both the balanced and imbalanced cases.</p><formula xml:id="formula_10">K init =3 K init =10 K init =30</formula><p>In the imbalanced case, moVB scored a slightly better K but its results (see <ref type="table">Table 4</ref>) were worse. For the parametric methods, <ref type="table">Table 5</ref> also shows the K value of the best silhouette score. The unsupervised silhouette metric is commonly used for model selection (NMI/ACC/ARI are supervised, hence inapplicable for model selection). As <ref type="table">Table 5</ref> shows, DeepDPM yielded a more accurate K than that approach. Running Times. Our running time is comparable with a single run of SCAN (the SOTA deep parametric method); e.g., on ImageNet-50, SCAN (with 2 NVIDIA 3090 GPUs) trains for ?8 [hr] while ours (with 1 weaker NVIDIA 2080 GPU) takes ?11 [hr]. However, training SCAN multiple times with a different K each time (as needed for model selection) took more than 3 days. Thus, DeepDPM's value and positive environmental impact are clear. <ref type="table">Table 6</ref> quantifies the performance gains due to the different parts of DeepDPM through an ablation study done on Fashion-MNIST (in the setting described earlier). It shows the effect of disabling splits, merges and both; e.g., merges  help even when initializing with K = 3. In fact, the large moves made by splits/merges help even when K init = 10. Also, replacing the subclustering nets with K-means (using K = 2) results in deterioration. Likewise, either turning off the priors when computing the cluster parameters, or using an isotropic loss instead of L cl , hurts performance and (while not shown here) often destabilizes the optimization. Finally, <ref type="figure" target="#fig_2">Figure 4</ref> demonstrates, on three different datasets, DeepDPM's robustness to the initial K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Ablation Study and Robustness to the Initial K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Limitations. As with most clustering methods, if Deep-DPM's input features are poor it would struggle to recover. Also, if K is known and the dataset is balanced, parametric methods (e.g., SCAN) may be a slightly better choice.</p><p>Future work. An interesting direction is adapting Deep-DPM to streaming data (e.g., similarly to how <ref type="bibr" target="#b19">[20]</ref> handled streaming DPM inference) or hierarchical settings <ref type="bibr">[7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr">61]</ref>. Moreover, our results may improve given a more sophisticated framework for building split proposals (e.g., see <ref type="bibr" target="#b66">[67]</ref>).</p><p>Broader impact. We hope our work will inspire the deepclustering community to adopt the nonparametric approach as well as raise awareness to issues with the parametric one. Nonparametrics also has an environmental positive impact: obviating the need to repeatedly train deep parametric methods for model selection drastically reduces resource usage.</p><p>Summary. We presented a deep nonparametric clustering method, a dynamic architecture that adapts to the varying K values, and a novel loss based on new amortized inference in mixture models. Our method outperforms deep and non-deep nonparametric methods and achieves SOTA results. We demonstrated the issues with parametric clustering, especially the sensitivity to the assumed K, and the added value the nonparametric approach brings to deep clustering. We showed the robustness of our method to both class imbalance and the initial K. Finally, we demonstrated the scalability of DeepDPM by being the first method of its kind to report results on ImageNet. Our code is publicly available.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Comparison with Classical Clustering Methods: the Inferred K Value in the Imbalanced Case</head><p>In the paper, we showed how DeepDPM outperforms classical (i.e. non-deep) clustering methods, including K-means, GMM, DBSCAN <ref type="bibr">[7]</ref>, moVB <ref type="bibr">[9]</ref> and a State-Of-The-Art (SOTA) DPM sampler <ref type="bibr">[6]</ref>. We also showed how, in the balanced case, DeepDPM inferred a more accurate estimate of K. Here, to complete the picture, we provide the results for inferring K in the imbalanced setting. In this case too, DeepDPM inferred the most accurate K value; see <ref type="table" target="#tab_4">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">When Parametric Methods Break: the ARI and NMI Metrics</head><p>In the paper, we investigated how parametric methods operate with an unknown K, on both balanced and imbalanced datasets. The parametric methods we compared with were K-means, DCN++, an improved variant of DCN <ref type="bibr">[16]</ref>, and SCAN <ref type="bibr">[14]</ref>. As the reader may recall, we ran the parametric methods (which require specifying K) with different K values ranging between 5 and 350 (the exact values we used were: <ref type="bibr">5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, and 350)</ref>. As DeepDPM is a nonparametric method, instead of using a known fixed value of K, it infers it. Specifically, on the ImageNet-50 dataset, DeepDPM inferred K = 55.3 ? 1.53 (the mean and std. dev. across 3 different runs) in the balanced case and 46.3 ? 2.52 in the imbalanced one. In both cases, our results were fairly close to the GT value (K = 50).</p><p>Here we provide the complementary metrics which were not shown in the paper due to page limits: the ARI (see <ref type="figure" target="#fig_6">Figure 9</ref>) and NMI (see <ref type="figure" target="#fig_3">Figure 10</ref>) metrics.</p><p>While the ARI (similarly to clustering accuracy) penalizes over-and under-clustering similarly, the NMI metric is not as sensitive to over-clustering as it is to under-clustering. For more details, see ? 2.1.2. Thus, the NMI score of parametric methods remains relatively-stable when K ? <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">250]</ref> for both the balanced case and the imbalanced one. Moreover, as can be seen in <ref type="figure" target="#fig_3">Figure 10a</ref>, for the parametric SOTA method, SCAN, the NMI misleadingly peaks at K = 70 (recall the true K is 50). Despite not having access to the additional information used (and required) by the parametric methods -that is, the value of K -and despite the fact that NMI is relatively insensitive to over-clustering, DeepDPM still reaches comparable performance to the parametric methods in both the ARI and NMI metrics, especially in the imbalanced case.  <ref type="figure" target="#fig_3">Figure 10</ref>. Mean NMI of 3 runs (? std. dev.) on 50 classes of ImageNet. Note that the NMI metric is not sufficiently sensitive to over clustering; e.g., in the balanced case, SCAN's NMI peaks around K = 70 while the true K is 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Ablation study: NMI, ARI and the Final Value of K</head><p>We provide in <ref type="table">Table 2</ref> the ARI, NMI and final K values for the ablation study described in the paper. The ACC values already appear in the paper. </p><formula xml:id="formula_11">NMI ARI final K K init =3 K init =10 K init =30 K init =3 K init =10 K init =30 K init =3 K init =10 K init =30</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Metrics and Datasets Used in the Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Evaluation Metrics</head><p>In our evaluations we used three common supervised clustering metrics: clustering accuracy (ACC), Normalized Mutual Information (NMI) and Adjusted Rand Index (ARI). The ACC and NMI range between 0 and 1, and the ARI ranges between -1 and 1. For all metrics the higher the better and all of them can accommodate for different numbers of classes between the prediction and the ground truth. We also used the silhouette score, which is an unsupervised metric, in order to find (in an unsupervised way) the best K for the parametric methods. The silhouette score ranges between -1 and 1 (the higher the better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">ACC</head><p>ACC is defined by:</p><formula xml:id="formula_12">ACC = max m N i=1 1(y i = m(z i )) N<label>(1)</label></formula><p>where N is the number of data points, y i is the Ground-Truth (GT) class label of data point i, z i is the predicted cluster assignment according to the clustering algorithm under consideration, 1(?) is the indicator function, and m is defined by all possible one-to-one mappings between the predicted class membership and the ground-truth one. Thus, this metric can be compared to the standard accuracy measure used in the supervised-learning settings, with class mapping, where the mapping used is the best match between the GT classes and the predicted ones. To find the best match, we use the popular Hungarian matching algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">NMI</head><formula xml:id="formula_13">Let z = (z i ) N i=1 and let y = (y i ) N i=1 . NMI is defined by: NMI = 2 ? I(y; z) H(y) + H(z)<label>(2)</label></formula><p>where H(.) stands for entropy and I(.; .) denotes Mutual Information (MI). One problem with this metric, however, is that the MI term, which appears in the numerator, does not penalize large cardinalities (i.e., over clustering). The denominator partially fixes this, but not entirely. Thus, NMI is not sensitive enough to over clustering. See for example <ref type="figure" target="#fig_3">Figure 10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">ARI</head><p>The Rand index (RI) quantifies the percentage of "correct" decisions for each pair of data points. A decision is correct if two examples either belong to the same GT class and the same cluster assignment (a true positive, TP), or being from two different GT classes and assigned to two different clusters (a true negative, TN). Similarly, clustering errors are false positives (FP) and false negatives (FN). Then RI is computed by:</p><formula xml:id="formula_14">RI = T P + T N T P + T N + F P + F N .<label>(3)</label></formula><p>The ARI measure is the corrected-for-chance version of the Rand index. Given a set S of N elements, and two groupings or partitions (e.g. y and z) of these elements, the overlap between y and z can be summarized in a contingency table [c kl ] where each entry c kl denotes the number of objects in common between y k and z k : c kl = |y k ? z k |. Let a k be the sum of each row, meaning, a k = l c kl , and b k the sum of each column, i.e. b k = k c kl .</p><p>The ARI measure is then calculated by:</p><formula xml:id="formula_15">ARI = kl n kl 2 ? [ k a k 2 l b l 2 ]/ n 2 1 2 [ k a k 2 + l b l 2 ] ? [ k a k 2 l b l 2 ]/ n 2<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.4">Silhouette Score</head><p>So far we have discussed supervised evaluation scores, meaning, ones that require the GT labels. However, in unsupervised cases where the GT is unknown in training, one often needs a metric to evaluate the model; e.g. when performing hyperparameter tuning or model selection for parametric methods. In this case, usually a parametric model is run using a range of different values for K, and an unsupervised criterion is used to choose the best model (best value for K).</p><p>One of the most common unsupervised clustering metrics is the silhouette score, which quantifies the clustering quality by measuring the amount of "cohesion" within a cluster, and "separation" between different clusters. Meaning, the more data points within each cluster are closely-packed and different clusters are well-separated, the higher the silhouette score is. More formally, given data X = (x i ) N i=1 ? R N ?d and its clustering prediction z, for data point x i with cluster label k (z i = k), let</p><formula xml:id="formula_16">a(i) = 1 |N k | ? 1 xj :zj =k d(x i , x j )<label>(5)</label></formula><p>be the mean distance between x i and all other data points in the same cluster, where |N k | is the number of points hard assigned to cluster k, and and d(i, j) is the distance between data points x i and x j .</p><formula xml:id="formula_17">Let b(i) = min k ? :k ? ? =k 1 N k ? xj :zj =k ? d(x i , x j )<label>(6)</label></formula><p>be the smallest mean distance of datapoint x i to all points in any other cluster, of which x i is not a member. Now, the silhouette score of x i is defined as:</p><formula xml:id="formula_18">s(i) = b(i)?a(i) max(a(i),b(i)) , if N k &gt; 1 0, otherwise<label>(7)</label></formula><p>Thus, s(i) ? [?1, 1]. Finally, the total silhouette score is the average of all values for s(i). Note that the silhouette score does not use the GT labels, and thus it is an unsupervised metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Datasets for Evaluation</head><p>Datasets. We evaluate our method on text and image datasets in varying scales. The summary statistics on the datasets are available in <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Train samples Val samples Data Dimension GT K MNIST <ref type="bibr">[5]</ref> 60,000 10,000 28  <ref type="table">Table 3</ref>. Descriptive properties of the datasets used for evaluation.</p><p>Remarks regarding the ImageNet and ImageNet-50 datasets. The creators of ImageNet <ref type="bibr">[4]</ref> do not hold the copyright of all images, and the usage of that dataset is governed by the terms of the ImageNet license https://image-net.org/ download. ImageNet-50 is a subset of 50 randomly-selected classes of ImageNet, curated by <ref type="bibr">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Train and Validation Sets</head><p>In general, we train and evaluate using the train and validation split respectively for most of the comparisons. However, when comparing with deep nonparametric methods (which we had problems to run their code and had to resort to use their reported results), to allow for a fair comparison, we computed the evaluation metrics on the entire dataset (as this is what their reported numbers referred to), meaning, combining the train and validation sets into one set. Note that in this case too, we still trained our model only on the training set. Also recall the training (of both our method and the competitors) is unsupervised and used neither the GT labels nor the GT K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Creating Imbalanced Datasets</head><p>To create imbalanced datasets for the smaller datasets, i.e., MNIST, USPS and Fashion-MNIST, we undersampled some of the classes using random proportions. Concretely, for MNIST and USPS we sampled 10%, 5%, 20%, and 30% of the total amount of examples of classes 8, 2, 5, and 9, respectively. All the other classes were used in full. For Fashion-MNIST dataset, classes 0, 3, 5, 7, and 8 were under-sampled with 37%, 19%, 41%, 54%, 19% of the total number of examples per class (the classes and percentages were chosen randomly).</p><p>To create an imbalanced version of ImageNet-50, we sampled a 50-bin normalized histogram from a uniform Dirichlet distribution (not to be confused with a Dirichlet process) over the 50-dimensional probability simplex. This means that any histogram was equally probable (not to be confused with a uniform histogram). The random nonuniform histogram we sampled is shown in <ref type="figure" target="#fig_3">Figure 11a</ref>. For comparison, <ref type="figure" target="#fig_3">Figure 11b</ref> shows the original class distribution of ImageNet-50 which is almost perfectly balanced (i.e., almost uniform).   <ref type="figure" target="#fig_3">Figure 11</ref>. The balanced vs. imbalanced histograms for ImageNet-50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The NIW Prior, Marginal Data Likelihood, the Weighted Bayesian Estimates, and the Concentration Parameter</head><p>To make our work self-contained, below we provide the details for the key Bayesian calculations that we use. For more details about the known theoretical results in ? 3.1, ? 3.2, and ? 3.4, see <ref type="bibr">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Normal Inverse Wishart Distribution</head><p>In the Dirichlet Process Gaussian Mixture Model (DPGMM), like in the Bayesian GMM, each component's parameters, (? k , ? k ), where ? k = (? k , ? k ) denote the mean and covariance matrix, and ? k is the mixture weight, are assumed to be drawn from a certain prior distribution. A common choice for a prior for ? k is the Normal-Inverse-Wishart (NIW) distribution. This is because the latter is a conjugate priorto the multivariate normal distribution with an unknown mean and an unknown covariance matrix. The conjugacy property guarantees that the posterior probability will be in the same distribution family as the NIW prior, and provides a closed-form expression for it, which is algebraically convenient for inference.</p><p>The probability density function (pdf) of the Inverse-Wishart (IW) distribution over d ? d Symmetric and Positive Definite (SPD) matrices, evaluated at the d ? d SPD matrix ? k , is</p><formula xml:id="formula_19">W ?1 (? k ; ?, ?) = |??| ? 2 2 ?d 2 ? d ( ? 2 ) |? k | ? ?+d+1 2 e ? 1 2 tr(??? ?1 k )<label>(8)</label></formula><p>where ? &gt; d ? 1, ? ? R d?d is SPD, and ? d is the (d-dimensional) multivariate gamma function. Equivalently, we may write</p><formula xml:id="formula_20">? k ? W ?1 (?, ?) .<label>(9)</label></formula><p>The positive real number ? and the SPD matrix ? are called the hyperparameters of the IW distribution. Now let ? k ? R d . The vector ? k and the SPD matrix ? k are said to be Normal-Inverse-Wishart distributed if their joint pdf is</p><formula xml:id="formula_21">p(? k , ? k ; ?, m, ?, ?) = NIW(? k , ? k ; ?, m, ?, ?) ? p(? k |? k ;?,m) N (? k ; m, 1 ? ? k ) p(? k ;?,?) W ?1 (? k ; ?, ?)<label>(10)</label></formula><p>where m ? R d and ? &gt; 0 (while ? and ? are as before) and N (? k ; m, 1 ? ? k ) is a d-dimensional Gaussian pdf, evaluated at ? k , with mean m and covariance 1 ? ? k .</p><p>Equivalently, we may write</p><formula xml:id="formula_22">(? k , ? k ) ? NIW(m, ?, ?, ?) .<label>(11)</label></formula><p>The elements of the tuple ? ? (m, ?, ?, ?) are called the hyperparameters of the NIW distribution. Particularly, ? and ? are called the pseudocounts of that distribution. Loosely speaking, the higher ? and ? are, the more the distribution is peaked (roughly) around ? and around m, respectively.</p><p>Remark: do not confuse k (the index of the Gaussian/cluster) with ? ("kappa", a hyperparameter of the NIW distribution).</p><p>Assuming, for a moment, a hard-assignment setting, let N k = |{i : z i = k}| and let X k = (x i ) i:zi=k denote N k i.i.d. draws from N (? k , ? k ). The key reason why the NIW distribution is widely used <ref type="bibr">[8]</ref> as a prior over the parameters, (? k , ? k ), is conjugacy (to the Gaussian likelihood). Namely, the posterior distribution over these parameters is also NIW,</p><formula xml:id="formula_23">p(? k , ? k |X k ) = NIW(? k , ? k ; ? * , m * k , ? * , ? * k ) ,<label>(12)</label></formula><p>and its so-called posterior hyperparameters are given in closed form:</p><formula xml:id="formula_24">? * k = ? + N k (13) m * k = 1 ? * k ?m + i:zi=k x i (14) ? * k = ? + N k<label>(15)</label></formula><formula xml:id="formula_25">? * k = 1 ? * ?? + ?mm T + i:zi=k x i x T i ? ? * k m * k (m * k ) T .<label>(16)</label></formula><p>Importantly, when ? and ? are much smaller than N , then the specific choice of ? k and ? becomes negligible, implying a very weak prior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Marginal Likelihood Function</head><p>When marginalizing over the parameters of a Gaussian (i.e., its mean and covariance), one obtains the marginal data likelihood (given the hyperparameters of the NIW prior):</p><formula xml:id="formula_26">f x ((x i ) N i=1 ; ?) = f x ((x i ) N i=1 ; m, ?, ?, ?) = p((x i ) N i=1 |? k , ? k )p(? k , ? k ; ?)d(? k , ? k ) = 1 ? N d 2 ? d (? * /2) ? d (?/2) |??| ?/2 |? * ? * k | ? * /2 ? ? * d/2<label>(17)</label></formula><p>where ? d is the d-dimensional Gamma function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Weighted MAP Estimates of the Parameters</head><p>We now provide the details of the M step. More concretely, below we explain how we compute the weighted Maximum-a-Posteriori (MAP) estimates of the clusters' and subclusters' parameters, where the weighting is done according to the output of our deep nets.</p><p>Let ? = (m, ?, ?, ?) be the NIW hyperparams. In the unweighted case, the MAP estimates of ? and ? are:</p><formula xml:id="formula_27">? k = ? * ? * k ? * ? d + 1 (18) ? k = m * k .<label>(19)</label></formula><p>In our case, the MAP estimates are obtained in a similar way, but with the following differences. Rather than using hard assignments (as in ? 3.1), we use soft assignments; i.e., the MAP estimates take all N points into consideration, but with an appropriate weighting of each point. This is nearly identical to the weighted MAP estimates in the standard computation of the </p><formula xml:id="formula_28">? * k = ? + N i=1 r i,k<label>(20)</label></formula><formula xml:id="formula_29">m * k = 1 ? * k ?m + N i=1 r i,k x i<label>(21)</label></formula><formula xml:id="formula_30">? * k = ? + N i=1 r i,k<label>(22)</label></formula><formula xml:id="formula_31">? * k = 1 ? * ?? + ?mm T + N i=1 r i,k x i x T i ? ? * k m * k (m * k ) T .<label>(23)</label></formula><p>The parameters of the subclusters are updated in a very similar way, except that the soft subcluster assignments (i.e., ( r i,j )) are used instead of the soft cluster assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The Concentration Parameter of the Dirichlet Process</head><p>The concentration parameter of the Dirichlet Process, ? &gt; 0, is a user-defined hyperparameter that, when sampling from the prior, controls (the expected number of) the number of clusters. In short, the higher ? is, the more clusters are expected. However, when doing posterior calculations, if ? ? N , where N is the number of data points, then the importance of ? diminishes. Particularly, when computing the Hastings ratios for the splits and merges, the importance of ? ? N is usually negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">The Factors Affecting K; Our Weak Prior</head><p>In a DPGMM, the number of clusters is affected not only by ? and the data but also the NIW prior. For example, if ? is very high and ? is small, then the model will favor small clusters and thus K is likely to be high (so the small clusters will efficiently cover the data). Conversely, if ? is very hight and ? is large, then the model will favor large clusters so K will tend to be small (only few large clusters can cover the entire data). However, we emphasize that in all cases, our choices (see below) for the values of the NIW hyperparameters and the concentration parameter ? correspond to a very weak prior. That is, our ?, ? and ? are all much smaller than N in all the datasets we experimented with: across all the datasets, the smallest N was 13,000 (in STL-10). As a result, the main factor in determining the inferred K is the data itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Merge Proposals</head><p>In the paper, we explained how K is changed via splits and merges, and described how to compute the acceptance probability of split proposals. Here, we provide the complementary details on merge proposals. In a merge step, we sequentially propose pairs of clusters to merge. To avoid sequentially considering all possible merges, we consider (sequentially) the merges of each cluster with only its 3 nearest neighbors.</p><p>The proposal to merge a pair of clusters, k 1 and k 2 , is a accepted with probability (1, H m ), where</p><formula xml:id="formula_32">H m = 1 H s = ?(N k1 + N k2 )f x (X {k1,k2} ; ?) ??(N k1 )f x (X k1 ; ?)?(N k2 )f x (X k2 ; ?)<label>(24)</label></formula><p>is the Hastings ratio, ? is the Gamma function, N k,1 and N k,2 are the number of points in clusters 1 or 2, respectively, X k = (x i ) i:zi=k denotes the points in cluster k, and X {k1,k2} = (x i ) i:zi?{k1,k2} denotes the points in clusters k 1 and k 2 . As for f x (?; ?), this is the marginal likelihood where ? represents the NIW hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Feature Extraction</head><p>Here we provide more details on the feature-extraction process. In general, there are two main paradigms: an end-to-end approach in which the features and clustering are learned simultaneously, and a two-step approach in which clustering is performed on pre-computed latent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">End-to-end Approach: Jointly Learning Features and Clustering</head><p>Here, we loosely follow DCN <ref type="bibr">[16]</ref> and similarly start by training an Autoencoder (AE) with a reconstruction loss,</p><formula xml:id="formula_33">L AErecon = 1 N N i=1 ?g(f (x i )) ? x i ? 2 ?2 (25)</formula><p>where f is the encoder and g is the decoder. Then, while DCN performs K-means on the resulted embeddings to obtain initial cluster centers and assignments, we use our more flexible DeepDPM (which, unlike K-means, assumes neither isotropic classes, uniform weights, nor a known K). Next, we utilize the pipeline of <ref type="bibr">[16]</ref> and refine the AE's latent space by training it with both L recon and an additional clustering loss:</p><formula xml:id="formula_34">L AE clus = ?f (x i ) ? ? zi ? 2 ?2 ,<label>(26)</label></formula><p>where z i is the cluster assignment of x i , and ? zi is the cluster's center. This loss encourages f to create small intra-class distances in the latent space. The overall loss is L AErecon + ? 2 L AE clus where ? &gt; 0 is a tunable parameter. While in <ref type="bibr">[16]</ref> new cluster assignments and centers are computed after each epoch of the AE, we keep them fixed during this stage, changing only the embeddings. While the pipeline suggested in <ref type="bibr">[16]</ref> ends here, we add an alternation scheme where we alternate between clustering the updated embeddings using DeepBNP (keeping the AE frozen) and training the AE (i.e., perform feature learning, while keeping the clustering fixed). We repeat this process several times. Intuitively, during training, DeepDPM is likely to change K, thus, adapting the embeddings accordingly may reveal inter-and intra-class structures which can be useful, in turn, for the clustering module to find meaningful clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Two-Step Approach: Training on Latent Features.</head><p>Another approach for deep clustering is using a pretrained feature extractor backbone. As our method is DL-based, it is easy to concatenate any DL backbone before our DNN. Thus, we follow <ref type="bibr">[14]</ref> and use MoCo <ref type="bibr">[2]</ref> for (unsupervised) feature extraction.</p><p>We provide the specific values we used for the feature extractions below in ? 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Implementation Details and Hyperparameters</head><p>We detail here all the training configurations and hyperparameters used in our experiments. For all the experiments, our clustering net used the following MLP architecture where the MLP had an input layer, a single hidden layer, and an output layer. The number of neurons in the input layer was d (the dimension of the input to the clustering module). The number of hidden units was always 50 in all our experiments (changing that number had little effect on the results). The (changing) number of neurons in the output layer was K (which corresponds to the changing number of clusters). In addition, in all our experiments we used a batch size of 128, a learning rate (lr) of 0.0005 for the clustering net, and an lr of 0.005 for the subclustering nets. As for the prior hyperparams, for the DP's ? we chose ? = 10, and for the NIW hyperparams we used ? = 0.0001, set m to be the data mean, and ? to be d + 2. We used different ? values in each experiment, as detailed below.</p><p>Below in ? 7.4 we give some guidelines on how to choose the key hyperparameters for DeepDPM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Setup Used for Comparing with Classical Methods</head><p>Feature Extraction. For this experiment, we used the feature extraction procedure suggested by <ref type="bibr">[12]</ref> where we first trained a deep Autoencoder (AE) and then transformed its latent space using a UMAP transformation <ref type="bibr">[13]</ref>. We used the same configurations as in <ref type="bibr">[12]</ref>.</p><p>DeepDPM hyperparameters. For all experiments, we initialized DeepDPM with K = 1, DeepDPM was trained for 500 epochs. We set ? = I ? 0.005 in all the datasets, where I denotes the identity matrix. Note that we used the same configuration for the three datasets, in both the balanced and imbalanced cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Training on Latent Features</head><p>Here, we give the hyperparameters we used for evaluating our method on STL-10 and ImageNet. For both datasets we used the unsupervised pretrained feature extractor MoCo <ref type="bibr">[2]</ref> and trained DeepDPM on top of the resulting features. For STL-10 we pretrained it for 1000 epochs (on STL-10's train set) and for ImageNet we used the pretrained weights available online.  <ref type="table">Table 4</ref>. Implementation details for DeepDPM's experiments. D denotes the input dimension. For all datasets but ImageNet-50 it is the original data dimension. For ImageNet-50, D = 128, the output dimension of MoCo <ref type="bibr">[2]</ref> For STL-10, we initialized DeepDPM with K = 3 and trained it for 500 epochs with ? = I ? 0.05. For ImageNet, we initialized DeepDPM with K = 150 and trained it for 200 epochs with ? = I ? 0.001.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Jointly Learning Features and Clustering</head><p>As described in the paper, we adapted the feature learning pipeline from DCN <ref type="bibr">[16]</ref> to jointly learn features and clustering in alternation. <ref type="table">Table 4</ref> shows the AE architectures, the lr values, and the number of DeepDPM epochs. For ImageNet-50, we trained an AE on top of the features generated by MoCo <ref type="bibr">[2]</ref>. For all other datasets, it was trained on top of the original data dimension. For MNIST and Reuters10K we used three alternations, for ImageNet-50 we used two alternations in the balanced case and four alternations for the imbalanced. See below in ? 7.4 how we chose the number of alternations.</p><p>DeepDPM was initialized with K = 3 for MNIST, K = 1 for Reuters10K and K = 10 for ImageNet-50. As per the prior hyperparams, we chose ? = 0.005 for MNIST and Reuters10K. For ImageNet-50 we chose ? = I ? std(X ) ? 0.0001 , where std(X ) denotes the standard deviation of the data. Note that in both the balanced and imbalanced cases of ImageNet-50 we used the same hyperparameters, where the only difference was the number of alternations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">General Recommendations for Choosing Hyperparameters</head><p>Number of epochs. The number of epochs is chosen by the amount of epochs after which DeepDPM has converged to a certain K; i.e., no more split/merge proposal are accepted. We chose it empirically by training DeepDPM and measuring the average number of epochs it took for K to stabilize. We set the maximal number of epochs to 100 plus that average epoch number.</p><p>Number of alternations. When performing feature learning and clustering in alternation, we need to choose the number of times we perform the alternations (one alternation includes training the AE followed by the DeepDPM training). We stop the alternations once the DeepDPM's inferred K stabilizes.</p><p>The initial value for K. As we showed, the initial value of K has little effect on the final clustering that DeepDPM generates. Thus, it can be chosen arbitrarily. That said, the value of the initial K can affect the speed of convergence (if DeepDPM starts with a more accurate estimate then less splits and merges will happen and the DeepDPM will stabilize faster). A reasonable choice is to choose the initial K to be proportional to N , the number of datapoints; e.g., N/10000. Note that unlike parametric methods, this is used only as an initialization value which is expected to change.</p><p>Choosing ?. As a rule of thumb, we took ? to be proportional to I (i.e., the d ? d identity matrix) with the elements of the main diagonal being the data's standard deviation, scaled by an additional scalar s. The choice of s is based on empirically seeing that a substantial amount of both splits and merges proposals are accepted during the first few hundred epochs. Note that as ? is getting smaller, more splits will be accepted. Thus, if ? is too small, only splits will occur (and no merges) and if it is too large, no splits will be accepted.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 1. Mean clustering accuracy of 3 runs (? std. dev.) on ImageNet50. The Ground Truth K is 50. Parametric methods such as K-means, DCN++ (an improved variant of [71]) and SCAN [64], require knowing K. When given a poor estimate of K, they deteriorate in performance in a balanced dataset (a) and even more so in an imbalanced dataset (b). In contrast, the proposed DeepDPM does not require knowing K (it infers its value; e.g., K = 55.3 ? 1.53 in (a) and 46.3 ? 2.52 in (b)) and yet yields comparable results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Robustness to the initial K. GT K = 10 in all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 .</head><label>1</label><figDesc>Setup Used for Comparing with Classical Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 7.2. Training on Latent Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 7.3. Jointly Learning Features and Clustering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 7.4. General Recommendations for Choosing Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131. Additional Results1.1. Additional Visual Examples of Images Clustered TogetherHere, inFigures 1 to 8, we provide additional examples of images from the validation set of the ImageNet dataset that were clustered together using DeepDPM. These figures show how DeepDPM grouped images with similar semantic properties.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 .Figure 2 .Figure 3 .</head><label>123</label><figDesc>Examples from cluster #1 Examples from cluster #2 Examples from cluster #3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .Figure 5 .Figure 6 .Figure 7 .Figure 8 .</head><label>45678</label><figDesc>Examples from cluster #4 Examples from cluster #5 Examples from cluster #6 Examples from cluster #7 Examples from cluster #8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Mean ARI of 3 runs (? std. dev.) on 50 classes of ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>The random histogram we sampled from the uniform distribution over the 50-dimensional probability simplex. This histogram was used for creating the imbalanced version of ImageNet-50.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>The class histogram of the original ImageNet-50 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>69] K-means p .90? .02 .84? .05 .85?.06 .86?.01 .79?.05 .80?.06 .67?.01 .50?.03 .60?.04 GMM p .94?.00 .95?.00 .98?.00 .86?.02 .79?.05 .81?.06 .66?.01 .49?.02 .58?.03 92?.01 .91?.04 .93?.05 .87?.01 .82?.02 .83?.03 .67?.01 .49? .02 .59?.03 moVB .93?.00 .94?.00 .97?.00 .87?.02 .86?.04 .90?.04 .66?.02 .47?.03 .55?.03 DeepDPM (Ours) .94?.00 .95?.00 .98?.00 .88?.00 .86?.01 .89?.2 .68?.01 .51?.02 .62?.03 MNIST imb USPS imb Fashion-MNIST imb K-means p .89? .03 .84? .06 .83?.06 .82?.02 .71?.05 .71?.05 .62?.01 .46?.02 .56?.03 GMM p .94?.02 .95?.03 .96?.04 .83?.01 .74?.05 .76?.05 .62?.01 .46?.02 .57?.03 94?.02 .96?.02 .89?.02 .89?.06 .91?.04 .66?.01 .50? .01 .61?.01 moVB .94?.00 .95?.00 .96?.00 .88?.01 .89?.02 .91?.02 .63?.01 .44?.02 .53?.02 DeepDPM (Ours) .95?.01 .97?.01 .98?.01 .90?.00 .92?.00 .94?.00 .65?.00 .50?.00 .61?.00 Comparing the mean results (?std. dev.) of DeepDPM with classical clustering methods. The results are the mean of 10 independent runs. Methods marked with p are parametric (require K). Datasets marked with imb are imbalanced ones. Comparing the mean inferred value (?std. dev.) for K of 10 runs among nonparametric methods. GT K = 10.</figDesc><table><row><cell cols="4">DBSCAN DPM Sampler .DBSCAN .92?0 .93?0 DPM Sampler .93?.01 .Method .86?0 .92?0 Inferred K</cell><cell>.89?0 .94?0</cell><cell>.72?0 .84?0</cell><cell>.46?0 .79?0</cell><cell>.57?0 .80?0</cell><cell>.63?0 .62?0</cell><cell>-.32?0 .35?0</cell><cell>.39?0 .46?0</cell></row><row><cell></cell><cell>MNIST</cell><cell>USPS</cell><cell cols="3">Fashion-MNIST</cell><cell></cell><cell></cell></row><row><cell>DBSCAN DPM Sampler moVB DeepDPM (Ours)</cell><cell cols="2">9.0?0.00 6.0?0.00 11.3?0.82 8.5?0.85 14?1.00 11.2?1.08 10?0.00 9.2?0.42</cell><cell></cell><cell cols="2">4.0?0.00 12.4?0.97 16.9?2.30 10.2?0.79</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>avg .86?1.02 .84?2.35 N/A .75?0.53 .71?0.81 N/A .45?1.79 .43?5.73 N/A DCC ? [52] best avg .90?.02 .89?.07 .91?.07 .22?.00 .01?.00 .04?.00 .25?.00 .00?.00 .00?.00 DeepDPM (ours) avg .90?.01 .91?.02 .93?.03 .78?.004 .70?.01 .84?.01 .61?.00 .64?.01 .83?.00 DeepDPM (ours) best Comparing deep nonparametric methods. ?: reported in the papers. ?: obtained using their code. avg: mean (?std. dev.) of 5 runs. 52?.00 .09?.00 .24?.00 moVB .70?.01 .38?.01 .55?.02 DPM Sampler .72?.00 .43?.01 .57?.01 DeepDPM (ours) .75?.00 .49?.01 .64?.00 DeepDPM (ours) * .77?.00 .54?.01 .66?.01 ImageNet-50: Imbalanced DBSCAN .33?.00 .04?.00 .24?.00 moVB .68?.01 .44?.03 .52?.03 DPM Sampler .70?.00 .40?.01 .51?.00 DeepDPM (ours) .74?.01 .48?.02 .58?.01 DeepDPM (ours) * .75?.00 .51?.01 .60?.01 Comparison of nonparametric methods on ImageNet-50 and its imbalanced version. * marks results with AE alternation.</figDesc><table><row><cell>[43]</cell></row></table><note>Figure 3. Examples of ImageNet images clustered together by DeepDPM. Each panel stands for a different cluster.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>No splits/merges .29?.01 .59?.03 .46?.01 No splits .29?.01 .59?.02 .45?.03 No merges .46?.00 .58?.01 .47?.01 2-means instead of f sub .61?.00 .59?.02 .56?.02 No priors in the M step .58?.01 .57?.02 .58?.01 Isotropic loss instead of L cl .58?.00 .58?.00 .58?.02 DeepDPM (full method) .62?.03 .61?.00 .62?.01</figDesc><table /><note>Table 6. DeepDPM's performance under different ablations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 .</head><label>1</label><figDesc>Comparing the mean inferred value (?std. dev.) K value, across 10 runs, among the competing nonparametric methods. GT K = 10. The symbol imb marks imbalanced datasets.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Inferred K</cell><cell></cell></row><row><cell></cell><cell>MNIST imb</cell><cell>USPS imb</cell><cell>Fashion-MNIST imb</cell></row><row><cell>DBSCAN DPM Sampler moVB DeepDPM (Ours)</cell><cell>9.0?0.00 11.9?0.32 13.6?0.80 10.3?0.44</cell><cell>6.0?0.00 7.3?0.48 11.2?1.33 9.1?0.22</cell><cell>4.0?0.00 11.6?0.97 17.8?2.27 9.4?0.52</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>No splits/merges .53?.00 .67?.01 .64?.00 .22?.00 .49?.02 .means instead of f sub .68?.00 .68?.01 .67?.00 .50?.00 .51?.02 .48?.01 11?.00 12?.00 14.67?.58 No priors in the M step .65?.00 .66?.01 .66?.00 .48?.01 .48?.02 .50?.00 12?1.73 14?1.41 13.67?.58 Isotropic loss instead of L cl .67?.01 .67?.00 .67?.00 .50?.01 .49?.00 .49?.00 10?0.82 9?.00 9.25?.50 DeepDPM (full method) .68?.00 .67?.01 .68?.01 .50?.00 .51?.01 .52?.01 10.67?.58 11.67?1.15 14?.00</figDesc><table><row><cell>No splits No merges 2-</cell><cell>43?.01 .53?.00 .67?.01 .63?.00 .22?.00 .49?.02 .42?.02 .61?.00 .66?.00 .64?.01 .38?.00 .48?.01 .44?.01</cell><cell>3 3?.00 5?.00</cell><cell>10 10?.00 10?.00</cell><cell>30 23?.00 21.3?1.53</cell></row></table><note>Table 2. DeepDPM's performance under different ablations.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>M step in Bayesian EM-GMM, except that here the weighting is done using the (r i,k ) values (namely, the soft assignments which are our deep net's output). That is, we still use Eq. (18) and Eq.<ref type="bibr" target="#b18">(19)</ref>, but instead of the posterior hyperparameters from Eqs. (13)-(16), we use their weighted versions (where the weights are the (r i,k ) values).</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mixtures of Dirichlet processes with applications to Bayesian nonparametric problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Antoniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Vassilios Solachidis, Nicholas Vretos, and Petros Daras. Non-parametric clustering using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Avgerinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational inference for Dirichlet process mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bayesian analysis</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantically-aware aerial reconstruction from multi-modal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randi</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised pre-training of image features on non-curated data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Parallel sampling of HDPs using subcluster splits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Iii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sampling in computer vision and Bayesian nonparametric mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric intrinsic image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randi</forename><surname>Cabezas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parallel sampling of DP mixture models using sub-cluster splits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1501.03084</idno>
		<title level="m">Deep learning with nonparametric clustering</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dirichlet process mixture models on symmetric positive definite matrices for appearance clustering in video surveillance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassilios</forename><surname>Morellas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Papanikolopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bedros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<editor>Geoffrey Gordon, David Dunson, and Miroslav Dud?k</editor>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">M</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
		<idno>1977. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digit images for machine learning research. IEEE Signal Processing Magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scalable and flexible clustering of grouped data via parallel and distributed sampling in versatile hierarchical Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Dinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sampling in Dirichlet process mixture models for clustering streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Dinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS, 2022</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distributed MCMC inference in Dirichlet process mixture models using Julia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Dinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CCGRID Workshop on High Performance Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">You only train once: Loss-conditional training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josip</forename><surname>Djolonga</surname></persName>
		</author>
		<idno>ICLR, 2020. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A Bayesian analysis of some nonparametric problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferguson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From deformations to parts: Motion-based segmentation of 3D objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nonparametric clustering with distance dependent hierarchies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonparametric learning for layered segmentation of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental learning of nonparametric Bayesian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Monte Carlo sampling methods using Markov chains and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hastings</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Nonparametric object and parts modeling with Lie group dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>David S Hayden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Pacheco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastic variational inference. JMLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An incremental DPMM-based method for trajectory clustering, modeling, and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonparametric discovery of activity patterns from video collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Memoized online variational inference for Dirichlet process mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Streaming variational inference for Dirichlet process mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viet</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A split-merge Markov chain Monte Carlo procedure for the Dirichlet process mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonia</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">From patches to images: a nonparametric generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Two-view motion segmentation by mixtures of Dirichlet process with model selection and outlier removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Jian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Song</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Variational deep embedding: A generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuxi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huachun</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bangsheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanning</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning multiscale representations of natural scenes using Dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Jyri J Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Accelerated variational Dirichlet process mixtures. NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenichi</forename><surname>Kurihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Vlassis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimol: automatic online picture collection via incremental model learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Trajectories as topics: Multi-object tracking by topic discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Lihi Zelnik-Manor, and Shai Avidan. Graph embedded pose clustering for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Markovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">N2d:(not too) deep clustering via clustering the local manifold of an autoencoded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Piechocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Craddock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The dependent Dirichlet process mixture of objects for detection-free tracking and object modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willie</forename><surname>Neiswanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural clustering processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Pakman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Mitelut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Paninski</surname></persName>
		</author>
		<idno>PMLR, 2020. 3</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Combinatorial stochastic processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Pitman</surname></persName>
		</author>
		<idno>621</idno>
	</analytic>
	<monogr>
		<title level="j">Lecture notes</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Dept. Statistics, UC Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atul</forename><surname>Sohil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01449</idno>
		<title level="m">Deep continuous clustering</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Hierarchical Bayesian models for unsupervised scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><forename type="middle">B</forename><surname>Pizarro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Small-variance nonparametric clustering on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient global point cloud alignment using Bayesian nonparametric mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">P</forename><surname>How</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">The Manhattan frame model: Manhattan world inference in the space of surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Rosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Graphical models for visual object recognition and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudderth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Shared segmentation of natural scenes using dependent Pitman-Yor processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Describing visual scenes using transformed objects and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Erik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Video face clustering with unknown number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Hierarchical dirichlet processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">M</forename><surname>Beal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Bayesian adaptive superpixel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Uziel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meitar</forename><surname>Ronen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Visualizing data using t-SNE. JMLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">SCAN: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Deep embedding for determining the number of clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinwang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">DNB: A joint learning framework for deep Bayesian nonparametric clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeya</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Common failure modes of subcluster-based sampling in Dirichlet process gaussian mixture models -and a deep-learning solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Dinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deep clustering and representation learning that preserves geometric structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09590</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep clustering by Gaussian mixture variational autoencoders with graph embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ngai-Man</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Vsbdvm: an end-to-end bayesian nonparametric generalization of deep variational mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aria</forename><surname>Masoomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">G</forename><surname>Dy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03288</idno>
		<title level="m">Streaming adaptive nonparametric variational autoencoder</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Sampling in computer vision and Bayesian nonparametric mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<editor>Geoffrey Gordon, David Dunson, and Miroslav Dud?k</editor>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">The MNIST database of handwritten digit images for machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Distributed MCMC inference in Dirichlet process mixture models using Julia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Dinari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Freifeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CCGRID Workshop on High Performance Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aki</forename><surname>Dunson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald B</forename><surname>Vehtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Memoized online variational inference for Dirichlet process mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sudderth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">A database for handwritten text recognition research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jonathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Rcv1: A new benchmark collection for text categorization research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Russell-Rose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">N2d:(not too) deep clustering via clustering the local manifold of an autoencoded embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Mcconville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Santos-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Piechocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Craddock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<monogr>
		<title level="m" type="main">Umap: Uniform manifold approximation and projection for dimension reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leland</forename><surname>Mcinnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Melville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03426</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">SCAN: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Towards k-means-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nicholas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyi</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
