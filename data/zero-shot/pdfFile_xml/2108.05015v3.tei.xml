<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON CYBERNETICS 1 VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Feng</forename><surname>Wu</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON CYBERNETICS 1 VisEvent: Reliable Object Tracking via Collaboration of Frame and Event Flows</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Visual Tracking</term>
					<term>Neuromorphic Vision</term>
					<term>Dy- namic Vision Sensors</term>
					<term>Event Camera</term>
					<term>Self-attention and Trans- formers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Different from visible cameras which record intensity images frame by frame, the biologically inspired event camera produces a stream of asynchronous and sparse events with much lower latency. In practice, the visible cameras can better perceive texture details and slow motion, while event cameras can be free from motion blurs and have a larger dynamic range which enables them to work well under fast motion and low illumination. Therefore, the two sensors can cooperate with each other to achieve more reliable object tracking. In this work, we propose a large-scale Visible-Event benchmark (termed VisEvent) due to the lack of a realistic and scaled dataset for this task. Our dataset consists of 820 video pairs captured under low illumination, high speed, and background clutter scenarios, and it is divided into a training and a testing subset, each of which contains 500 and 320 videos, respectively. Based on VisEvent, we transform the event flows into event images and construct more than 30 baseline methods by extending current single-modality trackers into dualmodality versions. More importantly, we further build a simple but effective tracking algorithm by proposing a cross-modality transformer, to achieve more effective feature fusion between visible and event data. Extensive experiments on the proposed VisEvent dataset, FE108, and two simulated datasets (i.e., OTB-DVS and VOT-DVS), validated the effectiveness of our model. The dataset and source code have been released at our project page: https://sites.google.com/view/viseventtrack/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Visual tracking aims at locating the initialized object in the first frame with a bounding box and adjusting the box to better fit the target object for subsequent video frames. It has been widely used in many applications, including intelligent video surveillance, robotics, and autonomous vehicles. Although it has achieved great process with the help of deep learning, however, existing trackers <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref> still suffer from challenging scenarios such as low illumination, fast motion, background clutter.</p><p>To handle aforementioned challenges, some researchers resort to the biologically inspired event cameras like Dynamic Vision Sensors (DVS) <ref type="bibr" target="#b4">[5]</ref> for target object tracking <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b9">[10]</ref>. Different from regular visible cameras which record an intensity image in a frame manner (high latency, i.e., 10-20 ms), the event cameras output a stream of asynchronous events. The pixels of event cameras are sending information independently only when visual intensity changes (also called an event). Therefore, the event sensors excel at capturing the motion information with very low latency (1 ?s) and are almost free from the trouble of motion blur. It also requires much less energy, bandwidth, and computation. In addition, DVS sensors also outperform the visible cameras on dynamic range (140 vs 60dB), which enables them to work effectively even in poor illumination conditions. The advantages of the latency, resource consumption, and operation environments make the event cameras more suitable for target tracking in challenging scenarios. The comparison of the imaging quality and sampling mechanism of the two sensors are given in <ref type="figure" target="#fig_0">Fig.  1 and Fig. 2</ref>  motion or static objects and lack fine-grained texture information which is also very important for high-performance tracking. Therefore, the integration of visible cameras and DVS sensors is an intuitive idea for reliable object tracking.</p><p>There are already several works <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b11">[12]</ref> developed based on this setting, but their experiments are conducted on simulation data or several simple videos. Their performance on real data in the wild is still unknown. Recently, Zhang et al. propose a new dataset that contains 108 videos, termed FE108 <ref type="bibr" target="#b12">[13]</ref>, but tracking performance on this dataset is almost saturated. The development of this field is rather slow compared with visible camera based tracking due to the lack of a large-scale Visible-Event based object tracking dataset.</p><p>In this work, we first propose a large-scale neuromorphic tracking benchmark that contains 820 Visible and Event video sequence pairs, termed VisEvent. This dataset fully reflects the challenging factors in the real world like motion blur, fast and slow motion, low illumination, high dynamic range, background clutter, etc. It contains 17 attributes and mainly focuses on traffic scenes, thus the target objects are mainly people and vehicles. To construct a comprehensive benchmark, we also record some videos from the indoor scene. In total, our dataset contains 371, 128 frames. We split them into the training and testing subset, each of them containing 500 and 320 video pairs respectively. Due to the lack of baseline methods to be compared for future works, we extend currently visible camera based trackers into dual-modality versions with different fusion strategies like early, middle, and late fusion.</p><p>Based on the VisEvent dataset, we further build a novel and effective baseline method by developing a cross-modality transformer module. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, the cross-modality transformer boosts the message passing and feature fusion between visible frame and event flows significantly. Following the framework of binary classification based tracking <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, we train an online classifier to discriminate whether the given proposal is a target object or not. The tracking results can be ensured by choosing the best-scored proposal. Similar operations are conducted for the rest of frames and events until the end of testing video.</p><p>Generally speaking, the contributions of this paper can be concluded as the following three aspects 1 :</p><p>? We propose a large-scale neuromorphic tracking dataset that contains 820 Visible-Event videos. It is the first large-scale Visible-Event benchmark dataset collected from real world for single object tracking.</p><p>? We propose a simple but effective baseline tracker by developing a cross-modality transformer module that can fully exploit the unique information of different modalities for robust tracking. It is the first work to validate the successful application of cross-modality transformer in the visible-event tracking problem.</p><p>? We construct multiple dual-modality based trackers (more than 35) for future works to compare under various tracking pipelines (e.g., correlation filter based, binary classification based, and Siamese matching based trackers) and fusion strategies (e.g., early, middle, and late fusion).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section, we first give a brief review on Visible and Event camera based tracking respectively. Then, we discuss the trackers developed based on combining event and visible cameras. More related works can be found in recent surveys <ref type="bibr" target="#b15">[16]</ref>- <ref type="bibr" target="#b17">[18]</ref>.</p><p>Visible Camera based Tracking. Most of the current trackers are developed based on RGB cameras and track the target object frame by frame <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b21">[22]</ref>. Traditional RGB trackers use hand-crafted features for target representation but perform poorly in challenging scenarios. Among them, correlation filter (CF) based trackers dominate the tracking field due to their high efficiency and good performance. Many classical CF trackers <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> are propose in this stage, like Staple <ref type="bibr" target="#b24">[25]</ref>, CSR-DCF <ref type="bibr" target="#b25">[26]</ref>, etc. After that, the deep learning trackers, especially the Siamese network based trackers began to occupy the top positions of various benchmarks. From the early SiamFC <ref type="bibr" target="#b26">[27]</ref>, SINT <ref type="bibr" target="#b27">[28]</ref> to recent SiamRPN++ <ref type="bibr" target="#b28">[29]</ref>, DiMP <ref type="bibr" target="#b29">[30]</ref>, Siam RCNN <ref type="bibr" target="#b30">[31]</ref>, these trackers attain better results one after another. Shen et al. <ref type="bibr" target="#b31">[32]</ref> propose an attention based Siamese network which can improve matching performance by a sub-Siamese network. The feature learning and classification capabilities of extreme learning machine (ELM) is exploited by Deng et al. <ref type="bibr" target="#b32">[33]</ref> for efficient visual tracking. Liu et al. <ref type="bibr" target="#b23">[24]</ref> attempt to address the occlusion issue in the tracking task using correlation filtering and probabilistic finite state machines (FSMs). Zhou et al. <ref type="bibr" target="#b33">[34]</ref> propose a gradient-guided feature adjustment module to generate targetaware features for constructing the state estimation network, which achieves high-performance visual tracking. Li et al. <ref type="bibr" target="#b34">[35]</ref> develop a dual-regression tracking framework by combining the discriminative fully convolutional module and a finegrained correlation filter component. Li et al. <ref type="bibr" target="#b35">[36]</ref> treat the TIR tracking as a similarity verification task, and propose a Hierarchical Spatial-aware Siamese CNN (named HSSNet) for TIR tracking. However, their performance under low-illumination, fast motion, and low resolution is still unsatisfactory. Many works are proposed to handle these issues like active hard sample generation <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> and deblur <ref type="bibr" target="#b38">[39]</ref>, however, the existing algorithm can't address these issues well due to the bad imaging quality of RGB cameras. Other sensors are also explored for tracking task, including high frame rate cameras (short for HFR, larger than 200 FPS) <ref type="bibr" target="#b39">[40]</ref>, thermal cameras <ref type="bibr" target="#b40">[41]</ref>, and depth cameras <ref type="bibr" target="#b41">[42]</ref>, but HFR cameras are sensitive to illumination, thermal cameras are expensive, and depth cameras are also helpless for high speed and low light, which limit their wide applications in practical scenarios.</p><p>Event Camera based Tracking. Compared with RGB trackers, few people pay attention to tracking based on event cameras. Chen et al. <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref> propose the Adaptive Time-Surface with Linear Time Decay (ATSLTD) event-to-frame conversion algorithm for event frame construction and redetect the target object when model drifting. The synchronous Time-Surface with Linear Time Decay (TSLTD) representation is explored and fed into a CNN-LSTM network for 5-DoF object motion regression in <ref type="bibr" target="#b5">[6]</ref>. To handle the issue of local search in event based tracking, the authors of <ref type="bibr" target="#b42">[43]</ref> propose a data-driven, global sliding window based detector to help re-detect the target object when it re-enters the fieldof-view of the camera. <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref> explore the high speed feature tracking with DVS sensors. Cao et al. <ref type="bibr" target="#b46">[46]</ref> propose a target tracking controller based on spiking neural network which can be deployed on autonomous robots. Jiang et al. <ref type="bibr" target="#b47">[47]</ref> also propose a tracking framework that contains the offline-trained detector and an online-trained tracker which complement each other. Although these trackers work well in simple scenarios, however, their performance on large-scale tracking benchmarks is still unknown. Also, the performance of these models on track the objects that rarely move or are stationary is still alarming.</p><p>Tracking by Combining Visible and Event Cameras. Joint utilizing the two sensors for robust tracking is an intuitive idea and the initial verification has been obtained in the following work. For example, <ref type="bibr" target="#b48">[48]</ref>, <ref type="bibr" target="#b49">[49]</ref> first propose asynchronous photometric feature tracking with event and RGB sensors. Liu et al. <ref type="bibr" target="#b10">[11]</ref> also attempt to extract candidate ROIs from RGB frames and event flows simultaneously for more accurate tracking. Huang et al. <ref type="bibr" target="#b11">[12]</ref> develop an SVMbased tracker using re-constructed samples for an online update and candidate search locations mining from event flows with a CeleX sensor. DashNet <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b50">[50]</ref> is developed based on parallel SNN and CNN tracking and fusion which can run at 2083 FPS on neuromorphic chips. Their work fully demonstrates the vast potential of Visible-Event tracking in practical applications.</p><p>These works have made preliminary explorations in this direction, however, their experiments are conducted on several simple real videos or simulation data, as shown in <ref type="table" target="#tab_1">Table I</ref>. Their results on really challenging scenarios are still unknown, also, their work lacks proper baseline methods to compare. We believe our proposed dataset and baseline algorithm will be a good platform for the research in this direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. VISEVENT BENCHMARK DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Protocols</head><p>The VisEvent is developed to provide a dedicated platform for the training and evaluation of Visible-Event tracking algorithms. Therefore, we obey the following protocols when constructing our benchmark: 1). Large-scale: It is important to provide a huge amount of video sequences for data-hungry deep trackers. We collect 820 video pairs with average 450 frames for each video. 2). High-quality dense annotations: Our dataset is densely annotated for each frame and is independently checked by a professional labeling company and two PhDs. 3). Short-term &amp; long-term tracking: Our dataset contains 709 and 111 videos for short-term and long-term tracking which will be beneficial for constructing a robust and flexible tracker. 4). Long-tail distribution: In our real world, pedestrians, and vehicles are more related to our life and the two categories occupy the majority of our videos. 5). The balance between dual-modalities: Since the VisEvent contains two modalities, the balance of difficult videos for each modality is very important. The cases where tracking with single modality can already realize high performance should be avoided. 6). Comprehensive baselines: We construct multiple baselines for future work to compare by extending visible trackers into its dual-modality version with various fusion strategies. Also, we propose a simple but effective crossmodality transformer based tracker as our advanced baseline approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Collection and Annotation</head><p>Based on aforementioned protocols, we first collect multiple video sequences with DVS (Dynamic Vision Sensors, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>), which can output visible video frames and event flows simultaneously. It is worthy to note that two streams are generated from a single sensor and are already aligned by the hardware. Therefore, no external processing operations like registration in spatial and temporal views are needed. The resolution of dual modalities is 346 ? 260. The target objects are UAV, Hand, Pen, Bottle, Tank, Toy, Car, Tennis, Pedestrian, Badminton, Basketball, Book, Plant, Shoes, Phone, Laptop, Bag and Cat. Parts of them are visualized in <ref type="figure" target="#fig_0">Fig. 1</ref> and more samples can be found in our supplementary material and demo videos.</p><p>After acquiring these videos, we first transform the output file format * .aedat4 into RGB and event frames * .bmp and then select video clips that contain a consistent target object as one sequence. The annotation for each frame is fulfilled by a professional label company and two authors of this work checked all the annotations frame by frame. Rough annotations will be adjusted again to further ensure the accuracy of our dataset. Some samples of our dataset are visualized in <ref type="figure" target="#fig_2">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Attribute Definition</head><p>As shown in <ref type="table" target="#tab_1">Table II</ref>, there are totally 17 attributes defined in our proposed VisEvent dataset. For the RGB cameras, our dataset reflects many popular attributes in single object tracking, such as camera motion, rotation, scale variation, occlusion. For the event cameras, since it is challenging for tracking the static target object, we introduce the attribute NMO (NO Motion) to evaluate the tracking performance under this situation. Other motion related challenges are also considered, including FM (Fast Motion), MB (Motion Blur), BOM (background object motion). Our dataset also reflects the scenarios under different lighting conditions, such as LI (Low Illumination), OE (Over Exposure), and IV (Illumination Variation). It is worthy to note that all our videos are with low resolution (i.e., 346 ? 260) compared with resolution 1280?720 in GOT-10K <ref type="bibr" target="#b54">[53]</ref> due to the limitation of hardware. Therefore, we do not explicitly list low-resolution attributes in <ref type="table" target="#tab_1">Table II</ref>. The distribution of each attribute in our dataset will be presented in subsequent sections and visualized in <ref type="figure">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Statistical Analysis</head><p>As shown in <ref type="figure">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Baseline Construction</head><p>Due to there is no code released for event related tracking, in this work, we construct many baseline trackers by fusing visible frames and event flows for future works to compare.</p><p>Representative Trackers: Three kinds of representative tracking frameworks are explored in this work: 1). Binary Classification based Trackers: MDNet <ref type="bibr" target="#b13">[14]</ref>, RT-MDNet <ref type="bibr" target="#b14">[15]</ref>, VITAL <ref type="bibr" target="#b37">[38]</ref>, Meta-Tracker <ref type="bibr" target="#b55">[54]</ref>, and MANet <ref type="bibr" target="#b56">[55]</ref>. 2). Correlation Filter based Trackers: KCF <ref type="bibr" target="#b57">[56]</ref>, STRCF <ref type="bibr" target="#b58">[57]</ref>, MOSSE <ref type="bibr" target="#b59">[58]</ref>, CSK <ref type="bibr" target="#b60">[59]</ref>, CN <ref type="bibr" target="#b61">[60]</ref>, DAT <ref type="bibr" target="#b62">[61]</ref>, LDES <ref type="bibr" target="#b63">[62]</ref>. 3). Siamese Matching based Trackers: SiamFC <ref type="bibr" target="#b26">[27]</ref>, SiamFC++ <ref type="bibr" target="#b64">[63]</ref>, SiamRPN <ref type="bibr" target="#b65">[64]</ref>, SiamRPN++ <ref type="bibr" target="#b28">[29]</ref> (AlexNet, ResNet50, and Long-term versions), ATOM <ref type="bibr" target="#b66">[65]</ref>, DIMP <ref type="bibr" target="#b29">[30]</ref>, PrDIMP <ref type="bibr" target="#b67">[66]</ref>, SiamRCNN <ref type="bibr" target="#b30">[31]</ref>, Ocean <ref type="bibr" target="#b68">[67]</ref>, and SiamDW <ref type="bibr" target="#b69">[68]</ref>.</p><p>Various Fusion Strategies: Three kinds of fusion strategies are considered when extending the aforementioned trackers, including: 1). Early Fusion denotes the strategy that fuses the input data before feeding them into the tracking model. In this paper, two kinds of operations are explored, more detail, we first simply add or concatenate corresponding RGB and event frame as one unified data for tracking. Therefore, existing RGB trackers can be directly tested as baseline algorithms to compare for future works. 2). Middle Fusion is also termed Feature Fusion and it is widely used in current multimodal fusion methods. In this work, we consider the following approaches for fusion. 3). Late Fusion (or response fusion) target at combining the response score or activation map output from tracking model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion</head><p>In this section, we give a direct comparison between Visible-Event and other dual-modal tracking tasks, including RGB-Thermal <ref type="bibr" target="#b71">[70]</ref> and RGB-Depth. These two tasks also attempt to fuse dual-modalities for robust object tracking. For the RGB-Thermal, the thermal sensor can sense the temperature of the surface of the object and is not affected by the illumination. Therefore, it has a long sensing distance and works well in the nighttime. However, this sensor is sensitive to thermal cross-over, i.e., the image quality is bad when the target object has similar temperature with background, and motion blur. The high price is also one of the reasons restricting its wide applications. For the RGB-Depth, the depth sensors can perceive objects well in 3D space, however, it may only work well in local space due to the fact that its sensing distance is limited (usually less than 10 meters). Also, it can't handle the issue of low light and high speed. In contrast, the Event cameras, such as the Dynamic Vision Sensor (DVS) <ref type="bibr" target="#b4">[5]</ref>, are bio-inspired vision sensors that output pixel-level brightness changes instead of standard intensity frames. They offer significant advantages over standard cameras, namely a very high dynamic range, no motion blur, and a latency in the order of microseconds. The following tutorials are recommended to have a general understanding of Event cameras. <ref type="bibr" target="#b1">2</ref> The study on object tracking using Event cameras is a new arising topic, therefore, many problems are needed to be addressed to achieve reliable object tracking in challenging scenarios. For example, how to represent the event flows to fully exploit the spatio-temporal information, how to design efficient neural networks like spiking neural networks for effective feature learning. More importantly, there are still no public large-scale realistic visible-event dataset and baseline methods for object tracking which seriously limited the development of this research direction. In this work, we propose a large-scale benchmark dataset termed VisEvent to handle this problem, some sample images are visualized in <ref type="figure" target="#fig_2">Fig. 5</ref>. In addition, we also construct multiple baseline trackers by extending visible trackers into dual-modality versions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation and Overview</head><p>Based on our proposed VisEvent dataset, we first extend current trackers which are developed for RGB videos into dual-modality versions, and evaluate their results. According to the experimental results, we observe that existing trackers are less effective on our dataset even though deep neural networks and regular attention modules are used. For instance, as shown in <ref type="figure" target="#fig_5">Fig. 7</ref>, the SiamRPN++ <ref type="bibr" target="#b28">[29]</ref> and SuperDiMP <ref type="bibr" target="#b29">[30]</ref> only attains 0.576|0.410 and 0.489|0.320 on precision and success plot, respectively. MDNet <ref type="bibr" target="#b13">[14]</ref> with channel and spatial attention only achieves 0.456|0.273 and 0.455|0.270, as listed in <ref type="table" target="#tab_1">Table VI</ref>. How to design a more effective information fusion module for the visible-event tracking is still a question worth exploring.</p><p>Here, we would like to introduce the representation of event flows and visible images. We simply use the event images transformed from event flows to fuse with visible images. More importantly, a simple but effective feature fusion module is proposed to achieve interactive learning via information propagation between dual-modalities, termed cross-modality transformers (CMT). As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, we first extract the feature representations of visible and event images using CNN. Then, the candidate proposals are sampled and fed into the RoI align module for instance feature extraction. Afterwards, we attain the base vector m with element-wise multiplication operations based on the given feature representations of dual modalities. The base vector is used as the query vector to attend the two modalities (context vector) respectively to realize the interaction and transmission of information flows. After that, we use self-attention layers to boost the internal connections of each modality. Lastly, the two features are concatenated and fed into the classifier for tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Input Representation</head><p>Given the synchronous video frames and asynchronous event flows, how to represent and adaptively fuse the two modalities is the key for successful and reliable visual tracking on the VisEvent. From the perspective of perception principle, the visible cameras capture the global scene by recording the intensity of all pixels in a frame manner. Usually, the CNN is used to extract its feature representations, for example, the RT-MDNet <ref type="bibr" target="#b14">[15]</ref> uses three convolutional layers as their backbone network. Different from visible sensors, the event cameras asynchronously captures the variation in log-scale intensity (I), in other words, each pixel will output a discrete event independently when the visual change exceeds a threshold (?):</p><formula xml:id="formula_0">||log(It+1) ? log(It)|| ? ?<label>(1)</label></formula><p>In practice, we use a 4-tuple {x, y, t, p} to represent the discrete event of a pixel captured with DVS, where the x, y are spatial coordinates, t is the timestamp, and p is the polarity of brightness variation, i.e., 1 and -1 are used to denote the ON event (increase) and OFF event (decrease) respectively. A comparison of sampling mechanism of visible and event cameras is visualized in <ref type="figure">Fig. 2</ref>.</p><p>To fully utilize the benefits of CNN, previous event trackers <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b42">[43]</ref> usually transform the asynchronous event flows into synchronous event image by stacking the events in a fixed time interval. In this work, we also adopt such transformation to get the event images but focus on designing novel feature fusion modules for high-performance tracking. In the subsequent subsection, we will introduce our proposed crossmodality transformer for interactive dual-modal information propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross-Modality Transformer for Fusion</head><p>Following RT-MDNet <ref type="bibr" target="#b14">[15]</ref>, we take the three convolutional layers as the shared backbone of our tracker. Once we obtain the feature representation of dual modalities, we can directly fuse them for tracking. However, the dual features are extracted independently thus lack of interactive feature learning which may limit its representation ability. Many works demonstrate that joint feature learning between multi-modal data will bring more powerful feature representation. Inspired by previous works <ref type="bibr" target="#b72">[71]</ref>, <ref type="bibr" target="#b73">[72]</ref>, the Cross-Modality Transformer (termed CMT) is proposed in this work to enhance the message passing between dual-modalities. This module is developed based on attention mechanism which targets at retrieving information from context vectors y j based on query x. Usually, we can first compute the similarity score a j between the query x and context vector y j using MLP layers. Then, this score will be normalized with Softmax operator. Finally, the context vectors will be weighted and summed as the output of an attention layer: Att X?Y (x, {y j }) = j ? j y j . The widely used self-attention layer <ref type="bibr" target="#b74">[73]</ref> is a special case of attention family, as its query vector x is actually from the context vectors.</p><p>In our scenario, we have dual-modality features which can be used to attend to each other using cross-attention model, i.e., from RGB to event, and from event to RGB. As shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, the features of RGB frame and event flows are firstly fed into a cross-attention model to guide the information propagation along with both directions. Formally, we use F v and F e to denote the initial feature obtained from the CNN backbone network. Then, these two feature maps are added along the channel dimension and reshaped into feature vectors F v andF e . A base vector m can be attained by elementwise product between input features of dual-modalities, i.e., m =F v F e . The base vector m is used as the query feature to attend the context vectors, i.e., visible and event features respectively:</p><formula xml:id="formula_1">Fe = CrossAttV ?E (m, Fe),Fv = CrossAttE?V (m, Fv) (2)</formula><p>Therefore, the joint cross-modality representations can be attained with cross-attention model which can align the dualmodalities by exchanging the information.</p><p>To boost the internal connections, we introduce the selfattention layers <ref type="bibr" target="#b74">[73]</ref> based on the output of cross-attention model:</p><formula xml:id="formula_2">Fe = Self AttE?E(Fe,Fe), Fv = Self AttV ?V (Fv,Fv) (3)</formula><p>For simplicity, we take the event featureF e as an example and similar operations are implemented for visible features. Specifically speaking, we first use two FC layers with weights W h and W g to process the inputF e separately. The output will be fed into a Softmax layer and multiplied with the results of another branch (i.e., W o * F e ). Finally, we feed these results into an FC layer with weights W p to get the attended event features. The aforementioned process can be summarized as: Fe = Wp(Sof tmax((W h * Fe) * (Wg * Fe)) * (Wo * Fe)) <ref type="bibr" target="#b3">(4)</ref> where W p , W h , W g and W o are weights of different FC layers, * is the multiplication operation, Sof tmax denotes Softmax layer. Then, we feed the attended features into the FC layers to output final feature vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>In this work, we follow binary classification based tracking framework <ref type="bibr" target="#b14">[15]</ref> and conduct tracking by discriminating the given proposal is a target object or not. In the training phase, we introduce a set of domain-specific layers (DS layers) for each video sequence to learn the shared features which are only used in the training phase. The binary cross-entropy loss and instance embedding loss are used for the optimization of our network. we refer the readers to check their paper for the details of the two loss functions <ref type="bibr" target="#b14">[15]</ref>. For our baseline, we first extend MDNet into dual-modality and train it on training subset of VisEvent for 50 epochs. Learning rate is 0.0001, batch size is 8, other parameters are default. The training costs about 3 hours. For the first frame, we extract 500 positive and 5000 negative samples and train an online classifier for 50 iterations. For other trackers, the LF and MF based trackers are trained on VisEvent with its default settings. We adopt the pre-trained models of EF based trackers for the testing. All extended trackers have been released to help researchers re-produce our experiments <ref type="bibr" target="#b2">3</ref> .</p><p>In the testing phase, we train an online classifier using samples extracted from the first frame. For the subsequent frames, we extract proposals with the Gaussian sampling method around tracking result of the previous frame, then, feed them into the classifier to get the response score. The proposal with the maximum score will be chosen as the tracking result of the current frame. In addition, we also use hard sample mining and online update strategy for better tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Evaluation Metric</head><p>In this work, our tracker is trained on the training subset of our proposed VisEvent dataset which contains 500 video sequences. For the testing, we evaluated on the testing subset (320 videos) of VisEvent, and also two simulated tracking 3 https://github.com/wangxiao5791509/RGB-DVS-SOT-Baselines datasets, i.e., the OTB-DVS and VOT-DVS. We adopt simulation toolkit V2E <ref type="bibr" target="#b75">[74]</ref> to complete the conversion. We also report and compare with other state-of-the-art trackers on FE108 dataset 4 . It totally contains 108 videos and the authors separate them into 76 and 32 videos for the training and testing, respectively.</p><p>Two popular metrics are adopted for the evaluation of tracking performance, including Precision Plot and Success Plot. Specifically, Precision Plot illustrates the percentage of frames where the center location error between the object location and ground truth is smaller than a pre-defined threshold (default value is 20-pixel). Success Plot demonstrates the percentage of frames the IoU of the predicted and the ground truth bounding boxes is higher than a given ratio. The dataset, source code, and evaluation toolkit can be found at https: //github.com/wangxiao5791509/VisEvent_SOT_Benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Benchmark Comparison</head><p>Results on VisEvent dataset. In this work, we construct multiple baseline methods for future works to compare on our dataset and report part of these results in <ref type="figure" target="#fig_5">Fig. 7</ref>. Specifically, we can find that the correlation filter based trackers achieve lower scores on this benchmark due to the manually designed features used in their model, such as HOG, gray pixels, etc. With the help of the deep features, the binary classification based trackers achieve better performance than correlation filter based trackers. For example, the MDNet <ref type="bibr" target="#b13">[14]</ref>, VITAL <ref type="bibr" target="#b37">[38]</ref>, RT-MDNet <ref type="bibr" target="#b14">[15]</ref> get 0.627|0.426, 0.616|0.415, 0.560|0.352 respectively, while the CN <ref type="bibr" target="#b61">[60]</ref> and KCF <ref type="bibr" target="#b57">[56]</ref> only achieve 0.458|0.269 and 0.453|0.260.</p><p>Interestingly, we also find that the Siamese network based trackers usually occupy the top few rankings of other datasets, but are not always very prominent on our dataset. Among of them, the SiamRPN++ <ref type="bibr" target="#b28">[29]</ref> achieves 0.538|0.379, 0.576|0.410 and 0.539|0.387, when AlexNet <ref type="bibr" target="#b76">[75]</ref>, ResNet50 <ref type="bibr" target="#b77">[76]</ref>, and long-term versions are evaluated. These results are significantly worse than MDNet and its multiple extensions which may demonstrate that the number of layers of the backbone network is not the most important role for Visible-Event tracking. The long-term version of SiamRPN++ did not increase the overall score when comparing it with its short-term version. This phenomenon fully demonstrates the challenge of our proposed tracking dataset. Compared with these works, we can attain better results with the help of the CMT module, i.e., 0.632|0.430 on precision and success plot respectively. Our results are even better than DiMP50 <ref type="bibr" target="#b29">[30]</ref> which is a very strong tracker developed based on deep residual networks (50 layers, Ours: 3 convolutional layers). These results fully demonstrate the effectiveness and advantages of our proposed baseline tracker.</p><p>Results on FE108 dataset. As shown in <ref type="table" target="#tab_1">Table III</ref>, we compare with multiple strong Siamese trackers on FE108 dataset, including SiamRPN++ <ref type="bibr" target="#b28">[29]</ref>, SiamBAN <ref type="bibr" target="#b78">[77]</ref>, SiamFC++ <ref type="bibr" target="#b64">[63]</ref>, and KYS <ref type="bibr" target="#b79">[78]</ref>. We can find that the results of these trackers on this benchmark are poor. Our tracker based on MDNet and ATOM attains 0.578|0.351 and 0.794|0.543, respectively, which are significantly better than these trackers. These experiments on this benchmark also validated the advantages of our trackers on the RGB-DVS tracking.</p><p>Results on Artificial OTB-DVS <ref type="bibr" target="#b80">[79]</ref> and VOT-DVS <ref type="bibr" target="#b81">[80]</ref>. To comprehensively validate the effectiveness of our model, we also test it on two popular tracking datasets, including OTB-DVS and VOT-DVS dataset, and report their AUC score in <ref type="table" target="#tab_1">Table IV</ref>. Specifically, we can find that our model achieves 0.68 and 0.33 based on RGB videos and Event images, respectively, on the OTB-DVS. Better results can be obtained if both of them are used, i.e., 0.69 on this dataset. For the VOT-DVS, we get 0.39, 0.18, and 0.43 on these three settings, respectively. These results fully demonstrate the effectiveness of event flows for the improvement of tracking performance. It is worthy to note that we only conduct self-comparison on the two datasets and do not compare with other trackers, as the two datasets are all simulated data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization</head><p>In addition to the aforementioned quantitative analysis, in <ref type="figure">Fig. 8</ref>, we also give some visualization of tracking results of ours and compared trackers. We can find that current strong trackers PrDiMP50 <ref type="bibr" target="#b67">[66]</ref>, SuperDiMP <ref type="bibr" target="#b29">[30]</ref>, and SiamRPN++ <ref type="bibr" target="#b28">[29]</ref> are still suffer from motion blur, fast motion and low illumination, etc. The online trackers MDNet <ref type="bibr" target="#b13">[14]</ref>, VITAL <ref type="bibr" target="#b37">[38]</ref>, and ours can handle these scenarios with the help of event images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ablation Study</head><p>Influence of Input Modalities: In this work, we report the tracking results with single modality to validate the effectiveness of combing visible and event sensors. As shown in <ref type="table" target="#tab_5">Table V</ref>, the baseline tracker MDNet achieves 0.605|0.412 when only visible frames are used. If only the event frames are used, it can achieve 0.460|0.280. It is significantly worse than tracking results with visible cameras which demonstrates that only the event sensors are not enough for practical tracking. Because it can only capture where events occurred in the scene and provide outline shape information. This information is very important for tracking but we still need the appearance and detailed texture information to discriminate the target object from other distractors. When we combine both modalities for tracking, the overall performance can be improved to 0.627|0.426.</p><p>In addition, we also test the PrDIMP18 tracker based on RGB and event images only, and get the 0.554|0.407 and 0.404|0.256, respectively. When we fuse the two modalities with early fusion, we can get 0.578|0.421, which is significantly better than single modality only. It also attained better  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Influence of Challenging Factors:</head><p>In this work, 17 attributes are considered for the VisEvent tracking dataset. In this section, we report most of them in <ref type="figure" target="#fig_6">Fig. 9</ref>, including low illumination and fast motion. It is easy to find that our proposed tracker attains the top-5 even the best tracking performance under these attributes. These results demonstrate the effectiveness of our proposed modules for tracking under extremely challenging scenarios. More results can be found on our project page.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results of Various Fusion Strategies</head><p>In this section, we consider the following feature information fusion approaches based on MDNet <ref type="bibr" target="#b13">[14]</ref> for robust tracking: 1).Concatenate; 2).Add; 3).1 ? 1 Conv : the convolutional layer with kernel 1?1; 4).CAtt : channel attention; 5).SAtt : spatial attention; 6).CAM : cross attention module proposed in <ref type="bibr" target="#b70">[69]</ref>. According to Table VI, we can find that the simple concatenate of dual features can bring the best tracking performance, i.e., 0.627|0.426. Interestingly, the add, channel attention, and spatial attention all achieve inferior results. We think this may be caused by the fact that the event images only provide shape information and it may hurt the visible features by simple add, and two modal information are saved to the greatest extent with concatenating operation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Efficiency Analysis</head><p>Due to the proposed CMT is a general module for visibleevent tracking, therefore, its efficiency mainly depends on the used baseline tracker. For example, when integrating our CMT into dual-modality RT-MDNet, it can run at about 14 FPS. With the help of CMT, we achieve the best tracking performance on the proposed benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Failed Case Analysis</head><p>Although this paper achieves good results on some videos of our dataset, however, this problem is still far from being solved. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, the fast-moving tennis ball, and the star in the shaking cameras are hard to track. The tennis ball in RGB frames is almost invisible due to the influence of motion blur and clutter background. In contrast, the corresponding event images capture the moving ball well, but the tracker also failed to re-locate the target, as only a local search scheme is used in most of the evaluated trackers. For the star in the second group, the trackers are also easily influenced by distractors in the background. Therefore, how to represent the event streams more accurately is a worthy study problem. As we can see from the two cases, simply stacking the events in a fixed time interval also brings the "long-tail" issue. We leave this as our future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORKS</head><p>In this paper, we propose a new and large-scale object tracking benchmark by combing the visible and event sensors. It targets at providing the characteristic of high dynamic range and high temporal resolution for standard visible cameras with biologically inspired event cameras. This will widely extend the applications of current visual trackers in practical scenarios, such as high speed, low light, and clutter background. Our dataset contains 820 video pairs that are collected with DVS cameras and it involves multiple types of objects and scenarios. We provide multiple baseline methods by extending current state-of-the-art trackers into dual-modality versions. In addition, we also design a simple but effective baseline tracker by developing cross-modality transformer modules for interactive feature learning and fusion. Extensive experiments on the proposed VisEvent dataset fully demonstrate its good performance. We hope this work will boost the development of object tracking based on neuromorphic cameras. In our future works, we will continue to explore new architectures of pure spiking neural networks for this tracking task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Illustration of complementary characteristics of Visible and Event cameras. (a-c) are relatively simple for Event cameras, while (d) are easy for Visible cameras.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>(a). The DVS camera used for data collection; (b). RGB frames and event flows output from DVS sensor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Representative samples of our newly proposed VisEvent tracking dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a)Concat : We simply concatenate the features of dual-modalities to get the fused representation for tracking. (b)Add : The two features are added together as the final features. (c)1 ? 1 Conv : The convolutional layer with kernel 1 ? 1 is used for fusing the feature maps. (d)CAtten : The widely used channel attention is employed for fusion. (e)SAtten : Spatial attention is used for feature fusion. (f )CAM : Cross attention module proposed in [69].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>An overview of our proposed tracking framework via collaboration of visible frame and event flows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Tracking results on the proposed VisEvent dataset (part of the constructed baselines are reported in this figure). EF and MF denotes the early and middle fusion strategy used for the extension of corresponding tracker respectively. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Tracking results under different challenging scenarios. Best viewed by zooming in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 .</head><label>10</label><figDesc>Failed cases of our and the evaluated trackers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>respectively, to help readers better understand their unique advantages. Despite benefits, the event cameras can't capture slow-arXiv:2108.05015v3 [cs.CV] 28 Jun 2022 Fig. 2. Sampling mechanisms of visible and event cameras. Each pixel in visible image records the light intensity (I) in a synchronous way; while each pixel in the biologically inspired event camera asynchronously reflects the changes in lighting intensity. Usually, we use 1/-1 to denote ON/OFF event (enhancement/diminished light). V d is the neuronal membrane potential.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>OF EXISTING EVENT DATASETS FOR OBJECT TRACKING. # DENOTES THE NUMBER OF CORRESPONDING ITEM.</figDesc><table><row><cell>Datasets</cell><cell cols="5">Year #Videos #Frames #Resolution #Attributes</cell><cell>Aim</cell><cell>Absent Color Real Public</cell></row><row><cell>VOT-DVS [51]</cell><cell>2016</cell><cell>60</cell><cell>-</cell><cell>240 ? 180</cell><cell>-</cell><cell>Eval</cell></row><row><cell>TD-DVS [51]</cell><cell>2016</cell><cell>77</cell><cell>-</cell><cell>240 ? 180</cell><cell>-</cell><cell>Eval</cell></row><row><cell>Ulster [11]</cell><cell>2016</cell><cell>1</cell><cell>9,000</cell><cell>240 ? 180</cell><cell>-</cell><cell>Eval</cell></row><row><cell>EED [52]</cell><cell>2018</cell><cell>7</cell><cell>234</cell><cell>240 ? 180</cell><cell>-</cell><cell>Eval</cell></row><row><cell>FE108 [13]</cell><cell>2021</cell><cell>108</cell><cell>208,672</cell><cell>346 ? 260</cell><cell>-</cell><cell>Train/Eval</cell></row><row><cell cols="2">VisEvent (Ours) 2021</cell><cell>820</cell><cell>371,127</cell><cell>346 ? 260</cell><cell>17</cell><cell>Train/Eval</cell></row></table><note>Fig. 4. Distribution of the proposed VisEvent dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>(left sub-figure), our proposed VisEvent tracking dataset contains 820 video sequence pairs (371,128 RGB frames totally), the minimum, maximum, and average length are 18, 6246, and 450 frames, respectively. The frame rate of visible videos is about 25 FPS. For the distribution of video length, we have 276, 222, 76, 65, 75, 111 videos for TABLE II DESCRIPTION OF 17 ATTRIBUTES IN OUR VISEVENTDATASET.    We can find that our dataset is suitable for the evaluation of both short-term and long-term tracking. For the challenging factors, we have [136,<ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b74">73,</ref><ref type="bibr" target="#b76">75,</ref><ref type="bibr" target="#b65">64,</ref> 114,<ref type="bibr" target="#b58">57,</ref> 131, 211,<ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b71">70,</ref> 90,<ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b64">63,</ref><ref type="bibr" target="#b37">38,</ref> 239]  videos for the 17 attributes listed inTable II, respectively. Our dataset contains many videos with camera motion, background clutter, scale variation, occlusion, and motion of distractors. Experimental results in section V show that the visual tracking problem in these scenarios is far from being solved.</figDesc><table><row><cell>Attributes</cell><cell>Definition</cell></row><row><cell>01. CM</cell><cell>Abrupt motion of the camera</cell></row><row><cell>02. ROT</cell><cell>Target object rotates in the video</cell></row><row><cell>03. DEF</cell><cell>The target is deformable</cell></row><row><cell>04. FOC</cell><cell>Target is fully occluded</cell></row><row><cell>05. LI</cell><cell>Low illumination</cell></row><row><cell>06. OV</cell><cell>The target completely leaves the video sequence</cell></row><row><cell>07. POC</cell><cell>Partially occluded</cell></row><row><cell>08. VC</cell><cell>Viewpoint change</cell></row><row><cell>09. SV</cell><cell>Scale variation</cell></row><row><cell>10. BC</cell><cell>Background clutter</cell></row><row><cell>11. MB</cell><cell>Motion blur</cell></row><row><cell>12. ARC</cell><cell>The ratio of bounding box aspect ratio is outside the range [0.5, 2]</cell></row><row><cell>13. FM</cell><cell>The motion of the target is larger than the size of its bounding box</cell></row><row><cell>14. NMO</cell><cell>No motion</cell></row><row><cell>15. IV</cell><cell>Illumination variation</cell></row><row><cell>16. OE</cell><cell>Over exposure</cell></row><row><cell>17. BOM</cell><cell>Influence of background object motion for Event camera</cell></row><row><cell cols="2">diverse ranges, i.e., [1-100, 101-300, 301-500, 501-700, 701-</cell></row><row><cell cols="2">1000, 1000+].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III COMPARISON</head><label>III</label><figDesc>ON THE FE108 DATASET. THE RESULTS OF BASELINE TRACKERS ARE BORROWED FROM FE108 BENCHMARK.</figDesc><table><row><cell cols="3">SiamRPN++ [29] SiamBAN [77] SiamFC++ [63]</cell><cell>KYS [78]</cell><cell cols="3">ATOM [65] Ours (MDNet) Ours (ATOM)</cell></row><row><cell>0.335|0.218</cell><cell>0.374|0.225</cell><cell>0.391|0.238</cell><cell cols="2">0.410|0.266 0.713|0.465</cell><cell>0.578|0.351</cell><cell>0.794|0.543</cell></row></table><note>Fig. 8. Visualization of ours and other strong trackers.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV AUC</head><label>IV</label><figDesc>OF OTB-DVS AND VOT-DVS DATASETS. motion blur and low illumination attributes. These experimental results fully demonstrate the useful clues provided by event flows. Similar conclusions can also be drawn from the results based on RT-MDNet. Therefore, it will be an interesting research direction of reliable object tracking by the collaboration of video frame and event flows.Influence of Cross-Modality Transformer: To better understand the contributions of the feature fusion module in our proposed trackers, we integrate it with MDNet and RT-MDNet to check its influence on final tracking. The tracking results are reported inTable V. When integrated into MDNet, clearly, its results 0.627|0.426 can be improved to 0.632|0.430. It also helps RT-MDNet by improving the results from 0.560|0.352 to 0.564|0.359. These results demonstrate the effectiveness of the feature-level information fusion module CMT for tracking.</figDesc><table><row><cell>Dataset</cell><cell>RGB</cell><cell>Event</cell><cell>Both</cell><cell>Dataset</cell><cell>RGB</cell><cell>Event</cell><cell>Both</cell></row><row><cell>OTB-DVS</cell><cell>0.68</cell><cell>0.33</cell><cell>0.69</cell><cell>VOT-DVS</cell><cell>0.39</cell><cell>0.18</cell><cell>0.43</cell></row><row><cell cols="2">results on the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V</head><label>V</label><figDesc>UP: COMPONENT ANALYSIS OF OUR TRACKING MODEL; DOWN: MODALITY ANALYSIS OF PRDIMP18 ON ALL TESTING SET, MOTION BLUR (MB), AND LOW ILLUMINATION (LI) SUBSET.</figDesc><table><row><cell>Index Frame Event</cell><cell>CMT</cell><cell>Ours (MDNet)</cell><cell>Ours (RT-MDNet)</cell></row><row><cell>x</cell><cell></cell><cell>0.605|0.412</cell><cell>0.538|0.342</cell></row><row><cell>y</cell><cell></cell><cell>0.460|0.280</cell><cell>0.380|0.216</cell></row><row><cell>z</cell><cell></cell><cell>0.627|0.426</cell><cell>0.560|0.352</cell></row><row><cell>{</cell><cell></cell><cell>0.632|0.430</cell><cell>0.564|0.359</cell></row><row><cell cols="3">Index Frame Event PrDIMP18(ALL) PrDIMP18(MB)</cell><cell>PrDIMP18(LI)</cell></row><row><cell>|</cell><cell>0.554|0.407</cell><cell>0.443|0.337</cell><cell>0.483|0.361</cell></row><row><cell>}</cell><cell>0.404|0.256</cell><cell>0.339|0.227</cell><cell>0.312|0.202</cell></row><row><cell></cell><cell>0.578|0.421</cell><cell>0.471|0.359</cell><cell>0.517|0.375</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI RESULTS</head><label>VI</label><figDesc>WITH VARIOUS FUSION METHODS.</figDesc><table><row><cell>Method</cell><cell>Concat</cell><cell>Add</cell><cell>1 ? 1 Conv</cell><cell>CAtt</cell><cell>SAtt</cell><cell>CAM</cell><cell>CMT (Ours)</cell></row><row><cell>Pre. Plot</cell><cell>0.627</cell><cell>0.471</cell><cell>0.617</cell><cell>0.456</cell><cell>0.455</cell><cell>0.595</cell><cell>0.632</cell></row><row><cell>Suc. Plot</cell><cell>0.426</cell><cell>0.287</cell><cell>0.422</cell><cell>0.273</cell><cell>0.270</cell><cell>0.402</cell><cell>0.430</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The video tutorial of this work can be found at https://youtu.be/ vGwHI2d2AX0</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://youtu.be/D6rv6q9XyWU</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://zhangjiqing.com/dataset/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learn to match: Automatic matching network design for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards more flexible and accurate object tracking with natural language: Algorithms and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Tracking by joint local and global search: A target-aware attention based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic attention guided multi-trajectory analysis for single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A 128? 128 120 db 15 ?s latency asynchronous temporal contrast vision sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lichtsteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Posch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Solid-State Circuits</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="566" to="576" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end learning of object motion estimation from retinal events for event-based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">e-tld: Event-based framework for dynamic object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ussa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00855</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event-driven stereo visual tracking algorithm to solve object occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Camu?as-Mesa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serrano-Gotarredona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Ieng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benosman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Linares-Barranco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4223" to="4237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Asynchronous tracking-by-detection on adaptive time surfaces for event-based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="473" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Dashnet: A hybrid artificial and spiking neural network for high-speed object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.12942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Combined frame-and event-based detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Moeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbr?ck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Symposium on Circuits and Systems (ISCAS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2511" to="2514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Event-guided structured output tracking of fast-moving objects using a celex sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2413" to="2417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Object tracking by jointly exploiting frame and event domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F X W B Y B D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4293" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Real-time mdnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A survey of appearance models in visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acm Transactions on Intelligent Systems Technology</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="478" to="488" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep visual tracking: Review and experimental comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="323" to="338" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Eventbased vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bartolozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Conradt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Action-driven visual object tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2239" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A biologically inspired appearance model for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2357" to="2370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatially regularized structural support vector machine for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3024" to="3034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Location-aware and regularization-adaptive correlation filters for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2430" to="2442" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning scale-adaptive tight correlation filter for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="270" to="283" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Toward occlusion handling in visual tracking via probabilistic finite state machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1726" to="1738" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6309" to="6318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4282" to="4291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning discriminative model prediction for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6182" to="6191" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Siam r-cnn: Visual tracking by re-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual object tracking by hierarchical attention siamese network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3068" to="3080" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-performance visual tracking with extreme learning machine framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2781" to="2792" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Target-aware state estimation for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2908" to="2920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual-regression model for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="page" from="364" to="374" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hierarchical spatial-aware siamese network for thermal infrared object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="71" to="81" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sint++: Robust visual tracking via adversarial positive instance generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4864" to="4873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vital: Visual tracking via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8990" to="8999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Effects of blur and deblurring to visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07904</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rgb-t object tracking: benchmark and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page">106977</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Cdtb: A color and depth visual object tracking dataset and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kapyla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durmush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Long-term object tracking with a moving event camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Orchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Bmvc</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">241</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">High-speed event camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">O</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andrade-Cetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J. Sol?</forename><surname>Ortega</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The 31st British Machine Vision Virtual Conference</title>
		<meeting>the The 31st British Machine Vision Virtual Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Haste: multi-hypothesis asynchronous speeded-up tracking of events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alzugaray</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">31st British Machine Vision Virtual Conference (BMVC 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Institute of Robotics and Intelligent Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eth Zurich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">744</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Spiking neural network-based target tracking control for autonomous mobile robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing and Applications</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1839" to="1847" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Object tracking on event cameras with offline-online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAAI Transactions on Intelligence Technology</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Asynchronous, photometric feature tracking using events and frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rebecq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gallego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="750" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Asynchronous photometric feature tracking using events and frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">--</forename><surname>Eklt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="601" to="618" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A framework for the general design and computation of hybrid neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dvs benchmark datasets for object tracking, action recognition, and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroscience</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">405</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Event-based moving object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mitrokhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ferm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Parameshwara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Got-10k: A large high-diversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-adapter rgbt tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2262" to="2270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="583" to="596" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning spatialtemporal regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4904" to="4913" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="702" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van De Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1090" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">In defense of colorbased model-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Possegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mauthner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2113" to="2120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Robust estimation of similarity transformation for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8666" to="8673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Siamfc++: Towards robust and accurate visual tracking with target estimation guidelines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8971" to="8980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Atom: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4660" to="4669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Probabilistic regression for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7183" to="7192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Ocean: Object-aware anchor-free tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deeper and wider siamese networks for real-time visual tracking</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4591" to="4600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Proposal-free one-stage referring expression via grid-word cross-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Mfgnet: Dynamic modality-aware filter generation for rgb-t tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5103" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">V2e: From video frames to realistic dvs event camera streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Delbruck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07722</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Siamese box adaptive network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6668" to="6677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Know your surroundings: Exploiting scene information for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Goutam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Radu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The seventh visual object tracking vot2019 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Kamarainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Drbohlav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
