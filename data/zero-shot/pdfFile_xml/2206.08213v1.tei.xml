<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Closer Look at Smoothness in Domain Adversarial Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harsh</forename><surname>Rangwani</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumukh</forename><forename type="middle">K</forename><surname>Aithal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Mishra</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arihant</forename><surname>Jain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh Babu</surname></persName>
						</author>
						<title level="a" type="main">A Closer Look at Smoothness in Domain Adversarial Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain adversarial training has been ubiquitous for achieving invariant representations and is used widely for various domain adaptation tasks. In recent times, methods converging to smooth optima have shown improved generalization for supervised learning tasks like classification. In this work, we analyze the effect of smoothness enhancing formulations on domain adversarial training, the objective of which is a combination of task loss (eg. classification, regression etc.) and adversarial terms. We find that converging to a smooth minima with respect to (w.r.t.) task loss stabilizes the adversarial training leading to better performance on target domain. In contrast to task loss, our analysis shows that converging to smooth minima w.r.t. adversarial loss leads to sub-optimal generalization on the target domain. Based on the analysis, we introduce the Smooth Domain Adversarial Training (SDAT) procedure, which effectively enhances the performance of existing domain adversarial methods for both classification and object detection tasks. Our analysis also provides insight into the extensive usage of SGD over Adam in the community for domain adversarial training.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Domain Adversarial Training <ref type="bibr">(Ganin &amp; Lempitsky, 2015)</ref> (DAT) refers to adversarial learning of neural network based feature representations that are invariant to the domain. For example, the learned feature representations for car images from the Clipart domain should be similar to that from the Web domain. DAT has been widely useful in diverse areas (cited 4200 times) such as recognition * Equal contribution 1 Video Analytics Lab, Indian Institute of Science, Bengaluru, India 2 PES University, Bengaluru 3 Amazon, India (Work done at Indian Institute of Science, Bengaluru). Correspondence to: Harsh Rangwani &lt;harshr@iisc.ac.in&gt;.</p><p>Proceedings of the 39 th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022. Copyright 2022 by the author(s). <ref type="bibr" target="#b12">(Long et al., 2018;</ref><ref type="bibr">Cui et al., 2020;</ref><ref type="bibr" target="#b19">Rangwani et al., 2021)</ref>, fairness <ref type="bibr">(Adel et al., 2019)</ref>, object detection <ref type="bibr">(Saito et al., 2019)</ref>, domain generalization <ref type="bibr" target="#b10">(Li et al., 2018)</ref>, image-toimage translation <ref type="bibr" target="#b11">(Liu et al., 2017)</ref> etc. The prime driver of research on DAT is its application in unsupervised Domain Adaptation (DA), which aims to learn a classifier using labeled source data and unlabeled target data, such that it generalizes well on target data. Various enhancements like superior objectives <ref type="bibr">(Acuna et al., 2021;</ref><ref type="bibr">Zhang et al., 2019)</ref>, architectures <ref type="bibr" target="#b12">(Long et al., 2018)</ref>, etc. have been proposed to improve the effectiveness of DAT for unsupervised DA.</p><p>As DAT objective is a combination of Generative Adversarial Network (GAN) <ref type="bibr">(Goodfellow et al., 2014)</ref> (adversarial loss) and Empirical Risk Minimization (ERM) (Vapnik, 2013) (task loss) objectives, there has not been much focus on explicitly analyzing and improving the nature of optimization in DAT. In optimization literature, one direction that aims to improve the generalization focuses on developing algorithms that converge to a smooth (or a flat) minima <ref type="bibr">(Foret et al., 2021;</ref><ref type="bibr" target="#b4">Keskar &amp; Socher, 2017)</ref>. However, we find that these techniques, when directly applied for DAT, do not significantly improve the generalization on the target domain (Sec. 7).</p><p>In this work, we analyze the loss landscape near the optimal point obtained by DAT to gain insights into the nature of optimization. We first focus on the eigen-spectrum of Hessian (i.e. curvature) of the task loss (ERM term for classification) where we find that using Stochastic Gradient Descent (SGD) as optimizer converges to a smoother minima in comparison to Adam <ref type="bibr" target="#b6">(Kingma &amp; Ba, 2014)</ref>. Further, we find that smoother minima w.r.t. task loss results in stable DAT leading to better generalization on the target domain. Contrary to task loss, we find that smoothness enhancing formulation for adversarial components worsens the performance, rendering techniques <ref type="bibr">(Cha et al., 2021)</ref> which enhance smoothness for all loss components ineffective. Hence we introduce Smooth Domain Adversarial Training (SDAT) <ref type="figure" target="#fig_0">(Fig. 1)</ref>, which aims to reach a smooth minima only w.r.t. task loss and helps in generalizing better on the target domain. SDAT requires an additional gradient computation step and can be combined easily with existing methods. We show the soundness of the SDAT method theoretically through a generalization arXiv:2206.08213v1 <ref type="bibr">[cs.</ref>LG] 16 Jun 2022  <ref type="bibr">(SDAT)</ref>. We demonstrate that converging to smooth minima w.r.t. adversarial loss leads to sub-optimal DAT. Due to this conventional approaches which smooth combination of task loss and adversarial loss lead to sub-optimal results. Hence, we propose SDAT which only focuses on smoothing task loss, leading to stable training which results in effective generalization on target domain. 2 bound (Sec. 4) on target error. We extensively verify the empirical efficacy of SDAT over DAT across various datasets for classification (i.e., DomainNet, VisDA-2017 and Office-Home) with ResNet and Vision Transformer (Dosovitskiy et al., 2020) (ViT) backbones. We also show a prototypical application of SDAT in DA for object detection, demonstrating it's diverse applicability. In summary, we make the following contributions:</p><p>? We demonstrate that converging to smooth minima w.r.t. task loss leads to stable and effective domain alignment through DAT, whereas smoothness enhancing formulation for adversarial loss leads to suboptimal performance via DAT.</p><p>? For enhancing the smoothness w.r.t. task loss near optima in DAT, we propose a simple, novel, and theoretically motivated SDAT formulation that leads to stable DAT resulting in improved generalization on the target domain.</p><p>2 Figures for the smooth minima and sharp minima are from <ref type="bibr">(Foret et al., 2021)</ref> and used for illustration purposes only. from the source domain's labeled data that generalizes well on the unseen data from the target domain <ref type="bibr" target="#b12">(Long et al., 2018;</ref><ref type="bibr">Acuna et al., 2021;</ref><ref type="bibr">Zhang et al., 2019;</ref><ref type="bibr" target="#b9">Kundu et al., 2021;</ref>. One of the most prominent lines of work is based on <ref type="bibr">DAT (Ganin &amp; Lempitsky, 2015)</ref>. It involves using an additional discriminator to distinguish between source and target domain features. A Gradient Reversal layer (GRL) is introduced to achieve the goal of learning domain invariant features. The follow-up works have improved upon this basic idea by introducing a class information-based discriminator (CDAN <ref type="bibr" target="#b12">(Long et al., 2018)</ref>), introducing a transferable normalization function <ref type="bibr" target="#b18">(Wang et al., 2019)</ref>, using an improved Margin Disparate Discrepancy <ref type="bibr">(Zhang et al., 2019)</ref> measure between source and target domain, etc. In this work, we focus on analyzing and improving such methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smoothness of Loss Landscape:</head><p>As neural networks operate in the regime of over parameterized models, low error on training data does not always lead to better generalization . Often it has been stated <ref type="bibr">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr">1994;</ref><ref type="bibr">He et al., 2019;</ref><ref type="bibr">Dziugaite &amp; Roy, 2017</ref>) that smoother minima does generalize better on unseen data. But until recently, this was practically expensive as smoothing required additional costly computations. Recently, a method called Sharpness Aware Minimization (SAM) <ref type="bibr">(Foret et al., 2021)</ref> for improved generalization has been proposed which finds a smoother minima with an additional gradient computation step. However, we find that just using SAM naively does not lead to improved generalization on target domain (empirical evidence in <ref type="bibr">Tab. 5,</ref><ref type="bibr">12 and 11)</ref>. In this work, we aim to develop solutions which converge to a smooth minima but at same time lead to better generalization on target domain, which is not possible just by using SAM. and ?max indicate the presence of smoother loss surface). Low range of eigenvalues (x-axis), T r(H) and ?max for SGD compared to Adam indicates that it reaches a smoother minima, which leads to a higher target accuracy. D) Validation accuracy and ?max comparison for SDAT and DAT across epochs, SDAT shows significantly stable training with low ?max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminaries</head><p>We will primarily focus on unsupervised DA where we have labeled source data S = {(x s i , y s i )} and unlabeled target data T = {(x t i )}. The source samples are assumed to be sampled i.i.d. from source distribution P S defined on input space X , similarly target samples are sampled i.i.d. from P T . Y is used for denoting the label set which is {1, 2, . . . , k} in our case as we perform multiclass (k) classification. We denote y : X ? Y a mapping from images to labels. Our task is to find a hypothesis function h ? that has a low risk on the target distribution. The source risk (a.k.a expected error) of the hypothesis h ? is defined with respect to loss function l as:</p><formula xml:id="formula_0">R l S (h ? ) = E x?P S [l(h ? (x), y(x))].</formula><p>The target risk R l T (h ? ) is defined analogously. The empirical versions of source and target risk will be denoted byR l S (h ? ) andR l T (h ? ). All notations used in paper are summarized in App. A. In this work we build on the DA theory of <ref type="bibr">(Acuna et al., 2021)</ref> which is a generalization of <ref type="bibr">Ben-David et al. (2010)</ref>. We first define the discrepancy between the two domains. Definition 3.1 (D ? h ? ,H discrepancy). The discrepancy between two domains P S and P T is defined as following:</p><formula xml:id="formula_1">D ? h ? ,H (P S ||P T ) := sup h ?H [E x?P S [l(h ? (x), h (x))]]? [E x?P T [? * (l(h ? (x), h (x)))]]<label>(1)</label></formula><p>Here ? * is a frenchel conjugate of a lower semi-continuous convex function ? that satisfies ?(1) = 0, and H is the set of all possible hypothesis (i.e. Hypothesis Space).</p><p>This discrepancy distance D ? h ? ,H is based on variational formulation of f-divergence <ref type="bibr" target="#b15">(Nguyen et al., 2010)</ref> for the convex function ?. The D ? h ? ,H is the lower bound estimate of the f-divergence function D ? (P S ||P T ) (Lemma 4 in <ref type="bibr">(Acuna et al., 2021)</ref>). We state a bound on target risk <ref type="bibr">Acuna et al., 2021)</ref>:</p><formula xml:id="formula_2">R l T (h ? ) based on D ? h ? ,H discrepancy (</formula><formula xml:id="formula_3">Theorem 1 (Generalization bound). Suppose l : Y ? Y ? [0, 1] ? dom ? * . Let h * be the ideal joint clas- sifier with least ? * = R l S (h * ) + R l T (h * ) (i.e. joint risk) in H.</formula><p>We have the following relation between source and target risk:</p><p>The first term in practice is empirically approximated by using finite samplesR l S (h ? ) and used as task loss (classification) for minimization. The empirical estimate of the second term is adversarial loss which is optimized using GRL as it has a min-max form. (Overview in <ref type="figure" target="#fig_0">Fig. 1</ref>) The above procedure composes DAT, and we use CDAN <ref type="bibr" target="#b12">(Long et al., 2018)</ref> as our default DAT method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis of Smoothness</head><p>In this section, we analyze the curvature properties of the task loss with respect to the parameters (?). Specifically, we focus on analyzing the Hessian of empirical source risk H = ? 2 ?R l S (h ? ) which is the Hessian of classification (task) loss term. For quantifying the smoothness, we measure the trace T r(H) and maximum eigenvalue of Hessian (? max ) as a proxy for quantifying smoothness. This is motivated by analysis of which states that the low value of ? max and T r(H) are indicative of highly smooth loss landscape <ref type="bibr">(Jastrzebski et al., 2020)</ref>. Based on our observations we articulate our conjecture below:</p><p>Conjecture 1. Low ? max for Hessian of empirical source risk (i.e. task loss) ? 2 ?R l S (h ? ) leads to stable and effective DAT, resulting in reduced risk on target domainR l T (h ? ).</p><p>For empirical verification of our conjecture, we obtain the Eigen Spectral Density plot for the HessianR l T (h ? ). We show the ? max , T r(H) and Eigen Spectrum for different algorithms, namely DAT w/ Adam, DAT w/ SGD and our proposed SDAT (which is described in detail in later sections) in <ref type="figure" target="#fig_1">Fig. 2</ref>. We find that high smoothness leads to better generalization on the target domain (Additional empirical evidence in <ref type="figure" target="#fig_3">Fig. 3A</ref>). We hypothesize that enforcing smoothness of classifier h ? leads to a smooth landscape for discrepancy (d ? S,T ) as it is also a function of h ? . The smooth landscape ensures stable minimization (of Eq. 5), ensuring a decrease in (d ? S,T ) with each SGD step even for a large step size (similar to <ref type="bibr">(Chu et al., 2020)</ref>), this explains the enhanced stability and improved performance of adversarial training. For verifying the stabilization effect of smoothness, empirically we obtain ? max for SGD and proposed SDAT at both best (? best max ) and last epoch (? last max ) for adaptation from Infographic to Clipart Domain <ref type="figure" target="#fig_1">(Fig. 2D</ref>). We find that as ? max increases (decrease in smoothness of landscape), the training becomes unstable for SGD leading to a drop in validation accuracy. Whereas in the case of the proposed SDAT, the ? max remains low across epochs, leading to stable and better validation accuracy curve. We also provide additional validation accuracy curves for more adaptation tasks where we also observe a similar phenomenon in <ref type="figure">Fig. 6</ref>. To the best of our knowledge, our analysis of the effect of smoothness of task loss on the stability of DAT is novel.</p><p>We also find that SGD leads to low ? max (high smoothness w.r.t. task loss) in comparison to Adam leading to better performance. This also explains the widespread usage of SGD for DAT algorithms <ref type="bibr">(Ganin &amp; Lempitsky, 2015;</ref><ref type="bibr" target="#b12">Long et al., 2018;</ref><ref type="bibr" target="#b21">Saito et al., 2018a)</ref>, instead of Adam. More details about Hessian analysis is provided in App. D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Smoothing Loss Landscape</head><p>In this section we first introduce the losses which are based on Sharpness Aware Minimization (Foret et al., 2021) (SAM). The basic idea of SAM is to find a smoother minima (i.e. low loss in neighborhood of ?) by using the following objective given formally below:</p><formula xml:id="formula_4">min ? max || ||?? L obj (? + )<label>(6)</label></formula><p>here L obj is the objective function to be minimized and ? ? 0 is a hyperparameter which defines the maximum norm for . Since finding the exact solution of inner maximization is hard, SAM maximizes the first order approximation:</p><formula xml:id="formula_5">(?) ? arg max || ||?? L obj (?) + T ? ? L obj (?) = ?? ? L obj (?)/||? ? L obj (?)|| 2 (7)</formula><p>The? (?) is added to the weights ?. The gradient update for ? is then computed as ? ? L obj (?)| ?+? (?) . The above procedure can be seen as a generic smoothness enhancing formulation for any L obj . We now analogously introduce the sharpness aware source risk for finding a smooth minima:</p><formula xml:id="formula_6">max || ||?? R l S (h ?+ ) = max || ||?? E x?P S [ l(h ?+ (x), f (x))] (8)</formula><p>We also now define the sharpness aware discrepancy estimation objective below:</p><formula xml:id="formula_7">max ? min || ||?? d ?+ S,T<label>(9)</label></formula><p>As d ? S,T is to be maximized the sharpness aware objective will have min || ||?? instead of max || ||?? , as it needs to find smoother maxima. We now theoretically analyze the difference in discrepancy estimation for smooth version d ? S,T (Eq. 9) in comparison to non-smooth version d ? S,T (Eq. 4). Assuming D ? is a L-smooth function (common assumption for non-convex optimization (Carmon et al., 2020)), ? is a small constant and d * S,T the optimal discrepancy, the theorem states: Theorem 2. For a given classifier h ? and one step of (steepest) gradient ascent i.e.</p><formula xml:id="formula_8">? = ? + ?(?d ? S,T /||?d ? S,T ||) and ? = ? + ?(?d ? S,T | ?+? (?) /||?d ? S,T | ?+? (?) ||) d ? S,T ? d ? S,T ? ?(1 ? cos ?) 2L(d * S,T ? d ? S,T ) (10)</formula><p>where ? is the angle between ?d ? S,T and ?d ? S,T | ?+? (?) .  When compared to SGD, Adam converges to a non-smooth minima (high ?max), leading to a high error on target. Using Adam in comparison to SGD, converges to a non-smooth minima (high ?max) leading to high error on target. B) Domain Accuracy (vs iterations), it is lower when adversarial loss is smooth (i.e. SDAT w/ adv), which indicates suboptimal discrepancy estimation d ? s,t C) Target Accuracy on Art ? Clipart vs smoothness of the adversarial component. As the smoothness increases (?), the target accuracy decreases indicating that smoothing adversarial loss leads to sub-optimal generalization.</p><formula xml:id="formula_9">The d ? S,T (non-smooth version) can exceed d ? S,T (smooth discrepancy) significantly, as the term d * S,T ? d ? S,T ? 0, as the h ? objective is to oppose the convergence of d ? S,T to optima d * S,T (min-max training in Eq. 5). Thus d ? S,T</formula><p>can be a better estimate of discrepancy in comparison to d ? S,T . A better estimate of d ? s,t helps in effectively reducing the discrepancy between P S and P T , hence leads to reduced R l T (h ? ). This is also observed in practice that smoothing the discriminator's adversarial loss (SDAT w/ adv in <ref type="figure" target="#fig_3">Fig. 3B</ref>) leads to low domain classification accuracy (proxy measure for d ? s,t ) in comparison to DAT. Due to ineffective discrepancy estimation, SDAT w/ adv results in sub-optimal generalization on target domain i.e. high target error R l T (h ? ) ( <ref type="figure" target="#fig_3">Fig. 3B</ref>). We also observe that further increasing the smoothness of the discriminator w.r.t. adversarial loss (increasing ?) leads to lowering of performance on the target domain ( <ref type="figure" target="#fig_3">Fig. 3C</ref>). A similar trend is observed in GANs (App. E) which also has a similar min-max objective. The proof of the above theorem and additional experimental details is provided in App. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Smooth Domain Adversarial Training (SDAT)</head><p>We propose smooth domain adversarial training which only focuses on converging to smooth minima w.r.t. task loss (i.e. empirical source risk), whereas preserves the original discrepancy term. We define the optimization objective of proposed Smooth Domain Adversarial Training below:</p><formula xml:id="formula_10">min ? max ? max || ||?? E x?P S [l(h ?+ (x), y(x))] + d ? S,T<label>(11)</label></formula><p>The first term is the sharpness aware risk, and the second term is the discrepancy term which is not smooth in our procedure. The term d ? S,T estimates D ? h ? ,H (P S ||P T ) dis-crepancy. We now show that optimizing Eq. 11 reduces R l T (h ? ) through a generalization bound. This bound establishes that our proposed SDAT procedure is also consistent (i.e. in case of infinite data the upper bound is tight) similar to the original DAT objective (Eq. 5). Theorem 3. Suppose l is the loss function, we denote ? * := R l S (h * ) + R l T (h * ) and let h * be the ideal joint hypothesis:</p><formula xml:id="formula_11">R l T (h ? ) ? max || ||??R l S (h ?+ ) + D ? h ? ,H (P S ||P T )+ ?(||?|| 2 2 /? 2 ) + ? * .<label>(12)</label></formula><p>where ? : R + ? R + is a strictly increasing function.</p><p>The bound is similar to generalization bounds for domain adaptation <ref type="bibr">(Ben-David et al., 2010;</ref><ref type="bibr">Acuna et al., 2021)</ref>. The main difference is the sharpness aware risk term max || ||??R l S (h ? ) in place of source risk R l S (h ? ), and an additional term that depends on the norm of the weights ?(||?|| 2 2 /? 2 ). The first is minimized by decreasing the empirical sharpness aware source risk by using SAM loss shown in Sec. 4. The second term is reduced by decreasing the discrepancy between source and target domains. The third term, as it is a function of norm of weights ||?|| 2 2 , can be reduced by using either L2 regularization or weight decay. Since we assume that the H hypothesis class we have is rich, the ? * term is small.</p><p>Any DAT baseline can be modified to use SDAT objective just by using few lines of code (App. L). We observe that the proposed SDAT objective (Eq. 11) leads to significantly lower generalization error compared to the original DA objective (Eq. 5), which we empirically demonstrate in the following sections. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Adaptation for classification</head><p>We evaluate our proposed method on three datasets: Office-Home, VisDA-2017, and DomainNet, as well as by combining SDAT with two DAT based DA techniques: CDAN and CDAN+MCC. We also show results with ViT backbone on Office-Home and VisDA-2017 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Office-Home (Venkateswara et al., 2017): Office-Home consists of 15,500 images from 65 classes and 4 domains: Art (Ar), Clipart (Cl), Product (Pr) and Real World (Rw).</p><p>DomainNet <ref type="bibr" target="#b18">(Peng et al., 2019)</ref>: DomainNet consists of 0.6 million images across 345 classes belonging to six domains. The domains are infograph (inf), clipart (clp), painting (pnt), sketch (skt), real and quickdraw.</p><p>VisDA-2017 <ref type="bibr" target="#b17">(Peng et al., 2017)</ref>: VisDA is a dataset that focuses on the transition from simulation to real world and contains approximately 280K images across 12 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Domain Adaptation Methods</head><p>CDAN <ref type="bibr" target="#b12">(Long et al., 2018)</ref>: Conditional Domain Adversarial network is a popular DA algorithm that improves the performance of the DANN algorithm. CDAN introduces the idea of multi-linear conditioning to align the source and target distributions better. CDAN in <ref type="table" target="#tab_0">Table 1</ref> and 2 refers to our implementation of CDAN* <ref type="bibr" target="#b12">(Long et al., 2018)</ref> method.</p><p>CDAN + MCC <ref type="bibr" target="#b1">(Jin et al., 2020)</ref>: The minimum class confusion (MCC) loss term is added as a regularizer to CDAN. MCC is a non-adversarial term that minimizes the pairwise class confusion on the target domain, hence we consider this as an additional minimization term which is added to empirical source risk. This method achieves close to SOTA accuracy among adversarial adaptation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>We implement our proposed method in the Transfer-Learning-Library (Junguang Jiang &amp; Long, 2020) toolkit developed in PyTorch <ref type="bibr" target="#b16">(Paszke et al., 2019)</ref>. The difference between the performance reported in CDAN* and our implementation CDAN is due to the batch normalization layer in domain classifier, which enhances performance.</p><p>We use a ResNet-50 backbone for Office-Home experiments and a ResNet-101 backbone for VisDA-2017 and DomainNet experiments. Additionally, we also report the performance of ViT-B/16 (Dosovitskiy et al., 2020) backbone on Office-Home and VisDA-2017 datasets. All the backbones are initialised with ImageNet pretrained weights. We use a learning rate of 0.01 with batch size 32 in all of our experiments with ResNet backbone. We tune ? value in SDAT for a particular dataset split and use the same value across domains. The ? value is set to 0.02 for the Office-Home experiments, 0.005 for the VisDA-2017 experiments and 0.05 for the DomainNet experiments. More details are present in supplementary (refer App. F).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results</head><p>Office-Home: For the Office-Home dataset, we compare our method with other DA algorithms including DANN, SRDC, MDD and f-DAL in <ref type="table" target="#tab_0">Table 1</ref>. We can see that the addition of SDAT improves the performance on both CDAN and CDAN+MCC across majority of source and target domain pairs. CDAN+MCC w/ SDAT achieves SOTA adversarial adaptation performance on the Office-Home dataset with ResNet-50 backbone. With ViT backbone, the increase in accuracy due to SDAT is more significant. This may be attributed to the observation that ViTs reach a sharp  <ref type="table" target="#tab_1">Table 2)</ref>. Compared to the proposed method, TVT is computationally more expensive to train, contains additional adaptation modules and utilizes a backbone that is pretrained on ImageNet-21k dataset (App. J). On the other hand, SDAT is conceptually simple and can be trained on a single 12 GB GPU with ViT (pretrained on ImageNet). With ViT backbone, SDAT particularly improves the performance of source-target pairs which have low accuracy on the target domain (Pr)Cl, Rw)Cl, Pr)Ar, Ar)Pr in <ref type="table" target="#tab_0">Table 1</ref>).  <ref type="table" target="#tab_2">Table 3</ref> shows the results on the large and challenging DomainNet dataset across five domains. The proposed method improves the performance of CDAN significantly across all source-target pairs. On specific sourcetarget pairs like inf ) real, the performance increase is 4.5%. The overall performance of CDAN is improved by nearly 1.8% which is significant considering the large number of classes and images present in DomainNet. The improved results are attributed to stabilized adversarial training through proposed SDAT which can be clearly seen in <ref type="figure" target="#fig_1">Fig. 2D</ref>.</p><p>VisDA-2017: CDAN w/ SDAT improves the overall performance of CDAN by more than 1.5% with ResNet backbone and by 4.9% with ViT backbone on VisDA-2017 dataset <ref type="table" target="#tab_1">(Table 2)</ref>. Also, on CDAN + MCC baseline SDAT leads to 2.1% improvement over baseline, leading to SOTA accuracy of 89.8% across classes. Particularly, SDAT significantly improves the performance of underperforming minority classes like bicycle, car and truck. Additional baselines and results are reported in supplementary (App. G) along with a discussion on statistical significance (App. K).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Adaptation for object detection</head><p>To further validate our approach's generality and extensibility, we did experiments on DA for object detection. We use the same setting as proposed in <ref type="bibr">DA-Faster (Chen et al., 2018)</ref> with all domain adaptation components and use it as our baseline. We use the mean Average Precision at 0.5 IoU (mAP) as our evaluation metric. In object detection, the smoothness enhancement can be achieved in two ways (empirical comparison in Sec. 6.2) : </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Setup</head><p>We evaluate our proposed approach on object detection on two different domain shifts:</p><p>Pascal to Clipart (P ? C): Pascal (Everingham et al., 2010) is a real-world image dataset which consists images with 20 different object categories. Clipart (Inoue et al., 2018) is a graphical image dataset with complex backgrounds and has the same 20 categories as Pascal. We use <ref type="bibr">ResNet-101 (He et al., 2016)</ref> backbone for Faster R-CNN <ref type="bibr" target="#b20">(Ren et al., 2015)</ref> following <ref type="bibr">Saito et al. (2019)</ref>.</p><p>Cityscapes to Foggy Cityscapes (C ? F c): Cityscapes (Cordts et al., 2016) is a street scene dataset for driving, whose images are collected in clear weather. Foggy Cityscapes (Sakaridis et al., 2018) dataset is synthesized from Cityscapes for the foggy weather. We use <ref type="bibr">ResNet-50 (He et al., 2016)</ref> as the backbone for Faster R-CNN for experiments on this task. Both domains have the same 8 object categories with instance labels. <ref type="table" target="#tab_3">Table 4</ref> shows the results on two domain shifts with varying batch size (bs) during training. We find that only smoothing w.r.t. classification loss is much more effective (SDAT-Classification) than smoothing w.r.t. combined classification and regression loss (SDAT). On average, SDAT-Classification produces an mAP gain of 2.0% compared to SDAT, and 2.8% compared to DA-Faster baseline. The proposed SDAT-Classification significantly outperforms DA-Faster baseline and improves mAP by 1.3% on P ? C and by 2.8% on C ? F c. It is noteworthy that increase in performance of SDAT-Classification is consistent even after training with higher batch size (bs = 8) achieving improvement of 4.3% in mAP.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>How much smoothing is optimal?: <ref type="figure" target="#fig_5">Figure 4A</ref> shows the ablation on ? value (higher ? value corresponds to more smoothing) on the Ar)Cl from Office-Home dataset with CDAN backbone. The performance of the different values of ? is higher than the baseline with ? = 0. It can be seen that ? = 0.02 works best among all the different values and outperforms the baseline by at least 1.5%. We found that the same ? value usually worked well across domains in a dataset, but different ? was optimal for different datasets. More details about optimum ? is in App. I.</p><p>Which components benefit from smooth optima?: <ref type="figure" target="#fig_5">Fig.  4C</ref> shows the effect of introducing smoothness enhancement for different components in DAT. For this we use SAM on a) task loss (SDAT) b) adversarial loss (SDAT w/ adv) c) both task and adversarial loss (SDAT-all). It can be seen that smoothing the adversarial loss component (SDAT w/ adv) reduces the performance to 51.0%, which is significantly lower than even the DAT baseline.</p><p>Is it Robust to Label Noise?: In practical, real-world scenarios, the labeled datasets are often corrupted with some amount of label noise. Due to this, performing domain adaptation with such data is challenging. We find that smoother minima through SDAT lead to robust models which generalize well on the target domain. <ref type="figure" target="#fig_5">Figure 4B</ref> provides the comparison of SGD vs. SDAT for different percentages of label noise injected into training data.   techniques for ERM (LS, SAM and VAT) fail to provide significant consistent gain in performance which also confirms the requirement of specific smoothing strategies for DAT. We find that SDAT even outperforms SWAD on average by a significant margin of 1.2%. Additional details regarding the specific methods are provided in App. H.</p><p>Does it generalize well to other DA methods?: We show results highlighting the effect of smoothness (SDAT) on <ref type="bibr">DANN(Ganin et al., 2016)</ref> and <ref type="bibr">GVB-GD (Cui et al., 2020)</ref> in <ref type="table" target="#tab_7">Table 6</ref> with ResNet-50 and ViT backbone. DANN w/ SDAT leads to gain in accuracy on both DomainNet and Office-Home dataset. We observe a significant increase (average of +3.3%) with DANN w/ SDAT (ViT backbone) on Office-Home dataset. SDAT leads to a decent gain in accuracy on Office-Home dataset with GVB-GD despite the fact that GVB-GD is a much stronger baseline than DANN. We primarily focused on CDAN and CDAN + MCC for the main results as we wanted to establish that SDAT can improve on even SOTA DAT methods for showing its effectiveness. Overall, we have shown results with four DA methods (CDAN, CDAN+MCC, DANN, GVB-GD) and this shows that SDAT is a generic method that can applied on top of any domain adversarial training based method to get better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>In this work, we analyze the curvature of loss surface of DAT used extensively for Unsupervised DA. We find that converging to a smooth minima w.r.t. task loss (i.e., empirical source risk) leads to stable DAT which results in better generalization on the target domain. We also theoretically and empirically show that smoothing adversarial components of loss lead to sub-optimal results, hence should be avoided in practice. We then introduce our practical and effective method, SDAT, which only increases the smoothness w.r.t. task loss, leading to better generalization on the target domain. SDAT leads to an effective increase for the latest methods for adversarial DA, achieving SOTA performance on benchmark datasets. One limitation of SDAT is presence of no automatic way of selecting ? (extent of smoothness) which is a good future direction to explore. Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor, J., Cho*, K., and Geras*, K. The break-even point on A. Notation <ref type="table" target="#tab_10">Table   Table 7</ref> contains all the notations used in the paper and the proofs of theorems. </p><formula xml:id="formula_12">d ? S,T = E x?P S [log(D ? (g ? (x)))] + E x?P T [log(1 ? D ? (g ? (x)))]<label>(13)</label></formula><p>The above term is exactly the Eq. C.1 in <ref type="bibr">Acuna et al. (2021)</ref> where they show that optimal d ? S,T i.e.:</p><formula xml:id="formula_13">max ? d ? S,T = D JS (P S ||P T ) ? 2 log(2)<label>(14)</label></formula><p>Hence we can say from result in Eq. 4 is a consequence of Lemma 1 and Proposition 1 in <ref type="bibr">(Acuna et al., 2021)</ref>, assuming that D ? satisfies the constraints in Proposition 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proof of Theorems</head><p>In this section we provide proofs for the theoretical results present in the paper:</p><formula xml:id="formula_14">Theorem 1 (Generalization bound). Suppose l : Y ? Y ? [0, 1] ? dom ? * .</formula><p>Let h * be the ideal joint classifier with error ? * = R l S (h * ) + R l T (h * ). We have the following relation between source and target risk:</p><formula xml:id="formula_15">R l T (h ? ) ? R l S (h ? ) + D ? h ? ,H (P S ||P T ) + ? *<label>(15)</label></formula><p>Proof. We refer the reader to Theorem 2 in Appendix B of <ref type="bibr">Acuna et al. (2021)</ref> for the detailed proof the theorem.</p><p>We now introduce a Lemma for smooth functions which we will use in the proofs subsequently:</p><p>Lemma 1. For an L-smooth function f (w) the following holds where w * is the optimal minima:</p><formula xml:id="formula_16">f (w) ? f (w * ) ? 1 2L ||?f (w)|| 2</formula><p>Proof. The L-smooth function by definition satisfies the following:</p><formula xml:id="formula_17">f (w * ) ? f (v) ? f (w) + ?f (w)(v ? w) + L 2 ||v ? w|| 2</formula><p>Now we minimize the upper bound wrt v to get a tight bound on f (w * ).</p><formula xml:id="formula_18">D(v) = f (w) + ?f (w)(v ? w) + L 2 ||v ? w|| 2 after doing ? v D(v) = 0 we get: v = w ? 1 L ?f (w)</formula><p>By substituting the value of v in the upper bound we get:</p><formula xml:id="formula_19">f (w * ) ? f (w) ? 1 2L ||?f (w)|| 2</formula><p>Hence rearranging the above term gives the desired result:</p><formula xml:id="formula_20">f (w) ? f (w * ) ? 1 2L ||?f (w)|| 2</formula><p>Theorem 2. For a given classifier h ? and one step of (steepest) gradient ascent i.</p><formula xml:id="formula_21">e. ? = ? + ?(?d ? S,T /||?d ? S,T ||) and ? = ? + ?(?d ? S,T | ?+? (?) /||?d ? S,T | ?+? (?) ||) for maximizing d ? S,T ? d ? S,T ? ?(1 ? cos ?) 2L(d * S,T ? d ? S,T )<label>(16)</label></formula><p>where ? is the angle between ?d ? S,T and ?d ? S,T | ?+? (?) .</p><p>Proof of Theorem 2. We assume that the function is L-smooth (the assumption of L-smoothness is the basis of many results in non-convex optimization <ref type="bibr">(Carmon et al., 2020)</ref>) in terms of input x. As for a fixed h ? as we use a reverse gradient procedure for measuring the discrepancy, only one step analysis is shown. This is because only a single step of gradient is used for estimating discrepancy d ? S,T i.e. one step of each min and max optimization is performed alternatively for optimization. After this the h ? is updated to decrease the discrepancy. Any differential function can be approximated by the linear approximation in case of small ?:</p><formula xml:id="formula_22">d ?+?v S,T ? d ? S,T + ??d ? T S,T v<label>(17)</label></formula><p>The dot product between two vectors can be written as the following function of norms and angle ? between those:</p><formula xml:id="formula_23">?d ? T S,T v = ||?d ? S,T || ||v|| cos?<label>(18)</label></formula><p>The steepest value will be achieved when cos ? = 1 which is actually</p><formula xml:id="formula_24">v = ?d ? S,T (x) ||?d ? S,T (x)|| . Now we compare the descent in another direction v 2 = ?d ? S,T | w+ (w) ||?d ?</formula><p>S,T | w+ (w) || from the gradient descent. The difference in value can be characterized by:</p><formula xml:id="formula_25">d ?+?v S,T ? d ?+?v2 S,T = ?||?d ? S,T ||(1 ? cos ?)<label>(19)</label></formula><p>As ? is an angle between ?d ? S,T | w+ (w) (v 2 ) and ?d ? S,T (X) (v). The suboptimality is dependent on the gradient magnitude. We use the following result to show that when optimality gap d * S,T ? d ? S,T (x) is large the difference between two directions is also large.</p><p>For an L-smooth function the following holds according to Lemma 1:</p><formula xml:id="formula_26">f (w) ? f (w * ) ? 1 2L ||?f (w)|| 2</formula><p>As we are performing gradient ascent f (w) = ?d ? s,t , we get the following result:</p><formula xml:id="formula_27">(d * S,T ? d ? S,T ) ? 1 2L ||?d ? S,T (x)|| 2 2L(d * S,T ? d ? S,T ) ? (d ?+?v2 S,T ? d ?+?v S,T ) 2 (?(1 ? cos ?)) 2 ?(1 ? cos ?) 2L(d * S,T ? d ? S,T ) ? (d ? S,T ? d ? S,T )</formula><p>This shows that difference in value of by taking a step in direction of gradient v vs taking the step in a different direction v 2 is upper bounded by the d * S,T ? d ? S,T (x), hence if we are far from minima the difference can be potentially large. As we are only doing one step of gradient ascent d * S,T ? d ? S,T will be potentially large, hence can lead to suboptimal measure of discrepancy.</p><p>Theorem 3. Suppose l is the loss function, we denote ? * := R l S (h * ) + R l T (h * ) and let h * be the ideal joint hypothesis:</p><formula xml:id="formula_28">R l T (h ? ) ? max || ||??R l S (h ?+ ) + D ? h ? ,H (P S ||P T ) + ?(||?|| 2 2 /? 2 ) + ? * .<label>(20)</label></formula><p>where ? : R + ? R + is a strictly increasing function.</p><p>Proof of Theorem 3: In this case we make use of Theorem 2 in the paper sharpness aware minimization <ref type="bibr">(Foret et al., 2021)</ref> which states the following: The source risk R S (h) is bounded using the following PAC-Bayes generalization bound for any ? with probability 1 ? ?:</p><formula xml:id="formula_29">R S (h ? ) ? max || ||??R S (h ? ) + k log 1 + ? 2 2 ? 2 1 + log(n) k 2 + 4 log n ? +?(1) n ? 1<label>(21)</label></formula><p>here n is the training set size used for calculation of empirical riskR S (h), k is the number of parameters and ||?|| 2 is the norm of the weight parameters. The second term in equation can be abbreviated as ?(||?|| 2 ). Hence,</p><formula xml:id="formula_30">R S (h ? ) ? max || ||??R S (h ? ) + ?(||?|| 2 2 /? 2 )<label>(22)</label></formula><p>From the generalization bound for domain adaptation for any f-divergence (Acuna et al., 2021) (Theorem 2) we have the following result.</p><formula xml:id="formula_31">R l T (h ? ) ? R l S (h ? ) + D ? h ? ,H (P S ||P T ) + ? *<label>(23)</label></formula><p>Combining the above two inequalities gives us the required result we wanted to prove i.e.  Step <ref type="formula">50</ref>  . SNGAN performance on different datasets, smoothing discriminator in GAN also leads to inferior GAN performance (higher FID) across both datasets.</p><formula xml:id="formula_32">R l T (h ? ) ?R l S (h ? ) + D ? h ? ,H (P S ||P T ) + ?(||?|| 2 2 /? 2 ) + ? * .<label>(24)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Hessian Analysis</head><p>We use the PyHessian library <ref type="bibr">(Yao et al., 2020)</ref> to calculate the Hessian eigenvalues and the Hessian Eigen Spectral Density. For Office-Home experiments, all the calculations are performed using 50% of the source data at the last checkpoint. For DomainNet experiments <ref type="figure" target="#fig_1">(Fig. 2D)</ref>, we use 10% of the source data for Hessian calculation. The Maximum Eigenvalue is calculated at the checkpoint with the best validation accuracy (? best max ) and the last checkpoint (? last max ). Only the source class loss is used for calculating to clearly illustrate our point. The partition was selected randomly, and the same partition was used across all the runs. We also made sure to use the same environment to run all the Hessian experiments. A subset of the data was used for Hessian calculation mainly because the hessian calculation is computationally expensive <ref type="bibr">(Yao et al., 2020)</ref>. This is commonly done in hessian experiments. For example, (Chen et al., 2021) (refer Appendix D) uses 10% of training data for Hessian Eigenvalue calculation. The PyHessian library uses Lanczos algorithm <ref type="bibr">(Ghorbani et al., 2019)</ref> for calculating the Eigen Spectral density of the Hessian and uses the Hutchinson method to calculate the trace of the Hessian efficiently.</p><p>For further establishing the generality of sub-optimality of smooth adversarial loss, we also perform experiments on Spectral Normalised Generative Adversarial Networks (SNGAN) <ref type="bibr" target="#b13">(Miyato et al., 2018)</ref>. In case of SNGAN we also find that smoothing discriminator through SAM leads to suboptimal performance (higher FID) as in <ref type="figure">Fig. 5</ref>. The above evidences indicates that smoothing the adversarial loss leads to sub-optimality, hence it should not be done in practice. We use the same configuration for SNGAN as described in PyTorchStudioGAN <ref type="bibr" target="#b3">(Kang &amp; Park, 2020)</ref> for both CIFAR10 <ref type="bibr" target="#b7">(Krizhevsky et al., 2009)</ref> and TinyImageNet 3 with batch size of 256 in both cases. We then smooth the discriminator while discriminator is trained by using the same formulation as in Eq. 9. We find that smoothing discriminator leads to higher (suboptimal) Fr?chet Inception Distance in case of GANs as well, shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1. Image Classification</head><p>Office-Home: For CDAN methods with ResNet-50 backbone, we train the models using mini-batch stochastic gradient descent (SGD) with a batch size of 32 and a learning rate of 0.01. The learning rate schedule is the same as <ref type="bibr">(Ganin et al., 2016)</ref>. We train it for a total of 30 epochs with 1000 iterations per epoch. The momentum parameter in SGD is set to 0.9 and a weight decay of 0.001 is used. For CDAN+MCC experiments with ResNet-50 backbone, we use a temperature parameter <ref type="bibr" target="#b1">(Jin et al., 2020)</ref> of 2.5. The bottleneck dimension for the features is set to 2048.</p><p>VisDA-2017: We use a ResNet-101 backbone initialized with ImageNet weights for VisDA-2017 experiments. Center Crop is also used as an augmentation during training. We use a bottleneck dimension of 256 for both algorithms. For CDAN runs, we train the model for 30 epochs with same optimizer setting as that of Office-Home. For CDAN+MCC runs, we use a temperature parameter of 3.0 and a learning rate of 0.002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DomainNet:</head><p>We use a ResNet-101 backbone initialized with ImageNet weights for DomainNet experiments. We run all the experiments for 30 epochs with 2500 iterations per epoch. The other parameters are the same as that of Office-Home.</p><p>Additional experiments with a ViT backbone are performed on Office-Home and VisDA-2017 datasets. We use the ViT-B/16 architecture pretrained on ImageNet-1k, the implementation of which is borrowed from <ref type="bibr">(Wightman, 2019)</ref>. For all CDAN runs on Office-Home and VisDA, we use an initial learning rate of 0.01, whereas for CDAN+MCC runs, the initial learning rate of 0.002 is used. ? value of 0.02 is shared across all the splits on both the datasets for the ViT backbone. A batch-size of 24 is used for Office-Home and 32 for VisDA-2017.</p><p>To show the effectiveness of SDAT fairly and promote reproducibility, we run with and without SDAT on the same GPU and environment and with the same seed. All the above experiments were run on Nvidia V100, RTX 2080 and RTX A5000 GPUs. We used Wandb (Biewald, 2020) to track our experiments. We will be releasing the code to promote reproducible research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1.1. ARCHITECTURE OF DOMAIN DISCRIMINATOR</head><p>One of the major reasons for increased accuracy in Office-Home baseline CDAN compared to reported numbers in the paper is the architecture of domain classifier. The main difference is the use of batch normalization layer in domain classifier, which was done in the library <ref type="bibr">(Junguang Jiang &amp; Long, 2020)</ref>. <ref type="table" target="#tab_11">Table 8</ref> shows the architecture of the feature classifier and domain classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2. Additional Implementations Details for DA for Object detection</head><p>In SDAT, we modified the loss function present in <ref type="bibr">Chen et al. (2018)</ref> by adding classification loss smoothing, i.e. smoothing classification loss of <ref type="bibr">RPN and ROI, used in Faster R-CNN (Ren et al., 2015)</ref>, by training with source data. Similarly, we applied smoothing to regression loss and found it to be less effective. We implemented SDAT for object detection using Detectron2 <ref type="bibr">(Wu et al., 2019)</ref>. The training is done via SGD with momentum 0.9 for 70k iterations with the learning rate of 0.001, and then dropped to 0.0001 after 50k iterations. We split the target data into train and validation sets and report the best mAP on validation data. We fixed ? to 0.15 for object detection experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Additional Results</head><p>VisDA-2017: <ref type="table" target="#tab_12">Table 9</ref> shows the overall accuracy on the VisDA-2017 with ResNet-101 and ViT backbone. The accuracy reported in this table is the overall accuracy of the dataset, whereas the accuracy reported in the  <ref type="table" target="#tab_0">Table 10</ref> shows the results of the proposed method on DomainNet across five domains. We compare our results with ADDA and MCD and show that CDAN achieves much higher performance on DomainNet compared to other techniques. It can be seen that CDAN w/ SDAT further improves the overall accuracy on DomainNet by 1.8%. We have shown results with three different domain adaptation algorithms namely <ref type="bibr">DANN (Ganin &amp; Lempitsky, 2015)</ref>, CDAN <ref type="bibr" target="#b12">(Long et al., 2018)</ref> and CDAN+MCC <ref type="bibr" target="#b1">(Jin et al., 2020)</ref>. SDAT has shown to improve the performance of all the three DA methods. This shows that SDAT is a generic method that can applied on top of any domain adversarial training based method to get better performance.</p><p>Source-only: Source-only setting measures the performance of a model trained only on source domain directly on unseen target data with no further target adaptation. We compare the performance of models with and without smoothing the loss landscape for source-only experiments on VisDA-2017 <ref type="table" target="#tab_0">(Table 11</ref>) and Office-Home <ref type="table" target="#tab_0">(Table 12</ref>) datasets with a ViT backbone pretrained on ImageNet. Initial learning rate of 0.001 and 0.002 is used for Office-Home and VisDA-2017 dataset, respectively. ? value of 0.002 is used for ERM w/SAM run for both the datasets. It can be seen that ERM w/ SAM does not directly lead to better performance on the target domain. The idea behind SWA is that averaging weights across epochs leads to better generalization because it reaches a wider optima. The recently proposed SWA-Densely (SWAD) <ref type="bibr">(Cha et al., 2021)</ref> takes this a step further and proposes to average the weights across iterations instead of epochs. SWAD shows improved performance on domain generalization tasks. We average every 400 iterations in the SWA instead of averaging per epochs. We tried averaging across 800 iterations as well and the performance was comparable.</p><p>Difference between SWAD and SDAT: As SWAD performs Weight Averaging, it is not possible to selectively smooth only minimization (ERM) components with SWAD, as gradients for both the adversarial loss and ERM update weights of the backbone. Due to this, SWAD cannot reach optimal performance for DAT. For verifying this, we also compare our method by implementing SWAD for Domain Adaptation on four different source-target pairs of Office-Home dataset in <ref type="table" target="#tab_5">Table 5</ref>. On average, SDAT (Ours) gets 61.6% (+2.4% over DAT) accuracy in comparison to 60.4% (+1.2% over DAT) for SWAD.</p><p>Virtual Adversarial Training (VAT) <ref type="bibr" target="#b14">(Miyato et al., 2019)</ref>: VAT is regularization technique which makes use of adversarial perturbations. Adversarial perturbations are created using Algo. 1 present in <ref type="bibr" target="#b14">(Miyato et al., 2019)</ref>. We added VAT by optimizing the following objective:</p><formula xml:id="formula_33">min ? E x?P S [ max ||r||? D KL (h ? (x)||h ? (x + r))]<label>(25)</label></formula><p>This value acts as a negative measure of smoothness and minimizing this will make the model smooth. For training, we set hyperparameters to 15.0, ? to 1e-6, and ? as 0.  <ref type="table" target="#tab_0">Table 13</ref>. We also show results with smoothing parameter of 0.2 and observe comparable performance. We observe that label smoothing slightly improves the performance over DAT. <ref type="bibr">SAM (Foret et al., 2021)</ref>: In this method, we apply SAM directly to both the task loss and adversarial loss with ? = 0.05 as suggested in the paper. It can be seen that the performance improvement of SAM over DAT is minimal, thus indicating the need for SDAT.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. Comparison with TVT</head><p>TVT <ref type="figure" target="#fig_0">(Yang et al., 2021)</ref> is a recent work that reports performance higher than the other contemporary unsupervised DA methods on the publicly available datasets. This method uses a ViT backbone and focuses on exploiting the intrinsic properties of ViT to achieve better results on domain adaptation. Like us, TVT uses an adversarial method for adaptation Even without using external modules to promote the transferability and discriminability in the features learned using ViT, we are able to report higher numbers than TVT. This advocates our efforts to show the efficacy of converging to a smooth minima w.r.t. task loss to achieve better domain alignment. Moreover, TVT uses a batch size of 64 to train the network, causing a memory requirement of more than 35GB for efficient training, which is significantly higher than the 11.5GB memory used by our method on a batch-size of 24 for Office-Home to obtain better results. This allows our method to be trained using a standard 12GB GPU, removing the need of an expensive hardware. The ViT backbone used by TVT is pretrained on a much larger ImageNet-21k dataset, whereas we use the backbone pretrained on ImageNet-1k dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Significance and Stability of Empirical Results</head><p>To establish the empirical results' soundness and reliability, we run a subset of experiments (representative of each different source domain) on DomainNet. The experiments are repeated with three different random seeds leading to overall 36 experimental runs (18 for CDAN w/ SDAT (Our proposed method) and 18 for CDAN baseline). Due to the large computational complexity of each experiment (?20 hrs each), we have presented results for multiple trials on a subset of splits. We find (in <ref type="table" target="#tab_0">Table 16</ref>) that our method can outperform the baseline average in each of the 6 cases, establishing significant improvement across all splits. However, we found that due to the large size of DomainNet, the average increase (across three different trials) is close to the reported increase in all cases <ref type="table" target="#tab_0">(Table 16</ref>), which also serves as evidence of the soundness of reported results (for remaining splits). We also present additional statistics below for establishing soundness.</p><p>If the proposed method is unstable, there is a large variance in the validation accuracy across epochs. For analyzing the stability of SDAT, we show the validation accuracy plots in <ref type="figure">Figure 6</ref> on six different splits of DomainNet. We find that our proposed SDAT improves over baselines consistently across epochs without overlap in confidence intervals in later epochs. This also provides evidence for the authenticity and stability of our results. We also find that in some cases, like when using the Infographic domain as a source, our proposed SDAT also significantly stabilizes the training <ref type="figure">(Figure 6</ref> inf ) clp).</p><p>One of the other ways of reporting results reliably proposed by the concurrent work (Berthelot et al., 2021) (Section 4.4) involves reporting the median of accuracy across the last few checkpoints. The median is a measure of central tendency which ignores outlier results. We also report the median of validation accuracy for our method across all splits for the last five epochs. It is observed that we observe similar gains for median accuracy (in <ref type="table" target="#tab_0">Table 17</ref>) as reported in <ref type="table" target="#tab_2">Table 3</ref>.</p><p>As the Office-Home dataset is smaller (i.e., 44 images per class) in comparison to DomainNet we find that there exists some variance in baseline CDAN results (This is also reported in the well-known benchmark for DA <ref type="bibr">(Junguang Jiang &amp; Long, 2020)</ref>). For establishing the empirical soundness, we report results of 4 different dataset splits on 3 seeds. It can be seen in <ref type="table" target="#tab_0">Table 18</ref> that even though there is variance in baseline results, our combination of CDAN w/ SDAT can produce consistent improvement across different random seeds. This further establishes the empirical soundness of our procedure.  <ref type="figure">Figure 6</ref>. Validation Accuracy across epochs on different splits of DomainNet. We run on three different random seeds and plot the error bar indicating standard deviation across runs. CDAN w/ SDAT consistently outperforms CDAN across different splits of DomainNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L. PyTorch Pseudocode for SDAT</head><p>In the code snippet below, we show that with a few changes in the code, SDAT can be easily integrated with any DAT algorithm. SDAT requires an additional forward pass and gradient computation, as shown below.</p><p>1 # task_loss_fn refers to the function to calculate task loss. 2 # (For classification settings, this can be Cross Entropy Loss).</p><p>3 4 # optimizer refers to the smooth optimizer which contains parameters of the feature extractor and classifier. 5 optimizer.zero_grad() 6 # ad_optimizer refers to standard SGD optimizer which contains parameters of domain classifier. 7 ad_optimizer.zero_grad() <ref type="table" target="#tab_0">8  Table 16</ref>. DomainNet experiments over 3 different seeds (with ResNet backbone). We report the mean, standard deviation, reported increase and average increase in the accuracy (in %).</p><p>Split CDAN CDAN w/ SDAT Reported Increase ( </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Overview of Smooth Domain Adversarial Training</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Eigen Spectral Density plots of Hessian (? 2Rl S (h ? )) for Adam (A), SGD (B) and SDAT (C) on Art ) Clipart. Each plot contains the maximum eigenvalue (?max) and the trace of the Hessian (T r(H)), which are indicators of the smoothness (Low T r(H)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A) Error on Target Domain (y-axis) for Office-Home dataset against maximum eigenvalue ?max of classification loss in DAT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>a) DA-Faster w/ SDAT-Classification: Smoothness en-hancement for classification loss. b) DA-Faster w/ SDAT: Smoothness enhancement for the combined classification and regression loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Analysis of SDAT for Ar ? Cl split of Office-Home dataset. A) Variation of target accuracy with maximum perturbation ?. B) Comparison of accuracy of SDAT with DAT for different ratio of label noise. C) Comparison of accuracy when smoothing is applied to various loss components.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Figure 5. SNGAN performance on different datasets, smoothing discriminator in GAN also leads to inferior GAN performance (higher FID) across both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Accuracy (%) on Office-Home for unsupervised DA (with ResNet-50 and ViT backbone). CDAN+MCC w/ SDAT outperforms other SOTA DA techniques. CDAN w/ SDAT improves over CDAN by 1.1% with ResNet-50 and 3.1% with ViT backbone.</figDesc><table><row><cell>ResNet-50 (He et al., 2016)</cell><cell></cell><cell>34.9</cell><cell>50.0</cell><cell>58.0</cell><cell>37.4</cell><cell>41.9</cell><cell>46.2</cell><cell>38.5</cell><cell>31.2</cell><cell>60.4</cell><cell>53.9</cell><cell>41.2</cell><cell>59.9</cell><cell>46.1</cell></row><row><cell>DANN (Ganin et al., 2016)</cell><cell></cell><cell>45.6</cell><cell>59.3</cell><cell>70.1</cell><cell>47.0</cell><cell>58.5</cell><cell>60.9</cell><cell>46.1</cell><cell>43.7</cell><cell>68.5</cell><cell>63.2</cell><cell>51.8</cell><cell>76.8</cell><cell>57.6</cell></row><row><cell>CDAN* (Long et al., 2018)</cell><cell></cell><cell>49.0</cell><cell>69.3</cell><cell>74.5</cell><cell>54.4</cell><cell>66.0</cell><cell>68.4</cell><cell>55.6</cell><cell>48.3</cell><cell>75.9</cell><cell>68.4</cell><cell>55.4</cell><cell>80.5</cell><cell>63.8</cell></row><row><cell>MDD (Zhang et al., 2019) f-DAL (Acuna et al., 2021) SRDC (Tang et al., 2020) CDAN</cell><cell>ResNet-50</cell><cell>54.9 56.7 52.3 54.3</cell><cell>73.7 77.0 76.3 70.6</cell><cell>77.8 81.1 81.0 76.8</cell><cell>60.0 63.1 69.5 61.3</cell><cell>71.4 72.2 76.2 69.5</cell><cell>71.8 75.9 78.0 71.3</cell><cell>61.2 64.5 68.7 61.7</cell><cell>53.6 54.4 53.8 55.3</cell><cell>78.1 81.0 81.7 80.5</cell><cell>72.5 72.3 76.3 74.8</cell><cell>60.2 58.4 57.1 60.1</cell><cell>82.3 83.7 85.0 84.2</cell><cell>68.1 70.0 71.3 68.4</cell></row><row><cell>CDAN w/ SDAT</cell><cell></cell><cell>56.0</cell><cell>72.2</cell><cell>78.6</cell><cell>62.5</cell><cell>73.2</cell><cell>71.8</cell><cell>62.1</cell><cell>55.9</cell><cell>80.3</cell><cell>75.0</cell><cell>61.4</cell><cell>84.5</cell><cell>69.5</cell></row><row><cell>CDAN + MCC</cell><cell></cell><cell>57.0</cell><cell>76.0</cell><cell>81.6</cell><cell>64.9</cell><cell>75.9</cell><cell>75.4</cell><cell>63.7</cell><cell>56.1</cell><cell>81.2</cell><cell>74.2</cell><cell>63.9</cell><cell>85.4</cell><cell>71.3</cell></row><row><cell>CDAN + MCC w/ SDAT</cell><cell></cell><cell>58.2</cell><cell>77.1</cell><cell>82.2</cell><cell>66.3</cell><cell>77.6</cell><cell>76.8</cell><cell>63.3</cell><cell>57.0</cell><cell>82.2</cell><cell>74.9</cell><cell>64.7</cell><cell>86.0</cell><cell>72.2</cell></row><row><cell>TVT (Yang et al., 2021)</cell><cell></cell><cell>74.9</cell><cell>86.8</cell><cell>89.5</cell><cell>82.8</cell><cell>87.9</cell><cell>88.3</cell><cell>79.8</cell><cell>71.9</cell><cell>90.1</cell><cell>85.5</cell><cell>74.6</cell><cell>90.6</cell><cell>83.6</cell></row><row><cell>CDAN CDAN w/ SDAT</cell><cell>ViT</cell><cell>62.6 69.1</cell><cell>82.9 86.6</cell><cell>87.2 88.9</cell><cell>79.2 81.9</cell><cell>84.9 86.2</cell><cell>87.1 88.0</cell><cell>77.9 81.0</cell><cell>63.3 66.7</cell><cell>88.7 89.7</cell><cell>83.1 86.2</cell><cell>63.5 72.1</cell><cell>90.8 91.9</cell><cell>79.3 82.4</cell></row><row><cell>CDAN + MCC</cell><cell></cell><cell>67.0</cell><cell>84.8</cell><cell>90.2</cell><cell>83.4</cell><cell>87.3</cell><cell>89.3</cell><cell>80.7</cell><cell>64.4</cell><cell>90.0</cell><cell>86.6</cell><cell>70.4</cell><cell>91.9</cell><cell>82.2</cell></row><row><cell>CDAN + MCC w/ SDAT</cell><cell></cell><cell>70.8</cell><cell>87.0</cell><cell>90.5</cell><cell>85.2</cell><cell>87.3</cell><cell>89.7</cell><cell>84.1</cell><cell>70.7</cell><cell>90.6</cell><cell>88.3</cell><cell>75.5</cell><cell>92.1</cell><cell>84.3</cell></row></table><note>Method Ar)Cl Ar)Pr Ar)Rw Cl)Ar Cl)Pr Cl)Rw Pr)Ar Pr)Cl Pr)Rw Rw)Ar Rw)Cl Rw)Pr Avg</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Accuracy (%) on VisDA-2017 for unsupervised DA (with ResNet-101 and ViT backbone). The mean column contains mean across all classes. SDAT particularly improves the accuracy in classes that have comparatively low CDAN performance.Methodplane bcycl bus car horse knife mcyle persn plant sktb train truck mean</figDesc><table><row><cell>ResNet (He et al., 2016)</cell><cell></cell><cell>55.1 53.3 61.9 59.1 80.6 17.9 79.7 31.2 81.0 26.5 73.5 8.5</cell><cell>52.4</cell></row><row><cell>DANN (Ganin et al., 2016)</cell><cell></cell><cell>81.9 77.7 82.8 44.3 81.2 29.5 65.1 28.6 51.9 54.6 82.8 7.8</cell><cell>57.4</cell></row><row><cell>MCD (Saito et al., 2018b)</cell><cell></cell><cell cols="2">87.0 60.9 83.7 64.0 88.9 79.6 84.7 76.9 88.6 40.3 83.0 25.8 71.9</cell></row><row><cell>CDAN* (Long et al., 2018) MCC (Jin et al., 2020) CDAN CDAN w/ SDAT</cell><cell>ResNet-101</cell><cell cols="2">85.2 66.9 83.0 50.8 84.2 74.9 88.1 74.5 83.4 76.0 81.9 38.0 73.9 88.1 80.3 80.5 71.5 90.1 93.2 85.0 71.6 89.4 73.8 85.0 36.9 78.8 94.9 72.0 83.0 57.3 91.6 95.2 91.6 79.5 85.8 88.8 87.0 40.5 80.6 94.8 77.1 82.8 60.9 92.3 95.2 91.7 79.9 89.9 91.2 88.5 41.2 82.1</cell></row><row><cell>CDAN+MCC</cell><cell></cell><cell cols="2">95.0 84.2 75.0 66.9 94.4 97.1 90.5 79.8 89.4 89.5 86.9 54.4 83.6</cell></row><row><cell>CDAN+MCC w/ SDAT</cell><cell></cell><cell cols="2">95.8 85.5 76.9 69.0 93.5 97.4 88.5 78.2 93.1 91.6 86.3 55.3 84.3</cell></row><row><cell>TVT (Yang et al., 2021)</cell><cell></cell><cell cols="2">92.9 85.6 77.5 60.5 93.6 98.2 89.3 76.4 93.6 92.0 91.7 55.7 83.9</cell></row><row><cell>CDAN CDAN w/ SDAT CDAN+MCC</cell><cell>ViT</cell><cell cols="2">94.3 53.0 75.7 60.5 93.9 98.3 96.4 77.5 91.6 81.8 87.4 45.2 79.6 96.3 80.7 74.5 65.4 95.8 99.5 92.0 83.7 93.6 88.9 85.8 57.2 84.5 96.9 89.8 82.2 74.0 96.5 98.5 95.0 81.5 95.4 92.5 91.4 58.5 87.7</cell></row><row><cell>CDAN+MCC w/ SDAT</cell><cell></cell><cell cols="2">98.4 90.9 85.4 82.1 98.5 97.6 96.3 86.1 96.2 96.7 92.9 56.8 89.8</cell></row><row><cell cols="3">minima compared to ResNets (Chen et al., 2021). CDAN</cell></row><row><cell cols="3">+ MCC w/ SDAT outperforms TVT (Yang et al., 2021),</cell></row></table><note>a recent ViT based DA method and achieves SOTA re- sults on both Office-Home and VisDA datasets (</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on DomainNet with CDAN w/ SDAT. The number in the parenthesis refers to the increase in Acc. w.r.t. CDAN.</figDesc><table><row><cell>Target (?) Source (?)</cell><cell>clp</cell><cell>inf</cell><cell>pnt</cell><cell>real</cell><cell>skt</cell><cell>Avg</cell></row><row><cell>clp</cell><cell>-</cell><cell cols="5">22.0 (+1.4) (+2.6) (+1.5) (+2.3) (+2.0) 41.5 57.5 47.2 42.1</cell></row><row><cell>inf</cell><cell>33.9 (+2.3)</cell><cell>-</cell><cell cols="3">30.3 (+1.0) (+4.5) (1.5) 48.1 27.9</cell><cell>35.0 (+2.3)</cell></row><row><cell>pnt</cell><cell cols="2">47.5 (+3.4) (+0.9) 20.7</cell><cell>-</cell><cell cols="3">58.0 (+0.8) (+1.8) (+1.7) 41.8 42.0</cell></row><row><cell>real</cell><cell cols="3">56.7 (+0.9) (+0.7) (+0.4) 25.1 53.6</cell><cell>-</cell><cell cols="2">43.9 (+1.6) (+1.0) 44.8</cell></row><row><cell>skt</cell><cell cols="4">58.7 (+2.7) (+1.1) (+2.8) (+2.2) 21.8 48.1 57.1</cell><cell>-</cell><cell>46.4 (+2.2)</cell></row><row><cell>Avg</cell><cell cols="6">49.2 (+2.3) (+1.0) (+1.7) (+2.2) (+1.8) (+1.8) 22.4 43.4 55.2 40.2 42.1</cell></row><row><cell>DomainNet:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Results on DA for object detection.</figDesc><table><row><cell>Method</cell><cell cols="3">C ? F c P ? C P ? C (bs=2) (bs=2) (bs=8)</cell></row><row><cell>DA-Faster (Chen et al., 2018)</cell><cell>35.21</cell><cell>29.96</cell><cell>26.40</cell></row><row><cell>DA-Faster w/ SDAT</cell><cell>37.47</cell><cell>29.04</cell><cell>27.64</cell></row><row><cell>DA-Faster w/ SDAT-Classification</cell><cell>38.00</cell><cell>31.23</cell><cell>30.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table /><note>also shows that even DA-Faster w/ SDAT (i.e. smoothing both classifica- tion and regression) outperforms DA-Faster by 0.9 % on average. The improvement due to SDAT on adaptation for object detection shows the generality of SDAT across techniques that have some form of adversarial component present in the loss formulation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison across different loss smoothing techniques on Office-Home. SDAT (with ResNet-50 backbone) outperforms other smoothing techniques in each case consistently.</figDesc><table><row><cell cols="5">Method Ar)Cl Cl)Pr Rw)Cl Pr)Cl</cell><cell>Avg</cell></row><row><cell>DAT</cell><cell>54.3</cell><cell>69.5</cell><cell>60.1</cell><cell>55.3 59.2</cell></row><row><cell>VAT</cell><cell>54.6</cell><cell>70.7</cell><cell>60.8</cell><cell cols="2">54.4 60.1 (+0.9)</cell></row><row><cell>SWAD</cell><cell>54.6</cell><cell>71.0</cell><cell>60.9</cell><cell cols="2">55.2 60.4 (+1.2)</cell></row><row><cell>LS</cell><cell>53.6</cell><cell>71.6</cell><cell>59.9</cell><cell cols="2">53.4 59.6 (+0.4)</cell></row><row><cell>SAM</cell><cell>54.9</cell><cell>70.9</cell><cell>59.2</cell><cell cols="2">53.9 59.7 (+0.5)</cell></row><row><cell>SDAT</cell><cell>56.0</cell><cell>73.2</cell><cell>61.4</cell><cell cols="2">55.9 61.6 (+2.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Is it better than other smoothing techniques? To answer this question, we compare SDAT with different smoothing techniques originally proposed for ERM. We specifically compare our method against DAT, Label Smoothing (LS)(Szegedy et al., 2016),SAM (Foret et al., 2021)  and VAT<ref type="bibr" target="#b14">(Miyato et al., 2019)</ref>.Stutz et al. (2021)  recently showed that these techniques produce a significantly smooth loss</figDesc><table><row><cell>Accuracy (in %)</cell><cell>54.75 55.00 55.25 55.50 56.00 55.75</cell><cell>A) Value Ablation</cell><cell>Accuracy (in %)</cell><cell>45 50 25 30 35 40</cell><cell cols="3">B) Label Noise Ablation CDAN CDAN w/ SDAT</cell><cell>C) Smoothing Loss Components Smooth Task Smooth Adv Accuracy 54.3 51.0 55.7 54.9</cell></row><row><cell></cell><cell>54.50</cell><cell></cell><cell></cell><cell>20</cell><cell></cell><cell></cell></row><row><cell></cell><cell>54.25</cell><cell>0.00 0.01 0.02 0.03 0.04 0.05</cell><cell></cell><cell>15</cell><cell>10%</cell><cell>20% Label Noise</cell><cell>40%</cell></row></table><note>landscape in comparison to SGD. We also compare with a very recent SWAD (Cha et al., 2021) technique which is shown effective for domain generalization. For this, we run our experiments on four different splits of the Office-Home dataset and summarize our results in Table 5. We find that</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Analysing the effect of SDAT on DANN (on DomainNet and Office-Home with ResNet-50 and ViT-B/16 respectively) and GVB-GD (on Office-Home with ResNet-50).</figDesc><table><row><cell>DomainNet</cell><cell></cell><cell>clp)pnt</cell><cell>skt)pnt</cell><cell>inf)real</cell><cell>skt)clp</cell></row><row><cell>DANN DANN w/ SDAT</cell><cell>RN-50</cell><cell cols="4">37.5 38.9 (+1.4) 45.7 (+1.8) 39.6 (+1.9) 56.3 (+2.5) 43.9 37.7 53.8</cell></row><row><cell>Office-Home</cell><cell></cell><cell>Ar)Cl</cell><cell>Cl)Pr</cell><cell>Rw)Cl</cell><cell>Pr)Cl</cell></row><row><cell>GVB-GD GVB-GD w/ SDAT DANN</cell><cell>RN-50</cell><cell cols="4">56.4 57.6 (+1.2) 75.4 (+1.2) 60.0 (+1.0) 56.6 (+0.7) 74.2 59.0 55.9 52.6 65.4 60.4 52.3</cell></row><row><cell>DANN w/ SDAT</cell><cell></cell><cell cols="4">53.4 (+0.8) 66.4 (+1.0) 61.3 (+0.9) 53.8 (+1.5)</cell></row><row><cell>DANN DANN w/ SDAT</cell><cell>ViT</cell><cell cols="4">62.7 68.0 (+5.3) 82.4 (+0.6) 73.4 (+4.9) 68.8 (+2.3) 81.8 68.5 66.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>supervised learning and domain adaptation. arXiv preprint arXiv:2106.04732, 2021. 20 Biewald, L. Experiment tracking with weights and biases, 2020. URL https://www.wandb.com/. Software available from wandb.com. 17 Foret, P., Kleiner, A., Mobahi, H., and Neyshabur, B. Sharpness-aware minimization for efficiently improving generalization. In International Conference on Learning Representations, 2021. URL https:// openreview.net/forum?id=6Tm1mposlrM.</figDesc><table><row><cell></cell><cell></cell><cell>1,</cell></row><row><cell></cell><cell>2, 4, 8, 15, 19</cell><cell></cell></row><row><cell>Carmon, Y., Duchi, J. C., Hinder, O., and Sidford, A. Lower bounds for finding stationary points I. Mathe-matical Programming, 184(1):71-120, 2020. 4, 14</cell><cell cols="2">Ganin, Y. and Lempitsky, V. Unsupervised domain adapta-tion by backpropagation. In International conference on machine learning, pp. 1180-1189. PMLR, 2015. 1, 2, 4,</cell></row><row><cell>Cha, J., Chun, S., Lee, K., Cho, H.-C., Park, S., Lee, Y.,</cell><cell>18</cell><cell></cell></row><row><cell>and Park, S. Swad: Domain generalization by seeking flat minima. arXiv preprint arXiv:2102.08604, 2021. 1, 8, 19</cell><cell cols="2">Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., and Lem-pitsky, V. Domain-adversarial training of neural net-</cell></row><row><cell>Chen, X., Hsieh, C.-J., and Gong, B. When vision trans-</cell><cell cols="2">works. The journal of machine learning research, 17</cell></row><row><cell>formers outperform resnets without pretraining or strong</cell><cell cols="2">(1):2096-2030, 2016. 6, 7, 9, 16, 17</cell></row><row><cell>data augmentations. arXiv preprint arXiv:2106.01548, 2021. 7, 16</cell><cell cols="2">Ghorbani, B., Krishnan, S., and Xiao, Y. An investigation into neural net optimization via hessian eigenvalue den-</cell></row><row><cell>Chen, Y., Li, W., Sakaridis, C., Dai, D., and Van Gool, L.</cell><cell cols="2">sity. In International Conference on Machine Learning,</cell></row><row><cell>Domain adaptive faster r-cnn for object detection in the</cell><cell cols="2">pp. 2232-2241. PMLR, 2019. 16</cell></row><row><cell>wild. In Proceedings of the IEEE conference on com-</cell><cell></cell><cell></cell></row><row><cell>puter vision and pattern recognition, pp. 3339-3348,</cell><cell cols="2">Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,</cell></row><row><cell>2018. 7, 8, 17</cell><cell cols="2">Warde-Farley, D., Ozair, S., Courville, A., and Bengio,</cell></row><row><cell></cell><cell cols="2">Y. Generative adversarial nets. Advances in neural in-</cell></row><row><cell>Chu, C., Minami, K., and Fukumizu, K. Smoothness</cell><cell cols="2">formation processing systems, 27, 2014. 1</cell></row><row><cell>and stability in gans. arXiv preprint arXiv:2002.04185,</cell><cell></cell><cell></cell></row><row><cell>2020. 4</cell><cell cols="2">He, H., Huang, G., and Yuan, Y. Asymmetric valleys: be-</cell></row><row><cell>Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler,</cell><cell cols="2">yond sharp and flat local minima. In Proceedings of the 33rd International Conference on Neural Informa-</cell></row><row><cell>M., Benenson, R., Franke, U., Roth, S., and Schiele, B. The cityscapes dataset for semantic urban scene un-</cell><cell cols="2">tion Processing Systems, pp. 2553-2564, 2019. 2</cell></row><row><cell>derstanding. In Proceedings of the IEEE conference</cell><cell cols="2">He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-</cell></row><row><cell>on computer vision and pattern recognition, pp. 3213-</cell><cell cols="2">ing for image recognition. In Proceedings of the IEEE</cell></row><row><cell>3223, 2016. 8</cell><cell cols="2">conference on computer vision and pattern recognition,</cell></row><row><cell>Cui, S., Wang, S., Zhuo, J., Su, C., Huang, Q., and Qi,</cell><cell cols="2">pp. 770-778, 2016. 6, 7, 8</cell></row><row><cell>T. Gradually vanishing bridge for adversarial domain</cell><cell cols="2">Hochreiter, S. and Schmidhuber, J. Simplifying neural nets</cell></row><row><cell>adaptation. In Proceedings of the IEEE Conference on</cell><cell cols="2">by discovering flat minima. Advances in neural informa-</cell></row><row><cell>Computer Vision and Pattern Recognition, 2020. 1, 9</cell><cell cols="2">tion processing systems, 7, 1994. 2</cell></row><row><cell>Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,</cell><cell cols="2">Hochreiter, S. and Schmidhuber, J. Flat minima. Neural</cell></row><row><cell>D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,</cell><cell cols="2">computation, 9(1):1-42, 1997. 2</cell></row><row><cell>M., Heigold, G., Gelly, S., et al. An image is worth</cell><cell></cell><cell></cell></row><row><cell>16x16 words: Transformers for image recognition at</cell><cell cols="2">Inoue, N., Furuta, R., Yamasaki, T., and Aizawa, K. Cross-</cell></row><row><cell>scale. In International Conference on Learning Repre-</cell><cell cols="2">domain weakly-supervised object detection through pro-</cell></row><row><cell>sentations, 2020. 2, 6</cell><cell cols="2">gressive domain adaptation. In Proceedings of the IEEE</cell></row><row><cell></cell><cell cols="2">conference on computer vision and pattern recognition,</cell></row><row><cell>Dziugaite, G. K. and Roy, D. M. Computing nonvacuous</cell><cell>pp. 5001-5009, 2018. 8</cell><cell></cell></row><row><cell>generalization bounds for deep (stochastic) neural net-</cell><cell></cell><cell></cell></row><row><cell>works with many more parameters than training data.</cell><cell cols="2">Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D.,</cell></row><row><cell>arXiv preprint arXiv:1703.11008, 2017. 2</cell><cell>and Wilson, A. G.</cell><cell>Averaging weights leads to</cell></row><row><cell></cell><cell cols="2">wider optima and better generalization. arXiv preprint</cell></row><row><cell></cell><cell cols="2">arXiv:1803.05407, 2018. 19</cell></row><row><cell>88</cell><cell></cell><cell></cell></row><row><cell>(2):303-338, 2010. 8</cell><cell></cell><cell></cell></row></table><note>Everingham, M., Van Gool, L., Williams, C. K., Winn, J., and Zisserman, A. The pascal visual object classes (voc) challenge. International journal of computer vision,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Saito, K., Watanabe, K., Ushiku, Y., and Harada, T. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. Yao, Z., Gholami, A., Keutzer, K., and Mahoney, M. W. Pyhessian: Neural networks through the lens of the hessian. In 2020 IEEE International Conference on Big Data (Big Data), pp. 581-590. IEEE, 2020. 16 Zhang, Y., Liu, T., Long, M., and Jordan, M. Bridging theory and algorithm for domain adaptation. In International Conference on Machine Learning, pp. 7404-7413.</figDesc><table><row><cell></cell><cell></cell><cell>3723-</cell></row><row><cell cols="2">3732, 2018b. 7, 16</cell><cell></cell></row><row><cell cols="3">Saito, K., Ushiku, Y., Harada, T., and Saenko, K. Strong-</cell></row><row><cell cols="3">weak distribution alignment for adaptive object detec-</cell></row><row><cell cols="3">tion. In Proceedings of the IEEE/CVF Conference on</cell><cell>PMLR, 2019. 1, 2, 6</cell></row><row><cell cols="3">Computer Vision and Pattern Recognition, pp. 6956-</cell></row><row><cell>6965, 2019. 1, 8</cell><cell></cell><cell></cell></row><row><cell cols="3">Sakaridis, C., Dai, D., and Van Gool, L. Semantic foggy</cell></row><row><cell cols="3">scene understanding with synthetic data. International</cell></row><row><cell cols="3">Journal of Computer Vision, 126(9):973-992, 2018. 8</cell></row><row><cell cols="3">Stutz, D., Hein, M., and Schiele, B. Relating adversari-</cell></row><row><cell cols="3">ally robust generalization to flat minima. arXiv preprint</cell></row><row><cell cols="2">arXiv:2104.04448, 2021. 8, 19</cell><cell></cell></row><row><cell cols="3">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wo-</cell></row><row><cell cols="3">jna, Z. Rethinking the inception architecture for com-</cell></row><row><cell cols="3">puter vision. In Proceedings of the IEEE conference</cell></row><row><cell cols="3">on computer vision and pattern recognition, pp. 2818-</cell></row><row><cell cols="2">2826, 2016. 8, 19</cell><cell></cell></row><row><cell cols="3">Tang, H., Chen, K., and Jia, K. Unsupervised domain adap-</cell></row><row><cell cols="3">tation via structurally regularized deep clustering. In</cell></row><row><cell cols="3">Proceedings of the IEEE/CVF conference on computer</cell></row><row><cell cols="3">vision and pattern recognition, pp. 8725-8735, 2020. 6</cell></row><row><cell cols="3">Vapnik, V. The nature of statistical learning theory.</cell></row><row><cell cols="2">Springer science &amp; business media, 2013. 1</cell><cell></cell></row><row><cell cols="3">Venkateswara, H., Eusebio, J., Chakraborty, S., and Pan-</cell></row><row><cell cols="3">chanathan, S. Deep hashing network for unsupervised</cell></row><row><cell cols="3">domain adaptation. In (IEEE) Conference on Computer</cell></row><row><cell cols="3">Vision and Pattern Recognition (CVPR), 2017. 6</cell></row><row><cell cols="3">Wang, X., Jin, Y., Long, M., Wang, J., and Jordan, M. I.</cell></row><row><cell cols="3">Transferable normalization: towards improving transfer-</cell></row><row><cell cols="3">ability of deep neural networks. In Proceedings of the</cell></row><row><cell cols="3">33rd International Conference on Neural Information</cell></row><row><cell cols="2">Processing Systems, pp. 1953-1963, 2019. 2</cell><cell></cell></row><row><cell>Wightman, R.</cell><cell>Pytorch image models.</cell><cell>https:</cell></row><row><cell cols="3">//github.com/rwightman/pytorch-image-</cell></row><row><cell cols="2">models, 2019. 17</cell><cell></cell></row><row><cell cols="3">Wu, Y., Kirillov, A., Massa, F., Lo, W.-Y., and Gir-</cell></row><row><cell cols="3">shick, R. Detectron2. https://github.com/</cell></row><row><cell cols="3">facebookresearch/detectron2, 2019. 17</cell></row><row><cell cols="3">Yang, J., Liu, J., Xu, N., and Huang, J. Tvt: Transferable</cell></row><row><cell cols="3">vision transformer for unsupervised domain adaptation.</cell></row><row><cell cols="3">arXiv preprint arXiv:2108.05988, 2021. 6, 7, 19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>The notations used in the paper and the corresponding meaning.</figDesc><table><row><cell cols="2">Notation</cell><cell>Meaning</cell></row><row><cell>S</cell><cell></cell><cell>Labeled Source Data</cell></row><row><cell>T</cell><cell></cell><cell>Unlabelled Target Data</cell></row><row><cell cols="2">P S (or P T )</cell><cell>Source (or Target) Distribution</cell></row><row><cell>X</cell><cell></cell><cell>Input space</cell></row><row><cell>Y</cell><cell></cell><cell>Label space</cell></row><row><cell>y(?)</cell><cell></cell><cell>Maps image to labels</cell></row><row><cell>h ?</cell><cell></cell><cell>Hypothesis function</cell></row><row><cell cols="3">R l S (h ? ) (or R l T (h ? )) Source (or Target) risk</cell></row><row><cell cols="3">R l S (h ? ) (orR l T (h ? )) Empirical Source (or Target) risk</cell></row><row><cell>H</cell><cell></cell><cell>Hypothesis space</cell></row><row><cell cols="2">D ? h ? ,H (P S ||P T )</cell><cell>Discrepancy between two domains P S and P T</cell></row><row><cell>g ?</cell><cell></cell><cell>Feature extractor</cell></row><row><cell>f ?</cell><cell></cell><cell>Classifier</cell></row><row><cell>D ?</cell><cell></cell><cell>Domain Discriminator</cell></row><row><cell>d ? S,T</cell><cell></cell><cell>Tractable Discrepancy Estimate</cell></row><row><cell>? 2 ?R</cell><cell>l S (h ? ) (or H)</cell><cell>Hessian of classification loss</cell></row><row><cell cols="2">T r(H)</cell><cell>Trace of Hessian</cell></row><row><cell cols="2">? max</cell><cell>Maximum eigenvalue of Hessian</cell></row><row><cell></cell><cell></cell><cell>Perturbation</cell></row><row><cell>?</cell><cell></cell><cell>Maximum norm of</cell></row><row><cell cols="3">B. Connection of Discrepancy to d ? S,T (Eq. 4) in Main Paper</cell></row><row><cell cols="3">We refer reader to Appendix C.2 of Acuna et al. (2021) for relation of d ? S,T . The d ? S,T term defined in Eq. 4 given as:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 .</head><label>8</label><figDesc>Architecture used for feature classifier and Domain classifier. C is the number of classes. Both classifiers will take input from feature generator (g ? ).</figDesc><table><row><cell>Layer</cell><cell>Output Shape</cell></row><row><cell cols="2">Feature Classifier (f ? )</cell></row><row><cell>-</cell><cell>Bottleneck Dimension</cell></row><row><cell>Linear</cell><cell>C</cell></row><row><cell cols="2">Domain Classifier (D ? )</cell></row><row><cell>-</cell><cell>Bottleneck Dimension</cell></row><row><cell>Linear</cell><cell>1024</cell></row><row><cell>BatchNorm</cell><cell>1024</cell></row><row><cell>ReLU</cell><cell>1024</cell></row><row><cell>Linear</cell><cell>1024</cell></row><row><cell>BatchNorm</cell><cell>1024</cell></row><row><cell>ReLU</cell><cell>1024</cell></row><row><cell>Linear</cell><cell>1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 .</head><label>9</label><figDesc>Accuracy (%) on VisDA-2017 (ResNet-101 and ViT backbone).</figDesc><table><row><cell>Method</cell><cell></cell><cell>Synthetic ? Real</cell></row><row><cell>DANN (Ganin et al., 2016)</cell><cell></cell><cell>57.4</cell></row><row><cell>MCD (Saito et al., 2018b) CDAN* (Long et al., 2018) CDAN CDAN w/ SDAT CDAN+MCC (Jin et al., 2020)</cell><cell>ResNet-101</cell><cell>71.4 73.7 76.6 78.3 80.4</cell></row><row><cell>CDAN+MCC w/ SDAT</cell><cell></cell><cell>81.2</cell></row><row><cell>CDAN</cell><cell></cell><cell>76.7</cell></row><row><cell>CDAN w/ SDAT CDAN+MCC (Jin et al., 2020)</cell><cell>ViT</cell><cell>81.1 85.1</cell></row><row><cell>CDAN+MCC w/ SDAT</cell><cell></cell><cell>87.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>Accuracy(%) on DomainNet dataset for unsupervised domain adaptation (ResNet-101) across five distinct domains. The row indicates the source domain and the columns indicate the target domain.</figDesc><table><row><cell>ADDA</cell><cell>clp</cell><cell>inf</cell><cell>pnt</cell><cell>rel</cell><cell>skt</cell><cell>Avg</cell><cell>MCD</cell><cell>clp</cell><cell>inf</cell><cell>pnt</cell><cell>rel</cell><cell>skt</cell><cell>Avg</cell></row><row><cell>clp</cell><cell>-</cell><cell cols="5">11.2 24.1 41.9 30.7 27.0</cell><cell>clp</cell><cell>-</cell><cell cols="5">14.2 26.1 45.0 33.8 29.8</cell></row><row><cell>inf</cell><cell>19.1</cell><cell>-</cell><cell cols="4">16.4 26.9 14.6 19.2</cell><cell>inf</cell><cell>23.6</cell><cell>-</cell><cell cols="4">21.2 36.7 18.0 24.9</cell></row><row><cell>pnt</cell><cell>31.2</cell><cell>9.5</cell><cell>-</cell><cell cols="3">39.1 25.4 26.3</cell><cell>pnt</cell><cell cols="2">34.4 14.8</cell><cell>-</cell><cell cols="3">50.5 28.4 32.0</cell></row><row><cell>rel</cell><cell cols="3">39.5 14.5 29.1</cell><cell>-</cell><cell cols="2">25.7 27.2</cell><cell>rel</cell><cell cols="3">42.6 19.6 42.6</cell><cell>-</cell><cell cols="2">29.3 33.5</cell></row><row><cell>skt</cell><cell>35.3</cell><cell>8.9</cell><cell cols="2">25.2 37.6</cell><cell>-</cell><cell>26.7</cell><cell>skt</cell><cell cols="4">41.2 13.7 27.6 34.8</cell><cell>-</cell><cell>29.3</cell></row><row><cell>Avg</cell><cell cols="6">31.3 11.0 23.7 36.4 24.1 25.3</cell><cell>Avg</cell><cell cols="6">35.4 15.6 29.4 41.7 27.4 29.9</cell></row><row><cell>CDAN</cell><cell>clp</cell><cell>inf</cell><cell>pnt</cell><cell>rel</cell><cell>skt</cell><cell>Avg</cell><cell>CDAN w/ SDAT</cell><cell>clp</cell><cell>inf</cell><cell>pnt</cell><cell>rel</cell><cell>skt</cell><cell>Avg</cell></row><row><cell>clp</cell><cell>-</cell><cell cols="5">20.6 38.9 56.0 44.9 40.1</cell><cell>clp</cell><cell>-</cell><cell cols="5">22.0 41.5 57.5 47.2 42.1</cell></row><row><cell>inf</cell><cell>31.5</cell><cell>-</cell><cell cols="4">29.3 43.6 26.3 32.7</cell><cell>inf</cell><cell>33.9</cell><cell>-</cell><cell cols="4">30.3 48.1 27.9 35.0</cell></row><row><cell>pnt</cell><cell cols="2">44.1 19.8</cell><cell>-</cell><cell cols="3">57.2 39.9 40.2</cell><cell>pnt</cell><cell cols="2">47.5 20.7</cell><cell>-</cell><cell cols="3">58.0 41.8 42.0</cell></row><row><cell>rel</cell><cell cols="3">55.8 24.4 53.2</cell><cell>-</cell><cell cols="2">42.3 43.9</cell><cell>rel</cell><cell cols="3">56.7 25.1 53.6</cell><cell>-</cell><cell cols="2">43.9 44.8</cell></row><row><cell>skt</cell><cell cols="4">56.0 20.7 45.3 54.9</cell><cell>-</cell><cell>44.2</cell><cell>skt</cell><cell cols="4">58.7 21.8 48.1 57.1</cell><cell>-</cell><cell>46.4</cell></row><row><cell>Avg</cell><cell cols="6">46.9 21.4 41.7 52.9 38.3 40.2</cell><cell>Avg</cell><cell cols="6">49.2 22.4 43.4 55.2 40.2 42.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 5</head><label>5</label><figDesc>of the main paper refers to the mean of the accuracy across classes. CDAN w/ SDAT outperforms CDAN by 1.7% with ResNet-101 and by 4.4% with ViT backbone, showing the effectiveness of SDAT in large scale Synthetic ? Real shifts. With CDAN+MCC as the DA method, adding SDAT improves the performance of the method to 81.2% with ResNet-101 backbone.</figDesc><table /><note>DomainNet:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 11 .</head><label>11</label><figDesc>Accuracy (%) of source-only model trained with SGD (ERM) and SAM (ERM w/SAM) on VisDA-2017 for unsupervised DA with ViT-B/16 backbone H. Different Smoothing Techniques Stochastic Weight Averaging (SWA) (Izmailov et al., 2018): SWA is a widely popular technique to reach a flatter minima.</figDesc><table><row><cell>Method</cell><cell cols="3">plane bcybl bus</cell><cell cols="10">car horse knife mcyle persn plant sktb train truck mean</cell></row><row><cell>ERM</cell><cell>98.4</cell><cell cols="4">58.3 80.2 60.7 89.3</cell><cell>53.6</cell><cell>88.4</cell><cell>40.8</cell><cell cols="3">62.8 87.4 94.7</cell><cell>19.1</cell><cell>69.5</cell></row><row><cell>ERM w/ SAM</cell><cell>98.6</cell><cell cols="4">33.1 80.0 76.9 90.1</cell><cell>35.9</cell><cell>94.2</cell><cell>22.8</cell><cell cols="3">77.8 89.0 95.3</cell><cell>11.6</cell><cell>67.1</cell></row><row><cell cols="14">Table 12. Accuracy (%) of source-only model trained with SGD (ERM) and SAM (ERM w/SAM) on Office-Home for unsupervised DA</cell></row><row><cell cols="2">with ViT-B/16 backbone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="13">Ar)Cl Ar)Pr Ar)Rw Cl)Ar Cl)Pr Cl)Rw Pr)Ar Pr)Cl Pr)Rw Rw)Ar Rw)Cl Rw)Pr Avg</cell></row><row><cell>ERM</cell><cell>51.5</cell><cell>80.8</cell><cell>86.0</cell><cell>74.8</cell><cell>80.2</cell><cell>82.6</cell><cell>71.8</cell><cell>51.0</cell><cell>85.5</cell><cell>79.5</cell><cell>55.0</cell><cell>87.9</cell><cell>73.9</cell></row><row><cell>ERM w/ SAM</cell><cell>50.8</cell><cell>79.5</cell><cell>85.2</cell><cell>72.6</cell><cell>78.4</cell><cell>81.4</cell><cell>71.8</cell><cell>49.6</cell><cell>85.2</cell><cell>79.0</cell><cell>52.8</cell><cell>87.2</cell><cell>72.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>1. Label Smoothing (LS) (Szegedy et al., 2016): The idea behind label smoothing is to have a distribution over outputs instead of one hot vectors. Assuming that there are k classes, the correct class gets a probability of 1 -? and the other classes gets a probability of ?/(k ? 1) . (Stutz et al., 2021) mention that label smoothing tends to avoid sharper minima during training. We use a smoothing parameter (?) of 0.1 in all the experiments in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 13 .</head><label>13</label><figDesc>Different Smoothing techniques. We refer to(Stutz et al., 2021)  to compare the proposed SDAT with other techniques to show the efficacy of SDAT. It can be seen that SDAT outperforms the other smoothing techniques significantly. Other smoothing techniques improve upon the performance of DAT showing that smoothing is indeed necessary for better adaptation.</figDesc><table><row><cell>Method</cell><cell cols="4">Ar)Cl Cl)Pr Rw)Cl Pr)Cl</cell></row><row><cell>DAT</cell><cell>54.3</cell><cell>69.5</cell><cell>60.1</cell><cell>55.3</cell></row><row><cell>VAT</cell><cell>54.6</cell><cell>70.7</cell><cell>60.8</cell><cell>54.4</cell></row><row><cell>SWAD-400</cell><cell>54.6</cell><cell>71.0</cell><cell>60.9</cell><cell>55.2</cell></row><row><cell>LS (? = 0.1)</cell><cell>53.6</cell><cell>71.6</cell><cell>59.9</cell><cell>53.4</cell></row><row><cell>LS (? = 0.2)</cell><cell>53.5</cell><cell>71.2</cell><cell>60.5</cell><cell>53.2</cell></row><row><cell>SDAT</cell><cell>55.9</cell><cell>73.2</cell><cell>61.4</cell><cell>55.9</cell></row><row><cell>I. Optimum ? value</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 14</head><label>14</label><figDesc>and 15 show that ? = 0.02 works robustly across experiments providing an increase in performance (although it does not achieve the best result each time) and can be used as a rule of thumb.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 14 .</head><label>14</label><figDesc>? value for DomainNet on the unseen target data. On the contrary, they introduce additional modules within their architecture. The Transferability Adaption Module (TAM) is introduced to assist the ViT backbone in capturing both discriminative and transferable features. Additionally, the Discriminative Clustering Module (DCM) is used to perform discriminative clustering to achieve diverse and clustered features.</figDesc><table><row><cell>Split</cell><cell cols="3">DAT SDAT(? = 0.02) SDAT -Reported (? = 0.05)</cell></row><row><cell>clp)skt</cell><cell>44.9</cell><cell>46.7</cell><cell>47.2</cell></row><row><cell>skt)clp</cell><cell>56.0</cell><cell>59.0</cell><cell>58.7</cell></row><row><cell cols="2">skt)pnt 45.3</cell><cell>47.8</cell><cell>48.1</cell></row><row><cell>inf)rel</cell><cell>43.6</cell><cell>47.3</cell><cell>48.1</cell></row><row><cell></cell><cell cols="3">Table 15. ? value for VisDA-2017 Synthetic ) Real</cell></row><row><cell>Backbone</cell><cell cols="3">DAT SDAT (? = 0.02) SDAT Reported(? = 0.005)</cell></row><row><cell>CDAN</cell><cell>76.6</cell><cell>78.2</cell><cell>78.3</cell></row><row><cell cols="2">CDAN+MCC 80.4</cell><cell>80.9</cell><cell>81.2</cell></row><row><cell>to perform well</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 3 )Table 18 .</head><label>318</label><figDesc>Average Increase clp)pnt 38.9 ? 0.1 41.5 ? 0.3Table 17. Median accuracy of last 5 epochs on DomainNet dataset with CDAN w/ SDAT. The number in the parenthesis indicates the increase in accuracy with respect to CDAN. Office-Home experiments over 3 different seeds (with ResNet-50 backbone). We report the mean, standard deviation, reported increase and average increase in the accuracy (in %).</figDesc><table><row><cell>+2.6</cell><cell>+2.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.kaggle.com/c/tiny-imagenet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work was supported in part by SERB-STAR Project (Project:STR/2020/000128), Govt. of India. Harsh Rangwani is supported by Prime Minister's Research Fellowship (PMRF). We are thankful for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>id=r1g87C4KwB. 4</idno>
		<ptr target="https://openreview.net/forum" />
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Minimum class confusion for versatile domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Transferlearning-library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junguang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Baixu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<ptr target="https://github.com/thuml/Transfer-Learning-Library,2020.6" />
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Contrastive Learning for Conditional Image Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Contragan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving generalization performance by switching from adam to sgd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.07628</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Towards inheritable models for open-set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Venkat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Revanur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generalize then adapt: Source-free domain adaptive semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="7046" to="7056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised imageto-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="700" to="708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshida</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ishii</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2858821</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf.6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Wallach, H., Larochelle, H., Beygelzimer, A., d&apos;Alch?-Buc, F., Fox, E., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<title level="m">The visual domain adaptation challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Submodular subset selection for virtual adversarial active domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rangwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Aithal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>S3vaada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="7516" to="7525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faster R-Cnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adversarial dropout regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main"># Calculate task loss 10 class_prediction, feature = model(x)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">task_loss = task_loss_fn(class_prediction, label)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main"># Update parameters (Sharpness-Aware update) 25 optimizer</title>
		<imprint/>
	</monogr>
	<note>second_step(</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main"># Update parameters of domain classifier 27 ad_optimizer</title>
		<imprint/>
	</monogr>
	<note>step(</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
