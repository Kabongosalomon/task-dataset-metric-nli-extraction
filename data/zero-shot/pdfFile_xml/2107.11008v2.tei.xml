<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mousavi</surname></persName>
							<email>smousavi2@student.gsu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University Atlanta</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rolando</forename><surname>Estrada</surname></persName>
							<email>restrada1@gsu.edu</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Georgia State University</orgName>
								<address>
									<settlement>Atlanta</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SuperCaustics: Real-time, open-source simulation of transparent objects for deep learning applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Computer Vision</term>
					<term>Synthetic Data</term>
					<term>Dataset</term>
					<term>AI Simulation</term>
					<term>Virtual Environment</term>
					<term>Caustics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transparent objects are a very challenging problem in computer vision. They are hard to segment or classify due to their lack of precise boundaries, and there is limited data available for training deep neural networks. As such, current solutions for this problem employ rigid synthetic datasets, which lack flexibility and lead to severe performance degradation when deployed on real-world scenarios. In particular, these synthetic datasets omit features such as refraction, dispersion and caustics due to limitations in the rendering pipeline. To address this issue, we present SuperCaustics, a real-time, open-source simulation of transparent objects designed for deep learning applications. SuperCaustics features extensive modules for stochastic environment creation; uses hardware ray-tracing to support caustics, dispersion, and refraction; and enables generating massive datasets with multi-modal, pixel-perfect ground truth annotations. To validate our proposed system, we trained a deep neural network from scratch to segment transparent objects in difficult lighting scenarios. Our neural network achieved performance comparable to the state-of-the-art on a real-world dataset using only 10% of the training data and in a fraction of the training time. Further experiments show that a model trained with SuperCaustics can segment different types of caustics, even in images with multiple overlapping transparent objects. To the best of our knowledge, this is the first such result for a model trained on synthetic data. Both our open-source code and experimental data are freely available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Detecting transparent objects is one of the most challenging problems in computer vision, because these objects do not form disjoint boundaries with their environment. In addition, different types of transparent objects have unique characteristics based on the materials they're made of, and they come in varied thicknesses and densities that affects their appearance. Light passing through a transparent object will bounce multiple times before it reaches our eyes (or a camera), causing effects such as refraction, specular reflections, caustics and dispersion; and the dynamic nature of light makes them appear in radically different ways when exposed to light from different angles.</p><p>Deep learning has flourished into a rapidly thriving field over the last decade. In particular, it has achieved state-of-theart results in problems for which large amounts of (usually labelled) data is available. However, there has been relatively little work on applying deep learning to transparent object detection in part because gathering and labeling a sufficiently large dataset for this problem is cumbersome and difficult. Most recent publications on transparent object detection have utilized small, custom-made datasets of real data, gathered and labelled by the authors themselves to validate or test their algorithm or DCNN model. The actual training data consists primarily of large volumes of synthetic images created in 3D modelling software (e.g., Blender <ref type="bibr" target="#b0">[1]</ref>).</p><p>An important limitation of these rigid image datasets is their static nature. For instance, once an image has been generated, one cannot change its illumination, or replace the reflection maps in the environment. For real images, labeling the ground truth is also a significant challenge, especially for cameras with high pixel counts. Depth, in particular, is almost impossible to label by hand, so one is limited to active depth sensing (e.g., RGB-D sensors) or passive methods like disparity matching. However, depth cameras often register errors when encountering transparent objects due to their reflective and refractive properties <ref type="bibr" target="#b1">[2]</ref>. In computer vision applications where transparent objects are involved -such as robotic graspingprecise labeling is an integral part of the entire pipeline, and having fault-free data is crucial to the function of a robotics grasp algorithm. For example, the Cleargrasp pipeline <ref type="bibr" target="#b1">[2]</ref>, the current state-of-the-art for deep-learning based transparent object detection, uses a segmentation map to complement and remove faulty depth signals from a depth image.</p><p>Moreover, no matter how extensive and large a pre-made training dataset is, it can only cover so many lighting conditions. The positions of objects, type of surfaces, camera angles, and placement of light sources will only cover a small subset of all possible combinations. This is a challenge for transparent objects because their appearance is highly dependent on how they are lit. For example, they might cast shadows, caustics, or a combination of the two simply by varying the angle or intensity of the light sources. As such, models trained on synthetic datasets with only a small range of lightning scenarios suffer performance degradation when deployed on real-world data.</p><p>More specifically, as we review in Section II, clever deep learning solutions are often stifled by rigid datasets that cannot represent the full possibility space of the target domain, are difficult to change after acquisition, require significant human effort to produce or use proprietary technology for which the source-code and project files used to generate them are rarely released for free use by the public.</p><p>To address this gap, we propose SuperCaustics, an easy to use, highly customizable, easily extendable, open-source realtime simulation tool designed for generating dynamic, massive datasets in highly complex scenarios. As we detail further in Section III, SuperCaustics is a modular set of customizable classes developed within NVIDIA's RTX Branch of Unreal Engine 4 (Epic Games, USA) <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, allowing even those researchers who are unfamiliar with Unreal Engine to create, maintain and iterate on their own photorealistic customized datasets for vision tasks involving transparent objects. Super-Caustics features four modules: 1) A stochastic scene generation system that creates the geometry of a scene based on parameters like object type, number of objects, camera type, lighting, and background. 2) A prop manager that adds or removes customizable background items to increase scene complexity and variety. 3) A ground truth core that automatically overlays a wide variety of accurate, pixelwise ground-truth annotations. 4) A data ablation core <ref type="bibr" target="#b4">[5]</ref> that enables changing characteristics of the objects, background, environment, lighting, or rendering without affecting scene composition. Our proposed system allows for fine granular control over small details of the scene before and after generation. Each module has a set of customizable and extendable controls, including tools for data ablation. As detailed in <ref type="bibr" target="#b4">[5]</ref>, data ablation is the study of the effects of isolated changes in the features of data on the performance of a deep learning network (e.g. having the exact same image under a different lighting condition). Using the Data Ablation Core, we are able to change the characteristics of a scene in real-time, e.g., different lighting color, presence of sharp or soft caustics, light source angle, texture or shape of the backdrop, or changes in roughness/color of the glass surfaces (see <ref type="figure" target="#fig_3">Fig. 4</ref>). The tools in the Data Ablation Core can also be used to increase variety in the captured images.</p><p>A virtual dataset generated from SuperCaustics contains the features of the target domain, free from acquisition errors or labeling bias. More importantly, as our experiments confirm, a small, targeted, carefully crafted data-set can efficiently match the performance achieved by state-of-the-art methods that train their systems on massive datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>The old and the new -A problem that persists: Recognizing transparent objects has been a difficult problem to solve in vision. Early solutions used general features of transparent objects, e.g., specular reflections, for object recognition <ref type="bibr" target="#b5">[6]</ref>. More sophisticated approaches include estimating bounding boxes based on the refraction distortions that transparent objects' edges create on the background <ref type="bibr" target="#b6">[7]</ref>. Newer research trains deep convolutional neural networks on massive datasets to reach a pixel-wise labeling of transparent objects <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Also, some researchers have devised clever methods to exploit sensor failures in depth images to localize transparent objects <ref type="bibr" target="#b8">[9]</ref>. Regardless, often times researchers are forced to use low-quality real data for testing their algorithm or training a neural network <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b6">[7]</ref>, and other times researchers resort to creating rigid virtual datasets to train large models, such as a DCNN. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>.</p><p>Synthetic Data: Synthetic data has proven useful in various computer vision tasks, such as depth estimation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, surface normal estimation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, robotic grasping <ref type="bibr" target="#b10">[11]</ref>, and object segmentation <ref type="bibr" target="#b11">[12]</ref> by multiple independent researchers <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. Modern computer graphics can achieve near-photorealism, so synthetic data has become a viable alternative in situations where acquiring or labeling real data is difficult. However, there are still few datasets that meet the image quality required to be considered photo-realistic <ref type="bibr" target="#b7">[8]</ref>, and those that do, rarely include transparent objects because of the complexities they introduce in rendering and generating correct ground-truth labels. More importantly, to the best of our knowledge, there is no photorealistic, synthetic Core. Each of these modules comes with its own sub-modules that enable generation of a customizable scene in real-time. We use Blueprint, Unreal's visual scripting language, for the ground-truth annotations and data ablation controls. We use a separate Python interaction module-Controller-for data collection.</p><p>dataset that includes the tools and project files needed to generate the data itself. More often, these synthetic datasets are static and use proprietary code and technologies that does not allow for reproduction or change in the dataset. We believe flexibility in creating the image data is crucial since it allows for greater degrees of freedom when evaluating a specific method, and allows researchers to make corrections in the data and address weak spots in the model's performance.</p><p>The need for reliable data: One common factor in transparent object detection is the lack of reliable data. In older research, we see the use of personally-gathered real data that suffers from lack of variety, noise, or lack of labels. To get past this obstacle, we see various works generate their own synthetic datasets using tools like Blender <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> or NVIDIA Omniverse <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b14">[15]</ref>. As a result, these datasets consist of huge collections of static images. In addition, the developers of these datasets rarely release the project files or source code used to generate the data. Nevertheless, in most cases the target domain of the final algorithm is the real world. As such, to be able to generalize well into the target domain these datasets have to have tens of thousands of images, at least, making the training very expensive and time consuming. Despite all this data, we still see significant performance degradation when shifting from crisp, high-resolution synthetic data to blurry, grainy real images that are lit entirely differently. These synthetic datasets are non-changeable (static) and more often than not, they do not include graphically intense phenomena that happen in real-world scenarios (e.g., dispersion or caustics), which makes them very narrow in domain and makes the trained algorithms sensitive to these lighting conditions. <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b7">[8]</ref>.</p><p>SuperCaustics vs. Cleargrasp: Arguably, the most significant effort in modeling transparent objects is Cleargrasp <ref type="bibr" target="#b1">[2]</ref>. In this study, researchers trained deep learning models to estimate depth, surface normals, and occlusion boundaries using a mix of synthetic and real data. Their results indicate that in addition to a significant performance loss when shifting from a synthetic to a real domain, the models consistently misclassify caustics. In particular, they wrongly classify caustics as separate transparent objects. The authors of Cleargrasp speculated that this was because the same amount of sharp caustics were not present in the synthetic dataset due to limitations in the rendering software. A more flexible simulation tool, such as our proposed system, allows for addition or removal of such features. In particular, SuperCaustics can produce images with sharp caustics. More generally, our system can incorporate features of the real domain in the training data to make trained models more robust to such features in the target domain.</p><p>Overall, prior work on transparent object detection has been hampered by a lack of reliable, flexible data. To address this gap, we have developed SuperCaustics, an extendable, open-source, user-friendly simulation tool that allows for fine controls over every variable in the simulation. Our system can generate millions of unique images out of the box. In addition, it allows researchers to import 3D models of their choice, set their desired parameters, and capture synthetic data from fully customizable virtual environments. We describe our system in more detail, below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. SUPERCAUSTICS</head><p>SuperCaustics is a simulation tool made in Unreal Engine for generating massive computer vision datasets that include transparent objects. Unreal Engine is the engine of choice for projects with high-resolution, real-time 3D graphics. It is free for both commercial and non-commercial use and its source code is publicly available and extended by the community. We use one such extended version created for hardware raytracing by NVIDIA RTX graphics cards <ref type="bibr" target="#b3">[4]</ref>.</p><p>The key contributions of SuperCaustics are: 1. Open-source, free set of user-friendly, extendable tools for creating custom datasets. 2. Customizable photorealistic scenes using realtime hardware ray-tracing, 3. Automatic, multimodal, accurate ground-truth generation for visual tasks, 4. Python and UE4 interplay for data acquisition and processing.</p><p>Below, we detail each major module of SuperCaustics, and provide explanations of the different ground-truth annotations available in SuperCaustics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Generator Module</head><p>The Generator Module creates a scene based on given parameters like item type, shape, number of items and/or range of items. It leverages the physics engine inside UE4 to drop the items on the scene within a customizable distance offset. A random physics impulse can be added to the objects upon spawning to increase randomness and add variety to how they come to rest. The intensity of the impulse is also customizable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Props Manager</head><p>Similar to the Generator Module, a separate module manages the background items in the simulation. This module accepts different 3D objects as prop inputs, and manages their position and rotation within the first few frames of the simulation until they come to rest by the physics engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Ablation Core</head><p>Data Ablation is a powerful method for determining weak points and improving robustness in a neural network's performance <ref type="bibr" target="#b4">[5]</ref>. The Data Ablation Core in SuperCaustics allows for such experiments within the simulation. Additionally, the features of the Data Ablation Core can be used to further increase the variety in the final dataset. We can adjust the cameras (camera matrix), lighting, backdrops, reflection profiles (HDRI maps), and properties of the glass material (color, specular, thickness, roughness, texture). Additionally, all properties of the ray-tracing engine are exposed for experimentation within the Data Ablation module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Ground Truth Core</head><p>SuperCaustics also includes scripts for automatically estimating depth, surface normals (world space and camera space), object masks, outlines, occlusion boundaries and caustics segmentation. The Ground Truth Core can be readily extended by adding additional scripts (or modifying the available scripts). We use post-processing shaders (dubbed materials in UE4), to overlay these properties over the image, enabling pixel-perfect alignment between the data and the groundtruth labels. (See <ref type="figure" target="#fig_1">Figure 2)</ref> We detail each ground-truth mode below:</p><p>1) Depth Ground Truth: We calculate the normalized distance between each pixel that belongs to a specific object and the camera. By default, we set the real-life range of depth to 10 meters, which covers the entire environment. This range is customizable in the Ground Truth Core.</p><p>2) Surface Normals Ground Truth: We estimate the normal vector w.r.t to each 3D surface, then color each pixel to indicate the vector's direction. We use 6 main colors to show 6 axis of direction (positive and negative xyz). Surface normals can be calculated in 2 different spaces: camera-space and world-space. Normals of a pixel in world-space are defined with respect to the world Cartesian coordinate system. In world-space, each normal vector points towards a fixed axis in space, regardless of where it is being observed from. Cameraspace is the space in which points are defined with respect to the camera coordinate system. Each normal vector has the transformation of the camera itself applied to it, making the camera the origin point of the coordinate system. This means the transformations of the camera in world-space is implicitly applied to every normal vector in the image. This is a simplification compared to world-space surface normals. (See <ref type="figure" target="#fig_1">Figure 2.)</ref> 3) Object Mask Ground Truth: In Unreal Engine, it is easy to map visible pixels to their corresponding 3D objects. Our Blueprint script uses this mapping to automatically detect and overlay pixel-perfect semantic labels on the objects in the scene.</p><p>4) Outlines Ground Truth: The edges of transparent objects are sometimes hard to detect visually. In this Ground Truth pass, we can highlight the edges of objects with a customizable thickness (e.g., 1px or 20px).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Occlusion Boundaries Ground Truth:</head><p>This Ground Truth pass shows surfaces where transparent objects are touching another surface (like the backdrop). Specifically, this pass shows where there is a depth discontinuity. The logic of this ground truth pass is adapted from Cleargrasp <ref type="bibr" target="#b1">[2]</ref>. 6) Local &amp; Non-local Caustics Mask Ground Truth: In SuperCaustics, we have fine control over the characteristics of the image, and we can create the exact same image under different settings. We use the difference in the luminosity between two rendered images (one with caustics, and another without) to reach a pixel-wise segmentation of local caustics (inside transparent object) and non-local caustics (projected unto another surface) in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EVALUATION</head><p>To validate the usefulness of SuperCaustics, we trained a deep convolutional neural network (DCNN) to do a pixelwise labeling of transparent objects with both soft and sharp 2) Image acquisition: Using the features described in Section III, we generated 9000 1920?1080 synthetic images from 650 randomized scenarios. In each scenario, a reflection map and a backdrop was randomly chosen from a bank of 33 HDRI mappings and 33 backdrops. Then, a number of transparent objects were dropped into the scene from off-camera, so that they came to rest naturally using UE4's physics engine. Then, the Prop Manager module added the input props in random locations inside the scene. For gathering the images, 12 Intel Realsense cameras were simulated in various locations and angles. In every camera angle, the Data Ablation Core rotated the main light of the scene to cast shadows and caustics at various angles before capturing an image and its ground-truth values. For training, the images were split in the following ratio: 8000 for training and 1000 for validation.</p><p>3) Deep Neural Networks: We used an implementation of U-net <ref type="bibr" target="#b15">[16]</ref> as our segmentation neural network. In particular, we used the tools provided by the EasyTorch Library to prototype and run the experiments. EasyTorch is a Pytorchbased deep learning library used in <ref type="bibr" target="#b16">[17]</ref>- <ref type="bibr" target="#b18">[19]</ref>. We evaluated our performance on mean intersection-over-union (IOU) for the test set. For the loss function, we used the negative log likelihood loss (NLL). We trained our network for 30 epochs on purely synthetic data from SuperCaustics. Afterwards, we used the SuperCaustics model as a pre-trained starting point (labeled SuperCaustics-R in our results), which was trained on Cleargrasp Known Objects for an additional four epochs.  <ref type="table" target="#tab_0">Table I</ref> shows the object segmentation performance of our network trained on SuperCaustics data, and <ref type="table" target="#tab_0">Table II</ref> shows the equivalent results when segmenting caustics directly. In particular, the first table shows that our SuperCaustics-R model (which was also trained on a subset of real data from Cleargrasp) generalizes well to novel objects. Our network trained only with synthetic data achieves 88% IOU on the same domain, and it is robust to challenging image features like sharp caustics and segmenting multiple overlapping objects. Moreover, our SuperCaustics-R model achieves a performance of 53% IOU (comparable to Cleargrasp's 58%) on the Novel Objects test set. Below, we analyze these results in more detail. Performance, Efficiency and Scalability: Compared to <ref type="bibr" target="#b1">[2]</ref>, our training procedure is much more efficient. By producing higher quality, wider-domain data, we can achieve roughly the same performance on the Novel Objects test set using only 10% of the amount of data and in a fraction of the training time used by Cleargrasp (34 epochs on about 8000 images, compared to 100 epochs on 80000 images). Moreover, our training setup did not require 8 Tesla v100 cards as used by Cleargrasp. Achieving comparable results more efficiently supports our assertion that for difficult, complex tasks like labeling shiny transparent objects with caustics, using a rigid, massive dataset is not ideal, and one can always benefit Robustness to Caustics: <ref type="figure" target="#fig_4">Fig. 5</ref> shows examples of images with sharp caustics that were produced by our SuperCaustics system. Our trained model does not confuse caustics with separate transparent objects and is quite robust to caustics from various angles and light sources. This is due to the presence of natural caustics in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Results &amp; Discussion</head><p>Performance on the Real Domain: As <ref type="figure" target="#fig_4">Fig. 5</ref> shows, our trained model seems to have learned the general characteristics of transparent objects from the SuperCaustics synthetic data. However, as opposed to Cleagrasp where known objects were present in the synthetic data, our model was not exposed to the shape of these novel objects. Performance in the real domain was also affected by the grainy, noisy nature of the real-camera. Prior work has shown that segmentation models are very sensitive to fidelity <ref type="bibr" target="#b4">[5]</ref>, thus, a change in the general fidelity of the domain had a significant, but expected impact on the performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Solution to Cleargrasp's main problem -Segmenting</head><p>Non-local Caustics: In Cleargrasp's original experiments, their trained models falsely classify caustics as separate transparent objects <ref type="bibr" target="#b1">[2]</ref>. One possible solution for this problem is having a separate system that only classifies caustics, and because predictions in these regions tend to be invalid, masking the caustics out of the predictions. This setup is straightforward with SuperCaustics. To test this, we generated 2300 1280?720 images using the caustics segmentation mask ground-truth. We then trained a segmentation model to identify sharp, refractive, and reflective non-local caustics for 50 epochs. We then generated 60 images using novel objects (not seen before by the model) to test the generalization of the model. As <ref type="table" target="#tab_0">Table II</ref> shows, our model was able to segment caustics cast by novel objects at 68% IOU rate. As such, we believe that training a model with our SuperCaustics system will prove a feasible solution for masking out the erroneous pixels in Cleargrasp. Sensitivity to Brightness: Our trained model seems to be very sensitive to brightness in the features of the objects. In our tests with synthetic data, where the camera's exposure underexposes shadows, our model seems to make mistakes in segmenting transparent objects.</p><p>Side-Effect of the Patch Based Approach: In some of the images, there is some noise visible in the labeled pixels. We believe this is due to the patch-based approach used by the U-Net neural network; specifically, the noise is a result of the artifacts from stitching these patches together in the final image. In most cases, patches are surrounded by uniform neighboring pixels, which could be used as information in a context-based post processing algorithm to get rid of the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION &amp; FUTURE WORK Conclusion:</head><p>In this paper, we present SuperCaustics, an extendable, open-source, and user-friendly simulation tool for transparent objects that allows for fine controls over every variable in the simulation. Our system allows users to create customized datasets using its flexible simulation tools. With SuperCaustics, we hope to address the dearth of high-quality data for transparent objects, particularly data that can be modified post-acquisition. We demonstrated that by using a better curated data, a network can achieve performance comparable to when training with much larger rigid datasets.</p><p>In particular, we were able to achieve comparable performance to the state-of-the-art networks trained on Cleargrasp's data using a dataset of only 9000 RGB images. Specifically, we found that high-quality data is linked to better performance in segmentation, as our SuperCaustics-R model was able to approach the performance of state-of-the-art (cleargrasp) with only 34 epochs of training and 10% of the data, on far more reasonable hardware.</p><p>We believe that SuperCaustics will open exciting avenues for using synthetic data in machine learning, particularly for vision research related to transparent objects.</p><p>Future Work: Potential research possibilities enabled by the availability of readily-expandable, open-source simulations are endless. However, we are working on improving and developing our own deep learning pipeline for other tasks such as surface normal estimation and occlusion-boundary detection. As we have shown, an exciting avenue that can make full use of SuperCaustics is classifying projected local and non-local caustics in images of transparent objects. This can be useful in complex pipelines that are sensitive to caustics and need a masking pass to make sure only transparent objects are processed when estimating surface normals, depth, or segmentation boundaries. Other avenues for future research are performing data ablation studies, where isolated features in the data are changed to determine sensitive points in the performance of a specific neural network, and using the class based features developed in SuperCaustics to help solve problems such as active learning, lifelong learning and class incremental learning <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Finally, we will continue developing tools to expand the capabilities of our SuperCaustics system, as well as improving its stability and compatibility with different target domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Images rendered in SuperCaustics. Changing image features in real time is trivial in SuperCaustics. Note the changes in lighting, sharpness of caustics, realistic backdrop and specular reflections on the transparent objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Showcase of some of the available ground truth images in Su-perCaustics. First row (from left to right): caustics-enabled image, cameraspace normals, world-space normals. Second row (from left to right): caustics segmentation, depth image, segmentation mask. (Image best viewed on screen.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>As illustrated, our system has four components: (1) Scene Generator Module; (2) Prop Manager Module; (3) Ground-truth Core; (4) Data Ablation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>SuperCaustics allows fine controls over fundamental characteristics of every variable in the simulation. e.g., controlling the softness of the light-source and its caustics. Left: Very soft caustics, Middle: Moderately soft, Right: Very Sharp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Image (left), Prediction (middle), Ground-truth (right) First two rows show SuperCaustics Model on SuperCaustics data. Second two rows show SuperCaustics-R on Cleargrasp-Novel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Image (left), Ground-truth (middle), Prediction (right), SuperCaustics Model on Segmenting Caustics cast by Novel transparent objects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MODELS</head><label>I</label><figDesc>TRAINED WITH SUPERCAUSTICS DATA GENERALIZE WELL TO NOVEL SCENARIOS We conducted all our experiments in a Dell Precision 7920R server with two Intel Xeon Silver 4110 CPUs, two GeForce GTX 1080 Ti graphics cards, and 128 GBs of RAM.</figDesc><table><row><cell></cell><cell>Experiment</cell><cell></cell><cell cols="2">Scores Obtained</cell><cell></cell><cell></cell></row><row><cell>Training</cell><cell>Testing</cell><cell>Accuracy</cell><cell cols="2">F1 Score Precision</cell><cell>Recall</cell><cell>IOU</cell></row><row><cell>SuperCaustics</cell><cell>SuperCaustics (Known Objects)</cell><cell>0.9924</cell><cell>0.9361</cell><cell>0.9975</cell><cell cols="2">0.8818 0.8801</cell></row><row><cell>SuperCaustics-R</cell><cell>Cleargrasp-Real (Novel)</cell><cell>0.9560</cell><cell>0.6941</cell><cell>0.5627</cell><cell cols="2">0.9056 0.5316</cell></row><row><cell>ClearGrasp</cell><cell>Cleargrasp-Real (Novel)</cell><cell>NR</cell><cell>NR</cell><cell>NR</cell><cell>NR</cell><cell>0.5800</cell></row><row><cell cols="2">caustics. We chose pixel-wise labeling because it was more</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">suitable given the limited hardware we had available. To</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">demonstrate the flexibility of SuperCaustics, we aimed to test</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">our model on the real test set released by Cleargrasp. To do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">that, we set the angles of the simulated Intel real-sense cameras</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">to resemble similar perspectives and based the parameters of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">our simulation on the characteristics of the Cleargrasp's real-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">world dataset (i.e., occasional sharp caustics from hard lights,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">general soft lighting, presence of a tote box and wooden</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and cloth backdrops among synthetic backgrounds). Below,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">we compare our network's performance to the network pro-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">posed in Cleargrasp [2]. Our results indicate that our network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">achieved comparable results with much fewer training data.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A. Experimental Setup</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1) Hardware:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">CAUSTICS SEGMENTATION</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Experiment</cell><cell></cell><cell cols="2">Scores Obtained</cell></row><row><cell>Training</cell><cell>Testing</cell><cell cols="3">Accuracy F1 Score Precision</cell><cell>Recall</cell><cell>IOU</cell></row><row><cell cols="2">Caustic Segmentation Caustic Segmentation (Known Objects)</cell><cell>0.9981</cell><cell>0.9386</cell><cell>0.9541</cell><cell>0.9237 0.8844</cell></row><row><cell cols="2">Caustic Segmentation Caustic Segmentation (Novel Objects)</cell><cell>0.9958</cell><cell>0.8102</cell><cell>0.8682</cell><cell>0.7594 0.6809</cell></row><row><cell cols="2">from the availability of a better curated, higher quality</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">dataset. That is why open-source, user-friendly simulations</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">like SuperCaustics are important, because for every use-case</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">scenario, a well-made custom dataset is going to achieve</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">comparable results quicker and cheaper than traditional,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>massive datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Blender -a 3D modelling and rendering package, Blender Foundation, Stichting Blender Foundation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">O</forename><surname>Community</surname></persName>
		</author>
		<ptr target="http://www.blender.org" />
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cleargrasp: 3d shape estimation of transparent objects for manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Sajjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Unreal engine</title>
		<ptr target="https://www.unrealengine.com" />
		<imprint/>
	</monogr>
	<note>Epic Games</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">NvRTX branch of Unreal Engine</title>
		<ptr target="https://github.com/NvRTX/UnrealEngine/tree/NvRTXCaustics-4.26" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ai playground: Unreal enginebased data ablation tool for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="518" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using specularities for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacobs</forename><surname>Osadchy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramamoorthi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1512" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An additive latent feature model for transparent object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="558" to="566" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RGB-D local implicit function for depth completion of transparent objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mazhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Eenbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>abs/2104.00622</idno>
		<ptr target="https://arxiv.org/abs/2104.00622" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Friend or foe: exploiting sensor failures for transparent object localization and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Seib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barthen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paulus</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2266255</idno>
		<ptr target="https://doi.org/10.1117/12.2266255" />
	</analytic>
	<monogr>
		<title level="m">2016 International Conference on Robotics and Machine Vision</title>
		<editor>A. V. Bernstein, A. Olaru, and J. Zhou</editor>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10253</biblScope>
			<biblScope unit="page" from="94" to="98" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Soccer on your tabletop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<idno>abs/1806.00890</idno>
		<ptr target="http://arxiv.org/abs/1806.00890" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dex-net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Doan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Ojea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goldberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-driven simulations for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeravasarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rothkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Visvanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Winter Conference on Applications of Computer Vision (WACV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1063" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Framework for generation of synthetic ground truth data for driver assistance applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Haltakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Virtual worlds as proxy for multi-object tracking analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nvidia omniverse platform</title>
		<ptr target="https://developer.nvidia.com/nvidia-omniverse-platform" />
		<imprint/>
		<respStmt>
			<orgName>NVIDIA</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Easy torch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<ptr target="https://github.com/sraashis/easytorch" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dynamic deep networks for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
		<idno type="DOI">https:/www.frontiersin.org/article/10.3389/fcomp.2020.00035%7D,DOI={10.3389/fcomp.2020.00035},ISSN={2624-9898}</idno>
		<idno>ISSN={2624-9898}</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fcomp.2020.00035%7D" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computer Science</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Online</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Optic disc segmentation using disk-centered patch augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Motevali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khanal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Self-net: Lifelong learning via continual self-modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Mandivarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
		<idno type="DOI">https:/www.frontiersin.org/article/10.3389/frai.2020.00019</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/frai.2020.00019" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep active learning via open set recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mandivarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Continual learning with deep artificial neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Mandivarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Estrada</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient document image classification using region-based graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Mandivarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bunch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
