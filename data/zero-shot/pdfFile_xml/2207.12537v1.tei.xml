<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouping</forename><surname>Wang</surname></persName>
							<email>zpwang@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Augmented Cognition Lab (ACLab)</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Ostadabbas</surname></persName>
							<email>ostadabbas@ece.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Augmented Cognition Lab (ACLab)</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Live Stream Temporally Embedded 3D Human Body Pose and Shape Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D Human body pose and shape estimation within a temporal sequence can be quite critical for understanding human behavior. Despite the significant progress in human pose estimation in the recent years, which are often based on single images or videos, human motion estimation on live stream videos is still a rarely-touched area considering its special requirements for real-time output and temporal consistency. To address this problem, we present a temporally embedded 3D human body pose and shape estimation (TePose) method to improve the accuracy and temporal consistency of pose estimation in live stream videos. TePose uses previous predictions as a bridge to feedback the error for better estimation in the current frame and to learn the correspondence between data frames and predictions in the history. A multi-scale spatio-temporal graph convolutional network is presented as the motion discriminator for adversarial training using datasets without any 3D labeling. We propose a sequential data loading strategy to meet the special start-to-end data processing requirement of live stream. We demonstrate the importance of each proposed module with extensive experiments. The results show the effectiveness of TePose on widely-used human pose benchmarks with state-of-the-art performance. 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating human pose and shape is a crucial yet challenging task for many human-centric 3D computer vision applications <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>. With the recent developments in this area, model-based methods are designed to estimate 3D human pose and shape with a few parameters. Despite the improvements of accuracy and robustness for single images and video clips, live stream, as a major realtime processing media, has lagged to be explored for improving the accuracy and temporal consistency of human pose and shape estimation.</p><p>Single image-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b0">1</ref> Code available at https://github.com/ostadabbas/TePose. <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29]</ref> have widely been explored to recover 3D human pose and shape. Some of them have achieved impressive performance on the standard of spatial errors. However, the errors of estimation for each image are highly independent, even being applied to consecutive frames of a video or live stream. Minor spatial fluctuations for estimations in a frame sequence will be conspicuous for human observation as the fluctuations can be in different directions. The lack of temporal information makes single image-based methods much harder to get temporally consistent with stable pose and shape estimation.</p><p>Some recent works have been proposed to extend the single image-based methods to the video cases <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b35">36]</ref>. A sequence of frames is fed into pretrained single imagebased 3D human pose and shape estimation networks to get the static features for each frame <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b24">25]</ref>. Then a temporal encoder is utilized to extract temporal features from these static features. Finally, a mesh parameter regressor outputs the parameters of a template-based model, called skinned multi-person linear (SMPL) <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b41">42]</ref>, for each frame by inputting temporal features. Choi et al. <ref type="bibr" target="#b8">[9]</ref> improved the temporal consistency of 3D human motion by leveraging the temporal information from past and future frames excluding the static feature of the current frame. However, using information of future frames for prediction is not allowed in live stream scenarios. The temporal information of these video-based methods is limited by the length of the input sequence. To extract affluent temporal relevance to assist stable pose and shape recovery, the input video is usually quite long, which impedes their real-time applications.</p><p>We propose that the pose and shape predictions for previous frames can be a bridge to bring history information into account for current frame prediction, called temporally embedded 3D human body pose and shape estimation (TePose). TePose uses both the video frames and their corresponding pose and shape from previous predictions as the input for the current frame prediction. All the static features of input frames except current frame are concatenated with their pose and shape from previous predictions. A temporal encoder and subsequent regressor will predict the pose and shape for current frame using these input in-formation. Given that the static features are extracted by network trained on large datasets, the SMPL parameters inherited in the static features will have sample-based shift. The previous pose and shape predictions will play a role as feedback for the temporal encoder to learn sample-based correspondence between previous frames with the pose and shape predictions for fixing this shift.</p><p>Compared to methods using only video frames as model input, our model needs to use the human body pose and shape predictions from previous frames as input for prediction at the current frame. And the predicted pose and shape as the input are inaccurate especially at the beginning of the network training, which could make the convergences of our network even harder. In order to address this problem, we present a sequential data loading strategy and use mixed sources of pose and shape as the input for prediction. The proportion of input using ground truth pose and shape helps the model to converge at the beginning of training. The pose and shape predictions for previous frames come back as part of the input to make the network gradually fit into the real-world scenarios.</p><p>Kocabas et al. introduced in-the-wild datasets for the training of 3D human pose and shape estimation network by using adversarial training strategy <ref type="bibr" target="#b23">[24]</ref>. The adversarial loss can make up for 3D prediction evaluation whenever there is no 3D labeling for in-the-wild data. Graph neural network has long been proved to have strong ability for classification applications on different modalities of graphs including human pose skeleton. In this work, we use multi-scale spatial-temporal graph convolutional network as the motion discriminator to distinguish real human skeleton sequences against predicted ones. The real skeletons from AMASS dataset <ref type="bibr" target="#b37">[38]</ref> are used for training our motion discriminator.</p><p>In summary, this work introduces temporally embedded 3D human body pose and shape estimation (TePose), with the key contributions as following:</p><p>? presenting a temporally embedded human body pose and shape recovery framework using previous predictions as a bridge to feedback the error for current frame and to learn the correspondence between data frames and predictions in history.</p><p>? developing a sequential data loading strategy with mixed mesh parameter inputs to help the proposed model fitting from ideal cases to real-world cases.</p><p>? introducing a graph convolutional network (GCN) based motion discriminator to provide 3D prediction evaluation in absence of 3D labeling.</p><p>? comparing different temporal embedded architectures for 3D human body pose and shape recovery.</p><p>? and achieving the state-of-the-art performance on widely-used benchmarks for live stream human body pose and shape recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previous methods predict the human body pose and shape from a small set of video or live stream frames, which is the current frame for single image-based methods and several adjacent frames for video clip-based methods. The independence of these inputs make it hard to further improve the temporal consistency and accuracy for the prediction. Using more frames from longer period of time as input is beneficial for temporal information learning, however it is inefficient and time-consuming. Below, we give an overview of the state-of-the-art works in each category.</p><p>3D human body pose and shape from a single image.</p><p>Model-based methods that predict the parameters of predefined human body model <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55]</ref> have been widely used for 3D human body pose and shape estimation. The mapping from 2D monocular image to 3D parameters is an ill-posed problem and lacks of in-the-wild 3D supervision. Bogo et al. <ref type="bibr" target="#b3">[4]</ref> proposed SMPLify by optimizing the SMPL parameters to fit the estimated 2D keypoints, which relieves the need for manual assist <ref type="bibr" target="#b13">[14]</ref>. This kind of selfsupervision strategy has further been extended to fit silhouette <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b26">27]</ref> and 2D features for face, hands, and feet <ref type="bibr" target="#b41">[42]</ref>. Kolotouros et al. introduced a CNN-based regressor into SMPLify framework to form a self-improving system <ref type="bibr" target="#b24">[25]</ref>. Some works presented intermediate representations including keypoint heatmaps, silhouette <ref type="bibr" target="#b42">[43]</ref>, and semantic segmentation <ref type="bibr" target="#b40">[41]</ref> to split the whole mapping into two separate parts, which can be trained with broader supervisions. Kanazawa et al. <ref type="bibr" target="#b19">[20]</ref> used adversarial loss to supervise the human mesh recovery with an end-to-end framework comprised of convolutional encoder and SMPL parameter regressor. HoloPose <ref type="bibr" target="#b14">[15]</ref> presented body-part-based prior and iterative fitting for DensePose <ref type="bibr" target="#b15">[16]</ref> and 2D/3D joints to improve the regression. Georgakis et al. <ref type="bibr" target="#b12">[13]</ref> provided hierarchical kinematic prior to improve the repression performance. Li et al. proposed the HybrIK <ref type="bibr" target="#b28">[29]</ref> by using the hybrid inverse kinematics solution to bridge the gap between body mesh recovery and 3D keypoint estimation so as to get regression improvement.</p><p>There are also some model-free methods directly recovering the human body mesh instead of regressing body model parameters. Kolotouros et al. <ref type="bibr" target="#b25">[26]</ref> directly regressed the vertex coordinates for a mesh template using graph convolutional network (GCN) <ref type="bibr" target="#b22">[23]</ref>. Saito et al. <ref type="bibr" target="#b44">[45]</ref> aligned pixels of 2D images with the global context of their corresponding 3D object to infer both 3D surface and texture from a single image. Moon and Lee <ref type="bibr" target="#b39">[40]</ref> predicted the perlixel (line+pixel) likelihood on 1D heatmaps for each mesh vertex coordinate instead of directly regressing the parameters. Choi et al. <ref type="bibr" target="#b9">[10]</ref> estimated the 3D coordinates of human mesh vertices directly from the 2D human pose using a GCN-based system. Shortcomings-The single-image based methods have achieved impressive per-frame accuracy. However, as they do not utilize temporal information, lacking of temporal consistency restricts their performance in real-scenario video or live stream applications.</p><p>3D human body pose and shape from video. Some recent approaches have extended the SMPLify to exhibit a spatial-temporal manner. Tung et al. <ref type="bibr" target="#b48">[49]</ref> used selfsupervised losses driven by differentiable keypoint, segmentation, and motion reprojection errors, against detected 2D keypoints, 2D segmentation and 2D optical flow, respectively. Kanazawa et al. <ref type="bibr" target="#b20">[21]</ref> used ResNet-based encoder <ref type="bibr" target="#b17">[18]</ref> to learn human motion dynamics by supervising the dynamics prediction over time. Arnab et al. <ref type="bibr" target="#b2">[3]</ref> exploited the temporal context with bundle adjustment using the internet videos annotated by modified SMPLify. Doersch and Zisserman <ref type="bibr" target="#b10">[11]</ref> proved that synthetic data with cues about the person's motion, notably as optical flow and the motion of 2D keypoints, can help improving the performance for human mesh recovery. Sun et al. <ref type="bibr" target="#b46">[47]</ref> proposed a skeletondisentangling based framework to reduce the complexity and decouple the skeleton, a self-attention based temporal convolution network to efficiently exploit the short and long-term temporal cues, and a shuffled frame order recovery strategy to promote the learning of motion dynamics. Luo et al. <ref type="bibr" target="#b35">[36]</ref> proposed a motion estimation method via variational autoencoder (MEVA) using auto-encoder-based motion compression and a residual representation learned through motion refinement. Kocabs et al. presented the VIBE <ref type="bibr" target="#b23">[24]</ref> approach by introducing an adversarial learning framework with self-attention mechanism that leverages AMASS dataset <ref type="bibr" target="#b37">[38]</ref> to supervise the temporal human body pose and shape regression networks. Choi et al. proposed a temporally consistent mesh recovery (TCMR) <ref type="bibr" target="#b8">[9]</ref> system to recover the mesh for the middle frame of a video clip by fusing different kinds of temporal encoders and removing the temporal-inconsistent effect getting from residual connection. Lee et al. <ref type="bibr" target="#b27">[28]</ref> proposed to employ a view-invariant probabilistic encoder that can present 2D pose features as distribution of uncertainty in 2D space and a decoder that divides the body into five different local regions to estimate the 3D motion dynamics of each region. Shortcomings-However, both TCMR and this method need future frames as input for current frame estimation. In addition, most of video-based systems need long sequence of frames as input to get enough temporal information.</p><p>Graph neural network (GNN) for human pose skeleton. In the last decade, GNNs have been developed with many different forms to extract features from data in a graph <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b11">12]</ref>. GNNs can be roughly divided into spectral GNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref> or spatial GNNs <ref type="bibr" target="#b16">[17]</ref>. Graph convolution network (GCN) <ref type="bibr" target="#b22">[23]</ref> was initially proposed as a firstorder approximation for localized spectral convolutions. Its simplicity of structure and calculation makes it popular for graph-involved applications <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b55">56]</ref>. Multi-scale spatial GNN <ref type="bibr" target="#b31">[32]</ref> has been proposed to capture non-local features. Spatial-temporal GCN <ref type="bibr" target="#b53">[54]</ref> uses human skeleton sequences as input for action classification task, which has been improved by Liu et al. <ref type="bibr" target="#b32">[33]</ref> after extending the Multi-scale spatial-temporal graph convolution network (MS-GCN) <ref type="bibr" target="#b6">[7]</ref> into its disentangled version known as MS-G3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Introducing TePose</head><p>The overall architecture of temporally embedded 3D human body pose and shape estimation (TePose) is shown in <ref type="figure">Figure 1</ref>. It mainly comprises two parts, one a temporally embedded encoder and the other one a series of GCN-based motion discriminators. When a live stream or video sequence is given to estimate the human body pose and shape in each of its frame, TePose realizes this goal following a frame-by-frame manner, as described below.</p><p>Problem Formulation-We define the just-received frame for live stream or the frame in estimation for video as current frame I t . The T frames before current frames, {I t?T , ..., I t?1 }, and their corresponding predicted SMPL parameters, {? t?T , ..., ? t?1 }, are used to help the human body pose and shape estimation for the current frame. At the start of each live stream or video, we utilize the VIBE <ref type="bibr" target="#b23">[24]</ref>, which is a video-based inference model for body pose and shape estimation, to provide the SMPL parameters for the first T frames. Now, we introduce the detailed description of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Temporally Embedded Encoder</head><p>Given a sequence of RGB frames {I t?T , ..., I t?1 , I t } and previous SMPL parameter predictions {? t?T , ..., ? t?1 }, a pretrained ResNet is used to extract the static image features {f I t?T , ..., f I t } with dimension R 2048 for each frame <ref type="bibr" target="#b24">[25]</ref>. The previously predicted SMPL parameters {? ? } for each frame, which include camera parameters ? ? ? R 3 , pose parameters ? ? ? R 72 and shape parameters ? ? ? R 10 , are recognized as another input to concatenate with the image features as:</p><formula xml:id="formula_0">f i = Cat(f I i , f? i ), i = t ? T, ..., t ? 1.<label>(1)</label></formula><p>The static image features of current frame are concatenated with zeros vector to match the dimension with other frames. Then the concatenated features {f t?T , ..., f t } are sent into two gated recurrent units (GRU) <ref type="bibr" target="#b7">[8]</ref> in the time order to compute hidden states {g t?T , ..., g t }. One of the <ref type="figure">Figure 1</ref>: The overall architecture of our temporally embedded 3D human body pose and shape estimation (TePose). The network uses current frame, T frames of previous frames, and previously predicted SMPL parameters as input to predict the human body pose and shape in the current frame. The temporal sequence of them is fed into two gated recurrent units (GRU) in time order. Graph convolution network (GCN) based motion discriminator is incorporated to include adversarial loss.</p><p>GRU is uni-directional and another one is bi-directional. The initial hidden states of these GRUs are set to zeros. The output hidden states of GRUs for current frame g t are inputted into a SMPL parameter regressor <ref type="bibr" target="#b24">[25]</ref>. Following previous work <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>, The regressor is initialized with the mean pose and shape. Finally, the regressor outputs the predictions for the SMPL parameters? i = {? i ,? i ,? i }, 2D joints, 3D joints and the SMPL mesh coordinates M(? i ,? i ) ? R 6890?3 . A human body pose and shape model with neutral gender is used as in previous works. During the training process, we get two different predictions for features extracted from the two GRUs. Both of the predictions are used for calculating loss. However, we average the output features of two GRUs during evaluation so that we can get identical prediction for each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">GCN-based Motion Discriminator</head><p>One of the major problems faced by human body pose and shape estimation is the lack of in-the-wild datasets with accurate 3D annotations. Indoor datasets with 3D annotations have obvious differences compared with in-the-wild ones for data distributions including backgrounds and variety of subjects. The pseudo labels for some in-the-wild datasets are not reliable. To relieve this problem, we introduce adversarial loss to constrain the 3D human pose estimation. A multi-scale spatial-temporal graph convolu-tional network is used to distinguish the predicted 3D human skeleton sequences with the ones from human motion capture datasets. Our temporally embedded encoder is trained to generate 3D skeletons as real as possible to cheat the motion discriminator in an adversarial way.</p><p>The network structure of GCN-based motion discriminator is visualized in <ref type="figure" target="#fig_0">Figure 2</ref>. One of the outputs of the temporally embedded encoder introduced in last section is the predicted human skeleton sequence {? t?T , ...,? t }. We unify the predicted skeleton sequence with real ones from motion capture datasets as X ? R (T +1)?N ?C , where N is the number of joints and C is the coordinate dimension for each joint. The skeleton sequences are firstly processed by three consecutive GCN blocks. Then the global average pooling layer and fully connected layer are used to output the final decision if the input skeleton sequence is real or generated. For each GCN block, graph features are processed by multi-scale graph convolutional network (MS-GCN) <ref type="bibr" target="#b6">[7]</ref> and multi-scale graph 3D convolutional network (MS-G3D) <ref type="bibr" target="#b32">[33]</ref>. Then their outputs are added with residual connection. Finally, the graph features are processed with activation function to get the output for next step.</p><p>The MS-GCN is used to apply graph convolution on the human skeleton for each frame, namely spatial dimension. It can be represented as:</p><formula xml:id="formula_1">X t = ? K k=0D ? 1 2 (k)? (k)D ? 1 2 (k) X t W (k) .<label>(2)</label></formula><p>The ? is the activation function, and the? (k) is the kadjacency matrix for the graph of human skeleton,</p><formula xml:id="formula_2">? (k) m,n = ? ? ? ? ? 1 if d(X (m) t , X (n) t ) = k, 1 if m = n, 0 otherwise,<label>(3)</label></formula><p>where the d(X  <ref type="bibr" target="#b22">[23]</ref>. And the W (k) ? R C?C denotes the learnable weight matrix for the graph convolution.</p><p>The MS-G3D is used to apply graph convolution on the human skeletons in a time range ? , namely both on spatial and temporal dimensions. It can be represented as:</p><formula xml:id="formula_3">X t?? /2 = ? K k=0 D (? ) (k) ? 1 2?(? ) (k) D (? ) (k) ? 1 2 X t?? /2 W (k) .<label>(4)</label></formula><p>Compared with Eq. 2, the input skeleton data in Eq. 4 is extended to be spatial-temporal X t?? /2 ? R ? ?N ?C . The adjacency matrix for MS-G3D is also extend to be:</p><formula xml:id="formula_4">A (? ) (k) = ? ? ?? (k) . . .? (k) . . . . . . . . . A (k) . . .? (k) ? ? ? ? ?? .<label>(5)</label></formula><p>D (? ) (k) is the diagonal degree matrix for? (? ) (k) . By using zero padding at temporal dimension, the output skeleton data of each MS-G3D share the same spatial and temporal dimensions with the input X ? R (T +1)?N ?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Functions</head><p>The network is trained on both the datasets with 3D annotations and the ones without 3D annotations. Motivated by previous works <ref type="bibr" target="#b23">[24]</ref>, the total loss of proposed model is:</p><formula xml:id="formula_5">L = L 2D + 3D L 3D + ? L ? + (1 ? ? )L adv ,<label>(6)</label></formula><p>where { 3D , ? } are indicators to show if 3D keypoint labels and SMPL parameter labels exist respectively. A sample with 3D keypoint label will have loss function with 3D = 1, otherwise 0. And a sample with SMPL parameter label will have loss function with ? = 1, otherwise 0. We use the L2 losses for 2D, 3D and SMPL losses:</p><formula xml:id="formula_6">L 3D = X t ? X t 2 , L 2D = x t ? x t 2 , L ? = ? t ? ? t 2 + ? t ? ? t 2 ,<label>(7)</label></formula><p>where x t is the ground truth for 2D keypoints, andx t = s?RX t + ? is the 2D keypoint predictions deriving from 3D reprojection. [s, ?, R] are scale, transition and global rotation matrix from camera parameters. ? represents the orthographic projection. The adversarial loss can be represented as</p><formula xml:id="formula_7">L adv = D M X ? 1 2 ,<label>(8)</label></formula><p>where D M denotes the proposed GCN-based motion discriminator. And the cross-entropy loss is used for training this motion discriminator:</p><formula xml:id="formula_8">LD M = EX (DM (X) ? 1) 2 + EX DM (X) 2 .<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Sequential Data Loading Strategy</head><p>In our approach, we have to use both the previous image frames and their corresponding SMPL parameter predictions as inputs to train the network for the pose and shape estimation of current frame. The sequential training moving from the start to the end of each video is needed in this scenario as only the predictions for previous frames have got then the subsequent frames can be processed. Different from the common data loading method for machine learning, video sequences in our training procedure are no longer selected in a totally random way. We use the current training iteration j to get the current frame position t = mod(j, H), where H is defined to be the maximum video length we select to reach. The sequential data loading strategy of proposed method is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The long videos are kept selecting with a random way, and the samples are in the current frame positions of each selected long video. If there are selected long video shorter than the H, we do not include these ones into the batch for training and squeeze the batch size. In order to better control the convergence of our network, we use a mixed way to select the source of previous human body pose and shape. There is a probability of ? to use previously predicted SMPL parameters as in Eq. 1 and a probability of 1 ? ? to use ground truth labels for network input as</p><formula xml:id="formula_9">f i = Cat(f I i , f ? i ), i = t ? T, ..., t ? 1.<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We follow some of the settings as the previous works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b8">9]</ref>. The frame rate of video sequences are 25-30 frames per second. The backbone and regressor in the model is initialized with pretrained SPIN model <ref type="bibr" target="#b24">[25]</ref>. All the input human frames are cropped using ground truth bounding boxes and resized to 224?224 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref>. Occlusion augmentation is applied to the cropped frames for better generalization of the model <ref type="bibr" target="#b45">[46]</ref>. 2-layer GRUs <ref type="bibr" target="#b7">[8]</ref> with hidden size of 1024 are used in our temporally embedded encoder. The regressor has 2 fully-connected layers with 1024 neurons for each layer, and a final layer to output SMPL parameters <ref type="bibr" target="#b24">[25]</ref>. To reduce the time for waiting buffer frames and computational complexity for prediction, we choose a small temporal length T + 1 = 6. The number of kernel scales for all the MS-GCN and MS-G3D units in the discriminator are set to be K = 13 and K = 6 respectively. And the temporal window size ? is set to be 3 for all the MS-G3D units. The probability of using previously predicted SMPL parameters as input is set to ? = 0.9. The network is trained for 80 epochs with one NVIDIA Tesla V100 SXM2 GPU. During each epoch, 1/8 of videos from 3D human body pose estimation datasets are used for training. Maximum video length H is set to 505. The learning rates for predictor and discriminator are initialized to 5 ?5 and 1 ?4 , and reduced by a factor of 10 when the 3D pose accuracy does not improve after every 8 epochs. Adam optimizer <ref type="bibr" target="#b21">[22]</ref> is used for updating the network weights with a mini-batch size of 32.</p><p>The number of learnable parameters for the 2-layer GRUs in our system is O(18(len(g t ) 2 + len(g t ) ? len(f t ) + len(g t ))) = O(18?(2048 2 +2048?(2048+75)+2048)). The number of learnable parameters for each MS-GCN is O(K 1 CC ), where K 1 = 13 is the number of kernel scales. C and C are the numbers of input and output channels. The number of learnable parameters for each MS-G3D is O(K 2 CC ) while K 2 = 6. The total number of learnable parameters for proposed discriminator is</p><formula xml:id="formula_10">O((K 1 + K 2 )(CC 1 + C 1 C 2 + C 2 C 3 ) = O((13 + 6)(3 ? 64 + 64 ? 128 + 128 ? 256))).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>We use both 3D and 2D human body pose estimation datasets for our model training. 3DPW <ref type="bibr" target="#b50">[51]</ref> is an in-thewild dataset with 3D mesh labels, and Human3.6M <ref type="bibr" target="#b18">[19]</ref> also has SMPL parameter labels obtained from Mosh <ref type="bibr" target="#b33">[34]</ref>. MPI-INF-3DHP <ref type="bibr" target="#b38">[39]</ref> is 3D dataset consisting of both indoor and complex outdoor scenes. InstaVariety <ref type="bibr" target="#b20">[21]</ref> is a dataset with pseudo 2D keypoints. PoseTrack <ref type="bibr" target="#b0">[1]</ref> is the only ground-truth 2D video datasets we use. 3D datasets and 2D datasets are mixed together for supervision. The ratio between the numbers of 3D and 2D data samples is set to 4 : 6. Pose sequences from AMASS <ref type="bibr" target="#b37">[38]</ref> are used as real samples for adversarial training. The test sets of 3DPW, Hu-man3.6M and MPI-INF-3DHP are also used for evaluation.</p><p>For performance evaluation, we use both per-framebased metrics, including mean per joint position error (MPJPE), Procrustes-aligned MPJPE (PA-MPJPE) and mean per vertex position error (MPVPE), and temporalbased metrics, acceleration error (ACCEL) <ref type="bibr" target="#b20">[21]</ref>, to calculate the prediction errors. These position errors are measured in millimeter between the estimated and groundtruth 3D coordinates after aligning the root joint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State-of-the-art</head><p>Based on the work of Luo et al. <ref type="bibr" target="#b35">[36]</ref>, we compare our proposed Tepose approach with state-of-the-art models in two different training settings. In <ref type="table" target="#tab_0">Table 1</ref>  [9] predicts the body pose and shape of middle frame from 16-frame input, which impede them from real-time applications considering they need future frames as input. In contrast, our approach can infer without using future frames, and only 6 frames are needed as model input. Comparing with VIBE <ref type="bibr" target="#b23">[24]</ref>, which can also do real-time prediction, proposed TePose is better on both per-frame-based metrics and temporal-based metrics under the settings of <ref type="table" target="#tab_0">Table 1</ref>.</p><p>In  <ref type="bibr" target="#b33">[34]</ref> are applied with SMPL loss. We compare our TePose with both frame-based models and video-based models. As shown in the table, our method provides competitive results comparing with previous methods. The acceleration error is high for frame-based methods, which is reasonable as they do not use temporal information. For all the three test sets, our model has better performance than other methods on PA-MPJPE and Accel. For the other metrics, our model also provides close performance compared with state-of-the-art. And for all the other video-based models, they need much longer videos as input, which means more waiting time and more complex computation. The quantitative results show that proposed system outperforms the previous state-of-the-art methods in both per-frame prediction accuracy and temporal consistency by introducing temporally embedded encoder and GCN-based adversarial training.</p><p>To further qualitatively demonstrate the improvement of proposed method, we compare the pose and shape estimation of TePose with VIBE <ref type="bibr" target="#b23">[24]</ref> and TCMR <ref type="bibr" target="#b8">[9]</ref> in <ref type="figure" target="#fig_3">Figure 4</ref>. Three in-the-wild samples from 3DPW test set and three from MPI-INF-3DHP test set are selected to show the difference. The results show that the inferences of proposed TePose are more accurate and stable than VIBE and TCMR. Both quantitative and qualitative results demonstrate the effectiveness of proposed method.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we show how each module of our proposed system contributes to the improvement of human body pose and shape estimation performance. In <ref type="table" target="#tab_4">Table 3</ref>, we show the performance of 4 different module combinations. These models are all trained on MPI-INF-3DHP, Hu-man3.6M and InstaVariety while all of them are evaluated on the test set of Human3.6M, which has the same setting with the last three columns in <ref type="table" target="#tab_1">Table 2</ref>. The first row in <ref type="table" target="#tab_4">Table 3</ref> removes all the proposed modules. It uses single uni-directional GRU as temporal encoder. Compared with VIBE <ref type="bibr" target="#b23">[24]</ref>, the number of frames for model input is 6 instead of 16 in VIBE, and the adversarial learning of VIBE is also removed. It is reasonable to have worse performance than both VIBE and proposed TePose. The second row utilizes two GRUs as temporal encoder. One GRU is unidirectional and another one is bi-directional. It improves prediction performance compared with single-GRU model. When we add the temporally embedded encoder in the third row, SMPL parameter predictions for previous frames are needed as input for current-frame pose and shape estimation, which means the sequential data loading strategy also has to be added to make the training feasible. By adding the temporally embedded encoder and sequential data loading strategy in the third row of <ref type="table" target="#tab_4">Table 3</ref>, we demonstrate the effectiveness of these two proposed modules. In the last row of this table, we further add the adversarial learning module using proposed GCN-based motion discriminator to make it become the complete TePose. The results also demonstrate it can improve the accuracy of pose prediction. In conclusion, the results from different combinations of proposed modules demonstrate the effectiveness for each of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present TePose to estimate 3D human body pose and shape for live stream and video data. TePose utilizes mesh parameter predictions from previous frames as bridge to bring temporal information into prediction network for improving the accuracy and temporal consistency of overall estimation. Graph convolutional network is used to distinguish estimated pose sequences from real ones in dataset, so as to introduce extra adversarial loss for data samples without 3D supervision. A sequential data loading strategy is presented to make the convergence of the model using mixture input from previous mesh predictions and groundtruth mesh parameters. Compared to the previous methods, our approach provides competitive performance on accuracy and temporal consistency. By using temporal embedded input, TePose leads to better human pose and body estimation with less previous frames as input. It will be helpful especially for live stream scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Motion discriminator architecture. It comprises three graph convolution network (GCN) blocks followed with global average pooling and one fully connected layer. Each GCN block adds the outputs of MS-G3D and MS-GCN units with residual connection. The number of output channels C for the three GCN blocks are 64, 128 and 256 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>t</head><label></label><figDesc>, and the k denotes the kernel scale on the spatial graph.D (k) is the diagonal degree matrix of? (k) to get normalized k-adjacency matrix D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The sequential data loading strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative comparisons between VIBE<ref type="bibr" target="#b23">[24]</ref> (top), TCMR<ref type="bibr" target="#b8">[9]</ref> (middle) and TePose (bottom) on in-the-wild sequences. The three frames on the left are from test set of 3DPW<ref type="bibr" target="#b50">[51]</ref>, and the three frames on the right are from test set of MPI-INF-3DHP<ref type="bibr" target="#b38">[39]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>, all the models are trained with multiple datasets including 3DPW train set, while tested on the test sets of 3DPW, MPI-INF-3DHP and Human3.6M respectively. Specifically, for our model evaluating on 3DPW test set, we use train sets of 3DPW, MPI-INF-3DHP, Human3.6M and InstaVariety for training. only 3DPW with SMPL parameter labels is applied with SMPL loss L ? . The samples from MPI-INF-3DHP and Human3.6M without true mesh labels use adversarial loss L adv instead. For our model evaluating on MPI-INF-3DHP and Human3.6M test sets, it is trained with 3DPW, MPI-INF-3DHP, Human3.6M, InstaVariety and PoseTrack train sets. And the SMPL parameters of Human3.6M obtained from Mosh<ref type="bibr" target="#b33">[34]</ref> are added for supervision through SMPL loss L ? . As shown in the table, proposed TePose has best performance on all the per-frame-based metrics. MEVA<ref type="bibr" target="#b35">[36]</ref> needs at least 90 frames as model input, and TCMR MPJPE ? MPVPE ? Accel ? PA-MPJPE ? MPJPE ? Accel ? PA-MPJPE ? MPJPE ? Accel ? T + 1 Evaluation of state-of-the-art methods on 3DPW, MPI-INF-3DHP, and Human3.6M test datasets. All methods use 3DPW train set for training. " * " denotes the methods cannot predict real-time. " ? " shows unavailable results. T + 1 in the last column represents the number of input frames for each method. MPJPE ? MPVPE ? Accel ? PA-MPJPE ? MPJPE ? Accel ? PA-MPJPE ? MPJPE ? Accel ?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3DPW</cell><cell></cell><cell></cell><cell cols="2">MPI-INF-3DHP</cell><cell></cell><cell>Human3.6M</cell></row><row><cell cols="4">Methods PA-MPJPE ? VIBE [24] 57.6</cell><cell>91.9</cell><cell>-</cell><cell>25.4</cell><cell>68.9</cell><cell>103.9</cell><cell>27.3</cell><cell>53.3</cell><cell>78.0</cell><cell>27.3</cell><cell>16</cell></row><row><cell cols="2">MEVA [36]  *</cell><cell>54.7</cell><cell></cell><cell>86.9</cell><cell>-</cell><cell>11.6</cell><cell>65.4</cell><cell>96.4</cell><cell>11.1</cell><cell>53.2</cell><cell>76.0</cell><cell>15.3</cell><cell>90</cell></row><row><cell cols="2">TCMR [9]  *</cell><cell>52.7</cell><cell></cell><cell>86.5</cell><cell>102.9</cell><cell>7.1</cell><cell>63.5</cell><cell>97.3</cell><cell>8.5</cell><cell>52.0</cell><cell>73.6</cell><cell>3.9</cell><cell>16</cell></row><row><cell cols="2">TePose (Ours)</cell><cell>52.3</cell><cell></cell><cell>84.6</cell><cell>100.3</cell><cell>11.4</cell><cell>63.1</cell><cell>96.2</cell><cell>16.7</cell><cell>47.1</cell><cell>68.6</cell><cell>12.1</cell><cell>6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3DPW</cell><cell></cell><cell></cell><cell cols="2">MPI-INF-3DHP</cell><cell></cell><cell cols="2">Human3.6M</cell></row><row><cell cols="5">Methods HMR [20] GraphCMR [41] SPIN [25] I2L-MeshNet [40] PA-MPJPE ? Frame-based 76.7 70.2 59.2 57.7 Pose2Mesh [10] 58.3</cell><cell>130.0 -96.9 93.2 88.9</cell><cell>--116.4 110.1 106.3</cell><cell>37.4 -29.8 30.9 22.6</cell><cell>89.9 -67.5 --</cell><cell>124.2 -105.2 --</cell><cell>-----</cell><cell>56.8 50.1 41.1 41.1 46.3</cell><cell>88.0 --55.7 64.9</cell><cell>--18.3 13.4 23.9</cell></row><row><cell></cell><cell>HMMR [21]</cell><cell></cell><cell>72.6</cell><cell></cell><cell>116.5</cell><cell>139.3</cell><cell>15.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>56.9</cell><cell>-</cell><cell>-</cell></row><row><cell>Temporal</cell><cell cols="2">Doersch et al. [11] Sun et al. [47] VIBE [24] TePose (Ours)</cell><cell>74.7 69.5 56.5 56.1</cell><cell></cell><cell>--93.5 93.9</cell><cell>--113.4 115.9</cell><cell>--27.1 11.7</cell><cell>--63.4 62.9</cell><cell>--97.7 99.5</cell><cell>--29.0 17.2</cell><cell>-42.4 41.5 41.2</cell><cell>-59.1 65.9 61.6</cell><cell>--18.3 12.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Evaluation of state-of-the-art methods on 3DPW, MPI-INF-3DHP, and Human3.6M test datasets. All methods do not use 3DPW<ref type="bibr" target="#b50">[51]</ref> train set for training. " ? " shows unavailable results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>, all the methods do not use 3DPW for training. We use different settings for our model training. Specifically, we use train sets of MPI-INF-3DHP, Human3.6M, In-staVariety and PoseTrack for training models evaluating on 3DPW and MPI-INF-3DHP test sets. The model evaluating on Human3.6M test set is trained with MPI-INF-3DHP, Human3.6M and InstaVariety train sets. Both 3DPW with true SMPL parameter labels and Human3.6M with pseudo SMPL parameter labels from Mosh</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiments with different combinations of proposed modules. Models are trained on MPI-INF-3DHP, Human3.6M and In-staVariety while all of them are evaluated on the test set of Human3.6M.</figDesc><table /><note>'Embedded' represents the temporally embedded encoder. 'Seq' denotes the sequential data loading strategy. 'Adv' denotes the adversarial learning module using GCN-based motion discriminator.</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scape: shape completion and animation of people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jim</forename><surname>Rodgers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH 2005 Papers</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="408" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Integration of motion control techniques for virtual human and avatar real-time animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Boulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>B?cheiraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Emering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM symposium on Virtual reality software and technology</title>
		<meeting>the ACM symposium on Virtual reality software and technology</meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="111" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Le-Cun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chaolong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cui</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Wenming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xu Chunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond static features for temporally consistent 3d human pose and shape from a video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1964" to="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pose2mesh: Graph convolutional network for 3d human pose and mesh recovery from a 2d human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsuk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="769" to="787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">international conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical kinematic human mesh recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srikrishna</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Ko?eck?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="768" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Alexandru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Holopose: Holistic 3d human reconstruction in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Riza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Guler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10884" to="10894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>R?za Alp G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7297" to="7306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7122" to="7131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panna</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5614" to="5623" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammed</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Convolutional mesh regression for single-image human shape reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4501" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federica</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6050" to="6059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Uncertainty-aware human mesh recovery from video by learning part-based 3d dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Gun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong-Whan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12375" to="12384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Hybrik: A hybrid analytical-neural inverse kinematics solution for 3d human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3383" to="3393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10166" to="10175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pastanet: Toward human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="382" to="391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanczosnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01484</idno>
		<title level="m">Multi-scale deep graph convolutional networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="143" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mosh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">3d human motion estimation via motion compression and refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Golestaneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning features combination for human action recognition from skeleton sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Diogo Carbonera Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tabia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="13" to="20" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Amass: Archive of motion capture as surface shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nikolaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Troje</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5442" to="5451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">I2l-meshnet: Imageto-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyeongsik</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kyoung Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="752" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Neural body fitting: Unifying deep learning and model based human pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Expressive body capture: 3d hands, face, and body from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10975" to="10985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to estimate 3d human pose and shape from a single color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="459" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">3d skeleton-based human action classification: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliana</forename><forename type="middle">Lo</forename><surname>Presti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><forename type="middle">La</forename><surname>Cascia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="130" to="147" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunsuke</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shigeo</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2304" to="2314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How robust is 3d human pose estimation to occlusion?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Istv?n</forename><surname>S?r?ndi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timm</forename><surname>Linder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><forename type="middle">O</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.09316</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yili</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Indirect deep structured learning for 3d human body shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun Kai Vince</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignas</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Self-supervised learning of motion capture. Advances in Neural Information Processing Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Wei</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">S3: Neural shape, skeleton, and skinning fields for 3d human modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivabalan</forename><surname>Manivasagam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13284" to="13293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
