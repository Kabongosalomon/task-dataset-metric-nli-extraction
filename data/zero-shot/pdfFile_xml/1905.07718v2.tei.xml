<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Geometric Pose Affordance: Monocular 3D Human Pose Estimation with Scene Constraints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92617</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92617</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaurya</forename><surname>Rathore</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92617</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeyun</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92617</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless</forename><surname>Fowlkes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>92617</postCode>
									<settlement>Irvine</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Geometric Pose Affordance: Monocular 3D Human Pose Estimation with Scene Constraints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Accurate estimation of 3D human pose from a single image remains a challenging task despite many recent advances. In this paper, we explore the hypothesis that strong prior information about scene geometry can be used to improve pose estimation accuracy. To tackle this question empirically, we have assembled a novel Geometric Pose Affordance dataset, consisting of multi-view imagery of people interacting with a variety of rich 3D environments. We utilized a commercial motion capture system to collect gold-standard estimates of pose and construct accurate geometric 3D models of the scene geometry. To inject prior knowledge of scene constraints into existing frameworks for pose estimation from images, we introduce a view-based representation of scene geometry, a multi-layer depth map, which employs multi-hit ray tracing to concisely encode multiple surface entry and exit points along each camera view ray direction. We propose two different mechanisms for integrating multi-layer depth information into pose estimation: input as encoded ray features used in lifting 2D pose to full 3D, and secondly as a differentiable loss that encourages learned models to favor geometrically consistent pose estimates. We show experimentally that these techniques can improve the accuracy of 3D pose estimates, particularly in the presence of occlusion and complex scene geometry.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Accurate estimation of human pose in 3D from image data would enable a wide range of interesting applications in emerging fields such as virtual and augmented reality, humanoid robotics, workplace safety, and monitoring mobility and fall prevention in aging populations. Interestingly, many such applications are set in relatively controlled environments (e.g., the home) where large parts of the scene geometry are relatively static (e.g., walls, doors, heavy furniture). We are interested in the following question, "Can strong knowledge of scene geometry improve our estimates of human pose from images?".</p><p>Consider the images in <ref type="figure">Fig. 1 a.</ref> Intuitively, if we know the 3D locations of surfaces in the scene, this should constrain our estimates of pose. Hands and feet should not interpenetrate scene surfaces, and if we see someone sitting on a surface of known height we should have a good estimate of where their hips are even if large parts of the body are occluded. This general notion of scene affordance 1 has been explored as a tool for understanding functional and geometric properties of a scene <ref type="bibr" target="#b12">(Gupta et al., 2011;</ref><ref type="bibr" target="#b10">Fouhey et al., 2012;</ref><ref type="bibr" target="#b43">Wang et al., 2017;</ref><ref type="bibr" target="#b19">Li et al., 2019)</ref>. However, the focus of such work has largely been on using estimated human pose to infer scene geometry and function.</p><p>Surprisingly, there has been little demonstration of how scene 1 "The meaning or value of a thing consists of what it affords." -JJ Gibson <ref type="bibr">(1979)</ref>   We simultaneously captured 3 RGBD and 2 RGB video streams and ground-truth 3D pose from a VICON marker-based mocap system. Cameras are calibrated with respect to a 3D mesh model of scene geometry. knowledge can constrain pose estimation. Traditional 3D pose estimation models have explored kinematic and dynamic constraints which are scene agnostic and have been tested on datasets of people freely performing actions in large empty spaces. We posit one reason that scene constraints have not been utilized is lack of large-scale datasets of annotated 3D pose in rich environments. Methods have been developed on datasets like Human3.6M <ref type="bibr" target="#b17">(Ionescu et al., 2014)</ref> and MPI-INF-3DHP <ref type="bibr" target="#b23">(Mehta et al., 2017)</ref>, which lack diverse scene geometry (at most one chair or sofa) and are generally free from scene occlusion. Recent efforts have allowed for more precise 3D pose capture for in-the-wild environments <ref type="bibr" target="#b20">(von Marcard et al., 2018)</ref> but lack ground-truth scene geometry, or provide scene geometry but lack extensive ground-truth pose estimates <ref type="bibr" target="#b13">(Hassan et al., 2019)</ref>.</p><p>Instead of tackling human pose estimation in isolation, we argue that systems should take into account available information about constraints imposed by complex environments. A complete solution must ultimately tackle two problems: (i) estimating the geometry and free space of the environment (even when much of that free space is occluded from view), (ii) integrating this information into pose estimation process. Tools for building 3D models of static environments are well developed and estimation of novel scene geometry from single-view imagery has also shown rapid progress. Thus, we focus on the second aspect under the assumption that high-quality geometric information is available as an input to the pose estimation pipeline.</p><p>The question of how to represent geometry and incorporate the constraints it imposes with current learning-based approaches to modeling human pose is an open problem.</p><p>There are several candidates for representing scene geometry: voxel representations of occupancy <ref type="bibr" target="#b27">(Pavlakos et al., 2017)</ref> are straightforward but demand significant memory and computation to achieve reasonable resolution; Point cloud <ref type="bibr" target="#b3">(Chan et al., 2013)</ref> representations provide more compact representations of surfaces by sampling but lack topological information about which locations in a scene constitute free space. Instead, we propose to utilize multi-layer depth maps <ref type="bibr" target="#b33">(Shin et al., 2019)</ref> which provide a compact and nearly complete representation of 3 scene geometry that can be readily queried to verify pose-scene consistency.</p><p>We develop and evaluate several approaches to utilize information contained in the multi-layer depth map representation. Since multi-layer depth is a view-centered representation of geometry, it can be readily incorporated as an additional input feature channel. We leverage estimates of 2D pose either as a heatmap or regressed coordinate and query the multilayer depth map directly to extract features encoding local constraints on the z-coordinates of joints that can be used to predict geometry-aware 3D joint locations. Additionally, we introduce a differentiable loss that encourages a model trained with such features to respect hard constraints imposed by scene geometry. We perform an extensive evaluation of our multi-layer depth map models on a range of scenes of varying complexity and occlusion. We provide both qualitative and quantitative evaluation on real data demonstrating that these mechanisms for incorporating geometric constraints improves upon sceneagnostic state-of-the-art methods for 3D pose estimation.</p><p>To summarize our main contributions: 1. We collect and curate a unique, large-scale 3D human pose estimation dataset with rich ground-truth scene geometry and a wide variety of pose-scene interactions (see e.g. <ref type="figure">Fig. 1</ref>) 2. We propose a novel representation of scene geometry constraints: multi-layer depth map, and explore multiple ways to incorporate geometric constraints into contemporary learning-based methods for predicting 3D human pose. 3. We experimentally demonstrate the effectiveness of integrating geometric constraints relative to two state-of-the-art scene-agnostic pose estimation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion capture for ground-truth 3D pose. The work of <ref type="bibr" target="#b34">Sigal et al. (2010)</ref> introduced one of the first large-scale 3D human pose estimation datasets with synchronized images and groundtruth 3D keypoint locations. <ref type="bibr" target="#b17">Ionescu et al. (2014)</ref> scaled their dataset up to 3.6 million images covering a range of subjects and actions along with depth images and 3D body scans of the human subjects. To overcome the limitations of marker-based data collection such as constrained clothing and capture environment, several marker-less approaches have also been used. <ref type="bibr" target="#b18">Joo et al. (2016)</ref> utilize an indoor "panoptic studio" to capture poses from 10 calibrated RGBD cameras. <ref type="bibr" target="#b23">Mehta et al. (2017)</ref> utilized multi-view marker-less capture to collect pose data for subjects wearing a variety of clothing against both indoor and outdoor backgrounds. <ref type="bibr" target="#b30">Rhodin et al. (2018b)</ref> utilized calibrated PTZ cameras and human annotators to triangulate joint locations skiers over a large area of a ski-slope. <ref type="bibr" target="#b50">Zhou et al. (2018a)</ref> also explores motion capture both indoor and outdoor using a Drone. Synchronized inertial measurement sensor (IMU) data can be used to further enhance marker-less capture. <ref type="bibr" target="#b38">Trumble et al. (2017)</ref> develop an approach to fusing inertial measurement sensors with multi-view recording in a studio environment. <ref type="bibr" target="#b23">Mehta et al. (2017)</ref> use an IMU-based system along with a single synchronized mobile camera video stream to capture 3D human pose "in the wild".</p><p>These data collection efforts have largely focused on covering a diverse range of poses and actions, but actions take place in simple environments (i.e., an empty room) which minimize occlusion and impose very few geometric affordance constraints on human pose. Recent "in the wild" markerless capture data such as <ref type="bibr" target="#b23">Mehta et al. (2017)</ref> encompass much richer environments, but the scene geometry is unknown. In contrast, our dataset provides gold-standard, marker-based 3D pose of subjects in richer environments with ground-truth scene geometry, offering a controlled test-bed for research in 3D human pose estimation with rich geometric affordance. Taheri et al.</p><p>(2020) collects a dataset for grasping, with the markers placed both on hands and on bodies to capture whole-body pose during grasping and object manipulation. This is complementary to our dataset as it provides object geometry and grasping contacts while our dataset samples whole-body affordance. We provide a summary comparison of recent 3D human pose estimation datasets in <ref type="table">Table 1</ref>.</p><p>Modeling scene affordances. The term "affordance" was coined by J Gibson <ref type="bibr" target="#b11">(Gibson, 1979)</ref> to capture the notion that the meaning and relevance of many objects in the environment  Our work is also closely related to earlier work on scene context for object detection. <ref type="bibr" target="#b15">Hoiem et al. (2005</ref><ref type="bibr" target="#b16">Hoiem et al. ( , 2006</ref> introduces scene geometry penetration and contact constraints in an energy-based framework for fitting parameters of a kinematic body model to estimate pose. In our work, we explore a complementary approach which uses CNN-based regression models that are trained to directly predict valid pose estimates</p><p>given image and scene geometry as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Geometric Pose Affordance Dataset (GPA)</head><p>To collect a rich dataset for studying interaction of scene geometry and human pose, we designed a set of action scripts performed by 13 subjects, each of which takes place in one of 6 scene arrangements. In this section, we describe the dataset components and the collection process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Human Poses and Subjects</head><p>We designed three action scripts that place emphasis on semantic actions, mechanical dynamics of skeletons, and posescene interactions. We refer to them as Action, Motion, and In-  <ref type="bibr" target="#b10">et al., 2012;</ref><ref type="bibr" target="#b12">Gupta et al., 2011)</ref>. The 13 subjects included 9 males and 4 female with roughly the same age and medium variations in heights approximately from 155cm to 190cm, giving comparable subject diversity to Human3.6M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image Recording and Motion Capture</head><p>This motion capture studio layout is also illustrated in <ref type="figure">Fig. 1</ref> c. We utilized two types of camera, RGBD and RGB, placed at 5 distinct locations in the capture studio. All 5 cameras have a steady 30fps frame rate but their time stamps are only partially synchronized, requiring additional post-processing described below. The color sensors of the 5 cameras have the same 1920x1080 resolution and the depth sensor of the Kinect v2 cameras has a resolution at 640x480. The motion capture system was a standard VICON system with 28 pre-calibrated cameras covering the capture space which are used to estimate the 3D coordinates of IR-reflective tracking markers attached to the surface of subjects and objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Scene Layouts</head><p>Unlike previous efforts that focus primarily on human poses without other objects present (e.g. <ref type="bibr" target="#b17">(Ionescu et al., 2014;</ref><ref type="bibr" target="#b23">Mehta et al., 2017)</ref>), we introduced a variety of scene geometries with arrangements of 9 cuboid boxes in the scene. The RGB images captured from 5 distinct viewpoints exhibit substantially more occlusion of subjects than existing datasets (as illustrated in <ref type="figure">Fig   1 and Fig 2)</ref> and constrain the set of possible poses. We captured 1 or 2 subjects interacting with each scene and configured a total of 6 distinct scene geometries.</p><p>To record static scene geometry, we measured physical dimension of all the objects (cuboids) as well as scanning the scene with a mobile Kinect sensor. We utilized additional motion-capture markers attached to the corners and center face of each object surface so that we could easily align geometric models of the cuboids with the global coordinate system of the 6 <ref type="figure">Fig. 2</ref>. The 5 camera views from the same scene with the first 3 layers of corresponding multi-layer depth map (for visualization clarity, we plot inverse depth). 2nd column corresponds to a traditional depth map, recording the depth of the first visible surface in the scene from the camera viewpoint of 1st column. 3rd column is when the multi-hit ray leaves the first layer of objects (e.g. the backside of the boxes). 4th column is when the multi-hit ray hits another object.</p><p>motion capture system. We also use the location of these markers, when visible in the RGB capture cameras, in order to estimate extrinsic camera parameters in the same global coordinate system. This allows us to quickly create geometric models of the scene which are well aligned to all calibrated camera views and the motion capture data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Scene Geometry Representation</head><p>Mesh models of each scene were initially constructed in global coordinates using modeling software (Maya) with assistance from physical measurements and reflective markers attached to scene objects. To compactly represent the scene geometry from the perspective of a given camera viewpoint, we utilize a multi-layer depth map. Multi-layer depth maps are defined as a map of camera ray entry and exit depths for all surfaces in a scene from a given camera viewpoint (illustrated in <ref type="figure">Fig 4)</ref>. Unlike standard depth-maps which only encode the geometry of visible surfaces in a scene (sometimes referred to as 2.5D), multi-layer depth provides a nearly 2 complete, viewer-centered description of scene geometry which includes occluded surfaces.</p><p>The multi-layer depth representation can be computed from the scene mesh model by performing multi-hit ray tracing from a specified camera viewpoint. Specifically, the multi-hit ray tracing sends a ray from the camera center towards a point on the image plane that corresponds to the pixel at (x, y) and outputs distance values {t 1 , t 2 , t 3 , ..., t k } where k is the total number of polygon intersections along the ray. Given a unit ray direc-2 Surfaces tangent to a camera view ray are not represented </p><formula xml:id="formula_0">D 1 , D 2 , D 3 , D 4 , D 5 ).</formula><p>tion r and camera viewing direction v, the depth value at layer</p><formula xml:id="formula_1">i is D i (x, y) = t i r ? v if i &lt;= k and D i (x, y) = ? if i &gt; k.</formula><p>In our scenes, the number of multi-layer depth maps is set to 15 which suffices to cover all scene surfaces in our dataset. We visualize 5 camera viewpoints together with first 3 layers of depth map in the same scene in <ref type="figure">Fig 2.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Processing Pipeline</head><p>The whole data processing pipeline includes validating mo- truth pose streams to image capture streams, we first had annotators to manually correspond 10-20 pose frames to image frames. Then we estimated temporal scaling and offset parameters using <ref type="bibr">RANSAC Fischler and Bolles (1981)</ref>, and regress all timestamps to a single global timeline.</p><p>The RGB camera calibration was performed by having annotators mark corresponding image coordinates of visible markers (whose global 3D coordinates are known) and estimating extrinsic camera parameters from those correspondences. We performed visual inspection on all clips to check that the estimated camera parameters yield correct projections of 3D markers to their corresponding locations in the image. With estimated camera distortion parameters, we correct the radial and lens distortions of the image so that they can be treated as projections from ideal pinhole cameras in later steps. Finally, the scene geometry model was rendered into multi-layer depth maps for each calibrated camera viewpoint. We performed visual inspection to verify that the depth edges in renderings were precisely aligned with object boundaries in the RGB images.</p><p>After temporal and geometric calibration, we generated a unified dataset by using an adaptive sampling approach to select non-redundant frames. We consider frames with sufficiently different poses from adjacent ones as "interesting". Here, the measure of difference between two skeleton poses is defined </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Dataset Visualization and Statistics</head><p>A video demonstrating the output of this pipeline is available online 4 . The video shows the full frame and a crop with ground-truth joints/markers overlayed, for 10 sample clips from the 'Action' and 'Motion' sets. The video also indicates various diagnostic metadata including the video and mocap time stamps, joint velocities, and number of valid markers (there are 53 markers and 34 joints for VICON system). Since we have an accurate model of the scene geometry, we can also automatically determine which joints and markers are occluded from the camera viewpoint. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Geometry-aware Pose Estimation</head><p>We now introduce two approaches for incorporating geometric affordance in CNN-based pose regression, building on the baseline architecture of <ref type="bibr" target="#b49">Zhou et al. (2017)</ref>. Given an image I of a human subject, we aim to estimate the 3D human pose represented by a set of 3D joint coordinates of the human skeleton, P ? R J?3 where J is the number of joints. We follow the convention of representing each 3D coordinate in the local camera coordinate system associated with I. The first two coordinates are given by image pixel coordinates and the third coordinate is the joint depth in metric coordinates (e.g., millimeters) relative to the depth of a specified root joint. We use P XY and P Z respectively as short-hand notations for the components of P.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pose Estimation Baseline Model</head><p>We adopt one popular ResNet-based network described by <ref type="bibr" target="#b46">Xiao et al. (2018)</ref> as our 2D pose estimation module. The network output is a set of low-resolution heat-maps? ? R 64?64?J , 9</p><p>where each map? [:, :, j] can be interpreted as a probability distribution over the j-th joint location. At test time, the 2D pre-dictionP XY is given by the most probable (arg max) locations in S . This heat-map representation is convenient as it can be easily combined (e.g., concatenated) with the other spatial feature maps. To train this module, we utilize squared error loss</p><formula xml:id="formula_2">2D (? |P) = ? ? G(P XY ) 2<label>(1)</label></formula><p>where G(?) is a target distribution created from ground-truth P by placing a Gaussian with ? = 3 at each joint location.</p><p>To predict the depth of each joint, we follow the approach of  <ref type="bibr" target="#b28">Ren et al. (2015)</ref> given by:</p><formula xml:id="formula_3">1s (P|P) = ? ? ? ? ? ? ? ? ? ? ? ? ? 1 2 P Z ? P Z 2 P Z ? P Z ? 1 P Z ? P Z ? 1 2 o.w.</formula><p>(2)</p><p>Alternate baseline:. We also evaluated two alternative baseline architectures. First, we used the model of <ref type="bibr" target="#b21">Martinez et al. (2017)</ref> which detects 2D joint locations and then trains a multi-layer perceptron to regress the 3D coordinates P from the vector of 2D coordinates P XY . We denote this simple lifting model as </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Geometric Consistency Loss and Encoding</head><p>To inject knowledge of scene geometry we consider two approaches, geometric consistency loss which incorporates scene geometry during training, and geometric encoding which assumes scene geometry is also available as an input feature at test time.</p><p>Geometric consistency loss:. We design a geometric consistency loss (GCL) that specifically penalizes errors in pose estimation which violate scene geometry constraints. The intuition is illustrated in <ref type="figure" target="#fig_7">Fig. 6</ref>. For a joint at 2D location (x, y), the estimated depth z should lie within one of a disjoint set of intervals defined by the multi-depth values at that location.</p><p>To penalize a joint prediction P j = (x, y, z) that falls inside a region bounded by front-back surfaces with depths D i (x, y) and D i+1 (x, y) we define a loss that increases linearly with the penetration distance inside the surface:</p><formula xml:id="formula_4">G(i) (P j |D) = min(max(0,P j Z ? D i (P j XY )), max(0, D i+1 (P j XY ) ?P j Z ))<label>(3)</label></formula><p>Our complete geometric consistency loss penalizes predictions which place any joint inside the occupied scene geometry</p><formula xml:id="formula_5">G (P|D) = j max i?{0,2,4,...} G(i) (P j |D)<label>(4)</label></formula><p>Assuming {D i } is piece-wise smooth, this loss is differentiable almost everywhere and hence amenable to optimization with stochastic gradient descent. The gradient of the loss "pushes" joint location predictions for a given example to the surface of occupied volumes in the scene.</p><p>Encoding local scene geometry:. When scene geometry is available at test time (e.g., fixed cameras pointed at a known scene), it is reasonable to provide the model with an encoding of the scene geometry as input. Our view-centered multi-depth representation of scene geometry can be naturally included as an additional feature channel in a CNN since it is the same dimensions as the input image. We considered two different encodings of multi-layer depth.</p><p>(1) We crop the multi-layer depth map to the input frame, re-sample to the same resolution as the 2D heatmap using nearest-neighbor interpolation, and offset by the depth of the skeleton root joint.</p><p>(2) Alternately, we con- For the joint depth regression-based models (ResNet-*) we simply concatenated the encoded multi-depth as additional feature channels. For the lifting-based models (SIM-*), we query the multi-depth values at the predicted 2D joint locations and use the results as additional inputs to the lifting network.</p><p>In our experiments we found that the simple and memory efficient multi-layer depth encoding (1) performed the same or better than volumetric encoding with ground-truth root joint offset. However, the volumetric encoding (2) was more robust when there was noise in the root joint depth estimate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Overall Training</head><p>Combining the losses in Eq. 1, 2, and 4, the total loss for each training example is Close-to-Geometry (C2G) 1,727 We follow <ref type="bibr" target="#b49">Zhou et al. (2017)</ref>  </p><formula xml:id="formula_6">(P,? |P, D) = 2D (? |P) + 1s (P|P) + G (P|P, D)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Training data:. Our Geometric Pose Affordance (GPA) dataset has 304.8k images of which 82k images are used for held-out test evaluation. In addition, we use the MPII dataset <ref type="bibr" target="#b1">(Andriluka et al., 2014)</ref>, a large scale in-the-wild human pose dataset for training the 2D pose module. It contains 25k training images and 2,957 validation images. For the alternative baseline model (SIM), we use the MPII pre-trained ResNet <ref type="bibr" target="#b46">Xiao et al. (2018)</ref> to detect the 2D key points. We also evaluate performance when using the ground truth 2D human pose, which serves as an upper-bound for the lifting-based method <ref type="bibr" target="#b21">Martinez et al. (2017)</ref>.</p><p>Implementation details:. We take a crop around the skeleton from the original 1920 ? 1080 image and isotropically resize to Models are implemented in PyTorch with Adam as the optimizer. For the lifting-based method we use the same process as above to detect 2D joint locations and train the lifting network using normalized inputs and outputs by subtracting mean and dividing the variance for both 2D input and 3D ground-truth following <ref type="bibr" target="#b21">Martinez et al. (2017)</ref>.</p><p>Evaluation metrics:. Following standard protocols defined in <ref type="bibr" target="#b23">(Mehta et al., 2017;</ref><ref type="bibr" target="#b17">Ionescu et al., 2014)</ref>, we consider two evaluation metrics for experiments: MPJPE (mean per-joint position error) and the 3DPCK (percent correctly localized keypoints) with a distance threshold of 150 mm. In computing the evaluation metrics, root-joint-relative joint locations are evaluated according to the each method original paper evaluation protocol.</p><p>Evaluation subsets:. In addition to the three subsets -Action, Motion, and Interaction -that are inherited from the global split of the dataset based on script contents, we also report test performance on 4 other subsets of the test data: cross-subject (CS), cross-action (CA), occlusion, and close-to-geometry (C2G).</p><p>These are non-orthogonal splits of the test data which allow for finer characterizations of model performance and generalization in various scenarios: (1) CS subset includes clips from held-out subjects to evaluate generalization ability on unseen subjects and scenes;</p><p>(2) CA subset includes clips of held-out actions from same subjects from the training set;</p><p>(3) Occlusion subset includes frames with significant occlusions (at least 10 out of 34 joints are occluded by objects); (4) Close-to-geometry subset includes frames where subjects are close to objects (i.e.</p><p>at least 8 joints have distance less than 175 mm to the nearest surface).</p><p>Statistics of these testing subsets are summarized in <ref type="table" target="#tab_6">Table 2</ref>.</p><p>Ablative study:. To demonstrate the contribution of each component, we evaluate four variants of each model: the baseline models ResNet / SIM-P / SIM-G where G stands for groundtruth 2D joint input while P stands for predicted 2D joint input;</p><p>ResNet-E / SIM-P-E / SIM-G-E / PoseNet-E, models with encoded scene geometry input; ResNet-C / SIM-P-C / SIM-G-C / PoseNet-C, the models with geometric consistency loss (GCL); ResNet-F / SIM-P-F / SIM-G-F /PoseNet-F, our full model with both encoded geometry priors and GCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Baselines</head><p>To evaluate the difficulty of the GPA and provide context, was not available, we evaluated their released model. To account for systematic differences in the body joint definitions, we utilized the average of hip joints as the DOPE coordinate origin (H36M-based models typically use the pelvis root joint  on the PoseNet architecture achieves the lowest estimation error.</p><p>We break down the performance of the ResNet-based joint regression baseline on different subsets of data in <ref type="table">Table 3</ref>. We also list the corresponding PCK3D in <ref type="table">Table 7</ref>, which follows a similar pattern. The motion, occlusion and close-to-geometry subsets prove to be the most challenging as they involve large numbers of frames where subjects interact with the scene geometry.</p><p>Cross-dataset Generalization. We find that pose estimators show a clear degree of over-fitting to the specific datasets on fewer training examples <ref type="bibr" target="#b44">(Wang et al., 2020)</ref>. We attribute this to the greater diversity of poses, occlusions and scene interactions present in GPA.</p><p>which they are trained on <ref type="bibr" target="#b44">(Wang et al., 2020</ref>  <ref type="table">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of geometric affordance</head><p>From <ref type="table">Table 3</ref> we observe that incorporating geometric as an input (ResNet-E) and penalizing predictions that violate constraints during training (ResNet-C) both yield improved performance across all test subsets. Not surprisingly, the full model (ResNet-F) which is trained to respect geometric context provided as an input achieves the best performance. We can see from <ref type="table">Table 3</ref> that the full model, ResNet-F decreases the MPJPE by 2.1mm over the whole test set. Among 4 subsets, the most significant improvement comes on the occlusion and close-to-geometry subsets. Our geometry-aware method decreases MPJPE in occlusion and C2G set by 5.4mm / 6.6mm</p><p>and increase the PCK3D about 2% / 3%. Similar results hold for the SIM model. The MPJPE is reduced when using either the predicted (SIM-P-F) or ground-truth 2D joint locations (SIM-P-F) by 3mm and 3.6mm respectively (PCK3D improves 1.2% and 1.1%). The improvement from SIM-G model is overall larger than SIM-P model due to the more accurate 2D location and better geometry information provided to the network.</p><p>Controlling for Visual Context. One confounding factor in interpreting the power of geometric affordance for the ResNetbased model is that while the baseline model doesn't use explicit geometric input, there is a high degree of visual consistency between the RGB image and the underlying scene geometry (e.g., floor is green, boxes are brighter white on top than on vertical surfaces). As a result, the baseline model may well be implicitly learning some of the scene geometric constraints from images alone and consequently decreasing the apparent size of the performance gap.</p><p>To further understand whether the background pixels are useful or not for 3d pose estimation, we utilize <ref type="bibr">Grabcut Rother et al. (2004)</ref> to mask out background pixels. Specifically, we label the pixel belonging to markers, joints that are not occluded by the first-layer of multi-layer depth map as foreground, and occluded ones as background. Additionally, we dilate the skeleton constructed by all the joints and markers, use the inverse area as background area. We send these labels together with the image to OpenCV implementation Grabcut and get the foreground mask. We set the background color as green for better visualization as shown in <ref type="figure" target="#fig_9">Fig 7.</ref> We use the model <ref type="bibr" target="#b25">Moon et al. (2019)</ref>, and train and test on the masked background images.</p><p>We observe increased error on C2G from 70.7 mm to 78.7 mm MPJPE, which suggests that baseline models do take significant advantage of visual context in estimating pose.</p><p>Errors by joint type:. We partition the 16 human joints into the limb joints which are more likely to be interacting with scene geometry (out group) and the torso and hips (in group). The performance on these two subsets of joints as well as individual joints is illustrated for the SIM model in <ref type="table">Table 9</ref>. This verifies our assumption that limb joint estimation (wrist, elbow, knees, ankles) benefits more from incorporating geometric scene affordance.</p><p>Error in predicted root joint:. Since our models predict joint depths relative to the root joint, it is necessary to offset the multi-layer depth map values when encoding them as input. To make our evaluation more realistic, we also evaluated models using predicted root joint locations instead of using the ground- 35.2mm respectively. <ref type="table">Table 8</ref> shows the result of using this predicted root joint depth during encoding to offset the multi-depth map. Using predicted depth results in a loss of performance of about 1% over the three methods (with the largest effect for ResNet) but does not eliminate the benefits of geometric context.</p><p>Computational Cost:. We report the average runtime over 10 randomly sampled images on a single 1080Ti in <ref type="table">Table 10</ref>.   <ref type="table">Table 9</ref>. Performance of the lifting network-based model <ref type="bibr" target="#b21">(Martinez et al., 2017)</ref> broken down by individual joints and joint subsets. Baseline prediction error is higher for extremities (e.g., wrists and ankles) which are inherently more difficult to localize. These same joints typically show the largest reduction in error from introducing geometric context. baseline fails.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and Conclusion</head><p>In this work, we introduce a large-scale dataset for exploring geometric pose affordance constraints. The dataset provides multi-view imagery with gold-standard 3D human pose and scene geometry, and features a rich variety of human-scene interactions. We propose using multi-layer depth as a concise camera-relative representation for encoding scene geometry, and explore two effective ways to incorporate geometric constraints into training in an end-to-end fashion. There are, of course, many alternatives for representing geometric scene constraints which we have not yet explored. We hope the availability of this dataset will inspire future work on geometry-aware feature design and affordance learning for 3D human pose estimation.</p><p>Broadly speaking, our techniques for encoding geometry yielded only modest reductions joint localization error (? 2 ? 6% depending on the base model). We might have hoped for greater gains, but we expect that even the baseline models are implicitly learning something about scene constraints that are common across our dataset. Indeed, masking out the background yielded an ? 11% increase in baseline error. There has been substantial success in training models that predict scene depth (2.5D) from monocular RGB inputs <ref type="bibr" target="#b7">(Eigen et al., 2014;</ref><ref type="bibr" target="#b5">Chen et al., 2016)</ref> as well as full 3D representations such as voxels <ref type="bibr" target="#b35">(Song et al., 2017;</ref><ref type="bibr" target="#b39">Tulsiani et al., 2018)</ref> or multilayer depth <ref type="bibr" target="#b33">(Shin et al., 2019)</ref>. This suggests that when geometric supervision is available, it may be useful to explore training systems that jointly estimate scene structure and 3D human pose in a multi-task setup.</p><p>In our experiments we focused on a setting where the scene geometric constraints were available as input and highly accurate. While such prior knowledge is not available in general (e.g., for a random photo on the web), we believe such data is readily accessible in many practical scenarios. The successful development of robust structure from motion, SLAM, and specialized stereo or time-of-flight depth sensors makes geometric scene information increasingly prevalent and easy to acquire. Assuming known camera and scene geometry as input appears practical in commercial applications where, e.g. robots navigate a well-mapped environment interacting with people or fixed cameras monitor human activity in a static workspace. 16 <ref type="figure">Fig. 9</ref>. Visualization of the input images with the ground truth pose overlaid in the same view (blue and red indicate right and left sides respectively).</p><p>Columns 2-4 depict the first 3 layers of multi-layer depth map. Column 5 is the baseline model prediction overlaid on the 1st layer multi-layer depth map.</p><p>Column 6 is the ResNet-F model prediction. The red rectangles highlight locations where the baseline model generates pose predictions that violate scene geometry or are otherwise improved by incorporating geometric input.</p><p>We expect finding better techniques to incorporate such "side information" will offer a way to improve cross-scene/crossdataset generalization and avoid some of the common overfitting we currently observe when training and testing on individual datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head><p>This project is supported by NSF grants IIS-1813785, IIS1618806, IIS-1253538, CNS-1730158 and a hardware donation from NVIDIA. We thank reviewers for the valuable suggestions. We thank Shu Kong and Minhaeng Lee for helpful</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a: Samples from our data set featuring scene constrained poses: stepping on the stairs, sitting on the tables and touching boxes. b: Sample frame of a human interacting with scene geometry, and visualization of the corresponding 3D scene mesh with captured human pose. c: Motion capture setup.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>focused on capturing a diverse range human motions, actions, abd subjects using optical markers and/or IMUs to establish ground-truth pose. Our dataset focuses on interactions between humans and static scene geometry and includes both ground-truth 3D pose and a complete description of the scene geometry.are largely defined in relation to the ways in which an individual can functionally interact with them. For computer vision, this suggests scenarios in which the natural labels for some types of visual content may not be semantic categories or geometric data but rather functional labels, i.e., which human interactions they afford.<ref type="bibr" target="#b12">Gupta et al. (2011)</ref> present a humancentric paradigm for scene understanding by modeling physical human-scene interactions. Fouhey et al. (2012) rely on pose estimation methods to extract functional and geometric constraints about the scene and use those constraints to improve estimates of 3D scene geometry. Wang et al. (2017) collects a large-scale dataset of images from sitcoms which contains multiple images of the same scene with and without humans present. Leveraging state-of-the-art pose estimation and generative model to infer what kind of poses each sitcom scene affords. Li et al. (2019) build a fully automatic 3D pose synthesizer to predict semantically plausible and physically feasible human poses within a given scene. Monszpart et al. (2018) applies an energy-based model on synthetic videos to improve both scene and human motion mapping. Cao et al. (2020) construct a synthetic dataset utilizing a game engine. They first sample multiple human motion goals based on a single scene image and 2D pose histories, plan 3D human paths towards each goal, and finally predict 3D human pose sequences following each path. Rather than labeling image content based on observed poses, our approach is focused on estimating scene affordance directly from physical principles and geometric data, and then subsequently leveraging affordance to constrain estimates of human pose and interactions with the scene.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) used estimates of ground-plane geometry to reason about location and scales of objects in an image. More recent work such as Wang et al. (2015); D?az et al. (2016); Matzen and Snavely (2013) use more extensive 3D models of scenes as context to improve object detection performance. Geometric context for human pose estimation differs from generic object detection in that humans are highly articulated. This makes incorporating such constraints more complicated as the resulting predictions should simultaneously satisfy both scene-geometric and kinematic constraints. Constraints in 3D human pose estimation. Estimating 3D human pose from monocular image or video is an ill-posed problem that can benefit from prior constraints. Recent examples include Fang et al. (2018) who model kinematics, symmetry and motor control using an RNN when predicting 3D human joints directly from 2D key points. Yang et al. (2018) propose an adversarial network as an anthropometric regularizer. Wang et al. (2018); Zhou et al. (2018b) construct a graphical model encoding priors to fit 3D pose reconstruction. Rogez et al. (2019); Chen and Ramanan (2017) first build a large set of valid 3Dhuman poses and treat estimation as a matching or classification problem.<ref type="bibr" target="#b0">Akhter and Black (2015)</ref>;<ref type="bibr" target="#b29">Rhodin et al. (2018a)</ref> explore joint constraints in 3D and geometric consistency from multi-view images.<ref type="bibr" target="#b49">Zhou et al. (2017)</ref> improve joint estimation by adding bone-length ratio constraints.To our knowledge, there is relatively little work on utilizing scene constraints for 3D human pose.<ref type="bibr" target="#b48">Zanfir et al. (2018)</ref> utilize an energy-based optimization model for pose refinement which penalizes ankle joint estimates that are far above or below an estimated ground-plane. The recent work of<ref type="bibr" target="#b13">Hassan et al. (2019)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Overview of model architecture: we use ResNet-50 as our backbone to extract features from a human centered cropped image. The feature map is used to predict 2D joint location heatmaps and is also concatenated with encoded multi-layer depth map. The concatenated feature is used to regress the depth (z-coordinate) of each joint. The model is trained with a loss on joint location (joint regression loss) and scene affordance (geometric consistency loss). The 2d joint heatmaps are decoded to x,y joint locations using an argmax. The geometric consistency loss is described in more detail inFig 6 (a)andSection 4.2. Fig. 4. Illustration of multi-layer depth map. For each image pixel we record the depth of all surface intersections along the view ray (e.g.,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>summarizes statistics on the number of occluded joints as well as the distribution of which multi-depth layer is closest to a joint. While the complete scene geometry requires 15 depth layers, as the figure shows only the first 5 layers are involved in 3 The dataset is available online: https://wangzheallen.github.io/ GPA 4 Video Link: https://youtu.be/ZRnCBySt2fk Fig. 5. Top: Distribution of the number of joints occluded in training and testing frames. Bottom: Distribution of the index of the depth layer closest to each pose. High index layers, which often correspond to hidden surfaces such as the bottom side of platforms, seldom constrain pose. 90% of the interaction between body joints and scene geometry. The remaining layers often represent surfaces which are inaccessible (e.g., bottoms of cuboids).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc><ref type="bibr" target="#b49">Zhou et al. (2017)</ref>, which combines the 2D joint heatmap and the intermediate feature representations in the 2D pose module as input to a joint depth regression module (denoted ResNet in the experiments). These shared visual features provide additional cues for recovering full 3D pose. We train with a smooth 1 loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>SIM</head><label></label><figDesc>in the experiments. To detect the 2D locations we utilized the ResNet model of<ref type="bibr" target="#b46">Xiao et al. (2018)</ref> and also considered an upper-bound based on lifting the ground-truth 2D joint locations to 3D. Second, we trained the PoseNet model proposed in<ref type="bibr" target="#b25">Moon et al. (2019)</ref> which uses integral regression<ref type="bibr" target="#b36">(Sun et al., 2018)</ref> in order to regress pose from the heat map directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>sider a volumetric encoding of the scene geometry by sampling 64 depths centered around the root joint using a range based on the largest residual depth between the root and any other joint seen during training (approx. +/ ? 1m). For each (x, y) location and depth, we evaluate the geometric consistency loss G at that (a) is the illustration of the geometry consistency loss as a function of depth along a specific camera ray corresponding to a predicted 2D joint location. In (b) the green line indicates the ray corresponding to the 2D location of the right foot. Our multi-depth encoding of the scene geometry stores the depth to each surface intersection along this ray (i.e., the depth values Z 0 , Z 1 , Z 2 , Z 3 , Z 4 ). Valid poses must satisfy the constraint that the joint depth falls in one of the intervals:Z J &lt; Z 0 or Z 1 &lt; Z J &lt; Z 2 or Z 3 &lt; Z J &lt; Z 4 .The geometric consistency loss pushes the prediction Z J towards the closest valid configuration along the ray, Z J = Z 2 . point. This resulting encoding is of size H ?W ?64 and encodes the local volume occupancy around the pose estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>and adopt a stage-wise training approach: Stage 1 initializes the 2D pose module using 2D annotated images (i.e., MPII dataset); Stage 2 trains the 3D pose estimation module, jointly optimizing the depth regression module as well as the 2D pose estimation module; Stage 3 of training adds the geometry-aware components (encoding input, geometric consistency loss) to the modules trained in stage 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>We adopt Grabcut<ref type="bibr" target="#b32">Rother et al. (2004)</ref> and utilize the ground truth (joints, multi-layer depth, and markers) we have to segment subjects from background. If the joints and markers are occluded by the first-layer of multi-layer depth, we set them as background, otherwise they are set as foreground in grabcut algorithm.256 ? 256, so that projected skeletons have roughly the same size. Ground-truth target 2D joint location are adjusted accordingly. For ResNet-based method, following<ref type="bibr" target="#b49">Zhou et al. (2017)</ref>, the ground truth depth coordinates are normalized to [0, 1]. The backbone for all models is ResNet-50<ref type="bibr" target="#b14">(He et al., 2016)</ref>. The 2D heat map/depth map spatial resolution is 64?64 with one output channel per joint. For test time evaluation, we scale each model prediction to match the average skeleton bone length observed in the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>we trained and evaluated a variety of recently proposed architectures for pose estimation including: DOPE Weinzaepfel et al. (2020), Simple baseline Martinez et al. (2017), ResNet-Baseline Zhou et al. (2017), PoseNet Moon et al. (2019), and I2L Moon and Lee (2020). As data and code for training DOPE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Distribution of prediction error (MPJPE) for ResNet-F and the baseline on the close-to-geometry test set. Examples are sorted in increasing order of baseline MPJPE (red) with corresponding ResNet-F performances (GCL + encoding, in blue). We also highlight 3 qualitative results, from left to right:(a) case shows ResNet-F improve over the baseline with respect to the depth prediction. (b,c) cases show ResNet-F improves over the baseline in all x, y, z axes. Furthermore, (b) demonstrates ResNet-F can even resolve ambiguity under heavy occlusions with the aid of geometry information. We show the image with the estimated 2D pose (after cropping), 1st layer of multi-layer depth map and whether the joint is occluded or not. Legend: hollow circles: occluded joints; solid dots: non-occluded joints; dotted lines: partially/completely occluded body parts; solid lines: non-occluded body parts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>Timings for SIM do not include 2D keypoint detection. For comparison, we also include the run time for the PROX model of Hassan et al. (2019) which uses an optimization-based approach to perform geometry-aware pose estimation. Qualitative results:. We show qualitative examples that highlight interaction with geometry in Fig 8 along with the distributions of the mean prediction error for the baseline and ResNet-F model over the close2geometry subset. The geometry aware model is able to show most improvement for hard examples where the baseline error is large. Further visualization of model predictions along with scene geometry encodings are shown in Fig 9. These examples demonstrate that ResNet-F has better accuracy in both xy localization and depth prediction and is often able to resolve ambiguity under heavy occlusion where the 15 MPJPE (mm) SIM-G SIM-G-F SIM-P SIM-P-F</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1905.07718v2 [cs.CV] 9 Dec 2021</figDesc><table><row><cell></cell><cell>2</cell></row><row><cell></cell><cell>Capture</cell></row><row><cell>Control</cell><cell>Space</cell></row><row><cell>Center</cell><cell></cell></row><row><cell></cell><cell>Control Center</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>tion capture pose estimates, camera calibration, joint temporal alignment of all data sources, and camera calibration. Unlike previous marker-based mocap datasets which have few occlusions, many markers attached to the human body are occluded in the scene during our capture sessions due to scene geometry.</figDesc><table><row><cell>We spent 4 months on pre-processing with help of 6 annota-</cell></row><row><cell>tors in total. There are three stages of generating ground truth</cell></row><row><cell>joints from recorded VICON sessions: (a) recognizing and la-</cell></row><row><cell>beling recorded markers in each frame to 53 candidate labels</cell></row><row><cell>which included three passes to minimize errors; (b) applying</cell></row><row><cell>selective temporal interpolation for missing markers based on</cell></row><row><cell>annotators' judgement. (c) removing clips with too few tracked</cell></row><row><cell>markers. After the annotation pipeline, we compiled record-</cell></row><row><cell>ings and annotations into 61 sessions captured at 120fps by the</cell></row><row><cell>VICON software. To temporally align these compiled ground-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 .</head><label>2</label><figDesc>Numbers of frames in each test subset. We evaluate performance</figDesc><table><row><cell>on different subsets of the test data split by the scripted behavior (Ac-</cell></row><row><cell>tion/Motion/Interaction), subjects that were excluded from the training</cell></row><row><cell>data (cross-subject) and novel actions (cross-action). Finally, we evaluate</cell></row><row><cell>on a subset with significant occlusion (Occlusion) and a subset where many</cell></row><row><cell>joints were near scene geometry (Close-to-Geometry).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Table 10. We compare the running time for our baseline backbone, our method, and another geometry-aware 3d pose estimation method PROX<ref type="bibr" target="#b13">Hassan et al. (2019)</ref> averaged over 10 samples evaluated on a single GPU.</figDesc><table><row><cell>Method</cell><cell>Average Run Time</cell></row><row><cell>SIM Martinez et al. (2017)</cell><cell>0.57 ms</cell></row><row><cell>SIM-F</cell><cell>0.64 ms</cell></row><row><cell>ResNet Zhou et al. (2017)</cell><cell>0.29 s</cell></row><row><cell>ResNet-F</cell><cell>0.36 s</cell></row><row><cell>PROX Hassan et al. (2019)</cell><cell>47.64 s</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>discussion, John Crawford and Fabio Paolizzo for providing support on the motion capture studio, and all the UCI friends who contribute to collection of the dataset.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<title level="m">2d human pose estimation: New benchmark and state of the art analysis</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Long-term human motion prediction with scene context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A 3d-point-cloud feature for humanpose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S G</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">3d human pose estimation = 2d pose estimation + matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Lifting gis maps into strong geometric context for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>D?az</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">People watching: Human actions as a cue for single-view geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Delaitre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
			<pubPlace>Boston; Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">From 3d scene geometry to human workspace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Resolving 3d human pose ambiguities with 3d scene constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Geometric context from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Putting objects in perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Total capture: A 3d deformation model for tracking faces, hands, and bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Putting humans in a scene: Learning affordance in 3d indoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Nyc3dcars: A dataset of 3d vehicles in geographic context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">imapper: Interaction-guided joint scene and human motion mapping from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Monszpart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guerrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>arxiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Camera distance-aware top-down approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">I2l-meshnet: Image-to-lixel prediction network for accurate 3d human pose and mesh estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sp?rri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lcr-net++: Multi-person 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">grabcut&quot; interactive foreground extraction using iterated graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>ToG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">3d scene reconstruction with multi-layer depth and epipolar transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Humaneva: Synchronizedvideo and motion capture dataset and baseline algorithm forevaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semantic scene completion from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lis Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">GRAB: A dataset of whole-body human grasping of objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Taheri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tzionas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Total capture: 3d human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Factoring shape, pose, and layout from the 2d image of a 3d scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aefros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Robust 3d human pose estimation from single images or video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Holistic 3d scene understanding from a single geo-tagged image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Binge watching: Scaling affordance learning from sitcoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Predicting camera viewpoint improves cross-dataset generalization for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="ECCV work" to=" shop" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">DOPE: Distillation Of Part Experts for whole-body 3D pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Br?gier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Combaluzier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
	<note>3d human pose estimation in the wild by adversarial learning</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Monocular 3d pose and shape estimation of multiple people in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marinoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Human motion capture using a drone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICRA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Monocap: Monocular human motion capture using a cnn coupled with a geometric prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
