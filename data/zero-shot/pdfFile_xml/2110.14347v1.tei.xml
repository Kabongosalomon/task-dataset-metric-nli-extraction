<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><forename type="middle">Shyam</forename><surname>Chanduri</surname></persName>
							<email>chanduriss@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences (HTW)</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeeshan</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Applied Sciences (HTW)</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suri</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Vozniak</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Agents and Simulated Reality (ASR) German Research Center for Artificial Intelligence (DFKI GmbH</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Vozniak@dfki</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>De</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>M?ller</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Agents and Simulated Reality (ASR) German Research Center for Artificial Intelligence (DFKI GmbH</orgName>
								<address>
									<settlement>Saarbr?cken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><forename type="middle">Mueller@dfki</forename><surname>De</surname></persName>
						</author>
						<title level="a" type="main">CamLessMonoDepth: Monocular Depth Estimation with Unknown Camera Parameters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>CHANDURI ET AL.: CAMLESSMONODEPTH 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Perceiving 3D information is of paramount importance in many applications of computer vision. Recent advances in monocular depth estimation have shown that gaining such knowledge from a single camera input is possible by training deep neural networks to predict inverse depth and pose, without the necessity of ground truth data. The majority of such approaches, however, require camera parameters to be fed explicitly during training. As a result, image sequences from wild cannot be used during training. While there exist methods which also predict camera intrinsics, their performance is not on par with novel methods taking camera parameters as input. In this work, we propose a method for implicit estimation of pinhole camera intrinsics along with depth and pose, by learning from monocular image sequences alone. In addition, by utilizing efficient subpixel convolutions, we show that high fidelity depth estimates can be obtained. We also embed pixel-wise uncertainty estimation into the framework, to emphasize the possible applicability of this work in practical domain. Finally, we demonstrate the possibility of accurate prediction of depth information without prior knowledge of camera intrinsics, while outperforming the existing state-of-the-art approaches on KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Perceiving accurate depth is a prerequisite for many application domains like robotics and autonomous driving. Traditionally, such systems rely on information fused from depth sensors such as LiDAR, because of their accuracy and robustness. However, such sensors have limited applicability in extreme weather conditions (fog or heavy rain), limited range, higher cost, and complexity <ref type="bibr">[10,</ref><ref type="bibr" target="#b14">27]</ref>. Estimating depth from RGB images can partially mitigate some of these limitations <ref type="bibr">[21]</ref>. While supervised learning methods <ref type="bibr" target="#b56">[11,</ref><ref type="bibr" target="#b57">12,</ref><ref type="bibr" target="#b60">15,</ref><ref type="bibr" target="#b11">24,</ref><ref type="bibr" target="#b41">54]</ref> have shown to estimate depth without sensors during inference, they still need ground truth supervision to complete the training. However, acquiring such a large ground truth dataset is a formidable challenge, moreover, expensive and time-consuming. The utilization of synthetic datasets with "free" ground truth data is a possible solution. However, such dataset are lacking representability compared to natural dataset, owing the difficulty in generating photo-realistic synthetic images <ref type="bibr" target="#b22">[35]</ref>. Recent advancements in deep learning helped in alleviating these issues by estimating depth from unlabelled image sequences alone, leading to self-supervised approaches like <ref type="bibr">[20,</ref><ref type="bibr">21,</ref><ref type="bibr">60,</ref><ref type="bibr">64]</ref>.</p><p>Monocular self-supervised approaches offer a more attractive solution than stereo approaches because of the widespread availability of image sequences available for training. In addition, these monocular approaches require no synchronization of cameras. However, they come with their own set of challenges. They operate under a static scene assumption, meaning, the camera is moving, and the scene is static and violation of such assumption predicts 'holes' in depth maps <ref type="bibr">[21]</ref>. They also suffer from issues including scaling to metricdepth when ground truth data is not available, brightness changes because of non-Lambertian and reflective surfaces and, occlusions. Over time, many methods with complex network architectures <ref type="bibr" target="#b10">[23,</ref><ref type="bibr">64]</ref>, engineered loss functions <ref type="bibr">[21,</ref><ref type="bibr" target="#b36">49]</ref>, masking moving objects [21, <ref type="bibr" target="#b23">36,</ref><ref type="bibr">64]</ref>, using ground truth supervision <ref type="bibr" target="#b10">[23]</ref> or post-processing <ref type="bibr" target="#b26">[39,</ref><ref type="bibr" target="#b44">57]</ref> for addressing scale ambiguity, etc., have tried to address such challenges. Yet, current monocular approaches significantly lag behind their counterpart supervised approaches like <ref type="bibr">[2,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b60">15]</ref>.</p><p>Another main limitation in many of the earlier approaches <ref type="bibr">[7,</ref><ref type="bibr">21,</ref><ref type="bibr">60,</ref><ref type="bibr">64]</ref> is the necessity of precise camera calibration parameters as input, for training accurate depth estimation models. This eliminates the plausible usage of potential data from the wild for training. Previous works <ref type="bibr">[8,</ref><ref type="bibr">22,</ref><ref type="bibr" target="#b38">51]</ref> have addressed such issues in the past, however, are not on par with the other approaches which use ground-truth calibrated data. In our work, we primarily address this issue by removing the necessity of pre-calibrated data and also focus on refining monocular depth estimation accuracy. The contributions of this work are fourfold: First, we learn depth from monocular image sequences even when camera intrinsics are unknown, in a self-supervised fashion, motivated from <ref type="bibr">[22]</ref>. We combine it with minimum re-projection loss and auto masking, as proposed by <ref type="bibr">Godard et al. [21]</ref>, in order to deal with occlusions and static pixels. Second, we reiterate the importance of using Efficient Sub-Pixel Convolution Networks (ESPCN) (adapted from <ref type="bibr">[1,</ref><ref type="bibr" target="#b35">48]</ref>) for upsampling purposes to obtain sharper depth estimates. Such a method can leverage super-resolution, that can be more accurate in comparison with interpolation approaches <ref type="bibr" target="#b35">[48]</ref>. Third, we extend our approach to estimate heteroscedastic pixel-wise uncertainty for depth map, capturing by brightness changes due to specular regions or sudden illumination changes, etc. Such information can be leveraged by the agents to take optimal decisions when they are under-confident in predicting depth values in unknown environments, which can avoid fatalities. For example, in an autonomous driving scenario, uncertainty estimation could be beneficial in handling erroneous estimations from the system to prevent accidents <ref type="bibr" target="#b30">[43]</ref>. Finally, with our exhaustive experiments, we demonstrate that our models, to our best knowledge, outperform the state-of-the-art in monocular depth estimation and closes-in the gap with full-supervised methods on the standard KITTI benchmark [18] even without the need of camera intrinsics as input.</p><p>2 Related Work Monocular Depth Estimation. Monocular depth estimation is an ill-posed problem, as multiple plausible depths could correspond to the same pixel on the image plane. Recent works showed the possibility of accurate depth estimation by analysing patterns in appearances using image sequences alone. For instance, Eigen et al. <ref type="bibr" target="#b57">[12]</ref> proposed the first deep learning method using multiscale CNNs which regressed depth output by taking only a single image as input. Many such subsequent supervised approaches like <ref type="bibr" target="#b60">[15,</ref><ref type="bibr" target="#b11">24,</ref><ref type="bibr" target="#b41">54]</ref> were later proposed, which further extended this formulation. These approaches demand huge amounts of training data, where obtaining ground truth data is both expensive and time-consuming. To mitigate this problem, <ref type="bibr" target="#b25">[38]</ref> was proposed which makes the use of synthetic data for generating ground truth. However, it lacks representability compared to the natural training data. To tackle this constraint, <ref type="bibr">[16,</ref><ref type="bibr">20]</ref> were proposed by utilizing the self-supervised strategy that involved learning depth using stereo images for monocular depth inference. In a nutshell, the right images were warped onto the left images using a differentiable sampler as in <ref type="bibr" target="#b13">[26]</ref> which enables learning the depth in an end-to-end manner. Later, Zhou et al.</p><p>[64] extended this idea by proposing a strategy to learn the depth along with the pose, to handle training completely with monocular settings only. Furthermore, <ref type="bibr">Godard et al. [21]</ref> proposed another set of extensions, where the multiscale approach and per-pixel minimum reprojection loss were adopted for the better handling of occlusions. Other self-supervised monocular depth estimation methods like <ref type="bibr">[7,</ref><ref type="bibr" target="#b10">23,</ref><ref type="bibr" target="#b29">42,</ref><ref type="bibr" target="#b32">45,</ref><ref type="bibr" target="#b39">52,</ref><ref type="bibr">60]</ref> were proposed over the time, which included more robust architectures, additional loss terms and constraints.</p><p>Learning from Videos in Wild. Most of the self-supervised learning works <ref type="bibr">[7,</ref><ref type="bibr">21,</ref><ref type="bibr">60,</ref><ref type="bibr">64]</ref> require camera intrinsics to learn depth, making it difficult to train on multiple datasets at a time. While Ranftl et al. <ref type="bibr" target="#b31">[44]</ref> proposed a method to train on multiple datasets at once, their approach to learn depth is supervised, requiring ground-truth depth maps. The lack of camera parameters in generic videos, for instance, a YouTube video, where image sequences are captured from unknown camera setups, limits the usage of such data for training. Traditionally, camera parameters are estimated using techniques which involve calibration targets <ref type="bibr" target="#b27">[40,</ref><ref type="bibr" target="#b34">47,</ref><ref type="bibr">61]</ref>, geometric structures <ref type="bibr">[4,</ref><ref type="bibr">62]</ref> or separate neural networks <ref type="bibr">[5,</ref><ref type="bibr" target="#b59">14]</ref> and are fed as input to the depth estimation models. The main disadvantages of using such approaches include either the necessity of additional data (for calibration), geometric assumptions, or additional complexity and training time. A recent work, proposed by Gordon et al.</p><p>[22] eliminates this need of pre-calibrated camera parameters as input for depth estimation, where camera parameters along with depth, pose and object motion are simultaneously learnt in a fully self-supervised manner. As camera parameters are learnt in a fully self-supervised manner, even image sequences from wild can be used, where the network generalizes better. Later, other works, applicable to image sequences from wild, such as in <ref type="bibr">[8]</ref> and <ref type="bibr" target="#b38">[51]</ref> were proposed which followed an approach similar to <ref type="bibr">[22]</ref> in camera parameters estimation. Our approach is similar to these, but does not need object motion mask as input <ref type="bibr">[22,</ref><ref type="bibr" target="#b38">51]</ref> or online refinement <ref type="bibr">[8]</ref>, to output accurate depth maps, while assuming a pinhole camera model with minimum or no distortion.</p><p>Depth Super-resolution. The problem of image enhancement has been a primary challenge in the fraternity of computer vision. Approaches like <ref type="bibr">[9,</ref><ref type="bibr" target="#b17">30,</ref><ref type="bibr" target="#b21">34,</ref><ref type="bibr" target="#b37">50]</ref> have shown to solve this problem while improving upon traditional methods like interpolation. However, such methods demand huge training times and are infeasible when used in combination with applications like depth estimation. For faster and accurate results, Shi et al. <ref type="bibr" target="#b35">[48]</ref> proposed ESPCN and later <ref type="bibr" target="#b29">[42]</ref> has extended it to depth estimation, but requires camera parameters as input. In our work, we exploit ESPCN along with initialization technique from <ref type="bibr">[1]</ref> to remove checkerboard artifacts. Consequently, it improves depth estimation accuracy and also, enables faster training and inference compared to other interpolation approaches <ref type="bibr" target="#b35">[48]</ref>.</p><p>Uncertainty. The two types of uncertainties in Bayesian world, namely epistemic and aleatoric, are discussed in recent works <ref type="bibr" target="#b15">[28,</ref><ref type="bibr" target="#b16">29]</ref>. Later Klodt et al. <ref type="bibr" target="#b19">[32]</ref> proposed an approach to model aleatoric uncertainty which also served for increasing depth estimation accuracy. In this paper, the authors have used a Bayesian framework to model photometric uncertainty as predictive variance. Another approach <ref type="bibr" target="#b14">[27]</ref> made use of discrete disparity volume module and to model depth uncertainties. Later, Poggi et al. <ref type="bibr" target="#b30">[43]</ref> explored various approaches including both empirical and predictive techniques for depth uncertainty estimation in selfsupervised methods. Inspired from <ref type="bibr" target="#b16">[29]</ref>, we propose an approach to model heteroscedastic aleatoric uncertainty as photometric variance by considering noise inherent in the input data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Self-supervised depth estimation approaches exempt the necessity of hard ground truth by generating a supervision signal using a moving camera setup and by measuring the overlap between temporal image sequences. This Structure-From-Motion (SFM) problem is modelled by performing a novel image synthesis of the target frame from image source point of view. However, this objective is ill-posed since knowledge of accurate pose and camera intrinsics can reconstruct this novel view even with incorrect depth estimations <ref type="bibr">[21]</ref>. To address such ambiguity, appearance matching and smoothness losses were proposed. However, it demands the knowledge of pre-calibrated camera intrinsics for novel view synthesis.</p><p>Motivated from earlier approaches <ref type="bibr">[21,</ref><ref type="bibr">22]</ref>, we propose a framework to simultaneously learn depth, camera pose, uncertainty and camera intrinsics from input video sequences, as seen in <ref type="figure" target="#fig_0">Figure 1</ref>(a). The depth network predicts a disparity map (inverse depth) D ?1 t of the target image I t and it's corresponding uncertainty map ? t . Simultaneously, the pose network takes any two consecutive frames I s (either I t+1 or I t?1 ) and I t , outputs a 6 Degreesof-Freedom (DOF) rigid transformation T t?s ? SE(3) from the target image plane to the source image plane, containing rotational R ? SO(3) and transitional t ? R 3 information. The camera network takes the pose encoder features as input and outputs the camera intrinsics matrix K, containing principal offset and focal length information. After obtaining these predictions, we reconstruct I t from the source image's point of view (POV). This novel view synthesis involves lifting the target image into 3D using the predicted depth map D t and the intrinsics matrix's inverse K ?1 . Then, using the predicted transformation matrix T t?s , and K, we project the 3D scene onto the source's POV, thus warping the source image into the target image <ref type="bibr">[21,</ref><ref type="bibr">64]</ref>. We call this warped target image I t . Such a transformation from the target image homogeneous coordinates p t to the source ones p s ? R 3 , is summarized by the relation p s</p><formula xml:id="formula_0">! = KRK ?1 D t (p t )p t + Kt [64]</formula><p>, where denotes the sampling operator, forms the backbone of this approach.</p><p>Relying on a static scene assumption, the warped target image I t is compared against the original target image I t , using a photometric error metric L p (I t , I t ), which is minimized. Following previous approaches [21, 64], we use a convex combination of L1 loss and a structural similarity(SSIM) loss <ref type="bibr" target="#b42">[55]</ref> for calculating the photometric error, with ? = 0.85. In order to handle occlusions, we follow Godard et al.'s [21] use of per-pixel minimum reprojection which compares views generated from multiple source frames to the target frame and considers the minimum of the per-pixel photometric error, as in Equation 1. This computation prevents high error values when correspondences are good <ref type="bibr">[21]</ref>.</p><formula xml:id="formula_1">L p (I t , I s ) = min I s L p (I t , I t ) = min I s ? 1 ? SSIM(I t , I t ) 2 + (1 ? ?) I t ? I t 1<label>(1)</label></formula><p>We learn a heteroscedastic Bayesian uncertainty map ? t , for input</p><formula xml:id="formula_2">I t , ? i j ? [0, 1] ? ? i j ? ? t ,</formula><p>capturing the photometric variance caused by specular objects or by sudden illumination change, etc., following from Kendall et al.'s <ref type="bibr" target="#b16">[29]</ref> residual weighting, as shown in Equation 2. Because of such modelling, the pixels corresponding to such brightness changes are down weighted [59] and as a result, the uncertainty values are higher at regions with high photometric reprojection error.</p><formula xml:id="formula_3">L p (I t , I s ) * = min I s L p (I t , I t ) 2? 2 t + 1 2 log ? t + 1.5<label>(2)</label></formula><p>Real world scenes do not always follow a static scene assumption, which is necessary for self-supervised monocular depth estimation. The performance of these methods suffers greatly in presence of static camera or object motion in same direction as camera which can manifest infinite depth results <ref type="bibr">[21]</ref>. To handle such scenarios where the relative motion between camera and objects is zero, we make use of binary mask ? ? {0,1} from [21] to filter out static pixels shown in Equation 3. Here [ ] represents the Iverson bracket. Such static pixels are removed by identifying pixels with higher warped reprojection loss between the warped source frame I t and target frame I t than the unwarped reprojection loss between the source and target frames.</p><formula xml:id="formula_4">? = min I s L p (I t , I s ) &gt; min I s L p (I t , I t )<label>(3)</label></formula><p>In order to encourage the smoothness of disparity estimations, especially in textureless and low-image gradient areas, and for regularizing inverse depth to prevent divergent values, we add a combination of first and second order smoothness losses L s . Equation 4 represents such a disparity smoothness term, weighted by the image's gradients, in order to preserve edges. The first-order gradient term (from) ? 1 ? ? x + ? y , is an L1 penalty on disparity gradients used to account for the depth discontinuities which often occur at strong image gradients <ref type="bibr" target="#b12">[25]</ref>. Adding a second order gradient ? 2 ? ? xx + ? xy + ? yx + ? yy , in addition, encourages better smoothness of these gradients with larger convergence radii, leading to better optimization <ref type="bibr" target="#b36">[49]</ref>. To discourage shrinking of the estimated depth [21], mean normalized inverse</p><formula xml:id="formula_5">depthD * t = D ?1 t / D t is considered, where D ?1 t</formula><p>represents inverse depth or disparity and D t represents the mean disparity.</p><formula xml:id="formula_6">L s = ? 1D * t e ?0.5|? 1 I t | + ? 2D * t e ?0.5|? 2 I t | (4)</formula><p>Final Training Loss. The final SFM objective now, is the combination of the net perpixel minimum photometric loss with uncertainty (Equation 2) and the disparity smoothness loss (Equation 4) given by L t = ?L * p + ? L s . Here, ? is the smoothness regularizer, to be chosen as a hyperparameter. This total loss L t is averaged over the total number of scales used in depth decoder, which is 4 in our work.</p><p>Network details. The depth and pose networks follows U-net type encoder-decoder architecture with skip connections in between the encoder and the decoder which facilitates better learning of deep abstract features along with spatial information. We use ResNext-50 <ref type="bibr" target="#b43">[56]</ref> depth encoder with a cardinality of 32 and base width of 4 and ResNet-50 pose encoder, unless stated otherwise. Following earlier works like [21, <ref type="bibr" target="#b11">24,</ref><ref type="bibr" target="#b14">27,</ref><ref type="bibr" target="#b20">33]</ref> in depth estimation, we also employ pre-trained ImageNet <ref type="bibr" target="#b33">[46]</ref> weights for initialization for the encoder. However, we use weights pre-trained in a semi-weakly supervised fashion as proposed in <ref type="bibr" target="#b45">[58]</ref> for both depth and pose networks unlike earlier works [20, 21, 23], which has improved depth estimation accuracy for us. Depth decoder uses a multiscale architecture to output inverse-depth and uncertainty at 4 scales similar to [21] with two primary modifications: (i) nearest-neighbour interpolation replaced with ESPCN for upsampling following ideas from <ref type="bibr" target="#b35">[48]</ref> and <ref type="bibr">[1]</ref> and, (ii) modification of final layer in the decoder to output depth uncertainty along with disparity. Pose decoder follows architecture from <ref type="bibr">[21]</ref>. The camera network is inspired from [22] but the architecture is slightly different. More network details are mentioned in supplementary section. The features obtained from the pose encoder are passed to this camera network, followed by squeeze operation to reduce the number of channels to 256. Two independent 3x3 convolution layers stem from this squeeze layer to estimate the normalized focal lengths f Efficient Sub-pixel convolutions. Since we use an encoder-decoder architecture for depth, it is necessary to upsample the outputs in decoder layers. Earlier approaches like [21, 53, 60, 64] rely on a nearest-neighbour interpolation for faster inference. Such techniques could compromise the output at object boundaries, as they combine the values from foreground and background. Moreover, the filters are not learnable, thus, limiting the upsampling effect. Transpose convolutions, on the contrary, are learnable, but suffer from checkerboard artifacts. To compensate for these artifacts, usually they are initialized with interpolation outputs, which come at the cost of training time. Hence, we make use of ESPCN <ref type="bibr" target="#b35">[48]</ref> to perform convolutional learning at low resolution. We then perform a pixel shuffle operation to upsample only on the final step. To alleviate the checkerboard artifacts caused by random initialization of these filters, we make use of Initialization to Convolution Nearest-neighbour Resize (ICNR) from <ref type="bibr">[1]</ref> which provides an initialization similar to nearest-neighbour. Such operation results in faster convergence and leads to a better minimum compared to transpose convolutions and other interpolation approaches <ref type="bibr" target="#b35">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Setup. Our PyTorch <ref type="bibr" target="#b28">[41]</ref> models were trained on 4 Nvidia GeForce RTX 2080 Ti GPUs, in a distributed setting, using the Adam optimizer <ref type="bibr" target="#b18">[31]</ref>, with a learning rate of 10 ?4 for the first 15 epochs and 10 ?5 for the last 10. The batch size is set to 12 and the default resolution is 640x192 (width x height), unless stated otherwise. The models are trained with data augmentation with a 50% chance of horizontal flips, random brightness (?0.2), random contrast (?0.2), hue jitters (?0.1) and, saturation (?0.2) variations. The input trio of frames at times t ? 1, t, and t + 1, are all applied with the same augmentation settings, but these augmentations are not used for computing the photometric loss. The max. and min. depths are set to 0.1 and 100 respectively. The remaining training parameters are set as in [21]. <ref type="figure" target="#fig_5">Figure 2</ref>: The input images, their corresponding disparity maps, and uncertainty maps (bottom) are shown with blocks. Our depth results are compared with MonoDepth2 [21]), OmegaNet <ref type="bibr" target="#b38">[51]</ref> and PackNet-SFM <ref type="bibr" target="#b10">[23]</ref>. For disparity maps, brighter pixels(orange) indicate nearer regions while darker indicate for farther. It can be observed that our approach shows high-quality depth maps especially at object boundaries. For uncertainty maps, brighter regions(orange) indicate pixels with higher uncertainty. Such uncertainty is more pronounced along object boundaries, when further from the centre of camera, also where brightness change occur, e.g. at reflective surfaces (a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KITTI Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lower is better</head><p>Higher is better The KITTI benchmark has become a de facto standard for depth evaluation in recent times <ref type="bibr" target="#b10">[23]</ref>. Hence, for evaluation, we make use of KITTI 2015 dataset [17] which con-sists of 200 different scenes of driving data captured using RGB cameras along with sparse ground truth depth maps captured by a Velodyne LiDAR sensor. As the pre-processing step, we follow the previous works [64] to remove static frames. This resulted in 39810 monocular raw images for training and 4424 for validation. In all the evaluation experiments, we make use of seven different metrics as proposed by Eigen et al. <ref type="bibr" target="#b58">[13]</ref>, which are now commonly used and accepted evaluation indicators for comparison among various depth estimation approaches <ref type="bibr">[63]</ref>. A depth cap of 80 meters is used in all our experiments.</p><p>Evaluation. In this section, we compare the effectiveness of the proposed method with SOTA using KITTI test dataset and Eigen split <ref type="bibr" target="#b56">[11]</ref>. Eigen split contains 697 images with reprojected LIDAR points, which are used for evaluation. For depth evaluation, per-image median ground truth scaling (as proposed in <ref type="bibr">[21]</ref>) is utilized, to handle the unavailability of scale information in self-supervised monocular approaches [21, 64]). No further postprocessing steps were involved. We compare the results of our two variants, one without uncertainty estimation (model V1) and the other with uncertainty estimation (model V2) for evaluation on KITTI benchmark. As shown in <ref type="table">Table 1</ref>, both our models achieved significant gains in both uncalibrated and calibrated scenarios in comparison to their corresponding baseline methods -OmegaNet <ref type="bibr" target="#b38">[51]</ref> and MonoDepth2 [21] respectively. In an uncalibrated scenario, where ground truth camera intrinsics are not known, our camera network predicts such values taking in a pair of images each time. Using such method, along with the proposed considerations, produced results which outperform the SOTA not only in uncalibrated but also in calibrated settings. In addition, a vivid improvement in depth map quality over baseline methods can be observed from qualitative results shown in <ref type="figure" target="#fig_5">Figure 2</ref>. This shows the effectiveness of the proposed method, especially at object boundaries, primarily due to the proposed ESPCN layers. Notably, we achieved such competing metric scores using ResNext-50 <ref type="bibr" target="#b43">[56]</ref> encoder, on contrary to high-end architectures used in <ref type="bibr" target="#b10">[23]</ref> and <ref type="bibr" target="#b14">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lower is better</head><p>Higher is better  <ref type="table" target="#tab_5">Table 2</ref>: KITTI benchmark comparison with our baseline methods at different resolutions Input resolution. In this experimental study, we have considered two common groups with image resolutions of 640x192 and 1024x320 pixels, respectively. In accordance to the reported results in <ref type="table" target="#tab_5">Table 2</ref>, the proposed method (model V2) outperforms baselines ([21], <ref type="bibr" target="#b38">[51]</ref>) and other novel methods, at lower and higher resolutions. While using a higher input image resolution has not improved results significantly in the baseline methods, it has showed positive effect with our approach. Moreover, this improvement due to a change in resolution is highest among the group. This manifests the effectiveness of efficient sub-pixel convolutions at higher resolutions. Even when regressed to an inferior ResNet-18 pose encoder   <ref type="table" target="#tab_7">Table 3</ref>: Ablation studies on KITTI benchmark with baseline referring to [21], where "Intr" indicates training with intrinsics network. "SR" indicates the usage of super resolution, and "Uncert" stands for training with uncertainty.</p><p>architecture at higher resolution owing to computational limitations, the results achieved along all performance metrics are the best among all our models. KITTI ablation experiments. To evaluate the significance of each of the components proposed in our approach, we have performed ablation studies on KITTI test data with Eigen split <ref type="table" target="#tab_7">(Table 3</ref>). We evaluated depth by varying three main components in this study: (i) with intrinsics network implying that external feed of pre-calibrated camera intrinsic parameters is unnecessary, (ii) using efficient sub-pixel convolutions and, (iii) with uncertainty estimation. Our approach with the networks proposed and training camera intrinsics has itself improved results compared to baseline (here [21]). It can be observed that our model trained without any of our components is not better than MonoDepth2, suggesting that these 3 contributions lead to improvement. The main improvement is associated with the usage of efficient subpixel convolutions in the depth decoder, resulting in sharper depth maps at higher resolutions. Crucially, this improvement is even more valuable in terms of achieved performance when trained camera network. This finding is a key result for proving the effectiveness of our proposed method to leverage its usage for potentially unlimited datasets including videos from internet. Adding the heteroscedastic aleatoric uncertainty estimation, has not improved depth estimation significantly, where the results remained roughly the same.</p><p>Generalization capability. We perform qualitative evaluation using our model trained only on KITTI dataset, while testing on unseen data from unseen videos similar to <ref type="bibr">[8]</ref>. However, we make use of two additional datasets -NuScenes <ref type="bibr">[6]</ref> and Audi a2d2 [19], along with a random road scene video from internet. Qualitative results of this experiment are demonstrated in <ref type="figure" target="#fig_2">Figure 3</ref>. Our model results are particularly exceptional along the object boundaries and in identifying thin objects compared to our baseline method <ref type="bibr">[21]</ref>. This evidence supports our claim that our method is generalizable to learn structures and scenes collected using different cameras, and also without the knowledge of camera intrinsics. For demonstrating that our method can be used for videos even without ground-truth intrinsics, we have gathered 57 stock videos from the internet, predominantly covering road scenes which involve minimal distortion. The URLs of these videos will be made public. After pre-processing, it resulted in 7623 and 1373 images for training and test respectively. This dataset is particularly challenging as it involves different camera setups, also with varying camera heights, obstacles present throughout some videos (for e.g. car hood), highly varying image resolutions, and even weather conditions (day, rain, snow, night etc.) as they are from the wild. For training, these images are resized to have a width of 640 and then, centre cropped to 640x192. This step assures that the input image is not stretched, although some vertical field of view is lost. Images from KITTI 2015 dataset [17], without intrinsics, involving similar pre-processing steps are used together for training along with this internet data to handle the variability and unavailability of large training dataset from the internet alone. Qualitative results, are demonstrated in <ref type="figure" target="#fig_3">Figure 4</ref>, justify that the proposed approach can be used even when camera parameters are not known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Videos from Internet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose a self-supervised monocular approach to learn depth for pinhole camera with minimum distortion, even without explicitly given intrinsics. This makes it possible to utilize videos from wild for training, which can be leveraged to obtain potentially unlimited data. Besides, we demonstrate that by incorporating ESPCN instead of interpolation in the depth decoder, robust and sharper depth maps can be obtained. In addition, we include an approach to estimate pixel-wise depth uncertainties, which could play a crucial role in robotics and autonomous driving tasks in tacking optimal decisions. Our contributions lead to significant improvement on KITTI benchmark even when ground truth camera intrinsics data is not available. Furthermore, through our experiments, we show that our best results are obtained at a higher input resolution of 1024x320 and to our knowledge, this result significantly outperforms the other state-of-the-art self-supervised monocular depth estimation methods on KITTI benchmark among methods which neither use ground truth data nor online refinement techniques for training.  <ref type="table">Table 1</ref>: The evaluation of results on KITTI test data with improved ground truth. The corresponding metrics are taken from their corresponding papers and <ref type="bibr">[2]</ref>, where the best results along each metric are highlighted in bold. and ResNext-101(P) architectures of our proposed method, where (B) indicates baseline approaches and (P) stands for our proposed method. All the ResNet and ResNext architectures (from both (B) and (P)) employ pretrained weights on ImageNet <ref type="bibr">[9]</ref> while Packnet <ref type="bibr">[5]</ref> is not using any pretraining. From findings reported in <ref type="table" target="#tab_5">Table 2</ref>, the proposed method involving ResNet-18(P) depth encoder with just 11.69 million(M) trainable parameters, has achieved comparable performance in regard to the high-end baseline (B) methods including Packnet with 128 M trainable parameters. Meanwhile, the proposed approach with ResNext-50 (P) depth encoder outperforms all the baseline (B) approaches significantly, while ResNext-101(P) depth encoder improve results even further. Comparison of depth performance with various architectures and their corresponding trainable parameters is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. Comparison with MonoDepth2 <ref type="bibr">[2]</ref>. In this experiment, we compare our model results with our baseline, MonoDepth2 <ref type="bibr">[2]</ref> to show the significance of our approach. <ref type="table" target="#tab_7">Table 3</ref> shows the results of baseline and the proposed method at both, lower (640x192) and higher input resolutions (1024x320) images. Here, the baseline <ref type="bibr">[2]</ref> has not shown greater improvement with change in input resolution, nevertheless, our approach manifested significant improvement due to our use of sub-pixel convolutions for upsampling. We have also compared our  approach with baseline at edge cases involving thin objects, object boundaries and colour saturated regions, and this qualitative analysis is demonstrated in <ref type="figure" target="#fig_5">Figure 2</ref>. Our model, especially for higher resolution input, exemplified better depth results at thin objects and the objects nearer to the camera. Also, sharper depth results can be observed at object boundaries, credit to our efficient sub-pixel convolutions which enabled better super-resolution for upsampling in between decoder layers. In colour saturated regions (such as in <ref type="figure" target="#fig_5">Figure 2(d))</ref>, the low resolution model of MonoDepth2 shows notable artifacts which are somewhat compensated with the higher resolution model. Both our models, however, do not exhibit any such artifacts and result in smoother and robust depth maps compared to the baseline approach. To support the above listed statements, we additionally rendered 3D (X,Y,Z) perspectives for the generated depth images <ref type="figure" target="#fig_2">(Figure 3)</ref>, where Z coordinate stands for the normalized value for each pixel of the generated depth image. As seen from <ref type="figure" target="#fig_2">Figure 3</ref> our approach shows less uncertainty (less dense points) in boundary regions of the objects. In other words, the proposed approach predicts better the class assignment for each depth point, e.g. vehicle vs background. Moreover, it demonstrates a better accuracy in terms of the reconstructed shape of scene objects.    <ref type="bibr">[3]</ref> consists of 22 driving sequences, of which only 11 sequences have ground truth trajectories and the remaining 11 sequences are unlabelled. We make use of only labelled sequences for both, training and evaluation. Similar to the prior approaches <ref type="bibr">[2,</ref><ref type="bibr" target="#b60">15]</ref>, we first train our models on sequences -00 to 08 using the KITTI Odometry split which has 36630 images and then evaluate separately on sequence 09 (1591 images) and sequence 10 (1201 images) data. Performance Metrics. For odometry evaluation, we use Absolute Trajectory Error (ATE) as performance metric as proposed in <ref type="bibr" target="#b60">[15]</ref>. This metric computes the Root Mean Square Error (RMSE) between the ground truth and the estimated trajectory. ATE can be computed with any of these: translation, rotation or velocity. All these parameters return the same single error metric, making it easier to compare <ref type="bibr" target="#b59">[14]</ref>. We opt translation to compute ATE similar to previous approaches <ref type="bibr">[2]</ref> for odometry evaluation. <ref type="figure" target="#fig_2">Figure 3</ref>: The 3D qualitative comparison of our approach in contrary to MonoDepth2 <ref type="bibr">[2]</ref> in 3D (X, Y, Z as in point cloud datasets) format, where the normalized color value for every single pixel from the predicted images is used as depth (Z) value during 3D visualization. In addition, the rotation of the point cloud data has been applied during renderings in order to emphasize the improvements of the proposed approach over the baseline, namely less uncertainty on neighbour object boundary regions as well as more accurate reconstruction of shapes.</p><p>Evaluation. For odometry evaluation, we follow <ref type="bibr">[2]</ref> and predict on five-frame test sequence, which is used in <ref type="bibr" target="#b60">[15]</ref>. We calculate the ATE of our predictions for each of the four pairs in a five-frame test sequence, and then combine them to report the metric's mean and standard deviation. Results of odometry evaluation are reported in <ref type="table" target="#tab_9">Table 4</ref>. For this experiment, we have employed default settings of the proposed method. Our proposed approach exemplifies similar performance with our baseline <ref type="bibr">[2]</ref>. There is a significant improvement in depth results when compared with our baseline, however, not in pose, which is predominantly impacted by the use of common encoder for both camera pose and intrinsics estimation. In addition, when compared with other approaches <ref type="bibr">[7,</ref><ref type="bibr">8,</ref><ref type="bibr" target="#b58">13]</ref>, similar to <ref type="bibr">[2]</ref>, the improvement is not significant because of (i) not using custom architectures for odometry evaluation as this particular pose network is designed particularly for better depth estimation and (ii) using only two frames for pose estimation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Camera Intrinsics Evaluation</head><p>We opt for KITTI odometry dataset <ref type="bibr">[3]</ref> for camera intrinsics evaluation similar to our odometry evaluation (refer subsection A.2). Here, the model is trained with sequences -00 to 08, and we use sequences 09 and 10 for evaluation. The mean and standard deviation of focal length parameters ( f x and f y ) and principal offsets (x 0 and y 0 ) are reported with each of the evaluation sequences, and these parameters are compared with the ground truth data. Distortion parameters are not modelled, hence are not reported. For this setup, we use ResNext-50 depth encoder and ResNet-18 pose encoder, with input resolution to the framework as 1024x320. The quantitative evaluation findings for this experiment are shown in <ref type="table" target="#tab_11">Table 5</ref>. It can be observed that the results are impacted significantly by using a light-weight camera network and also due to the usage of a common encoder for camera pose and intrinsics. Nevertheless, the depth estimation results were improved, with this intrinsics estimations, when compared to given camera intrinsics as input (shown in KITTI ablation experiments as part of the main paper).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Implementation Details</head><p>Depth Network. We embed efficient sub-pixel convolutions into the depth decoder for better upsampling in contrary to the nearest-neighbour interpolation followed in previous approaches <ref type="bibr">[</ref>   in extracting feature information necessary to perform super-resolution. Just before the final pixel shuffle operation, the output is arranged to have the channels multiplied by a factor of, r 2 where r is the upsample factor. This output is finally shuffled across the pixels along channels to obtain image super-resolution.This operation is shown in <ref type="figure" target="#fig_3">Figure 4(a)</ref>. The depth network, in overall, takes in a single image as input and outputs disparity or inverse-depth along with a pixel-wise uncertainty map. These outputs are obtained at four scales, i.e. at 1/8, 1/4, 1/2 and 1, scaling with input image resolution as shown in <ref type="figure" target="#fig_3">Figure 4(b)</ref>. Such multi-scale decoder is employed to prevent the gradient locality of the bilinear sampler  <ref type="table">Table 6</ref>: The network details of depth decoder used in our approach. Here, k indicates kernel size, p indicates padding, chns indicate number of channels for that layer, res stands for the downsampling factor, where 1 indicates full resolution, input stands for the input to that layer, [ps] indicates pixel shuffle operation with an upsample factor of 2, econv represents inputs from various levels of the encoder, and lastly activation indicates the activation function used for that corresponding layer. and to prevent the loss objective getting stuck at local minimum <ref type="bibr">[2]</ref>. The disparity map obtained is later converted to depth using D = 1/(a * ? + b) where D represents depth and ? represents the disparity map. Here, a and b are constants which are chosen such as to constrain the depth values between 0.1 and 100. The detailed network architecture is shown in <ref type="table">Table 6</ref>. inconv0 inconcat -----inconv1, inconv2 - <ref type="table">Table 7</ref>: The network details of the pose decoder and the camera network are shown. Here, k indicates the kernel size, p indicates the padding parameter, chns stands for the number of channels for that particular convolutional layer, res corresponds to the downsampling factor, where 1 indicates full resolution, input stands for the input to that layer, econv5 is the output from pose encoder, and lastly activation shows the used activation function</p><p>Pose and Camera Networks. The pose encoder is modified to take in a pair of images as input, which are concatenated channel-wise. Also, the weights in the expanded filter are divided by 2, which makes the output of the convolution similar to ResNet default output with a single image input <ref type="bibr">[2]</ref>. The camera network follows a light-weight architecture to estimate camera intrinsic parameters. Horizontal focal length ( f x ) and Horizontal centre (x 0 ) parameters are initialized to W /2, and Vertical focal length ( f y ) and Vertical centre (y 0 ) parameters are initialized to H/2 for better convergence following <ref type="bibr">[4]</ref>, where W and H represents width and height of the input image respectively. Network details of both pose and camera networks are shown in <ref type="table">Table 7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) shows the overall pipeline used in our approach. (b) depicts the inference model where the depth model can be used to predict inverse-depth and uncertainty from a single image. The pose and camera models can be used to predict camera pose and intrinsics respectively from two temporal frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>x , f y and principal offsets c x , cy in horizontal and vertical axes, normalized by the input image's width and height respectively. These are concatenated to output the camera intrinsics matrix K = f x 0 c x 0 f y c y 0 0 The softplus activation function f (x) = log (1 + exp (x)) is used to avert negative values for focal lengths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Unseen images from (a) Nuscenes[6], (b) Audi a2d2 [19] datasets respectively, and (c) a random video collected from Pexels. Better boundary separation (a), sharp results on thin objects (b) and accurate depth maps (c) suggest better applicability and generalizability of our model, trained just on KITTI dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Test results of our model, trained on multiple image sequences including KITTI and random internet videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Comparison of baseline (B) methods with proposed (P) methods, with absolute relative error against number of trainable parameters in millions(M). Here ResNet-18, ResNet-50 versions of [2] and Packnet from [5] are compared with ResNet-18, ResNext-50 and ResNext-101 depth encoders of our approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>The qualitative comparison of our approach in contrary to MonoDepth2[2]  with (a) near thin objects, (b) with close objects, (c) at object boundaries, (d) colour saturated regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>(a) shows the sub-pixel convolution operation adapted from[10]  depicting three 3x3 2D convolutions followed by a pixel-shuffling operation which rearranges pixels from higher number of channels into higher resolution of width and height. (b) shows the multiscale decoder architecture with 3x3 convolutions (blue) and sub-pixel convolutions (green) predicting outputs at four scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>[ 11 ]</head><label>11</label><figDesc>David Eigen and Rob Fergus. Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture. In Proceedings of the IEEE international conference on computer vision, pages 2650-2658, 2015. [12] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. Advances in neural information processing systems, 27:2366-2374, 2014. [13] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. arXiv preprint arXiv:1406.2283, 2014. Lukas von Stumberg, Rui Wang, and Daniel Cremers. D3vo: Deep depth, deep pose and deep uncertainty for monocular visual odometry. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1281-1292, 2020. [60] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1983-1992, 2018.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Lower is better</cell><cell></cell><cell></cell><cell>Higher is better</cell><cell></cell></row><row><cell>Method</cell><cell cols="7">Abs Rel Sq Rel RMSE RMSE log ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3</cell></row><row><cell>SFMlearner [15] Vid2Depth [7] GeoNet [13] DDVO [12] Ranjan [8] EPC++ [6] MonoDepth2 [2]</cell><cell>0.176 0.134 0.132 0.126 0.123 0.120 0.090</cell><cell>1.532 0.983 0.994 0.866 0.881 0.789 0.545</cell><cell>6.129 5.501 5.240 4.932 4.834 4.755 3.942</cell><cell>0.244 0.203 0.193 0.185 0.181 0.177 0.137</cell><cell>0.758 0.827 0.833 0.851 0.860 0.856 0.914</cell><cell>0.921 0.944 0.953 0.958 0.959 0.961 0.983</cell><cell>0.971 0.981 0.985 0.986 0.985 0.987 0.995</cell></row><row><cell cols="8">Single-View Depth. In The IEEE Conference on Computer Vision and Pattern Recog-nition (CVPR), June 2019. 4137-4145, 2015. ceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages multi-label energy optimization for fisheye image rectification and calibration. In Pro-[62] Mi Zhang, Jian Yao, Menghan Xia, Kai Li, Yi Zhang, and Yaping Liu. Line-based Brox, and Javier Civera. CAM-Convs: Camera-Aware Multi-Scale Convolutions for [14] Jose M. Facil, Benjamin Ummenhofer, Huizhong Zhou, Luis Montesano, Thomas tional Journal of Computer Vision, 78(1):89-105, 2008. catadioptric line images and sphere images with applications to calibration. Interna-[61] Xianghua Ying and Hongbin Zha. Identical projective geometric properties of central Ours without uncert.(V1) 0.082 0.451 3.666 0.126 0.925 0.986 0.996 Ours with uncert.(V2) 0.081 0.427 3.532 0.124 0.928 0.986 0.996</cell></row><row><cell cols="8">[63] ChaoQiang Zhao, QiYu Sun, ChongZhen Zhang, Yang Tang, and Feng Qian. Monocu-lar depth estimation based on deep learning: An overview. Science China Technological Sciences, pages 1-16, 2020.</cell></row><row><cell>1231-1237, 2013.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>):</cell></row><row><cell cols="8">[19] Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh Durgesh, Andrew S Chung, Lorenz Hauswald, Viet Hoang Pham, Maximilian M?hlegg, Se-bastian Dorn, et al. A2d2: Audi autonomous driving dataset. arXiv preprint arXiv:2004.06320, 2020.</cell></row><row><cell cols="8">[20] Cl?ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 270-279, 2017.</cell></row></table><note>[15] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002-2011, 2018.[16] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In European conference on computer vision, pages 740-756. Springer, 2016.[17] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driv- ing? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354-3361. IEEE, 2012.[18] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 32(11[21] Cl?ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE interna- tional conference on computer vision, pages 3828-3838, 2019.[22] Ariel Gordon, Hanhan Li, Rico Jonschkowski, and Anelia Angelova. Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras. In Proceedings of the IEEE International Conference on Computer Vision, pages 8977- 8986, 2019.[59] Nan Yang,[64] Tinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1851-1858, 2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Table showing evaluation of results when the model is trained with different types of encoders. (B) indicates baseline and (P) indicates proposed method. Evaluation of results is done on KITTI test data. Except for the ResNet-18 approaches, for all other, ResNet-50 pose encoder is employed.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>The evaluation results on KITTI benchmark with change in input resolution. For 1024x320 resolution, only ResNet-18 pose encoder is used due to computational limitations. For 640x192 resolution, ResNext-50 pose encoder is used.</figDesc><table><row><cell>A.2 Odometry Evaluation</cell></row></table><note>KITTI Odom Dataset. KITTI Odometry dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Pose evaluation performed with KITTI Odom data. Sequences 09 and 10 of this dataset are used for evaluation purposes, where #frames indicate the number of input frames for pose network. The mean ATE along with its standard deviation is reported for each approach.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Our camera network estimates of the intrinsics parameters, normalized by the input image resolution, are compared against the ground truth data. The mean of these normalized intrinsics estimates with standard deviations are reported for each parameter computed on all the images from KITTI Odom 09 and 10 sequences.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2110.14347v1 [cs.CV] 27 Oct 2021</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This research has been supported by the computer vision team in project SpuMo E2D (funding reference number 03EFLSL015) under HTW Saar, and in part by the autonomous driving team (Agents and Simulated Reality department) of German Research Center for Artificial Intelligence (DFKI GmbH, Saarbr?cken).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this document, we provide additional evaluations and ablations experiments in support of our work in Section A and expand on the network architectures and implementation details in Section B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Experiments</head><p>A.1 Depth KITTI Improved Ground Truth. For Eigen split evaluation proposed by Eigen et al. <ref type="bibr">[1]</ref> on KITTI benchmark, the authors have made use of reprojected LIDAR points for depth evaluation. However, such approach does not account for other important aspects like occlusions, moving objects and ego-vehicle motion <ref type="bibr">[2]</ref>. Hence, we make use of improved ground truth from <ref type="bibr" target="#b56">[11]</ref>, in which occlusions are handled by considering stereo pairs and high quality depth maps which are produced by accumulating five consecutive frames. This results in 652 images, which accounts to 93% of images of the Eigen split. Depth is capped to 80 meters similar to Eigen split evaluation and the error metrics also remain the same. The evaluation results using such improved ground truth data are reported in <ref type="table">Table 1</ref>, where both variants (V1 and V2) of our proposed method significantly outperform other benchmark methods in all evaluation metrics. Interestingly, when compared in between, version V2, namely the model with uncertainty estimation, exemplified better performance than model V1 without uncertainty estimation, in all the evaluation metrics. This infers the effectiveness of the uncertainty component on depth estimation.</p><p>Depth encoder variation. Using high-end architectures for the depth estimation is advantageous, nevertheless, would come at the cost of higher training and inference times. Here, we report the impact of such architectures on depth performance by varying depth encoders. Therefore, we compare depth performance of ResNet-18(B), Resnet-50(B) architectures from <ref type="bibr">[2]</ref> and Packnet(B) architecture from, <ref type="bibr">[5]</ref> with ResNet-18(P), ResNext-50 (P)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02937</idno>
		<title level="m">Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">High quality monocular depth estimation via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibraheem</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Enhancing self-supervised monocular depth estimation with traditional visual odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Andraghetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panteleimon</forename><surname>Myriokefalitakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pier</forename><forename type="middle">Luigi</forename><surname>Dovesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belen</forename><surname>Luque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Pieropan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="424" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric properties of central catadioptric line images and their application in calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Jo?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helder</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Araujo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1327" to="1333" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deepcalib: a deep learning approach for automatic intrinsic calibration of wide field-of-view cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bogdan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Charles</forename><surname>Bazin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGGRAPH European Conference on Visual Media Production</title>
		<meeting>the 15th ACM SIGGRAPH European Conference on Visual Media Production</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Depth prediction without the sensors: Leveraging structure for unsupervised learning from monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Casser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeren</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8001" to="8008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-supervised learning with geometric constraints in monocular video: Connecting flow, depth, and camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7063" to="7072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Computational principles of mobile robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Dudek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jenkin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning monocular depth by distilling cross-domain stereo networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="484" to="500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pm-huber: Patchmatch with huber regularization for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Heise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Klose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alois</forename><surname>Knoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02025</idno>
		<title level="m">Spatial transformer networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-supervised monocular trained depth estimation using self-attention and discrete disparity volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4756" to="4765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5574" to="5584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervising the new with the old: learning sfm from sfm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Klodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="698" to="713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevhen</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorg</forename><surname>Stuckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Desc: Domain adaptation for depth estimation via semantic consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Lopez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.01579</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Single view stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mude</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxiu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="155" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Hr-depth: High resolution self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.07356</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">What makes good synthetic training data for learning disparity and optical flow estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaus</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caner</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="942" to="960" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Calibrating self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mccraith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07714</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single view point omnidirectional camera calibration from planar grids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Rives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 2007 IEEE International Conference on Robotics and Automation</title>
		<meeting>2007 IEEE International Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3945" to="3950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d&apos;Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Superdepth: Self-supervised, superresolved monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rare?</forename><surname>Ambru?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9250" to="9256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the uncertainty of self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3227" to="3237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Towards robust monocular depth estimation: Mixing datasets for zero-shot crossdataset transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Lasinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hafner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A simple calibration procedure for fish-eye (high distortion) lens camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shishir</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1994 IEEE international Conference on Robotics and Automation</title>
		<meeting>the 1994 IEEE international Conference on Robotics and Automation</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3422" to="3427" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Feature-metric loss for selfsupervised learning of depth and egomotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="572" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Distilled semantics for comprehensive scene understanding from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filippo</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Pierluigi Zama Ramirez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><forename type="middle">Di</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattoccia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katerina</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07804</idno>
		<title level="m">Sfm-net: Learning of structure and motion from video</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Toward hierarchical selfsupervised monocular absolute depth estimation for autonomous driving applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Ang</surname></persName>
		</author>
		<idno type="DOI">10.1109/IROS45743.2020.9340802</idno>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2330" to="2337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Billion-scale semi-supervised learning for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>I Zeki Yalniz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mahajan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00546</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2283</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cl?ment</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3828" to="3838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Robust Stereo Visual Odometry through a Probabilistic Combination of Points and Line Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Ojeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Gonzalez-Jimenez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Depth from videos in the wild: Unsupervised monocular depth learning from unknown cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanhan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8977" to="8986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Allan Raventos, and Adrien Gaidon. 3d packing for self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitor</forename><surname>Guizilini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rares</forename><surname>Ambrus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudeep</forename><surname>Pillai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2485" to="2494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06125</idno>
		<title level="m">Every pixel counts++: Joint learning of geometry and motion with 3d holistic understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Competitive collaboration: Joint unsupervised learning of depth, camera motion, optical flow and motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Balles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12240" to="12249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sparsity invariant cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Miguel</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A tutorial on quantitative trajectory evaluation for visual (-inertial) odometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7244" to="7251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1851" to="1858" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
