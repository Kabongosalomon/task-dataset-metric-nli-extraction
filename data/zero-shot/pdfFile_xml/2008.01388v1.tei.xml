<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Cross-Modal Alignment for Multi-Person 3D Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jogendra</forename><surname>Nath</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kundu</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambareesh</forename><surname>Revanur</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitthal</forename><surname>Govind</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><forename type="middle">Mysore</forename><surname>Waghmare</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Venkatesh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh Babu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Video Analytics Lab</orgName>
								<orgName type="institution">Indian Institute of Science</orgName>
								<address>
									<settlement>Bangalore</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Cross-Modal Alignment for Multi-Person 3D Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a deployment friendly, fast bottom-up framework for multi-person 3D human pose estimation. We adopt a novel neural representation of multi-person 3D pose which unifies the position of person instances with their corresponding 3D pose representation. This is realized by learning a generative pose embedding which not only ensures plausible 3D pose predictions, but also eliminates the usual keypoint grouping operation as employed in prior bottom-up approaches. Further, we propose a practical deployment paradigm where paired 2D or 3D pose annotations are unavailable. In the absence of any paired supervision, we leverage a frozen network, as a teacher model, which is trained on an auxiliary task of multi-person 2D pose estimation. We cast the learning as a cross-modal alignment problem and propose training objectives to realize a shared latent space between two diverse modalities. We aim to enhance the model's ability to perform beyond the limiting teacher network by enriching the latent-to-3D pose mapping using artificially synthesized multi-person 3D scene samples. Our approach not only generalizes to in-the-wild images, but also yields a superior trade-off between speed and performance, compared to prior top-down approaches. Our approach also yields state-of-the-art multi-person 3D pose estimation performance among the bottom-up approaches under consistent supervision levels.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-person 3D human pose estimation aims to simultaneously isolate individual persons and estimate the location of their semantic body joints in a 3D space. This challenging task can aid a wide range of applications related to human behavior understanding such as surveillance <ref type="bibr" target="#b67">[58]</ref>, group activity recognition <ref type="bibr" target="#b41">[32]</ref>, sports analytics <ref type="bibr" target="#b21">[12]</ref>, etc. Existing multi-person pose estimation approaches can be broadly classified into two categories namely, top-down and bottom-up. In top-down approaches <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b47">38]</ref>, the first step is to detect persons using an offthe-shelf detector which is followed by predicting a 3D pose for each person using a single-person 3D pose estimator. Such approaches <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> are usually incapable of inferring absolute camera-centered distance of each human as they miss the global context. In contrast, the bottom-up approaches <ref type="bibr" target="#b45">[36]</ref>   Ours Sup.</p><p>On MuPoTS-3D dataset <ref type="figure">Fig. 2</ref>. We achieve a superior trade-off between speed and performance against the prior arts (Rogez <ref type="bibr" target="#b59">[50]</ref>, Rogez* <ref type="bibr" target="#b58">[49]</ref>, Mehta <ref type="bibr" target="#b45">[36]</ref>, Moon <ref type="bibr" target="#b47">[38]</ref>). See Section 5 joints, and then assign them to each individual person via a keypoint grouping operation. The bottom-up approaches yield suboptimal results as compared to top-down approaches, but have a superior run-time advantage against top-down methods <ref type="bibr" target="#b27">[18,</ref><ref type="bibr" target="#b57">48]</ref>. In this paper, we aim to leverage the computational advantage of bottom-up approaches while effectively eliminating the keypoint grouping operation via an efficient 3D pose representation. This results in a substantial gain in performance while maintaining an optimal computational overhead. Almost all multi-person 3D pose estimation approaches access large-scale datasets with 3D pose annotations. However, owing to the difficulties involved in capturing 3D pose in wild outdoor environments, many of the 3D pose datasets are captured in indoor settings. This restricts diversity in the corresponding images (i.e. limited variations in background, attires and pose performed by actors) <ref type="bibr" target="#b23">[14,</ref><ref type="bibr" target="#b24">15]</ref>. However, 2D keypoint annotations <ref type="bibr" target="#b28">[19,</ref><ref type="bibr" target="#b29">20]</ref> are available even for in-the-wild multi-person outdoor images. Hence, several approaches aim to design 2D-to-3D pose lifters <ref type="bibr">[4,</ref><ref type="bibr" target="#b43">34]</ref> by relying on an off-the-shelf, Image-to-2D pose estimator. Such approaches usually rely on geometric self-consistency of the projected 2D pose obtained from the lifter output, while imposing adversarial prior to assure plausible 3D pose predictions <ref type="bibr">[4,</ref><ref type="bibr" target="#b25">16]</ref>. However, the generalizability of such approaches is limited owing to the dataset bias exhibited by the primary Image-to-2D pose estimator which is trained in a fully-supervised fashion.</p><p>Our problem setting. Consider a scenario where a pretrained Image-to-2D pose estimator is used for the goal task of 3D pose estimation. There are two challenges that must be tackled. First, a pretrained Image-to-2D estimator would exhibit a dataset bias towards the training data. Thus, the deployment of such a model in an unseen environment (e.g. dancers in unusual costumes) is not guaranteed to result in an optimal performance. This curtails the learning of the 2D-to-3D pose lifter, especially in the absence of paired images from the unseen environment. Second, along with the Image-to-2D model, one can not expect to be provided with its labeled training dataset owing to proprietary <ref type="bibr" target="#b48">[39,</ref><ref type="bibr" target="#b40">31]</ref> or even memory <ref type="bibr">[9,</ref><ref type="bibr" target="#b37">28]</ref> constraints. Considering these two challenges, the problem boils down to performing domain adaptation <ref type="bibr" target="#b33">[24]</ref> by leveraging the pretrained Image-to-2D network (a.k.a the teacher network) in an unsupervised fashion, i.e. in the absence of any paired 2D or 3D pose annotations. Further, acknowledging the limitations of existing 2D-to-3D pose lifters, we argue that the 3D pose lifter should access the latent convolutional features instead of the final 2D pose output; owing to its greater task transferability <ref type="bibr" target="#b39">[30]</ref>.</p><p>Though it is easy to obtain unpaired multi-person images, acquiring a dataset of unpaired multi-person 3D pose is inconvenient. To this end, we synthesize multi-person 3D scenes by randomly placing the single-person 3D skeletons in a 3D grid as shown in <ref type="figure" target="#fig_2">Fig. 3B</ref>. We also formalize a systematic way to synthesize single-person 3D pose by accessing plausible ranges of parent-relative joint angle limits provided by biomechanic experts. This eradicates our dependency even on an unpaired 3D skeleton dataset. Our idea of creating artificial samples stems from the concept of domain randomization <ref type="bibr" target="#b53">[44,</ref><ref type="bibr" target="#b64">55]</ref> which is shown to be effective for generalizing deep models to unseen target environments. The core hypothesis is that the multi-person 3D pose distribution characterized by the artificially synthesized 3D pose scenes would subsume the unknown target distribution. Note that the proposed joint angle sampling would allow sampling of minimal implausible single-person poses as it does not adhere to the strong pose-conditioned joint angle priors formalized by Akhter et al . <ref type="bibr">[2]</ref>.</p><p>We posit the learning framework as a cross-modal alignment problem (see <ref type="figure" target="#fig_0">Fig. 1</ref>). To this end, we aim to realize a shared latent space V, which embeds samples from varied input modalities <ref type="bibr">[6]</ref>, such as unpaired multi-person image I and unpaired multi-person 2D pose K (i.e. camera projection on multi-person 3D pose P). Our training paradigm employs an auto-encoding loss on P (via K ? V ? P pathway), a distillation loss on K (via I ? V ? P ? K pathway) and an additional adaptation loss (non-adversarial) to minimize the cross-modal discrepancy at the latent space V. In further training iterations, we stop the limiting distillation loss and fine-tune the model on a self-supervised criteria based on the equivariance property <ref type="bibr" target="#b60">[51]</ref> of spatial-transformations on the image and its corresponding 2D pose representation. Extensive experiments of our ablations and comparisons against prior arts establish the superiority of this approach. In summary, our contributions are as follows:</p><p>-We propose an efficient bottom-up architecture that yields fast and accurate single-shot multi-person 3D pose estimation performance with structurally infused articulation constraints to assure valid 3D pose output. In absence of paired supervision we cast the learning as a cross-modal alignment problem and propose training objectives to realize a shared latent space between two diverse data-flow pathways.</p><p>-We enhance the model's ability to perform even beyond the limiting teacher network as a result of the enriched latent-to-3D-pose mapping using artificially synthesized multi-person 3D scene samples. -Our approach not only yields state-of-the-art multi-person 3D pose estimation performance among the prior bottom-up approaches but also demonstrates a superior trade-off between speed and performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-person 2D pose estimation works can be broadly classified into top-down and bottom-up methods. Top-down methods such as <ref type="bibr">[5,</ref><ref type="bibr" target="#b50">41,</ref><ref type="bibr" target="#b20">11,</ref><ref type="bibr" target="#b65">56]</ref> first detect the persons in the image and then estimate their poses. On the other hand, bottom-up methods <ref type="bibr" target="#b49">[40,</ref><ref type="bibr" target="#b55">46,</ref><ref type="bibr" target="#b22">13,</ref><ref type="bibr">3,</ref><ref type="bibr" target="#b51">42]</ref> predict the pose of all persons in a single-shot. Cao et al . <ref type="bibr">[3]</ref> use a non-parametric representation Part Affinity Field (PAF) and Part Confidence Map (PCM) to learn association between 2D keypoints and persons in the image. Similarly, Kocabas et al . <ref type="bibr" target="#b27">[18]</ref> proposed a bottom-up approach using pose residual network for estimating both keypoints and human detections simultaneously. Many approaches have been proposed for solving the problem of single-person 3D human pose estimation <ref type="bibr" target="#b62">[53,</ref><ref type="bibr" target="#b35">26,</ref><ref type="bibr" target="#b34">25,</ref><ref type="bibr" target="#b36">27,</ref><ref type="bibr" target="#b52">43,</ref><ref type="bibr" target="#b66">57]</ref>. Vnect <ref type="bibr" target="#b46">[37]</ref> is the first realtime 3D human pose estimation work that infers the pose by parsing location-maps and jointwise heatmaps. Martinez et al . <ref type="bibr" target="#b43">[34]</ref> proposed an effective approach to directly lift the ground-truth 2D poses to 3D poses. Few methods have been proposed so far for Multi-person 3D pose estimation. In <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref>, Rogez et al . proposed a top-down approach based on localization, classification and regression of 3D joints. These modules are pipelined to predict the final pose of all persons in the image. Mehta et al . <ref type="bibr" target="#b45">[36]</ref> proposed a single-shot approach to infer 3D poses of all people in the image using PAF-PCM representation. To handle occlusions, they introduced Occlusion Robust Pose Maps (ORPM) which allows full body pose inference under occlusions. Moon et al . <ref type="bibr" target="#b47">[38]</ref> proposed the first top-down camera-centered 3D pose estimation. Their framework contains three modules: DetectNet localizes multiple persons in the image, RootNet estimates camera-centered depth of root joint and PoseNet estimates root relative 3D pose of the cropped person. In RootNet, they use pinhole camera projection model to estimate absolute camera-centered depth. Dabral et al . <ref type="bibr">[7]</ref> proposed a 2D to 3D lifting based approach for camera-centric predictions. Rogez et al . <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50]</ref> and Moon et al . <ref type="bibr" target="#b47">[38]</ref> crop the detected person instances from the image and they do not leverage the global context information. All prior state-of-the-art works <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b59">50,</ref><ref type="bibr" target="#b45">36,</ref><ref type="bibr">7,</ref><ref type="bibr" target="#b47">38]</ref> require paired supervision. See <ref type="table" target="#tab_0">Table 1</ref> for a characteristic comparison against prior works. Cross-modal distillation. Gupta et al . <ref type="bibr">[10]</ref> proposed a novel method for enabling cross-modal transfer of supervision for tasks such as depth estimation. They propose alignment of representations from a large labeled modality to a sparsely labeled modality. In <ref type="bibr" target="#b61">[52]</ref>, Spurr et al . demonstrated the effectiveness of cross-modal alignment of latent space for the task of hand pose estimation. In a related work <ref type="bibr" target="#b54">[45]</ref>, Pilzer et al . proposed an unsupervised distillation based depth estimation approach via refinement of cycle-inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approaches</head><p>Our prime objective is to realize a learning framework for the task of multiperson 3D pose estimation without accessing any paired data (i.e. images with the corresponding 2D or 3D pose annotations). To achieve this, we plan to distill the knowledge from a frozen teacher network which is trained for an auxiliary task of multi-person 2D landmark estimation. Furthermore, in contrast to the general top-down approaches in fully-supervised scenarios, we propose an effective single-shot, bottom-up approach for multi-person 3D pose estimation. Such an architecture not only helps us maintain an optimal computational overhead but also lays a suitable ground for cross-modal distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Aiming to design a single-shot end-to-end trainable architecture, we draw motivation from the real-time object detectors such as YOLO <ref type="bibr" target="#b57">[48]</ref>. The output layer in YOLO divides the output spatial map into a regular grid of cells. The multi-dimensional vector at each grid location broadly represents two important attributes. Firstly, a confidence value indicating the existence of an object centroid in the corresponding input image patch upon registering the grid onto the spatial image plane. Secondly, a parameterization of the object properties, such as class probabilities and attributes related to the corresponding bounding box. In similar lines, for multi-person 3D pose estimation, each grid location of the output layer represents a heatmap indicating existence of a human pelvis location (or root) followed by a parameterization of the corresponding root-relative 3D pose. Here, the major challenge is how to parameterize root-relative human 3D pose in the efficient manner. We explicitly address it in the following subsection.</p><p>3.1.1 Parameterizing 3D pose via pose embedding. Root relative human 3D pose follows a complex structured articulation. Moreover, defining a parameterization procedure without accounting for the structural plausibility of the 3D pose would further add up to the inherent 2D to 3D ambiguity. Acknowledging this, we aim to devise a parameterization which selectively decodes anthropomorphically plausible human poses spanning a continuous latent manifold (see <ref type="figure" target="#fig_2">Fig. 3A</ref>). One of the effective ways to realize the above objective is to train a generative network <ref type="bibr" target="#b30">[21]</ref> which models the most fundamental form of human pose variations. Thus, we disentangle the root-relative pose p r into its rigid and non-rigid factors. The non-rigid factor, also known as the canonical pose p c is designed to be view-invariant. The rigid transformation is defined by the parameters c as required for the corresponding rotation matrix. In further granularity, according to the concept of forward kinematics <ref type="bibr" target="#b69">[60]</ref>, movement of each limb is constrained by the parent-relative joint-angle limits and the scale invariant fixed relative bone lengths. Thus, the unit vectors corresponding to each joint defined at their respective parent-relative local coordinate system <ref type="bibr">[2]</ref> is regarded as the most fundamental form of 3D human pose which is denoted by p l . Note that, the transformation p l ? p c is a fully-differentiable series of forward kinematic operations. We train a generative network <ref type="bibr" target="#b31">[22,</ref><ref type="bibr" target="#b32">23]</ref> following the learning procedure of adversarial auto-encoder {?, ? } (AAE <ref type="bibr" target="#b42">[33]</ref>) on samples of p l acquired from either a MoCap <ref type="bibr" target="#b0">[1]</ref> dataset or via a proposed Artificial-posesampling procedure (see <ref type="figure" target="#fig_2">Fig. 3A</ref>). We consider a uniform prior distribution i.e. by the teacher network or from the ground-truth depending on its availability. For each selected location r i , the corresponding ? i and c i are pooled from the relevant grid location to decode (via ? ) the corresponding root-relative 3D pose, p i r . First, the canonical pose, p i c is obtained by applying forward kinematics (denoted as FK in <ref type="figure">Fig. 4B</ref> in module M) on the decoded local vectors obtained from the pose embedding ? i . Following this, p i r is obtained after performing rigid transformation using c i , i.e. T R in <ref type="figure">Fig. 4B</ref>. Finally, the global 3D pose scene,</p><formula xml:id="formula_0">P = {p i g } N i=1</formula><p>, is constructed by translating the root-relative 3D poses to their respective root locations in the camera centered global coordinate system, i.e. T G in <ref type="figure">Fig. 4B</ref>. The 3D translation for each person i is obtained using (r i x , r i y , d i ), where r i x and r i y are the X and Y component obtained as a transformation of the spatial root location r i . In <ref type="figure">Fig. 4B</ref>, the series of fixed (non-trainable) differentiable operations to obtainP from the CNN output s is denoted as M. A weak perspective camera transformation, T K , of P provides us the corresponding multi-person 2D key-points denoted byk p . Inference. During inference, (r i x , r i y ) is obtained from the heatmap channel h predicted at the output of F. We follow the non-maximum suppression algorithm inline with Cao et al . <ref type="bibr">[3]</ref> to obtain a set of spatial root locations belonging to each person. Thus, the inference pathway during testing is as follows,P = M?H?E(I).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning cross-modal latent space</head><p>We posit the learning framework as a cross-modal alignment problem. Moreover, we aim to realize a shared latent space, V which embed samples from varied modality spaces, such as multi-person image I, multi-person 2D pose K, and multi-person 3D pose P. However, in absence of labeled samples (or paired samples) an intermediate representation of the frozen teacher network is treated as the shared latent embedding. Following this, separate mapping networks are trained to encode or decode the latent representation to various source modalities. Note that, the teacher network already includes the mapping of image to the latent space, E : I ? V and latent space to multi-person 2D pose, F : V ? K. We train two additional mapping networks, viz. a) multi-person 2D pose to latent space, G : K ? V and b) latent space to multi-person 3D pose,</p><formula xml:id="formula_1">(M ? H) : V ? P. Also note that, (T K ? M ? H) : V ? K.</formula><p>Available Datasets. We have access to two unpaired datasets viz. a) unpaired multi-person images I ? D I and b) unpaired multi-person 3D pose samples P syn ? D syn . Though it is easy to get hold of unpaired multi-person images, acquiring a dataset of unpaired multi-person 3D pose is inconvenient. Acknowledging this, we propose a systematic procedure to synthesize a large-scale multi-person 3D pose dataset from a set of plausible single-person 3D poses. A multi-person 3D pose sample constitute of a certain number of persons (samples of p i l ) with random rigid transformations (c i ) placed at different locations (i.e. r i x , r i y , d i ) in a 3D room. This is illustrated in <ref type="figure" target="#fig_2">Fig. 3B</ref>. Here, samples of p l can be obtained either from a MoCap dataset or by following Artificial-pose-sampling.</p><p>Broadly, we use two different data-flow pathways as shown in <ref type="figure">Fig. 4</ref>. Here, we discuss how these pathways support an effective cross-modal alignment.</p><p>a) Cross-modal distillation pathway for I ? D I . The objective of distillation pathway is to instill the knowledge of mapping an input RGB image to the corresponding multi-person 2D pose (i.e. from the teacher networkk q = F(v) where v = E(I)) into the newly introduced 3D pose estimation pipeline. Here,k q is obtained after performing bipartite matching inline with Cao et al . <ref type="bibr">[3]</ref>. We update the parameters of H by imposing a distillation loss betweenk q and the perceptively projected 2D posek p</p><formula xml:id="formula_2">= T K ? M ? H(v), i.e. L distl = |k p ?k q |.</formula><p>b) Auto-encoding pathway for P syn ? D syn . In the auto-encoding pathway, the objective is to reconstruct back the synthesized samples of multi-person 3D poses via the shared latent space. Owing to the spatially structured latent representation, for each non-spatial P syn we first generate the corresponding multi-person spatial heatmap (HM) and Part Affinity Map (PAF) inline with Cao et al . <ref type="bibr">[3]</ref>, denoted by m syn in <ref type="figure">Fig. 4A</ref>. Note that m syn represents the 2D keypoint locations of k syn which is the obtained as the camera projection of the P syn . Following this, we obtainP = M ? H(?) where? = G(m syn ). Parameters of both G and H are updated to minimize L recon = |P syn ?P |. c) Cross-modal adaptation. Notice that, H is the only common model updated in both pathways. Here, L distl is computed against the noisy teacher prediction that too in the 2D pose space. In contrast, L recon is computed against the true ground-truth 3D pose thus devoid of the inherent 2D to 3D ambiguity. As a result of this disparity, the model H differentiates between the corresponding input distributions, i.e. between P(v) and P(?), thereby learning separate strategies favouring the corresponding learning objectives. To minimize this discrepancy, we rely on the frozen teacher sub-network F. We hypothesize that, the energy computed via F, i.e. |F(?)?m syn | would be low if the associated input distribution of F, i.e. P(v = E(I)) aligns with the output distribution of G, i.e. P(? = G(m syn )).</p><p>Accordingly, we propose to minimize L adapt = |F ? G(m syn ) ? m syn | to realize an effective cross-modal alignment. Training phase-1 We update G and H to minimize all the three losses discussed above, i.e. L recon , L distl and L adapt each with different Adam <ref type="bibr" target="#b26">[17]</ref> optimizers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning beyond the teacher network</head><p>We see a clear limitation in the learning paradigm discussed above. The inference performance of the final model is limited by the dataset bias infused in the teacher network. We recognize L distl as the prime culprit which limits the ability of H by not allowing it to surpass the teacher's performance. Though one can rely on L recon to further improve H, this would degrade performance in the inference pathway as a result of increase in discrepancy between v and?. Considering this, we propose to freeze G thereby freezing its output distribution P(? = G(m syn )) in the second training phase.</p><p>Furthermore, in absence of the regularizing L distl we use a self-supervised consistency loss to regularize H for the unpaired image samples. For each image I we form a pair (I, I ) where I = T s (I) is the spatially transformed version (i.e. image-flip, random-crop, or in-place rotation) of I. Here, T s represents the differentiable spatial transformation. Next, we propose a consistency loss based on the equivariance property <ref type="bibr" target="#b60">[51]</ref> of the corresponding multi-person 2D pose, i.e.</p><formula xml:id="formula_3">L ss = |T s ? T K ? M ? H ? E(I) ? T K ? M ? H ? E ? T s (I)|</formula><p>The above loss is computed at the root-locations extracted using the teacher network for the original image I. Whereas, for I we use the spatial transformation T s on the extracted root locations of the original image. Training phase-2 We update the parameters of H (G is kept frozen from the previous training phase) to minimize two loss terms i.e. L recon and L ss .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe the experiments and results of the proposed approach on several benchmark datasets. Through quantitative and qualitative analysis, we demonstrate the practicality and performance of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>First, we explain the implementation details of synthetic dataset creation. Next, we provide the training details for learning the neural representation.</p><p>3D skeleton dataset. Artificial-pose-sampling is performed by sampling uniformly from joint wise angle limits defined at local parent relative <ref type="bibr">[2]</ref> spherical coordinate system (see <ref type="figure" target="#fig_2">Fig. 3A)</ref> i.e. [? 1 , ? 2 ], and [? 1 , ? 2 ] . For example, right-hip joint ? 1 = ? 2 = ? (i.e. 1-DoF) and ? 1 = ?/3, ? 2 = 2?/3 (See Suppl). Using these predefined limits, we construct a full 3D pose (via FK). A total of 1M poses are <ref type="table" target="#tab_8">Table 2</ref>. Quantitative analysis of different ablations of our approach on MuPoTS-3D. Unpaired means that there is no ground truth annotation available for an image. Paired means that there is a corresponding annotation available for an image. 3DPCK is Percentage of Correct 3D Keypoints predicted within 15cm. (higher 3DPCK is better). "sup." stands for supervision. MuCo-3DHP <ref type="bibr" target="#b45">[36]</ref> is used in fifth column. Red color indicates that configuration is less preferable for low data regime. (Best viewed in color ). Training. First, we train a pose decoder (see Section 3.1.1) either on artificial pose dataset (? arti ) or MoCap 3D dataset (? mocap ). The AAE modules are trained using a batch size of 32, with a learning rate of 1e-4 using Adam optimizers till convergence (See Suppl). The decoder ? is frozen for rest of the training. For training the neural representation, we choose the pretrained network of Cao et al . <ref type="bibr">[3]</ref> as the teacher network. We consider upto stage-1 "conv5-4-CPM" layer of <ref type="bibr">[3]</ref> as E. We concatenate the predictions of both heatmap and Part Affinity Field branches to obtain an embedding space of size 28?28?1024. We consider module F as from stage-1 "conv5-5-CPM" layer upto stage-2 "Mconv7-stage2" layer of <ref type="bibr">[3]</ref>. Using this teacher model, we train the modules {H, G} by minimizing the losses L distl , L recon , L adapt , L ss using separate Adam optimizers for each of the losses. We use a learning rate of 1e-4 upto 100k iterations and 1e-5 for the following 500k iterations while using a fixed batch size of 8 throughout the training. Further, we use batches of images from D syn and D I in alternate iterations while training the network. The input image size for D I is 224?224?3 and input PAF representation <ref type="bibr">[3]</ref> is of shape 28 ? 28 ? 43 for D syn . All transformations T K , T R , T G have been implemented using TensorFlow and are designed to be completely differentiable end-to-end. We have trained the entire pipeline on a Tesla-V100 GPU card in Nvidia-DGX station (See Suppl). <ref type="table">Table 3</ref>. Comparison of 3DPCK rel on MuPoTS-3D sequences. Our methods are highlighted in gray background color. Underlined values indicate that our unpaired learning (Ours-Us) approach performs better on that sequence. Ours-Fs (fully-supervised) achieves state-of-the-art in bottom up methods. Ours-Us approach performs competitively even when compared with prior fully supervised approaches.</p><p>Methods S1 S2 S3 S4 S5 S6 S7 S8 S9 S10 S11 S12 S13 S14 S15 S16 S17 S18 S19 S20 Avg Accuracy for all groundtruths Rogez <ref type="bibr" target="#b58">[49]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Studies</head><p>In order to study the effectiveness of our method, we perform extensive ablation study by varying levels of supervision, as shown in <ref type="table" target="#tab_8">Table 2</ref>. For all the ablations, we have used MuCo-3DHP images <ref type="bibr" target="#b45">[36]</ref> as I. Depending on the supervision setting, we either access none (for unsup. setting), a small fraction (semi sup. setting) or a complete set (full sup. setting) of 3D annotations in MuCo-3DHP dataset. Ours-Us (Using Unpaired images only): Our baseline model (see <ref type="table" target="#tab_8">Table 2</ref>) trained without accessing any annotated labels gives an overall 3DPCK of 53.3. We observe that L adapt + L ss gives a non-trivial boost of 4-6%. This demonstrates the importance of cross-modal alignment and self-supervised consistency.</p><p>Ours-Ws (Weakly supervised ): When supervised weakly by 2D ground truth (L 2D = |k p ?k p |), our approach obtains a 3DPCK of 67.9. Further, the performance of our approach that uses ? arti is on par with our performance with ? mocap indicating that ? arti has rich representation space, equivalent to ? mocap .</p><p>Ours-Fs (Fully supervised ): When we access the full training dataset of MuCo-3DHP and impose a 3D reconstruction loss by using L 3D = |P ?P |, we obtain a 3DPCK of 75.8, which is significantly better than the prior arts. <ref type="table">Table 6</ref>. Comparison of Absolute MPJPE (lower is better) on Human 3.6M evaluated on S9 and S11. The table is split into three parts: single-person 3D pose estimation approaches (No. 1 to 6), multi-person 3D pose estimation top-down approaches (No. 7 to 10), multi-person 3D pose estimation bottom-up approaches (No. 11 and 12). Our approach performs better than previous bottom-up multi-person pose estimation methods.    <ref type="bibr" target="#b58">[49,</ref><ref type="bibr" target="#b45">36,</ref><ref type="bibr" target="#b47">38]</ref>.</p><p>In the root-relative system, a joint keypoint prediction is considered as a correct prediction if the joint is present within the range of 15cm. For evaluating absolute location of human joints in camera coordinates, <ref type="bibr" target="#b47">[38]</ref> proposed 3DPCK abs in which a prediction is considered correct when the joint is within the range of 25cm. In <ref type="table">Table 3</ref> we have compared the results of our method against the state-of-theart methods. Our fully supervised approach yields state-of-the-art bottom-up performance (75.8 v/s Mehta <ref type="bibr" target="#b45">[36]</ref> 70.8) while being faster than the top-down approaches. In <ref type="table">Table 4</ref> we present joint-wise 3DPCK on MuPoTS-3D dataset. We compare against <ref type="bibr" target="#b47">[38]</ref> on 3DPCK abs metric in <ref type="table">Table 5</ref> as it is the only work that reported on 3DPCK abs . <ref type="figure" target="#fig_8">Fig. 5</ref>. Qualitative results on MuPoTS-3D (1st row), MS-COCO (2nd row), and "inthe-wild" images (3rd row) of our approach. Our approach is able to effectively handle inter-person occlusion and make reliable predictions for crowded images. Pink box highlights some failure cases. 1st row: presence of self-occlusion, 2nd row: rare multiperson interaction and 3rd row: joint location ambiguity.</p><p>Human 3.6M <ref type="bibr" target="#b23">[14]</ref> This dataset consists of 3.6 million video frames of single person 3D poses that have been collected in laboratory setting. In <ref type="table">Table 6</ref>, we show results on Protocol 2: MPJPE calculation on after alignment of root. As shown in <ref type="table">Table 6</ref>, our approach outperforms bottom-up multi-person works <ref type="bibr">(Mehta [36]</ref> 69.9 v/s Ours 67.9) and performs on par with top-down approaches <ref type="bibr">(Rogez [50]</ref> 63.5 and Dabral <ref type="bibr">[7]</ref> 65.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Fast and accurate inference. In <ref type="table" target="#tab_5">Table 8</ref>, we provide runtime complexity analysis of our model in comparison to prior works. All top-down approaches <ref type="bibr" target="#b47">[38,</ref><ref type="bibr" target="#b58">49,</ref><ref type="bibr" target="#b59">50]</ref> depend on a person detector model. Hence these methods have low fps in comparison to bottom-up approaches (See <ref type="figure">Fig. 2)</ref>. We outperform the previous bottom-up approach by a large margin in terms of 3DPCK, fps and model size. We achieve a superior real-time computation capability because our approach effectively eliminates the keypoint grouping operation usually performed in bottom-up approaches <ref type="bibr">[3,</ref><ref type="bibr" target="#b45">36]</ref>. All fps numbers reported in <ref type="table" target="#tab_5">Table 8</ref> were obtained on a Nvidia RTX 2080 GPU. In <ref type="table" target="#tab_5">Table 8</ref>, we also show the total number of parameters of the model used during inference time.</p><p>Is student network limited by teacher network? In <ref type="table" target="#tab_4">Table 7</ref> we report results of 2D pose estimation on both teacher model (k q ) and student model (k p ) by evaluating IoU, 2D-MPJPE and 2D-PCK on MuPoTS-3D dataset. We observe that a student model trained by minimizing L distl alone performs sub-optimally in comparison to the teacher. This result is not surprising as the student model is restricted by knowledge of the teacher model. However, in our complete loss formulation (Ours-Fs) our approach outperforms the teacher on the 2D task, validating the hypothesis that our approach can learn beyond the teacher network.</p><p>Qualitative results. We show qualitative results on the MS-COCO <ref type="bibr" target="#b38">[29]</ref>, MuPoTS-3D and frames taken from YouTube videos and other "in-the-wild" sources in <ref type="figure" target="#fig_8">Fig. 5</ref>. As seen in the <ref type="figure" target="#fig_8">Fig. 5</ref>, our model produces correct predictions on images with different camera viewpoints and on those images containing challenging elements such as inter-person occlusion. These qualitative results show that our model has generalized well on unseen images.</p><p>Two-stage refinement for performance-speed tradeoff. Top-down frameworks yield better performance as compared to the bottom-up approach while having substantial computational overhead <ref type="bibr" target="#b27">[18]</ref>. To this end we realize a hybrid framework which would provide flexibility based on the requirement. For example, the current single-shot (or single-stage) operates in a substantial computational superiority. To further improve its performance, we propose an additional pass of each detected persons through the full pipeline ( <ref type="figure" target="#fig_4">Fig. 6</ref>). Here, we train a separate H for the single-person pose estimation task which is operated on the cropped image patches of single human instances obtained from the Stage-1 predictions. By training the H network we obtain a 3DPCK of 76.9 (v/s Ours-Fs 75.8) with a runtime fps of 16.6 (v/s Ours-Fs 21.2 fps). (See <ref type="table">Table 3</ref> and <ref type="figure">Fig. 2)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper we have introduced an unsupervised approach for multi-person 3D pose estimation by infusing structural constraints of human pose. Our bottomup approach has real-time computational benefits and can estimate the pose of persons in camera-centric coordinates. Our method can benefit from future improvements on 2D pose estimation works in a plug-and-play fashion. Extending such a framework for multi-person human mesh recovery and extraction of appearance related mesh texture remains to be explored in future. Acknowledgement. This project is supported by a Indo-UK Joint Project (DST/INT/UK/P-179/2017), DST, Govt. of India and a WIRIN project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Cross-Modal Alignment for Multi-Person 3D Pose Estimation</head><p>The supplementary material is organized as follows:   We train an AAE to learn single-person pose embedding. The proposed framework for training the AAE using encoder ?, decoder ? and adversarial discriminator Disc is shown in <ref type="figure" target="#fig_0">Fig. 1B</ref>. The main motivation behind learning the single-person pose embedding is to disentangle enforcement of structural plausibility constraints for 3D human pose in the subsequent final task of multi-person pose estimation. This parameterization of 3D pose embedding not only guarantees generation of anthropomorphically plausible pose, but also follows the structural constraints <ref type="bibr" target="#b0">[1]</ref> such as joint angle limits, limb interpretation restrictions, etc.</p><p>a) View-invariant Canonical 3D Pose Representation. Let p g be a 3D pose in the global coordinate system, as shown in <ref type="figure" target="#fig_0">Fig. 1A(a)</ref>. The root-relative 3D pose p r (origin of coordinate system is located at root joint) as shown in <ref type="figure" target="#fig_0">Fig. 1A(b)</ref> is obtained after subtracting human pelvis location (a.k.a root) from p g . Then, the rigid transformation on p r , disentangles the root-relative pose into view invariant canonical pose p c . Let us consider a plane passing through the neck, left-hip and right-hip joints. Letn be a normal to this plane. In the canonical coordinate system, which is defined by axes X c , Y c and Z c in <ref type="figure" target="#fig_0">Fig. 1A(c)</ref>, the vectorn is canonically aligned with +ve X axis. This alignment makes the canonical pose p c view-invariant. Note that, the root-relative pose p r can be recovered from p c by performing a simple rigid transformation described by the corresponding rotation matrix. The rotation matrix itself can be described with Euler angles used to rotate p r to form p c . b) Local 3D pose representation. Inline with <ref type="bibr">[10]</ref>, the forward kinematic formulation expresses each body joint with respect to its parent joint. In the local coordinate system for each joint (see <ref type="figure" target="#fig_0">Fig. 1A(d)</ref>), the kinematic 3D structure of the human skeleton can be studied by capturing the limitations of joint movements relative to the corresponding parent joint. Further, every parent-child limb is assigned a fixed bone length. For example, the bone-length of the limb connecting the left-shoulder and left-elbow is fixed for all poses. A 3D pose expressed using this kinematic formulation is termed as local pose p l and is shown in <ref type="figure" target="#fig_0">Fig. 1A(d)</ref>. As p l is obtained from p c , it is both view-invariant and bone-length scale invariant. The local pose coordinate system X l , Y l and Z l is defined as follows: Each joint (except neck, pelvis, left-hip and right-hip) is expressed with respect to its parent joint, or in other words, the origin of the coordinate system is fixed at the parent joint. The coordinate axes are obtained by performing Gram-Schmidt orthogonalization of a vector joining parent-child and a normaln to the plane spanning neck, left-hip and right-hip joints. The transformation from canonical pose p c to local pose p l is given as T L : p c ? p l .</p><p>c) Training AAE. The architecture of AAE (see <ref type="figure" target="#fig_0">Fig. 1B</ref>) is based on a kinematic tree of limb-connections mentioned in <ref type="bibr">[3]</ref>. The pose embedding ? pose is 32 dimensional vector and obtained through tanh nonlinearity. We choose to train an AAE with an aim to learn pose embedding in continuous manner. This generative approach allows us to uniformly sample any random vector as ? ? U[?1, 1] 32 and predicts an anthropomorphically plausible human pose when decoded through ? . The plausible and implausible pose pattern obtained after sampling pose embedding is shown in <ref type="figure" target="#fig_0">Fig. 1C</ref>. We employ discriminator Disc to distinguish between real pose embedding ? real and pose embedding sampled through ? rand ? U[?1, 1] 32 . In order to enforce learning of an one-to-one mapping in a generative adversarial setup, we add cyclic reconstruction loss on both canonical pose p c and pose embedding ? pose as follows:</p><formula xml:id="formula_4">L cyc =| p c ? p c | + | ? pose ? ? pose | (1) Where, p c = F K ? ? ? ? ? T L (p c ), ? pose = ? ? ? (? pose )</formula><p>, FK: p l ? p c and T L : p c ? p l . We train encoder ? using L cyc and decoder ? using L cyc + L adv inline with <ref type="bibr">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>In this section, we describe network architectures of E, F, H, H , G.</p><p>Module E: We use a pre-trained model of Cao et al . <ref type="bibr">[2]</ref> as a teacher model as shown in <ref type="figure">Fig. 2</ref>. The teacher model uses VGG19 backbone, followed by separate branches of fully convolutional layers for heatmap and PAF. The concat operation concatenates the outputs of these branches into an output of shape 28 ? 28 ? 1024.</p><p>Module F: We use upto stage-2 of Cao et al . <ref type="bibr">[2]</ref> as F. As seen in <ref type="figure">Fig. 2</ref>, there are 8 convolutional layers in both HM and PAF branches. Each branch takes the input from the corresponding output branch of E in the distillation pathway and output of G in the auto-encoding pathway <ref type="figure">(Fig. 4</ref> of the main paper). Module G: It consists of five 7 ? 7 convolutional layers as shown in <ref type="figure">Fig. 2</ref>. The input m syn is of 28 ? 28 ? 43 dimension where 15 channels correspond to each of the 15 joints and 28 channels correspond to PAF representation for all limbs.</p><p>Module H and H : Both H and H network modules share the same architecture. These modules take an embedding v as an input and predict a tensor of shape 14 ? 14 ? 39. Further, these modules have a Channel-wise Fully Connected layer (Ch-FC) (similar to <ref type="bibr">[7]</ref>) where the layer connects all nodes of a given input channel to all nodes of corresponding output channel. In our architecture, this layer takes 7 ? 7 ? 128 as input tensor shape and outputs tensor of the same shape. Since each of the 128 channels has a spatial dimension of 7?7, the Ch-FC layer consists of 128 fully connected layers with 49 input nodes and 49 output nodes in each layer. The final layer of H uses an activation of tanh which ensures that the output space of H results in plausible 3D pose prediction (via ? ). All other layers in the module H use Leaky ReLU activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Differentiable transformation operations in M</head><p>Module M consists of frozen 3D pose embedding decoder ? , forward kinematics operation (FK), pose 3D rigid transformation T R and 3D scene composition by translating multiple root-relative 3D poses T G . a) Forward kinematics (FK) p l ? p c . Using forward kinematics, the local pose predicted by ? , is converted into view-invariant canonical 3D pose <ref type="bibr" target="#b0">[1]</ref>. b) Rigid rotation transformation T R : p c ? p r . Module H predicts sine and cosine angle components for 3 angle parameters (Euler angles, denoted as c) required to perform rigid rotation. Using the Euler angles, the canonical pose p c is transformed to the root-relative pose p r as described in Section 1. c) Global scene composition T G : p r ? p g . Using the predicted 2D rootkeypoints r x , r y and the depth d, the net translation of the pose is computed as a function of (r x , r y , d). This translation is performed on 3D pose of each person as inferred in the neural representation (i.e. where a root-joint can be inferred).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other implementation details</head><p>We develop a differentiable camera module with fixed configuration (focal lengths and center of camera are fixed based on input image size) for projecting the 3D scene. The unpaired 3D poses are normalized for keeping the bone length ratio fixed. As discussed previously, this dataset is used for training the pose decoder ? and also used for creating multi-person 3D skeleton scenes D syn . We first pretrain H using L distl for about 15k iterations before imposing all losses. Our phase-1 of training requires 450k iterations to converge. After training for 450k iterations, phase-2 of our training is started. As discussed in main paper, in phase-2 of our training, we impose only L ss and L recon while keeping G frozen.</p><p>3 Artificial-pose-sampling Artificial poses are created by sampling from joint-angle ranges specified by a biomechanic expert. These joint-angle limits are described in the local parentrelative system on the canonical pose representation (see <ref type="figure" target="#fig_0">Fig. 1</ref>). Therefore, the poses that are sampled from these angle limits provide us with diverse canonical poses. As described in Section 1, these poses can be used to train the AAE and to create the D syn , in a completely unsupervised setting where a 3D human pose dataset is inaccessible. In this section, we describe the sampling procedure and provide an analysis of the reliability of the Artificial-pose-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sampling Procedure</head><p>We use the joint-angle limits defined per joint in the local coordinate system and visualize the limits in <ref type="figure" target="#fig_2">Fig. 3A</ref>. As shown in <ref type="figure" target="#fig_2">Fig. 3A</ref>, every joint can be completely described in a spherical coordinate system using two angle limits (azimuth and elevation). We represent the angles as a range in azimuth [? 1 , ? 2 ] where ?180 ? &lt; ? ? 180 ? and elevation [? 1 , ? 2 ] where 0 ? ? ? ? 180 ? . As described in the Section 1, certain joints, such as the right hip joint has only 1DoF while some joints such as the neck joint has 0DoF. Note that 3D keypoint locations of the left hip and the left shoulder joints can be inferred in canonical pose directly without sampling, because the pelvis joint and neck joint are the mid-points of the hip joints and shoulder joints respectively.</p><p>There is one limitation in describing joint angle ranges in the spherical coordinate system: angle limits for certain joints span beyond the 180 ? limit of ?. For such joints we propose to use angle ranges that span on the opposite side (beyond 180 ? into negative ?) of the spherical coordinate system. For example, the ? range for the right shoulder joint is 120 ? and spans from ? 1 = 120 ? , but ? 2 goes beyond the 180 ? . Therefore, we set ? 2 to a value to a value that is equivalent to 240 ? (which is equal to -120 ? ).</p><p>We create artificial single-person pose dataset by sampling from these joint angle limits for all joints applying bone lengths, followed by forward kinematics operation to construct a canonical pose. For obtaining a variety of root-relative poses, we apply random rotation transformation operations on canonical poses.  <ref type="figure" target="#fig_2">Fig. 3</ref>. Single-person artificial pose dataset is created by sampling uniformly from joint wise angle limits defined at local parent-relative coordinate system <ref type="bibr" target="#b0">[1]</ref>. A. Since angle limits of left-body joints are symmetric to right-body joints, we present only right joints. Neck joint and right-hip joint have 0,1 DoF respectively. B. The artificial pose dataset subsumes all plausible poses and could contain a small fraction of implausible poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Analysis of artificial poses</head><p>Although sampled artificial poses may have certain degree of implausibility, because each joint angle is sampled independently of pose <ref type="bibr" target="#b0">[1]</ref>, we find that the artificial pose dataset subsumes all plausible poses <ref type="bibr">[8,</ref><ref type="bibr">9]</ref> (see <ref type="figure" target="#fig_2">Fig. 3B</ref>). This ensures that the AAE learns rich representations in embedding space ?. Our experimental analysis shown in Section 4.2 (in the main paper) confirms that having a certain degree of implausibility does not adversely affect the performance. Hence, if we are not provided an access to any unpaired 3D poses, our approach would still perform reliably by Artificial-pose-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Additional results</head><p>a) Results on 3DPW dataset. The 3D-Poses-in-the-Wild (3DPW) <ref type="bibr">[5]</ref> dataset consists of challenging outdoor in-the-wild video sequences. Compared to the MuPoTS-3D dataset, the 3DPW dataset contains larger volume of video sequences and outdoor scenes. In order to evaluate the generalizability of our model, we evaluate on the test set containing 24 sequences and show the results under the protocol All-Test-mode. Note that, as per the All-Test-mode protocol, we do not use 3DPW train set and 3DPW validation set for training our model. We use Mean Per-Joint Position Error (MPJPE) and Procrustes Mean Per-Joint Position Error as error metric (PMPJPE). The MPJPE metric is obtained as the average Euclidean distance of joints from corresponding ground-truth joint locations. In PMPJPE, the predicted pose is Procrustes aligned with the ground-truth pose before averaging the error over all joints. Therefore, PMPJPE does not consider global orientation of the predicted pose. b) 2D keypoint prediction. In this section, we extend the results presented in the <ref type="table" target="#tab_4">Table 7</ref> of the main paper. We present qualitative results in <ref type="figure">Fig. 4</ref> to compare the 2D keypoint estimation for teacher model and student model (Ours-Fs) on MuPoTS-3D dataset <ref type="bibr">[6]</ref>. The evaluation protocols used for 2D keypoint estimation are Intersection over Union (IoU), 2D-Mean Per-Joint Position Error (2D-MPJPE) and 2D-Percentage of correct keypoints (2D-PCK). IoU is the ratio of area of overlap between the predicted bounding box and the groundtruth bounding box to the area of union of the predicted bounding box and the ground-truth bounding box. 2D-MPJPE is average Euclidean distance between predicted 2D pose keypoints and ground-truth 2D pose keypoints. In 2D-PCK, a predicted keypoint is considered correct if it is present within a range of 25 pixels of ground-truth keypoint. All evaluations are done on keypoints that are shared by both teacher model and student model. c) Additional qualitative results. We present additional qualitative results for MuPoTS-3D dataset <ref type="figure" target="#fig_4">(Fig. 6</ref>), MS-COCO 2D keypoints dataset <ref type="figure">(Fig. 7)</ref>, and wild multi-person images from YouTube and other sources ( <ref type="figure">Fig. 8</ref>). For MuPoTS-3D dataset, we estimate poses of all persons in the image even if ground truth annotation is absent. These results not only show that our model is able to correctly predict depth and pose of persons, but also show generalizability of our model on unseen images.</p><p>5 Limitations of the proposed framework a) Estimation of pelvis (root) location. As discussed in Section 3.1.2 of the main paper, the neural representation of multi-person 3D pose is interpretable only in presence of a pelvis at the corresponding grid location. Therefore, in some scenarios where more than one person shares the same grid location, our model predicts only one pose for all persons in that grid. In rare cases, our model is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Ours Teacher Network <ref type="figure">Fig. 4</ref>. Comparison of teacher model and student model (Ours-Fs) results for the task of 2D keypoint estimation on MuPoTS-3D dataset. Erroneous predictions of the teacher model are highlighted using red ovals. Teacher model either fails to predict keypoint locations or fails to assign keypoint to the correct person. As the student model estimates 2D keypoints by projecting 3D pose, it does not involve any keypoint grouping operation usually employed in bottom-up methods, such as the teacher model. These results show that the our model is able to perform better than the teacher model.</p><p>unable to predict the root joint of some persons in a given image. This limitation is shown in <ref type="figure" target="#fig_8">Fig. 5(a)</ref> and <ref type="figure" target="#fig_8">Fig. 5(b)</ref>. The problem of having two pelvises in the same grid cell can be eliminated either by estimating two poses per grid-cell in the neural-representation or by increasing resolution of the output spatial map discussed in the Section 3.1 of the main paper.</p><p>b) Rare and ambiguous poses. <ref type="figure" target="#fig_8">Fig. 5(c)</ref> shows erroneous prediction on rarely occurring poses like acrobatic flips. The model fails to identify correct global orientation of the pose due to left-right symmetry ambiguity in lifting 2D pose to 3D pose. This limitation is also attributed to visibility of body parts. As the face of the person is not visible in the image of the <ref type="figure" target="#fig_8">Fig. 5(c)</ref>, the model is not able to estimate correct body orientation. Similar example of pose ambiguity is shown in <ref type="figure" target="#fig_8">Fig. 5(d)</ref>. The model predicts an ambiguous pose for the person tagged with c) Perception of depth based on bone lengths. As the proposed model is bone-length scale-invariant, it expects all 3D poses to be of the same size. Due to this, a person with small body-frame is assumed to be located far away from the camera. This drawback is illustrated in <ref type="figure" target="#fig_8">Fig. 5(d)</ref> wherein, a person tagged with dashed red line is assumed to be of the same body-frame size as that of remaining people in the image.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>first locate the body *Equal contribution. | Webpage: https://sites.google.com/view/multiperson3D arXiv:2008.01388v1 [cs.CV] 4 Aug 2020 We aim to realize a shared latent space V which embeds samples from varied input modalities i.e. the unpaired images and unpaired 3D poses. Auto-encoding pathway: K ? V ? P. Distillation pathway: from I ? V ? K to camera projection of I ? V ? P. Inference: I ? V ? P (red shadow).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>A. Learning continuous pose-embedding on MoCap or Artificially sampled pose dataset. B. Creating Dsyn: Each canonical pose pc is rigidly transformed through rotation and translation operation to form random 3D scenes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>U[? 1 , 1 ] 32 .Fig. 4 .</head><label>11324</label><figDesc>This ensures that any random vector ? ? [?1, 1] 32 decodes (via ? ) an anthropomorphically plausible human pose. (See Suppl)In the proposed Artificial-pose-sampling procedure, we use a set of joint angle limits (4 angles i.e. the allowed range of polar and azimuthal angles in the parent relative local pose representation) provided by the biomechanic experts. The angle for each limb is independently sampled from a uniform distribution defined by the above range values (see the highlighted regions on the sphere for each body joint inFig. 3A). Note that, the proposed joint angle sampling would allow sampling of minimal implausible single-person poses as it does not adhere to the pose-conditioned joint angle limits formalized by Akther et al .[2]. (See Suppl) 3.1.2 Neural representation of multi-person 3D pose. The last layer output of the single-shot latent to multi-person 3D pose mapper H, denoted as s, is a 3D tensor of size H ? W ? M (see block M Fig. 4B). The number of channels constitutes of 4 distinct components. The M dimensional vector for each grid location r i constitutes of 4 distinct components viz, a) a scalar heatmap intensity indicating existence of a skeleton pelvis denoted as h i , b) a 32 dimensional 3D pose embedding ? i , c) 6 dimensional rigid transformation parameters c i (sin and cos component of 3 rotation angles), and d) a scalar absolute depth d i associated with the skeleton pelvis. Note that, the last 3 components are interpretable only in presence of a pelvis at the corresponding grid location as denoted by the first component. Here, ? i is obtained through a tanh nonlinearity thus constraining it to decode (via frozen ? AAE from Section 3.1.1) only plausible 3D human poses. The model accesses a set of 2D pelvis key-point locations belonging to each person in the corresponding input image, denoted as {r i } N i=1 . Here, N denotes the total number of persons. These spatial locations are obtained either as estimated Proposed data-flow pathways. Distillation is performed from the teacher, {E, F} to the student {H}. Weights of H and F are shared across both the pathways.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>A hybrid framework for two-stage refinement which treats Stage-1 output as a person detector while Stage-2 performs single-person 3D pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>-Section 1 :</head><label>1</label><figDesc>Adversarial Auto-Encoder-Pose representations and training -Section 2: Architecture and implementation details -Section 3: Artificial poses-Sampling and analysis -Section 4: Additional results on 3DPW dataset and 2D pose estimation -Section 5: Limitations of the proposed framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>/ 1 BFig. 1 .</head><label>11</label><figDesc>pg 3D pose in global coordinate system pr 3D pose in root-relative coordinate system pc Canonical 3D pose representation p l 3D pose in local parent relative coordinate system Network E, F Frozen 2D pose estimation network G Encodes HM-PAF to intermediate representation H Learns neural representation ?, ? Adversarial Auto-Encoder Disc Pose Discriminator used to train AAE Transform -ations FK Forward Kinematics TR Rigid rotation operation on canonical pose TG Translation in global 3D space TL Canonical pose to local pose transformation TK Camera weak perspective projection Representation (space and samples) I Image space V Intermediate representation space P 3D space (of multi-person pose) K 2D space (of multi-person pose) msyn Synthetic HM-PAF representation for 2D pose rx, ry Root (pelvis joint) location s,s Neural representation kp,kq Student and Teacher 2D pose predictions respectively P,P Multi-person 3D pose GT and prediction v,? A sample in V space Others DoF Degrees of Freedom Dsyn Synthetic Dataset ?, ? Angle parameters in spherical coordinate system Disc 0A. 3D pose representation in 4 different coordinate systems-(a) Global, (b) Rootrelative, (c) Canonical and (d) Local. On the right, DoFs are shown for certain joints. Right-hip joint has only one DoF in local coordinate system. B. Training framework for AAE. C. The AAE trained with single-person pose datasets decodes a plausible pose when sampled in U[?1, 1] 32 . blue box: plausible pose, red box: implausible pose 1 Adversarial Auto-Encoder (AAE)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 .</head><label>5</label><figDesc>Limitations of the proposed framework. (a) Multiple pelvises in the same grid cell, (b) Missed pelvis detection, (c) Ambiguous pose and (d) Prediction on small body-frame sized person (d) Ambiguous pose for person tagged with dashed blue line a blue dashed line. In this case, the person's 3D pose cues in the image, such as the feet and facial orientation, are not clearly visible because of the limited spatial information owing to low-resolution of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results on MuPoTS-3D dataset. Note that even if ground truth annotation is absent, we predict poses of all people in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Qualitative results on MS-COCO Qualitative results on in-the-wild images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Characteristic comparison against prior works. without paired supervision implies the method does not need access to annotations.</figDesc><table><row><cell>Methods</cell><cell>Single shot</cell><cell>w/o paired supervision</cell><cell>Camera centric</cell></row><row><cell>Rogez [49]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mehta [36]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rogez [50]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dabral [7]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Moon [38]</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>67.7 49.8 53.4 59.1 67.5 22.8 43.7 49.9 31.1 78.1 50.2 51.0 51.6 49.3 56.2 66.5 65.2 62.9 66.1 59.1 53.8 Rogez[50] 87.3 61.9 67.9 74.6 78.8 48.9 58.3 59.7 78.1 89.5 69.2 73.8 66.2 56.0 74.1 82.1 78.1 72.6 73.1 61.0 70.6 Dabral[7] 85.1 67.9 73.5 76.2 74.9 52.5 65.7 63.6 56.3 77.8 76.4 70.1 65.3 51.7 69.5 87.0 82.1 80.3 78.5 70.7 71.3 Mehta[36] 81.0 60.9 64.4 63.0 69.1 30.3 65.0 59.6 64.1 83.9 68.0 68.6 62.3 59.2 70.1 80.0 79.6 67.3 66.6 67.2 66.0 Ours-Us 76.8 61.8 61.2 63.0 68.7 20.3 67.3 65.2 59.5 83.6 62.4 66.0 52.7 54.9 57.5 73.6 70.9 70.1 70.4 60.8 63.3 Ours-Ws 79.6 62.3 54.2 55.9 69.3 36.1 69.1 67.7 58.4 80.2 75.3 68.7 53.6 56.5 59.6 77.4 76.7 69.6 69.2 64.1 65.2 Ours-Fs 85.5 84.1 66.7 70.5 77.4 68.6 74.8 77.9 69.1 80.0 78.4 75.4 61.1 60.9 71.3 81.4 85.1 73.4 74.9 63.5 74.0 Accuracy only for matched groundtruths Rogez[49] 69.1 67.3 54.6 61.7 74.5 25.2 48.4 63.3 69.0 78.1 53.8 52.2 60.5 60.9 59.1 70.5 76.0 70.0 77.1 81.4 62.4 Rogez[50] 88.0 73.3 67.9 74.6 81.8 50.1 60.6 60.8 78.2 89.5 70.8 74.4 72.8 64.5 74.2 84.9 85.2 78.4 75.8 74.4 74.0 Dabral[7] 85.8 73.6 61.1 55.7 77.9 53.3 75.1 65.5 54.2 81.3 82.2 71.0 70.1 67.7 69.9 90.5 85.7 86.3 85.0 91.4 74.2 Mehta[36] 81.0 65.3 64.6 63.9 75.0 30.3 65.1 61.1 64.1 83.9 72.4 69.9 71.0 72.9 71.3 83.6 79.6 73.5 78.9 90.9 70.8 Ours-Us 76.8 66.6 62.1 63.9 73.5 20.3 67.3 67.8 59.5 83.6 62.4 66.0 56.0 63.5 59.5 75.2 70.9 73.0 73.1 80.8 66.1 Ours-Ws 79.6 66.0 55.5 56.4 74.8 36.1 69.1 69.6 58.4 80.2 75.3 68.7 56.7 66.4 61.6 78.9 76.7 72.8 71.7 83.0 67.9 Ours-Fs 85.5 86.5 66.7 70.5 81.2 68.6 74.8 79.5 69.1 80.0 78.4 75.4 64.0 68.6 73.7 82.9 85.1 76.4 77.4 72.8 75.8 Joint wise analysis of 3DPCK rel on MuPoTS-3D (higher is better). Underlined values indicate that our unpaired learning (Ours-Us) performs better on that joint Methods Hd. Nck. Sho. Elb. Wri. Hip Kn. Ank. Avg Rogez[49] 49.4 67.4 57.1 51.4 41.3 84.6 56.3 36.3 53.8 Mehta[36] 62.1 81.2 77.9 57.7 47.2 97.3 66.3 47.6 66.0 Ours-Us 52.9 79.0 72.2 57.9 45.3 89.9 66.9 45.1 63.3 Ours-Ws 59.9 82.4 78.0 60.6 42.3 91.5 67.2 45.5 65.2 Ours-Fs 63.4 85.5 84.2 70.4 56.8 95.0 78.2 59.0 74.0 We report Camera Centric absolute 3DPCK abs metric on MuPoTS-3D. B/U means Bottomup. fps is runtime frames/second.</figDesc><table><row><cell cols="3">Methods B/U 3DPCKabs (?) fps (?)</cell></row><row><cell>Moon* [38]</cell><cell>9.6</cell><cell>7.3</cell></row><row><cell>Moon [38]</cell><cell>31.5</cell><cell>7.3</cell></row><row><cell>Ours-Us</cell><cell>23.6</cell><cell>21.2</cell></row><row><cell>Ours-Ws</cell><cell>24.3</cell><cell>21.2</cell></row><row><cell>Ours-Fs</cell><cell>28.1</cell><cell>21.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>No. Methods Dir. Dis. Eat Gre. Phon. Pose Pur. Sit SitD. Smo. Phot. Wait Walk WaD. WaP. Avg</figDesc><table><row><cell></cell><cell>Single-person approaches</cell></row><row><cell cols="2">1. Martinez [34] 51.8 56.2 58.1 59.0 69.5 55.2 58.1 74.0 94.6 62.3 78.4 59.1 65.1 49.5 52.4 62.9</cell></row><row><cell>2. Zhou [59]</cell><cell>54.8 60.7 58.2 71.4 62.0 53.8 55.6 75.2 111.6 64.1 65.5 66.0 51.4 63.2 55.3 64.9</cell></row><row><cell>3. Sun [53]</cell><cell>52.8 54.8 54.2 54.3 61.8 53.1 53.6 71.7 86.7 61.5 67.2 53.4 47.1 61.6 53.4 59.1</cell></row><row><cell>4. Dabral [8]</cell><cell>44.8 50.4 44.7 49.0 52.9 43.5 45.5 63.1 87.3 51.7 61.4 48.5 37.6 52.2 41.9 52.1</cell></row><row><cell cols="2">5. Hossain [47] 44.2 46.7 52.3 49.3 59.9 47.5 46.2 59.9 65.6 55.8 59.4 50.4 52.3 43.5 45.1 51.9</cell></row><row><cell>6. Sun [54]</cell><cell>47.5 47.7 49.5 50.2 51.4 43.8 46.4 58.9 65.7 49.4 55.8 47.8 38.9 49.0 43.8 49.6</cell></row><row><cell></cell><cell>Multi-person approaches</cell></row><row><cell>7. Rogez [49]</cell><cell>76.2 80.2 75.8 83.3 92.2 79.0 71.7 105.9 127.1 88.0 105.7 83.7 64.9 86.6 84.0 87.7</cell></row><row><cell>8. Rogez [50]</cell><cell>55.9 60.0 64.5 56.3 67.4 71.8 55.1 55.3 84.8 90.7 67.9 57.5 47.8 63.3 54.6 63.5</cell></row><row><cell>9. Dabral [7]</cell><cell>52.6 61.0 58.8 61.0 69.5 58.8 57.2 76.0 93.6 63.1 79.3 63.9 51.5 71.4 53.5 65.2</cell></row><row><cell>10. Moon[38]</cell><cell>51.5 56.8 51.2 52.2 55.2 47.7 50.9 63.3 69.9 54.2 57.4 50.4 42.5 57.5 47.7 54.4</cell></row><row><cell>11. Mehta[36]</cell><cell>58.2 67.3 61.2 65.7 75.8 62.2 64.6 82.0 93.0 68.8 84.5 65.1 57.6 72.0 63.6 69.9</cell></row><row><cell>12. Ours-Fs</cell><cell>55.8 61.4 58.4 71.9 67.6 65.2 67.7 86.7 84.3 68.3 78.9 67.9 51.8 77.9 55.2 67.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>2D keypoint result comparison of our student model with teacher network on MuPoTS-3D. ? indicates that higher is better and ? indicates that lower is better.</figDesc><table><row><cell>Methods</cell><cell cols="3">IoU (?) 2D-MPJPE (?) 2D-PCK (?)</cell></row><row><cell cols="2">Teacher (Cao [3]) 60.1</cell><cell>38.0</cell><cell>66.6</cell></row><row><cell>Ldistl (no Dsyn) Ours-Fs</cell><cell>51.9 81.6</cell><cell>49.6 19.5</cell><cell>60.3 74.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc></figDesc><table><row><cell>(?)</cell></row></table><note>Complexity analysis on MuPoTS- 3D. B/U stands for bottom-up approach. ? indicates that higher is better and ? indicates that lower is better.Methods B/U 3DPCK (?) fps (?) Model sizeMuCo-3DHP Training Set and MuPoTS-3D Test Set. Mehta et.al [36] proposed creation of training dataset by compositing images from 3D single- person dataset MPI-INF-3DHP [35]. MPI-INF-3DHP is created by marker-less motion capture for 8 subjects using 14 cameras. MuPoTS-3D [36] is a multi- person 3D pose test dataset that contains 20 sequences capturing upto 3 persons per frame. Each of these sequences include challenging human poses and also capture real world interactions of persons. For evaluating multi-person 3D person pose, 3DPCK rel (Percentage of Correct Keypoints) is widely employed</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>Notation Table.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>Pose Repr.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Both E and F are frozen while training G and H. S indicates stride.</figDesc><table><row><cell>C 7x7 128 S1</cell><cell>C 7x7 128 S1</cell><cell>C 7x7 256 S1</cell><cell>C 7x7 512 S1</cell><cell>C 7x7 1024 S1</cell><cell></cell><cell>C 3x3 512 S2</cell><cell>C 3x3 256 S1</cell><cell>C 3x3 128 S2</cell><cell>C 3x3 128 S1</cell><cell></cell><cell>Ch. FC 128</cell><cell>Concat</cell><cell>C 3x3 256 S1</cell><cell>DeC 3x3 128 S2</cell><cell>C 3x3 128 S1</cell><cell>C 1x1 40 S1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">Heatmap branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Heatmap branch</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>C</cell><cell>C</cell><cell>C</cell><cell>C</cell><cell></cell><cell></cell><cell>C</cell><cell>C</cell><cell>C</cell><cell>C</cell><cell>C</cell><cell>C</cell><cell>C</cell><cell>C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell>3x3</cell><cell>3x3</cell><cell>1x1</cell><cell></cell><cell></cell><cell>3x3</cell><cell>7x7</cell><cell>7x7</cell><cell>7x7</cell><cell>7x7</cell><cell>7x7</cell><cell>1x1</cell><cell>1x1</cell></row><row><cell>VGG 19 upto conv4_2</cell><cell>C 3x3 256</cell><cell>C 3x3 128</cell><cell>128 C</cell><cell>128 C</cell><cell>128 C</cell><cell>512 C</cell><cell>Concat</cell><cell></cell><cell>19 C</cell><cell>128 C</cell><cell>128 C</cell><cell>128 C</cell><cell>128 C</cell><cell>128 C</cell><cell>128 C</cell><cell>19 C</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3x3</cell><cell>3x3</cell><cell>3x3</cell><cell>1x1</cell><cell></cell><cell></cell><cell>3x3</cell><cell>7x7</cell><cell>7x7</cell><cell>7x7</cell><cell>7x7</cell><cell>7x7</cell><cell>1x1</cell><cell>1x1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>512</cell><cell></cell><cell></cell><cell>38</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>128</cell><cell>38</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PAF branch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PAF branch</cell><cell></cell></row><row><cell cols="17">Fig. 2. 'C' stands for Convolutional layer. 'Ch. FC' stands for Channel-wise Fully</cell></row><row><cell cols="17">Connected layer [7]. 'DeC' stands for Deconvolutional layer. Dashed connection indicates</cell></row><row><cell>skip-connection.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 .</head><label>2</label><figDesc>Evaluation on 3DPW test set under the protocol All-Test-mode. We report MPJPE (lower is better) and PMPJPE (lower is better).</figDesc><table><row><cell>Method</cell><cell>MPJPE</cell><cell>PMPJPE</cell></row><row><cell>Ours-Fs</cell><cell>100.7</cell><cell>77.6</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning of human actions as trajectories in pose embedding manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer of robotic control with dynamics randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCVW</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">References 1. CMU graphics lab motion capture database</title>
		<ptr target="http://mocap.cs.cmu.edu/6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2017) 4, 7, 8</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised 3D pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised cross-modal alignment of speech and text embedding spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">H</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Multi-person 3D human pose estimation from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Gundavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning 3D human pose from structure and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dabral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mundhada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Kusupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afaque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning without memorizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A hierarchical deep temporal model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3D human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nobuhara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end recovery of human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multiposenet: Fast multi-person pose estimation using pose residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karagoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Akbas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">14</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Object pose estimation from monocular image using multi-view keypoint correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">iSPA-Net: Iterative semantic pose alignment network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ganeshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACM Multimedia</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">GAN-Tree: An incrementally learned hierarchical generative framework for multi-modal data distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">BiHMP-GAN: Bidirectional 3D human motion prediction gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning of human actions as trajectories in pose embedding manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Uppala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>WACV</publisher>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">UM-Adapt: Unsupervised multi-task adaptation using adversarial cross-task distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lakkakula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised cross-dataset adaptation via probabilistic amodal 3D human pose completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Patravali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>WACV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Self-supervised 3D human pose estimation via part guided novel image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Kinematic-structure-preserved representation for unsupervised 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rahul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rakesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2935" to="2947" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<title level="m">Data-free knowledge distillation for deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">2d/3D pose estimation and action recognition using multitask deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Frey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Monocular 3D human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Single-shot multi-person 3D pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3D human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM TOG</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Camera distance-aware top-down approach for 3D multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV (2019) 1, 2, 4</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge distillation in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Mopuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sim-to-real transfer of robotic control with dynamics randomization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICRA</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Refine and distill: Exploiting cycleinconsistency and knowledge distillation for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pilzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lathuiliere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Exploiting temporal information for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rayat Imtiaz Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Lcr-net: Localization-classificationregression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2017) 1, 2, 4</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multi-person 2d and 3D pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CVPR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Cross-modal deep variational hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spurr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IROS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A dual-source approach for 3D pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Person reidentification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCVW</publisher>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
