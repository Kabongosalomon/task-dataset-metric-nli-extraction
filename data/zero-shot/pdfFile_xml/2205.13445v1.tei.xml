<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Information Divergence: A Unified Metric for Multimodal Generative Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
							<email>j1nhwa.kim@navercorp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunji</forename><surname>Kim</surname></persName>
							<email>yunji.kim@navercorp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
							<email>lee.j@navercorp.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Kang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">NAVER AI Lab Republic</orgName>
								<address>
									<country key="KR">Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab &amp; CLOVA, SNU AIIS Republic of Korea</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Mutual Information Divergence: A Unified Metric for Multimodal Generative Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Sang-Woo Lee NAVER AI Lab &amp; CLOVA, KAIST AI Republic of Korea</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-to-image generation and image captioning are recently emerged as a new experimental paradigm to assess machine intelligence. They predict continuous quantity accompanied by their sampling techniques in the generation, making evaluation complicated and intractable to get marginal distributions. Based on a recent trend that multimodal generative evaluations exploit a vison-and-language pre-trained model, we propose the negative Gaussian cross-mutual information using the CLIP features as a unified metric, coined by Mutual Information Divergence (MID). To validate, we extensively compare it with competing metrics using carefully-generated or human-annotated judgments in text-to-image generation and image captioning tasks. The proposed MID significantly outperforms the competitive methods by having consistency across benchmarks, sample parsimony, and robustness toward the exploited CLIP model. We look forward to seeing the underrepresented implications of the Gaussian cross-mutual information in multimodal representation learning and the future works based on this novel proposition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A multimodal generative model, including text-to-image generation <ref type="bibr" target="#b0">[1]</ref> and image captioning <ref type="bibr" target="#b1">[2]</ref> models, is an emerging research topic showing interpretative multimodal understanding, text or image retrieval, machine creativity, etc. The gist of learning multimodal generative models is to understand how to connect one modality to the other and generate the corresponding representations following the desired data distribution. However, measuring the distance or divergence between the model and data distributions is generally intractable due to the finite data and generation cost. Therefore, the proposed metrics attempt to approximate it with polynomial-sized data and generated samples <ref type="bibr" target="#b2">[3]</ref>.</p><p>For text-to-image generation, the widely-used metrics are Inception Score (IS) <ref type="bibr" target="#b3">[4]</ref> and Fr?chet Inception Distance (FID) <ref type="bibr" target="#b4">[5]</ref>. These metrics are originally proposed for non-conditional generative models, which are repurposed to measure the distance between the data and model conditional distributions using a validation split. This idea was supported by the effectiveness of deep features as a perceptual metric <ref type="bibr" target="#b5">[6]</ref>, although these metrics use the Inception V3 <ref type="bibr" target="#b6">[7]</ref>. Not surprisingly, there are the attempts to develop the more robust metrics using multimodal pre-trained models, such as object detectors <ref type="bibr" target="#b7">[8]</ref>, image captioning models <ref type="bibr" target="#b8">[9]</ref>, and vision-and-language pre-trained models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>.  <ref type="bibr" target="#b18">[19]</ref>. We vary the ratio of text-image shuffling where the counterparts in the selected pairs are deliberately shuffled, depicting misalignment. For CLIP-S, we observe varying slopes across the datasets depending on data domain while MI shows a relatively consistent tendency. The CUB and MM-CelebA-HQ describe birds and human faces, respectively, having narrow domains compared to COCO and LN-COCO with various objects.</p><p>Significantly, the image captioning method transforms text-to-image measurement into image-to-text measurement, providing a different viewpoint with cyclic consistency.</p><p>For image-to-text generation, or image captioning, the COCO Caption Evaluation toolkit seems to be the standard to measure the divergence from ground-truth captions having BLEU <ref type="bibr" target="#b11">[12]</ref>, METEOR <ref type="bibr" target="#b12">[13]</ref>, ROUGE <ref type="bibr" target="#b13">[14]</ref>, CIDEr <ref type="bibr" target="#b14">[15]</ref>, and SPICE <ref type="bibr" target="#b15">[16]</ref>. Similar to the IS and FID of text-to-image generation metrics, these metrics are merely n-gram-based statistical methods neglecting conditional images. The breakthrough in improving the correlation with human judgment comes from the utilization of the pre-trained vision-and-language models, e.g., TIGEr <ref type="bibr" target="#b16">[17]</ref>, ViLBERTScore-F <ref type="bibr" target="#b17">[18]</ref>, and RefCLIP-S <ref type="bibr" target="#b18">[19]</ref>. We speculate that these two directional metrics are getting closer to measuring the generative divergence of text-image alignment.</p><p>This paper proposes a unified metric for multimodal generative models. In probability theory and information theory, mutual information (MI) measures how much one random variable tells us about the other. In multimodal generation, the MI of two modalities quantitatively measures how much the generated is well-aligned with the condition. From this motive, we propose to use the Gaussian mutual information where the probability distributions are defined by the means and covariances of visual and textual features and borrow the idea of cross-mutual information <ref type="bibr" target="#b19">[20]</ref> to measure the MI divergence from the real data distribution, which is the expectation of point-wise mutual information with respect to evaluating samples. Surprisingly, the proposed method outperforms previous works with significant margins on the assorted benchmarks of text-to-image generation and image captioning, including standard human judgment correlation benchmarks.</p><p>Section 2 introduces the previous works on text-to-image generation and image captioning metrics, and a previous work on cross-mutual information. Section 3 describes the proposed method defining the continuous mutual information using multivariate Gaussian distributions and the negative crossmutual information, which is the Mutual Information Divergence (MID) what we term. Section 4 consists of two parts, evaluation on text-to-image generation and image captioning evaluation, including related discussions. Section 5 concludes the work with remarks.</p><p>We summarize our contributions as follows:</p><p>? To the best of our knowledge, we firstly propose the negative cross-mutual information under the Gaussian assumption as a unified metric for multimodal generative models.</p><p>? We provide three theoretical analyses on the proposed method MID, out-of-distribution detecting by the squared Mahalanobis distances, bias and variance decomposition, and its relation to the Kullback-Leibler divergence.</p><p>? We achieve the state-of-the-art on text-to-image generation and image captioning benchmarks including the generated and human Likert-scale judgment correlations, visual reasoning accuracy, Flickr8K-Expert, Flickr8K-CF, Pascal-50S, and FOIL hallucination detection.</p><p>2 Related work 2.1 Metrics for assessing text-to-image generation Traditional metrics. One of the widely used metrics is the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b4">[5]</ref> that measures the distributional difference between synthetic (fake) and real-world images (real). Training and validation distributions are independent, making a model that fails to match the conditional distributions if it does not reflect the given textual information. Although it measures fidelity along with Inception Score <ref type="bibr" target="#b3">[4]</ref>, it cannot directly measure the alignment of text and image. The alternative metrics <ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> are proposed to evaluate the fidelity and diversity. Text-to-image metrics. Dedicated to assessing text-to-image generation, the R-Precision exploits the Deep Attentional Multimodal Similarity Model (DAMSM) <ref type="bibr" target="#b23">[24]</ref> to calculate the top-1 retrieval accuracy from one hundred text candidates for the generated image as a query. Besides, the CLIP R-Precision <ref type="bibr" target="#b10">[11]</ref> exploits the CLIP <ref type="bibr" target="#b9">[10]</ref> showing a better retrieval performance and human judgment correlation. However, falsenegative candidates (accidentally correlated) or strong negative candidates (totally unrelated) may interfere with the accurate assessment <ref type="bibr" target="#b24">[25]</ref>. To evaluate the quality of individual objects, the SOA <ref type="bibr" target="#b7">[8]</ref> attempts to measure the object detection accuracy using the YOLOv3 <ref type="bibr" target="#b25">[26]</ref> based on the object classes that appeared in the text but cannot consider other factors. Caption generation <ref type="bibr" target="#b8">[9]</ref> is another approach using the vision-and-language pre-trained model. The motivation is the cyclic consistency that the generated caption from the generated image should match with the text for image generation. However, the model bias including object hallucination <ref type="bibr" target="#b26">[27]</ref> and the accumulated errors from metric are drawbacks. Diagnostic datasets. Park et al. <ref type="bibr" target="#b10">[11]</ref> provide the curated splits of the CUB <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> and Flowers <ref type="bibr" target="#b29">[30]</ref> to assess unseen color and shape compositions in the narrow domains. DALL-Eval <ref type="bibr" target="#b30">[31]</ref> proposed a diagnostic dataset PaintSkills to evaluate visual reasoning skills to assess models based on this dataset. Since the dataset is generated from a 3D simulator using limited configurations, the data distributions deviate from other real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Metrics for assessing image captioning</head><p>Reference-only metrics. Borrowing from machine translation literature, image captioning models are evaluated using reference-based textual metrics. Usually, five references are used to measure in these metrics. BLEU-4 <ref type="bibr" target="#b11">[12]</ref>, ROUGE-L <ref type="bibr" target="#b13">[14]</ref>, and METEOR <ref type="bibr" target="#b12">[13]</ref> are n-gram precision or recall-based metrics, CIDEr <ref type="bibr" target="#b14">[15]</ref> uses tf-idf weighting and stemming, while SPICE <ref type="bibr" target="#b15">[16]</ref> uses semantic parsing and scene graph analysis. Notably, BERT-S++ <ref type="bibr" target="#b31">[32]</ref> considers inter-reference variance using the fine-tuned BERTScore <ref type="bibr" target="#b32">[33]</ref> for image captioning. Reference-with-image metrics. The recently proposed metrics are considering the images used for generating captions. TIGEr <ref type="bibr" target="#b16">[17]</ref> uses a pre-trained SCAN <ref type="bibr" target="#b33">[34]</ref> while ViLBERTScore-F <ref type="bibr" target="#b17">[18]</ref> uses a pre-trained ViLBERT <ref type="bibr" target="#b34">[35]</ref> exploiting the vision-andlanguage alignments from large-scale data and multiple tasks. Similarly, CLIP-S and RefCLIP-S <ref type="bibr" target="#b18">[19]</ref> uses the pre-trained CLIP, a more powerful vision-and-language model, outperforming the previous methods. Implications. CLIP R-Precision for text-to-image generation and RefCLIP-S for image captioning share the same motivation exploiting the same vision-and-language pre-trained model. Here, we remark on the unifying metrics in two different tasks (e.g., CLIP-S), text-to-image generation and image captioning, and propose a new unified metric for the multimodal generative models based on the continuous mutual information considering the covariances of two modality groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Cross-mutual information in machine translation</head><p>Bugliarello et al. <ref type="bibr" target="#b19">[20]</ref> proposed the cross-mutual information (XMI) as a metric of machine translation exploiting a probabilistic view in neural machine translation models. XMI is an analogue of mutual information for cross-entropy defined as XMI(S ? T ) = H qLM (T ) ? H qMT (T |S). H qLM (T ) denotes the cross-entropy of the target sentence T under a language model q LM and H qMT (T |S) is the crossconditional entropy under a cross-lingual model q MT . In practice, they exploit two model distributions q LM (t) and q MT (t|s) to approximate the XMI as follows:</p><formula xml:id="formula_0">XMI(S ? T ) ? ? 1 N N i=1 log q LM (t (i) ) q MT (t (i) |s (i) )</formula><p>where N denotes the number of held-out evaluating samples. Since language models consider a finite size of vocabulary, the cross-entropy can be efficiently approximated using the target sentences; however, it is limited to readily apply to other generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Continuous mutual information</head><p>We introduce a unified metric for conditional generative models not depending on the modalities of condition and generation. To measure the alignment of condition and generation, we first consider the continuous mutual information of the condition x and generation y as follows:</p><formula xml:id="formula_1">I(X; Y) = E p(x,y) log p(x, y) p(x)p(y)<label>(1)</label></formula><p>where the probability and joint probability distributions are multivariate Gaussian, which is the maximum entropy distribution for the given mean ? and covariance ? <ref type="bibr" target="#b35">[36]</ref>. The first two moments are used for practical reason <ref type="bibr" target="#b4">[5]</ref>. The multivariate Gaussian distribution is defined as:</p><formula xml:id="formula_2">p(x) = 1 (2?) D det(?) exp ? 1 2 (x ? ?) ? ?1 (x ? ?)<label>(2)</label></formula><p>where D is the dimension of x. The mutual information with the Gaussian distributions is reduced to:</p><formula xml:id="formula_3">I(X; Y) = 1 2 log det(? x ) det(? y ) det(? z )<label>(3)</label></formula><p>where ? x and ? y ? R D?D are the covariances of the condition and generation and ? z ? R 2D?2D is the covariance matrix of the concatenation z of x and y representing the joint distribution. The proof can be found in Appendix A.1. Note that we use log det(?) = i log ? i for numerical stability, where ? i is the eigenvalue of ?.  CLIP (ViT-B/32) 10.9 10.9 CLIP-R-Precision <ref type="bibr" target="#b10">[11]</ref> CLIP (ViT-B/32) 5.9 6.8 OFA-Captioning+CLIP-S <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref> </p><formula xml:id="formula_4">OFA-Large + CLIP (ViT-B/32) 8.1 8.1 CLIP-S [19] CLIP (ViT-L/14) 11.1 11.1 InfoNCE [37]</formula><p>CLIP (ViT-L/14) 11.1 11.1 CLIP-R-Precision <ref type="bibr" target="#b10">[11]</ref> CLIP (ViT-L/14) 9.4 9.7 OFA-Captioning+CLIP-S <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b37">38]</ref>  By the way, we can show that E (x,?)?D PMI(x;?) is related to the Kullback-Leibler divergence as follows (The proof can be found in Appendix A.3):</p><formula xml:id="formula_5">E (x,?)?D PMI(x;?) =I(X;?) + D KL (p(x) p(x)) ? D KL (p(?) p(z))<label>(7)</label></formula><p>For simplicity, we denote our proposed method E (x,?)?D PMI(x;?) as Mutual Information Divergence (MID), comparable of the FID. In practice, we use Equation 5 using the double-precision CLIP features. For point-wise evaluation, we use the PMI(x,?) without the expectation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Evaluation on text-to-image generation</head><p>Implementation details. Without an explicit mention, we use the CLIP (ViT-L/14) to extract image and text embedding vectors. Note that it is crucial to use double-precision for numerical stability.</p><p>Generated Likert-scale judgments. To carefully assess the text-image alignment, we consider the four-scale alignment using the real and fake images from the COCO dataset <ref type="bibr" target="#b1">[2]</ref>. We regard the real images as a four-point set, the fake images generated by the ground-truth captions as a three-point set, the fake images generated by the foiled captions 2 <ref type="bibr" target="#b38">[39]</ref> as a two-point set, and the randomly sampled (misaligned) fake images as one-point set. We assume that the fake images generated by the foiled captions should be inferior compared with the fake images generated by the ground-truth  captions because the model cannot exploit the critical information to generate key objects. The current state-of-the-art LAFITE <ref type="bibr" target="#b39">[40]</ref> pre-trained on the COCO 3 is used for our text-to-image generation model. Notice that when we evaluate the metrics, the ground-truth captions are used to measure the text-image alignment of the generated images. We believe this generated benchmark can be a proxy to human judgments with a careful manipulation of the fake images using the foiled captions.</p><p>We randomly sample 30K captions from the FOIL dataset <ref type="bibr" target="#b38">[39]</ref> to build 120K judgments. We report the Kendall's ? coefficient <ref type="bibr" target="#b40">[41]</ref> to measure the rank correlation, a variant of ? c or ? b accounting for ties. In <ref type="table" target="#tab_0">Table 1</ref>, our method consistently outperforms competing methods. InfoNCE denotes the negative InfoNCE loss <ref type="bibr" target="#b36">[37]</ref> calculating the softmax function over the 30K captions. Remind that the InfoNCE maximizes a lower bound on mutual information <ref type="bibr" target="#b36">[37]</ref>. Since this is estimated using a smaller batch size when optimizing, it shows a limited capability as a metric for the text-image alignment. Please refer to <ref type="table" target="#tab_10">Table 8</ref> and <ref type="figure" target="#fig_10">Figure 8</ref> in Appendix for the VQ-Diffusion <ref type="bibr" target="#b41">[42]</ref> benchmark. Visual reasoning accuracy using the foiled caption trick. Inspired by the foiled caption trick, we extend to four visual reasoning skills for object, count, color, and spatial relationship. For each  <ref type="figure">Figure 3</ref>: The footprints of metrics across feature extractors using the normalized scores.</p><p>category, we define a set of tokens, and we foiled those tokens in the caption by randomly swapping to the other token. We build three sets of images, the real images, the fake images, and the foiled fake images. We measure the accuracy that getting one point for the foiled fake images having the lowest score, or zero for the other cases. <ref type="table" target="#tab_3">Table 3</ref> shows that our method achieved the best performance across all categories. The runner-up was InfoNCE, while SOA was ineffective to differentiate among the two fake images and the real image. Although DALL-Eval <ref type="bibr" target="#b30">[31]</ref> proposed to use a detector and its dedicated heads for the count, color, and spatial relationship tasks, this was limited to the 3D-generated images with their near-perfect detection ability. Remind that the accuracy of random guessing is 33.3%, where MID requires a powerful feature extractor of the CLIP ViT-L/14 to get meaningful performances on the count, color, and spatial relationship tasks. For the detail, please refer to the text in Appendix C.</p><p>Human Likert-scale judgment. We collect 10K one-to-four Likert-scale human judgments for 2K fake images using the LAFITE and VQ-Diffusion from the Amazon Mechanical Turk (AMT) for the fine-grained comparison with the competing metrics. For each image, we collect five annotations from unique workers taking its median for a reliable correlation measurement. <ref type="table" target="#tab_1">Table 2</ref> shows the consistent results with our generated judgment correlation benchmark. Since this benchmark aims for the fine-grained judgment among only fake images, overall scores are relatively lower than the generated benchmark. All correlation results are significant having the p-value &lt; 0.001. For the details of the collection procedure and data statistics, please refer to Appendix D, where <ref type="figure">Figure 10</ref> shows the visualization of some examples comparing with human judgment scores. We also report the comp-t2i benchmark <ref type="bibr" target="#b10">[11]</ref> results for the compositional evaluation of the CUB and Flower datasets in aspects of color and shape in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Discussions</head><p>Consistent metric across datasets. A distinct property of our method is the consistency across datasets. As shown in <ref type="figure">Figure 1</ref>, the cosine similarity-based method, CLIP-S suffers the inconsistent results. For example, the CUB <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> and MM-CelebA-HQ <ref type="bibr" target="#b42">[43]</ref> have narrow domains, birds and human faces, respectively, which is prone to get a similar score for all samples in the datasets by cosine similarity. To validate our hypothesis, we vary the ratio of text-image shuffling using the real datasets where the counterparts in the selected pairs are deliberately shuffled, depicting misalignment. For CLIP-S, we observe the inconsistency depending on datasets, while MI shows a consistent tendency. Notice that the expectation of PMI is reduced to MI for the real images (Appendix A.1).</p><p>Inspecting possible over-fitting with the CLIP features. <ref type="figure">Figure 3</ref> shows the normalized scores of metrics across feature extractors. To get the normalized score, we subtract the score of RN101 and divide by the standard deviation of four scores across feature extractors reminiscent of z-score. We expect the footprint of metrics should be consistent across different generative models if the model is not over-fitted to the metrics. LAFITE used both encoders of CLIP ViT-B/32 and VQ-Diffusion used the text encoder of CLIP ViT-B/32, while DM-GAN did none of them <ref type="bibr" target="#b43">[44]</ref>. InfoNCE and CLIP-R-Precision are related to the contrastive training losses (LAFITE, and DM-GAN for the DAMSM loss <ref type="bibr" target="#b23">[24]</ref>), which may lead to drastic change of the normalized scoring signature across the feature extractors. While the proposed MID was relatively stable across the generative models.</p><p>Comparison with the state-of-the-art models using MID. <ref type="table" target="#tab_11">Table 9</ref> in Appendix E exhibits the MID performance of the recent text-to-image generative models along with the other major metrics. 41.8 CIDEr <ref type="bibr" target="#b14">[15]</ref> 43.9 SPICE <ref type="bibr" target="#b15">[16]</ref> 44.9 LEIC (? b ) <ref type="bibr" target="#b44">[45]</ref> 46.6 BERT-S++ <ref type="bibr" target="#b31">[32]</ref> 46.7 TIGEr <ref type="bibr" target="#b16">[17]</ref> 49.3 NUBIA <ref type="bibr" target="#b45">[46]</ref> 49.5 ViLBERTScore-F <ref type="bibr" target="#b17">[18]</ref> 50.1 CLIP-S <ref type="bibr" target="#b18">[19]</ref> 51.2 RefCLIP-S <ref type="bibr" target="#b18">[19]</ref> 53.0 MID (ours) 54.9 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on image captioning</head><p>Implementation details. For a fair comparison with the current state-of-the-art evaluation metric (RefCLIP-S), we use the same pre-trained CLIP (ViT-B/32) used in the prior work to extract image and caption embedding vectors. We use the images and the corresponding reference captions to build the covariance matrices ? x , ? y and the joint covariance matrix ? z . For the numerical stability of the inverse of covariance matrix, we replace ? ?1 x with? ?1 x = (? x + I) ?1 , which handles the near-zero eigenvalues of covariance. We found that of 5e-4 generally works across all benchmark evaluations, except for the FOIL benchmark where we used of 1e-15, which was slightly better. Note that we use an identical prompt "A photo depicts" for all caption embeddings as employed in RefCLIP-S <ref type="bibr" target="#b18">[19]</ref>.</p><p>Flickr8K-Expert and Flickr8k-CF. We measure the correlation of the proposed method with the Likert-scale judgments, which indicate the relative correctness of given captions. Fliker8K-Expert <ref type="bibr" target="#b46">[47]</ref> provides 17K human expert judgments for 5,664 images with a four-scale where the higher is better. Following prior works, we flatten all human judgments to a list of 16,992 (5,664?3) samples, and we exclude 158 pairs where their captions appear in the reference set. Flickr8K-CF <ref type="bibr" target="#b46">[47]</ref> has 145K binary judgments from the CrowdFlower for 48K image-caption pairs. Each pair receives at least three judgments, and we take the proportion of positive as a corresponding score. Kendall's ? coefficient <ref type="bibr" target="#b40">[41]</ref> measures the rank correlation, and ? c and ? b are used for Fliker8K-Expert and Flickr8K-CF, respectively. Although ? c is more suitable when the underlying scales differ in two variables, we follow the previous works for a fair comparison. <ref type="table" target="#tab_5">Tables 4 and 5</ref> show the evaluation results. For both cases, our MID significantly improves the correlation of human judgments with 54.9 and 37.3, respectively. Notably, MID improves further than RefCLIP-S, which uses the same vision-and-language pre-trained CLIP.  Flickr8K reference parsimony. Compared with the other methods, our method does not directly rely on the corresponding references, but through the mean and covariance. Therefore, we could exploit the sample statistics with a limited number of references. <ref type="figure" target="#fig_4">Figure 4</ref> shows the Kendall's ? correlation utilizing a subset of the available references. Interestingly, even though 30-40% of images are available, it retains the majority of performance. The dashed lines indicate the correlations of RefCLIP-S, which exploits all references. Notice that our method can be positioned between the reference-with-image and reference-free metrics. Because a sufficient amount of samples are required to assess image captioning models, the sample statistics can be reliable from the sufficient samples.</p><p>Pascal-50S. For a different evaluation setting of accuracy, the Pascal-50S <ref type="bibr" target="#b14">[15]</ref> offers 4K pair-wise preference judgments between two captions, evenly splitting four categories, two human correct captions (HC), both human written, but one is incorrect (HI), one is from human, the other is by a  model (HM), and both are generated by machine (MM). For each pair, there are 48 human judgments and the majority of votes decides which caption is preferred, where ties are broken randomly. As in the previous work <ref type="bibr" target="#b18">[19]</ref>, we randomly sample 5 references among 48 candidates and average over five evaluations. <ref type="table" target="#tab_7">Table 6</ref> shows the consistent results outperforming competitive methods. Except for HI, we achieve the state-of-the-art while the margin of HI is 0.1 having a near-perfect score of 99.7. Object hallucination sensitivity. Rohrbach et al. <ref type="bibr" target="#b26">[27]</ref> argue that image captioning models prone to generate the objects not presented in the image due to learned bias. To assess this aspect, the FOIL-COCO <ref type="bibr" target="#b38">[39]</ref> builds the carefully modified captions from the COCO captions <ref type="bibr" target="#b1">[2]</ref> by swapping a single noun-phrase, e.g., substituting "cat" for "dog". To measure the accuracy whether assigning a higher score to the ground-truth caption over the FOIL caption, we evaluate 32K test images with exclusive four reference captions of the COCO dataset. <ref type="table" target="#tab_8">Table 7</ref> shows the competitive scores of our method. Since RefCLIP-S <ref type="bibr" target="#b18">[19]</ref> directly accesses the reference captions of the evaluating image, they can exploit the original words (before foiling) in the references, which is roughly 87% in four references and 67% in a randomly selected reference. The RefCLIP-S is defined as follows:</p><p>RefCLIP-S(x, y, R) = H CLIP-S(x, y), max(max r?R cos(r, y), 0)</p><p>where H is harmonic mean, x and y denotes image and caption embeddings, and R is a set of the references. <ref type="figure" target="#fig_5">Figure 5</ref>   <ref type="figure">Figure 11</ref> for the visualization).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We, to our best knowledge, firstly argue that the negative cross-mutual information with multivariate Gaussian distributions can be used as a unified metric for multimodal generative models. We provide the theoretical analyses of the proposed metric by out-of-distribution detecting by the squared Mahalanobis distances, bias and variance decomposition, and the relation to the Kullback-Leibler divergence, along with the empirical experiments. We achieve the state-of-the-art performances on text-to-image generation and image captioning benchmarks, the generated and human Likertscale judgment correlations, visual reasoning accuracy, Flickr8K-Expert, Flickr8K-CF, Pascal-50S, and FOIL hallucination detection. We look forward to seeing the future works on the Gaussian cross-mutual information in multimodal representation learning based on this novel proposition. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Proof of the mutual information with Gaussian distributions</head><p>The mutual information of two Gaussian distributions is defined as:</p><formula xml:id="formula_7">I(X; Y) = 1 2 log det(? x ) det(? y ) det(? z ) .<label>(9)</label></formula><p>Proof. Let the mutual information be:</p><formula xml:id="formula_8">I(X; Y) = E p(x,y) log p(x, y) p(x)p(y) .<label>(10)</label></formula><p>Using the definition of multivariate Gaussian distribution as follows,</p><formula xml:id="formula_9">p(x) = 1 (2?) D det(?) exp ? 1 2 (x ? ?) ? ?1 (x ? ?)<label>(11)</label></formula><p>we rearrange the equation to cancel out the constant terms. Then, the continuous mutual information is reduced to:</p><formula xml:id="formula_10">I(X; Y) = 1 2 log det(? x ) det(? y ) det(?) + 1 2 E p(x,y) D 2 M (x) + D 2 M (y) ? D 2 M (z)<label>(12)</label></formula><p>where D 2 M denotes the squared Mahalanobis distance defined by D 2</p><formula xml:id="formula_11">M (x) = (x ? ? x ) ? ?1 x (x ? ? x ),</formula><p>where ? x and ? x are the mean and covariance of x, and z denotes [x; y].</p><p>By the way, the expectation of the squared Mahalanobis distance is the dimension of samples, D.</p><formula xml:id="formula_12">E p(x) D 2 M (x) = 1 N tr X ? ?1 x X = 1 N tr ? ?1 x XX = tr ? ?1 x ? x = tr(I D ) = D<label>(13)</label></formula><p>where X ? R D?N is the samples, I D ? R D?D is the identity matrix. We use the cyclic property of trace where tr(ABC) = tr(BCA). Therefore, the second term reduces to zero as follows:</p><formula xml:id="formula_13">1 2 E p(x,y) D 2 M (x) + D 2 M (y) ? D 2 M (z) = 1 2 (D + D ? 2D) = 0.<label>(14)</label></formula><p>We conclude the proof.</p><p>By the way, the point-wise mutual information (PMI) with Gaussian distributions can be derived from Equation <ref type="bibr" target="#b11">12</ref>:</p><formula xml:id="formula_14">PMI(x; y) = I(X; Y) + 1 2 D 2 M (x) + D 2 M (y) ? D 2 M (z) .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 The bias-variance decomposition of the expectation of the squared Mahalanobis distance</head><p>The expectation of PMI with respect to evaluating samples needs to calculate the expectation of three terms of the squared Mahalanobis distances (SMD) with respect to the evaluating samplex. With a notation ofX ? R D?N for N evaluation samples, we can decompose the expectation of SMD with two terms of bias and variance as follows:</p><formula xml:id="formula_15">Ex D 2 M (x) = 1 N tr (X ? ? x 1 ) ? ?1 x (X ? ? x 1 )<label>(16)</label></formula><formula xml:id="formula_16">= 1 N tr ? ?1 x (X ? ? x 1 )(X ? ? x 1 )<label>(17)</label></formula><formula xml:id="formula_17">= 1 N tr ? ?1 x XX ? N ?x? x + N (?x ? ? x )(?x ? ? x )<label>(18)</label></formula><p>= tr ? ?1</p><formula xml:id="formula_18">x ?x + (?x ? ? x )(?x ? ? x )<label>(19)</label></formula><formula xml:id="formula_19">= (?x ? ? x ) ? ?1 x (?x ? ? x ) + tr(? ?1 x ?x)<label>(20)</label></formula><formula xml:id="formula_20">= (?x ? ? x ) ? ?1 x (?x ? ? x ) + tr(? ?1 x ?x) ? tr(? ?1 x ? x ) + tr(? ?1 x ? x )<label>(21)</label></formula><formula xml:id="formula_21">= (?x ? ? x ) ? ?1 x (?x ? ? x ) + tr ? ?1 x (?x ? ? x ) + D<label>(22)</label></formula><p>where 1 ? R N is a vector of ones. Remind that the expectation of SMD is D when the evaluating samplesx are following the distribution of x in Equation 13. However, the above equation shows that if the mean or covariance ofx deviates from x, the result may be smaller or larger than D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Relation to Kullback-Leibler divergence</head><p>The proposed method MID is related to Kullback-Leibler divergence (or relative entropy). Let N 0 (? 0 , ? 0 ) and N 1 (? 1 , ? 1 ) are two multivariate normal distributions having the same dimension of D, then the Kullback-Leibler divergence between the distributions is as follows [49]:</p><formula xml:id="formula_22">D KL (N 0 N 1 ) = 1 2 tr ? ?1 1 (? 0 ? ? 1 ) + (? 1 ? ? 0 ) T ? ?1 1 (? 1 ? ? 0 ) + log det ? 1 det ? 0 .</formula><p>Using the above equation and Equation <ref type="bibr" target="#b21">22</ref>, we rearrange Equation 12 as follows:</p><formula xml:id="formula_23">E (x,?)?D PMI(x;?) =I(X; Y) + D KL (p(x) p(x)) + D KL (p(?) p(y)) ? D KL (p(?) p(z)) ? 1 2 log det ? x det ?x + log det ? y det ?? ? log det ? z det ?? (23) =I(X; Y) + D KL (p(x) p(x)) + D KL (p(?) p(y)) ? D KL (p(?) p(z)) ? 1 2 log det ? x det ? y det ? z + 1 2 log det ?x det ?? det ?? (24) =I(X;?) + D KL (p(x) p(x)) ? D KL (p(?) p(z))<label>(25)</label></formula><p>where D KL (p(?) p(y)) = 0 since? and y are the same condition evaluating generations.</p><p>B Generated Likert-scale judgment correlation using VQ-Diffusion <ref type="table" target="#tab_10">Table 8</ref> and <ref type="figure">Figure 6</ref> show the results from the (foiled) fake images using VQ-Diffusion <ref type="bibr" target="#b41">[42]</ref>. While the proposed MID outperforms the competing methods, the portion of fake images that get higher scores than real images is decreased in InfoNCE and CLIP-S. This observation may attribute to the under-performance of VQ-Diffusion than LAFTIE or the side effect of the contrastive loss used in LAFITE. Remind that our method shows the consistency toward different models among the comparative metrics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C The details on visual reasoning accuracy</head><p>We describe the detail of visual reasoning accuracy in <ref type="table" target="#tab_3">Table 3</ref>. For the object task, we use randomly sampled 30K captions from the FOIL dataset <ref type="bibr" target="#b38">[39]</ref>. For the count task, we use a set of tokens "0", "1", "2", "3", "4", "one", "two", "three", and "four". For the color task, we use the sixteen basic 1 <ref type="figure">Figure 6</ref>: The histogram shows the frequencies of the competing methods using the CLIP of ViT-L/14 for the four-scale judgments from the COCO dataset. The real images and the original captions (Real), the fake images generated from the VQ-Diffusion <ref type="bibr" target="#b41">[42]</ref> using the original caption (Fake), the fake images generated from the VQ-Diffusion but using the foiled caption deliberately swapping some objects to the other words (Foiled Fake) (notice that the foiled fake images are still matching with the original captions to assess), and randomly matching the foiled fake to the original captions by shuffling (Misaligned Fake). For the detail of methods, please see the text in Section 4.</p><p>color keywords <ref type="bibr" target="#b4">5</ref> . For the spatial relationship task, we use "above", "below", "left", "right", "front", and "back". The number of samples are 30K, 1.3K, 4.6K, 1.5K for the object, count, color, spatial relationship tasks, respectively. The LAFITE <ref type="bibr" target="#b39">[40]</ref> generates the (foiled) fake images. (1: obviously misaligned, 2: partly aligned, 3: fairly aligned except for some parts, 4: well-aligned)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Time constraint. We anticipate that each task takes at least 150 seconds. Please utilize this time to maximize the judgment quality.</p><p>? Star rating. To rate with the stars interface, please click the right part of blank (gray) star. Notice that we don't allow zero star assessment as in the instruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Quality:</head><p>Text(below)-to-Image(left) Alignment:</p><p>Text: A man standing in the surf behind a boy standing on a surfboard. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Human judgment collection from the Amazon Mechanical Turk</head><p>In Section 4.1, the human Likert-scale judgments on the fine-grained text-image alignment are used to evaluate the metrics and reported the rank correlations in <ref type="table" target="#tab_1">Table 2</ref>. The following paragraphs describe the data collection procedure using the Amazon Mechanical Turk (AMT). Data. We randomly sample 1K validation captions from the COCO dataset <ref type="bibr" target="#b1">[2]</ref> to generate fake images using the LAFITE <ref type="bibr" target="#b39">[40]</ref> and VQ-Diffusion <ref type="bibr" target="#b41">[42]</ref>, which makes 2K fake images in total. Each task consists of 10 randomly-sampled fake images and the corresponding captions. A worker from the AMT is carefully instructed to annotate the visual quality of each fake image and the text-image alignment between the fake image and its caption. The visual quality annotation is designed as a preliminary task to reduce the quality bias in the text-image alignment assessment. There are five assignments per task to collect five annotations to decide a final judgment score for the image. We used the median of five annotations, which makes 2K evaluating samples on 1K captions. For the reference samples, we use randomly-sampled 30K captions and the corresponding images from the validation split of the COCO dataset as in the generated Likert-scale judgment experiments.</p><p>Interface. Each task presents with the instruction on top of the AMT task interface as in <ref type="figure" target="#fig_9">Figure 7</ref>.</p><p>We adopt a one-to-four Likert scale to assess the image quality and text-to-image alignment. There are descriptions of how to interpret each level of score. We encourage the workers to spend at least 150 seconds for 20 assessments (10 images, two assessments for each image). We observed a few mild violations of this time constraint in a pilot deployment, so we systemically forced the workers to spend at least 150 seconds before the submission. <ref type="figure" target="#fig_10">Figure 8</ref> shows one of ten samples in a assignment. Cost. We collected 1K assignments for 200 tasks. Considering reasonable earn per hour, we paid $0.21 for each assignment, $210 in total.</p><p>Statistics. <ref type="figure">Figure 9</ref> shows the number of assignments per worker (left) and the time spent per assignment. A worker did at most 37 assignments and the assignments are done within five minutes. After taking median, the mean and standard deviation of the quality judgments are 2.43 and 0.61, respectively, and these of the alignment judgments are 2.62 and 0.64, respectively.</p><p>Visualization <ref type="figure">Figure 10</ref> shows some examples comparing with human judgment scores. Note that we used the mean of three median annotations from workers for this visualization.</p><p>E Performance results of text-to-image generative models <ref type="table" target="#tab_11">Table 9</ref> illustrates the performance of text-to-image generative models including the proposed MID scores. The reports of FID, SOA-C, and SOA-I are from Hinz et al. <ref type="bibr" target="#b7">[8]</ref> while missing scores are from the corresponding cites. They "randomly sampled three times 30,000 images from the training set <ref type="figure">Figure 10</ref>: The visualization of generated images and their evaluation results. From the top, a caption for generation, a generative model type, a normalized rank and a raw score in parentheses for each metric. The normalized rank denotes the evaluated score rank divided by the number of samples (zero-to-one scale) for a fair comparison. bold indicates the closer to human judgments. and compared them to the statistics of the validation set." We found that LAFITE uses all 82,612 training images to get the statistics, so we also report FID (ours) for a fair comparison. We sampled 30,000 fake images from the validation captions. This different sampling strategy may attribute to the difference of the real images' upper bound (6.09 vs. 2.73). For the proposed MID, we randomly sampled 30,000 images and the captions for each image from the validation set as reference samples (X, Y). We trained the LAFITE model from scratch using the official code <ref type="bibr" target="#b5">6</ref> with the same hyperparameters and 1.5 times training longer to achieve a slightly better FID than the publicly released model. Interestingly, the filtered GLIDE underperforms with the worst FID; however, it outperforms some of the other models with MID. It may show that the data filtering severely affects FID while relatively retaining the performance of text-image alignment captured by MID.</p><p>F Performance comparison on the comp-t2i dataset <ref type="table" target="#tab_0">Table 10</ref> demonstrates the human judgement correlation for the comp-t2i dataset <ref type="bibr" target="#b10">[11]</ref>. We compute Pearson correlation coefficient (PCC), Spearman correlation coefficient (SCC), binary decision consensus accuracy (Acc.), and Kendall's ? coefficients between metric scores and human judgment scores. The human judgment scores are pre-processed using the ratio of n/5 where n is the number of votes. Pearson correlation coefficient is a statistic that measures the linear correlation, while Spearman correlation coefficient evaluates a monotonic relationship rather than the raw values. Following <ref type="bibr" target="#b10">[11]</ref>, the accuracy is measured for binary decision consensus for the seen and swapped captions. Additionally, we report two variants of the Kendall's ? having a better confidence interval for more precise comparison.</p><p>We clarify that even with the released asset, we cannot reproduce the reported scores for the CLIP-R-Precision of some splits. So, we report our reproduction along with their reported scores. Following the released code, we exploit available bounding boxes for the image pre-processing and sample the negative examples from a randomly-chosen different class for the CLIP-R-Precision. Notice that CLIP-R-Precision has a moderate variance since the sampled negative examples impact the score while risking some degree of false negative describing narrow domains of birds and flowers.</p><p>We conducted the experiment using both the pre-trained CLIP and the CLIP model further fine-tuned on the corresponding dataset. Note that CLIP model is based on ResNet 101. Since CLIP-R-Precision score is determined by a set of caption candidates, we repeat 10 times to construct the negative examples and report standard deviation. To measure the proposed MID, the training set for fine-tuning CLIP is used as the reference set (X, Y) in Equation 25 (Appendix), while the scored image-caption pairs are the evaluation samples (X,?). In most cases, regardless of whether it is fine-tuned or not, the proposed MID outperforms CLIP-S and CLIP-R-Precision. These results suggest that our MID has better generalization capabilities.  <ref type="bibr" target="#b10">[11]</ref>. 'FT' denotes whether the pre-trained CLIP model is further fine-tuned on the corresponding dataset which is available at https://github. com/Seth-Park/comp-t2i-dataset, and ? denotes our reproduction. The best result and the second best result are boldfaced and underlined, respectively.  <ref type="figure">Figure 11</ref>: The visualization of the FOIL hallucination detection. FO and GT denote FOIL and ground-truth captions, respectively. The foiled word is highlighted by red and its counterpart is by blue. For each caption, we report the scores of CLIP-S <ref type="bibr" target="#b18">[19]</ref>, RefCLIP-S <ref type="bibr" target="#b18">[19]</ref>, and MID (ours). First two columns show the corrected examples, while the third column shows an example that CLIP-S and MID are failed to detect. The smaller score is better for the FOIL captions. As pointed out in Section 4.2, RefCLIP-S directly exploits the reference captions where the counterpart of the foiled word is appeared. One of references of the third example was that "a woman is picking bananas from a basket." The fourth example shows that MID can be negative for unlikely samples since it is based on the definition of differential entropy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 RN101 14 MID 3 RN101 3 RN101 14 LAFITE 1 Figure 1 :</head><label>314331411</label><figDesc>ViT-B/32 ViT-B/16 ViT-L/ViT-B/32 ViT-B/16 ViT-L/14 ViT-B/32 ViT-B/16 ViT-L/Left. The schematic diagram of our proposed method. Right. The consistent property of Gaussian MI is in stark contrast to CLIP-S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 Figure 2 :</head><label>12</label><figDesc>The histogram shows the frequencies of the competing methods using the CLIP of ViT-L/14 for the four-scale judgments from the COCO dataset. For the detail of method, please see the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>B/32 ViT-B/16 ViT-L/14 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Flickr8K parsimony.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>FOIL parsimony.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>supports this. Our method is robust via covariance estimation in both 1-ref (green line) and 4-ref (blue line), while RefCLIP-S is reducing to its reference-free version of CLIP-S degrading performance. We interpolate the scores of RefCLIP-S and CLIP-S as a reference (yellow line). If we similarly utilize the term of max(max r?R cos(r, y), 0) as in Equation 8, MID gets 92.4 and 93.7, outperforming RefCLIP-S (Appendix G for the detail,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>2022/04/29 10:59 AM HIT Please read the instruction carefully. If you don't follow, you're at high risk of being rejected. For each pair of image and caption, please judge image quality and text-to-image alignment with a 1-to-4 scale by clicking a corresponding button. Image quality: This measures whether a given image looks realistic, allowing the deviation from its caption if any. (1: far from realistic, 2: partly realistic, 3: fairly realistic except for some parts, 4: obviously realistic) Text-to-image alignment: This measures whether the given text and image are correctly aligned. The image should explain the caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>The instruction appeared on top of the AMT task interface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>The one of ten samples in an assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 Figure 9 :</head><label>19</label><figDesc>Left. The number of assignments per worker. Right. Time spent per assignment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FO:FO:FO:</head><label></label><figDesc>A tall book tower with people walking down a city street. (CLIP-S/RefCLIP-S/ PMI=.737/.804/16.9) GT: A tall clock tower with people walking down a city street. (.738/.806/20.0) Some people a chair some bananas and plastic cups. (.747/.832/8.63) GT: Some people a table some bananas and plastic cups. (.756/.843/12.0) FO: a giraffe grazing on grass in an open field. (.707/.747/-4.89) GT: A zebra grazing on grass in an open field. (.718/.821/38.6) Large bowls of broccoli bunches being examined by a female buyer. (.774/.811/11.8) GT: Large bowls of banana bunches being examined by a female buyer. (.761/.815/9.61) A woman is tending to the different containers of food. A woman is picking bananas from a basket. A street vendor setting out buckets of bunches of bananas. A person putting bananas in baskets near a bench. FO: A boat that is flying in t sky. (.728/.791/12.7) GT: A airplane that is flying the sky. (.779/.845/37.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Generated Likert-scale judgment correlation using LAFITE. ? uses the i.i.d. samples having at least one detected object, which was 88.3% of samples, to calculate the SOA accuracy per image.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Kendall ?c Kendall ? b</cell></row><row><cell>SOA  ? [8]</cell><cell>YOLO-V3</cell><cell>47.3</cell><cell>51.8</cell></row><row><cell>CLIP-S [19]</cell><cell>CLIP (ViT-B/32)</cell><cell>40.8</cell><cell>35.3</cell></row><row><cell>InfoNCE [37]</cell><cell>CLIP (ViT-B/32)</cell><cell>44.1</cell><cell>38.2</cell></row><row><cell>CLIP-R-Precision [11]</cell><cell>CLIP (ViT-B/32)</cell><cell>66.0</cell><cell>56.1</cell></row><row><cell cols="2">OFA-Captioning+CLIP-S [19, 38] OFA-Large + CLIP (ViT-B/32)</cell><cell>72.0</cell><cell>62.3</cell></row><row><cell>CLIP-S [19]</cell><cell>CLIP (ViT-L/14)</cell><cell>52.2</cell><cell>45.2</cell></row><row><cell>InfoNCE [37]</cell><cell>CLIP (ViT-L/14)</cell><cell>64.8</cell><cell>56.1</cell></row><row><cell>CLIP-R-Precision [11]</cell><cell>CLIP (ViT-L/14)</cell><cell>69.6</cell><cell>58.1</cell></row><row><cell cols="2">OFA-Captioning+CLIP-S [19, 38] OFA-Large + CLIP (ViT-L/14)</cell><cell>73.7</cell><cell>63.8</cell></row><row><cell>MID (ours)</cell><cell>CLIP (ViT-B/32)</cell><cell>74.6</cell><cell>64.6</cell></row><row><cell>MID (ours)</cell><cell>CLIP (ViT-L/14)</cell><cell>87.3</cell><cell>75.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Kendall ?c Kendall ? b</cell></row><row><cell>SOA  ? [8]</cell><cell>YOLO-V3</cell><cell>5.6</cell><cell>7.2</cell></row><row><cell>CLIP-S [19]</cell><cell>CLIP (ViT-B/32)</cell><cell>10.6</cell><cell>10.6</cell></row><row><cell>InfoNCE [37]</cell><cell></cell><cell></cell><cell></cell></row></table><note>AMT human Likert-scale judgment correlation. Using the 2K fake images from LAFITE and VQ-Diffusion, we collected fine-grained text-image alignment judgments and measure the judgment correlation of competing metrics. ? 90.2% of samples having at least one detected object.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Visual reasoning accuracy using the foiled caption trick.</figDesc><table><row><cell></cell><cell cols="6">Object Count Color Spatial Object Count Color Spatial</cell></row><row><cell>Metric</cell><cell></cell><cell>CLIP ViT-B/32</cell><cell></cell><cell></cell><cell>CLIP ViT-L/14</cell><cell></cell></row><row><cell>CLIP-S [19]</cell><cell>0.318</cell><cell>0.026 0.068</cell><cell>0.025</cell><cell>0.585</cell><cell>0.157 0.209</cell><cell>0.169</cell></row><row><cell>CLIP-R-Precision [11]</cell><cell>0.168</cell><cell>0.016 0.031</cell><cell>0.019</cell><cell>0.238</cell><cell>0.046 0.058</cell><cell>0.041</cell></row><row><cell>SOA [8]</cell><cell>0.367</cell><cell>0.028 0.039</cell><cell>0.035</cell><cell>0.365</cell><cell>0.030 0.035</cell><cell>0.030</cell></row><row><cell>InfoNCE [37]</cell><cell>0.416</cell><cell>0.042 0.094</cell><cell>0.049</cell><cell>0.675</cell><cell>0.230 0.284</cell><cell>0.243</cell></row><row><cell>MID (ours)</cell><cell>0.792</cell><cell>0.290 0.332</cell><cell>0.280</cell><cell>0.843</cell><cell>0.443 0.481</cell><cell>0.457</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 2(top left) shows the histogram of the frequencies of PMI showing the four categories. Fake samples have generally lower values than Real's. Foiled Fake has lower values than Real and Fake having a long tail. We observe that foiled caption broadly impacts the text-image alignment as we expected. The negative PMI is often observed for the fake images, which are deviated from the distribution of real images. Whereas, the histogram of CLIP-S 4 (bottom right) and InfoNCE (bottom left) show Fake samples where its scores are higher than Real's, while the overlapping areas of Fake and Foiled Fake (62.2% and 61.6% for CLIP-S and InfoNCE, respectively) are greater than PMI's (55.2%), making it difficult to differentiate the degree of text-image alignment. For the caption generation method<ref type="bibr" target="#b8">[9]</ref>, we exploit the current state-of-the-art image captioning model of the OFA-Large<ref type="bibr" target="#b37">[38]</ref> pre-trained on a huge mixture of publicly available datasets to generate captions for the generated images. Then, CLIP-S<ref type="bibr" target="#b18">[19]</ref> is used to assess the quality of image captioning. Notice that CLIP-S outperforms traditional image captioning metrics as shown in Section 4.2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Flickr8K-Expert human judgment correlation.</figDesc><table><row><cell>Method</cell><cell>Kendall ?c</cell></row><row><cell>BLEU-1 [12]</cell><cell>32.3</cell></row><row><cell>BLEU-4 [12]</cell><cell>30.8</cell></row><row><cell>ROUGE-L [14]</cell><cell>32.3</cell></row><row><cell>BERT-S (RoBERTa-F)</cell><cell>39.2</cell></row><row><cell>METEOR [13]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Flickr8K-CF human judgment correlation.</figDesc><table><row><cell>Method</cell><cell>Kendall ? b</cell></row><row><cell>BLEU-4 [12]</cell><cell>16.9</cell></row><row><cell>CIDEr [15]</cell><cell>24.6</cell></row><row><cell>METEOR [13]</cell><cell>22.2</cell></row><row><cell>ROUGE-L [14]</cell><cell>19.9</cell></row><row><cell>SPICE [16]</cell><cell>24.4</cell></row><row><cell>BERT-S (RoBERTa-F)</cell><cell>22.8</cell></row><row><cell>LEIC [45]</cell><cell>29.5</cell></row><row><cell>CLIP-S [19]</cell><cell>34.4</cell></row><row><cell>RefCLIP-S [19]</cell><cell>36.4</cell></row><row><cell>MID (ours)</cell><cell>37.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Pascal-50S accuracy. Please see the text for the definition of subsets, HC, HI, HM, and MM.</figDesc><table><row><cell>Method</cell><cell>HC</cell><cell>HI</cell><cell cols="2">HM MM Mean</cell></row><row><cell>length</cell><cell cols="3">51.7 52.3 63.6 49.6</cell><cell>54.3</cell></row><row><cell>BLEU-4 [12]</cell><cell cols="3">60.4 90.6 84.9 54.7</cell><cell>72.6</cell></row><row><cell>SPICE [16]</cell><cell cols="3">63.6 96.3 86.7 68.3</cell><cell>78.7</cell></row><row><cell>METEOR [13]</cell><cell cols="3">63.8 97.7 93.7 65.4</cell><cell>80.1</cell></row><row><cell>ROUGE-L [14]</cell><cell cols="3">63.7 95.3 92.3 61.2</cell><cell>78.1</cell></row><row><cell>CIDEr [15]</cell><cell cols="3">65.1 98.1 90.5 64.8</cell><cell>79.6</cell></row><row><cell cols="4">BERT-S (RoBERTa-F) 65.4 96.2 93.3 61.4</cell><cell>79.1</cell></row><row><cell>TIGEr [17]</cell><cell cols="3">56.0 99.8 92.8 74.2</cell><cell>80.7</cell></row><row><cell cols="4">ViLBERTScore-F [18] 49.9 99.6 93.1 75.8</cell><cell>79.6</cell></row><row><cell>BERT-S++ [32]</cell><cell cols="3">65.4 98.1 96.4 60.3</cell><cell>80.1</cell></row><row><cell>CLIP-S [19]</cell><cell cols="3">56.5 99.3 96.4 70.4</cell><cell>80.7</cell></row><row><cell>RefCLIP-S [19]</cell><cell cols="3">64.5 99.6 95.4 72.8</cell><cell>83.1</cell></row><row><cell>MID (ours)</cell><cell cols="3">67.0 99.7 97.4 76.8</cell><cell>85.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>FOIL hallucination pairwise detection accuracy results.The methods utilize either one or four references.</figDesc><table><row><cell>Method</cell><cell cols="2">1-ref 4-ref</cell></row><row><cell>length</cell><cell>50.2</cell><cell>50.2</cell></row><row><cell>BLEU-4 [12]</cell><cell>66.5</cell><cell>82.6</cell></row><row><cell>METEOR [13]</cell><cell>78.8</cell><cell>85.4</cell></row><row><cell>ROUGE-L [14]</cell><cell>71.7</cell><cell>79.3</cell></row><row><cell>CIDEr [15]</cell><cell>82.5</cell><cell>90.6</cell></row><row><cell>SPICE [16]</cell><cell>75.5</cell><cell>86.1</cell></row><row><cell>BERT-S</cell><cell>88.6</cell><cell>92.1</cell></row><row><cell>CLIP-S [19]</cell><cell>87.2</cell><cell>87.2</cell></row><row><cell cols="2">RefCLIP-S [19] 91.0</cell><cell>92.6</cell></row><row><cell>MID (ours)</cell><cell>90.5</cell><cell>90.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>[48] Hanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo Jo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the mlaas platform with a real-world case study. arXiv preprint arXiv:1810.09957, 2018. [49] John Duchi. Derivations for linear algebra and optimization. Berkeley, California, 3(1): 2325-5870, 2007. [50] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [51] Ming Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiao-Yuan Jing, Fei Wu, and Bingkun Bao. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. arXiv preprint arXiv:2008.05865, 2020.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Generated Likert-scale judgment correlation using VQ-Diffusion. ? uses the i.i.d. samples having at least one detected object, which was 88.1%, to calculate the SOA accuracy per image.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="2">Kendall ?c Kendall ? b</cell></row><row><cell>SOA  ? [8]</cell><cell>YOLO-V3</cell><cell>37.0</cell><cell>38.4</cell></row><row><cell>CLIP-S [19]</cell><cell>CLIP (ViT-B/32)</cell><cell>70.3</cell><cell>60.9</cell></row><row><cell>InfoNCE [37]</cell><cell>CLIP (ViT-B/32)</cell><cell>74.2</cell><cell>64.3</cell></row><row><cell>CLIP-R-Precision [11]</cell><cell>CLIP (ViT-B/32)</cell><cell>66.5</cell><cell>54.5</cell></row><row><cell cols="2">OFA-Captioning+CLIP-S [19, 38] OFA-Large + CLIP (ViT-B/32)</cell><cell>73.8</cell><cell>63.9</cell></row><row><cell>CLIP-S [19]</cell><cell>CLIP (ViT-L/14)</cell><cell>70.9</cell><cell>61.4</cell></row><row><cell>InfoNCE [37]</cell><cell>CLIP (ViT-L/14)</cell><cell>78.0</cell><cell>67.6</cell></row><row><cell>CLIP-R-Precision [11]</cell><cell>CLIP (ViT-L/14)</cell><cell>68.5</cell><cell>56.5</cell></row><row><cell cols="2">OFA-Captioning+CLIP-S [19, 38] OFA-Large + CLIP (ViT-L/14)</cell><cell>74.2</cell><cell>64.3</cell></row><row><cell>MID (ours)</cell><cell>CLIP (ViT-B/32)</cell><cell>79.8</cell><cell>69.1</cell></row><row><cell>MID (ours)</cell><cell>CLIP (ViT-L/14)</cell><cell>82.0</cell><cell>71.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>The performance reports of text-to-image generative models. ? The publicly-released model using the filtered subset of training dataset (https://github.com/openai/glide-text2im). For the backbone, I, Y, C stands for Inception-V3<ref type="bibr" target="#b6">[7]</ref>, YOLO-V3<ref type="bibr" target="#b25">[26]</ref>, and CLIP<ref type="bibr" target="#b9">[10]</ref>.</figDesc><table><row><cell>Metric</cell><cell>FID ?</cell><cell cols="3">SOA-C ? SOA-I ? FID (ours) ?</cell><cell>MID ?</cell><cell>MID ?</cell></row><row><cell>Backbone</cell><cell>I</cell><cell>Y</cell><cell>Y</cell><cell>I</cell><cell cols="2">C (ViT-B/32) C (ViT-L/14)</cell></row><row><cell>GLIDE [50]  ?</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32.08?.05</cell><cell>-01.00?.16</cell><cell>-01.03?.06</cell></row><row><cell>AttnGAN [24]</cell><cell>33.10?.11</cell><cell>25.88</cell><cell>39.01</cell><cell>29.15?.06</cell><cell>0-8.90?.18</cell><cell>-65.20?.92</cell></row><row><cell>DM-GAN [44]</cell><cell>27.34?.11</cell><cell>33.44</cell><cell>48.03</cell><cell>22.90?.06</cell><cell>-03.51?.20</cell><cell>-44.66?.71</cell></row><row><cell>OP-GAN [8]</cell><cell>24.70?.09</cell><cell>35.85</cell><cell>50.47</cell><cell>22.14?.01</cell><cell>0-1.32?.10</cell><cell>-50.34?.84</cell></row><row><cell>DF-GAN [51]</cell><cell>21.42?.00</cell><cell>-</cell><cell>-</cell><cell>31.75?.06</cell><cell>-15.21?.12</cell><cell>-58.75?.16</cell></row><row><cell cols="2">VQ-Diffusion [42] 13.86?.00</cell><cell>-</cell><cell>-</cell><cell>13.13?.05</cell><cell>-05.77?.11</cell><cell>-19.63?.28</cell></row><row><cell>LAFITE [40]</cell><cell>08.12?.00</cell><cell>61.09</cell><cell>74.78</cell><cell>08.03?.01</cell><cell>-35.17?.20</cell><cell>-06.26?.69</cell></row><row><cell>Real</cell><cell>6.09?.05</cell><cell>74.97</cell><cell>80.84</cell><cell>02.73?.15</cell><cell>41.63?.06</cell><cell>-57.44?.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>The comp-t2i human judgment correlation</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For the details, please refer to the object hallucination section in Section 4.2 and Figure 11 in Appendix.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/drboog/Lafite 4 The CLIP-S [19] is defined as 2.5 ? cos(x, y) which is the scaled cosine similarity of the CLIP features.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://www.w3.org/TR/css-color-3/#html4</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://github.com/drboog/Lafite</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely thank Dongyoon Han for reviewing our manuscript and providing helpful comments. Also, we give thanks to Jung-Woo Ha for early discussions and suggestions throughout the project. The NAVER Smart Machine Learning (NSML) platform [48]  has been used in the experiments.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric</head><p>FT PCC SCC Acc. ?c ? b</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C-CUB Color</head><p>Human <ref type="bibr" target="#b10">[11]</ref> -0.5949?.000 0.5890?.000 81.7?.00 --DAMSM <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b23">24]</ref> -0.0503?.000 0.1224?.000 54.9?.00 --CLIP-S <ref type="bibr" target="#b18">[19]</ref> 0.1919?.000 0.1865?.000 67.0?.00 14.78?.000 13.59?.000 CLIP-R-Precision <ref type="bibr">[</ref> </p><p>where MID is parameterized by the referencing image and caption statistics. Notice that RefCLIP-S also has the hyper-parameter of 2.5 in the CLIP-S to balance with the cosine similarity term.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adversarial text-to-image synthesis: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislav</forename><surname>Frolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Raue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">J?rn Hees, and Andreas Dengel</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">144</biblScope>
			<biblScope unit="page" from="187" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Microsoft coco captions: Data collection and evaluation server</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00325</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Toward a generalization metric for deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Tung</forename><surname>Hoang Thanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS Workshop on &quot;I Can&apos;t Believe It&apos;s Not Better!</title>
		<meeting>the NeurIPS Workshop on &quot;I Can&apos;t Believe It&apos;s Not Better!</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">GANs trained by a two time-scale update rule converge to a local Nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6627" to="6638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semantic object accuracy for generative text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Heinrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wermter</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Benchmark for compositional text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samaneh</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note>Round 1</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</title>
		<meeting>the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Text Summarization Branches Out</title>
		<meeting>the ACL Workshop on Text Summarization Branches Out</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision</title>
		<meeting>the European conference on computer vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tiger: Text-to-image grounding for image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jana</forename><surname>Diesner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2141" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vilbertscore: Evaluating image caption using vision-and-language bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanhee</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Doo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluation and Comparison of NLP Systems</title>
		<meeting>the 1st Workshop on Evaluation and Comparison of NLP Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="34" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CLIPScore: A Reference-free Evaluation Metric for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Ronan Le Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empirical Methods in Natural Language Processing</title>
		<meeting>the Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">It&apos;s easier to translate out of english than into it: Measuring neural translation difficulty by cross-mutual information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Bugliarello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sabrina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonios</forename><surname>Mielke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Anastasopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naoaki</forename><surname>Cotterell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Okazaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1640" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Improved precision and recall metric for assessing generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Kynk??nniemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Reliable fidelity and diversity metrics for generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Muhammad Ferjad Naeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjung</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjey</forename><surname>Uh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7176" to="7185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attngan: Fine-grained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Eccv caption: Correcting false negatives by collecting machine-and-human-verified image-caption associations for ms-coco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsuk</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seong Joon</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.03359</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object hallucination in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4035" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="49" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Dall-eval: Probing the reasoning skills and social biases of text-to-image generative transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhay</forename><surname>Zala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.04053</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving image captioning evaluation by considering inter references variance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangyu</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinglu</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bertscore: Evaluating text generation with bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varsha</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Kilian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Maximum-entropy distributions having prescribed first and second moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wragg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="689" to="693" />
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingren</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03052</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Foil it! find one mismatch between image and language caption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandro</forename><surname>Pezzelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yauhen</forename><surname>Klimovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aur?lie</forename><surname>Herbelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaella</forename><surname>Bernardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="255" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changyou</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tensmeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lafite</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.13792</idno>
		<title level="m">Towards language-free training for text-to-image generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A new measure of rank correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Maurice</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kendall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="1938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Vector quantized diffusion model for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.14822</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tedigan: Text-guided diverse face image generation and manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing-Hao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2256" to="2265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning to evaluate image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guandao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5804" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Pelkins Ajanoh, and Mohamed Coulibali. Nubia: Neural based interchangeability assessor for text generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Kane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Muhammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Kocyigit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdalla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Evaluating NLG Evaluation</title>
		<meeting>the 1st Workshop on Evaluating NLG Evaluation</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="28" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Framing image description as a ranking task: Data, models and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
		<idno>LAFITE CLIP-S: 0.07 (0.60) MID (ours): 0.09 (-28.34) Human: 0.31 (2.33</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="853" to="899" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Orange fire hydrant on the sidewalk in a commercial area</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
