<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Moir? Photo Restoration Using Multiresolution Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Moir? Photo Restoration Using Multiresolution Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Moir? pattern</term>
					<term>neural network</term>
					<term>image restora- tion</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Digital cameras and mobile phones enable us to conveniently record precious moments. While digital image quality is constantly being improved, taking high-quality photos of digital screens still remains challenging because the photos are often contaminated with moir? patterns, a result of the interference between the pixel grids of the camera sensor and the device screen. Moir? patterns can severely damage the visual quality of photos. However, few studies have aimed to solve this problem. In this paper, we introduce a novel multiresolution fully convolutional network for automatically removing moir? patterns from photos. Since a moir? pattern spans over a wide range of frequencies, our proposed network performs a nonlinear multiresolution analysis of the input image before computing how to cancel moir? artefacts within every frequency band. We also create a large-scale benchmark dataset with 100, 000 + image pairs for investigating and evaluating moir? pattern removal algorithms. Our network achieves state-of-the-art performance on this dataset in comparison to existing learning architectures for image restoration problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Digital cameras and mobile phones enable us to conveniently record precious moments. While digital image quality is constantly being improved, taking high-quality photos of digital screens still remains challenging because the photos are often contaminated with moir? patterns, a result of the interference between the pixel grids of the camera sensor and the device screen. Moir? patterns can severely damage the visual quality of photos. However, few studies have aimed to solve this problem. In this paper, we introduce a novel multiresolution fully convolutional network for automatically removing moir? patterns from photos. Since a moir? pattern spans over a wide range of frequencies, our proposed network performs a nonlinear multiresolution analysis of the input image before computing how to cancel moir? artefacts within every frequency band. We also create a large-scale benchmark dataset with 100, 000 + image pairs for investigating and evaluating moir? pattern removal algorithms. Our network achieves state-of-the-art performance on this dataset in comparison to existing learning architectures for image restoration problems.</p><p>Index Terms-Moir? pattern, neural network, image restoration I. INTRODUCTION N OWADAYS, digital cameras and mobile phones play a significant role in people's lives. They enable us to easily record any precious moments that are interesting or meaningful. There exist many occasions when people would like to capture digital screens. Such occasions include taking photos of visual contents on a screen, or shooting scenes involving digital monitors. While image quality is constantly being improved, taking high-quality photos of digital screens still remains challenging. Such photos are often contaminated with moir? patterns <ref type="figure" target="#fig_3">(Fig. 4)</ref>.</p><p>A moir? pattern in the photo of a screen is the result of the interference between the pixel grids of the camera sensor and the device screen. It can appear as stripes, ripples, or curves of intensity and colour diversifications superimposed onto the photo. The moir? pattern can vary dramatically due to a slight change in shooting distance or camera orientation. This moir? artefact severely damages the visual quality of the photo. There is a large demand for post-processing techniques capable of removing such artefacts. In this paper, we call images of digital screens taken with digital devices moir? photos.</p><p>It is particularly challenging to remove moir? patterns in photos, which are mixed with original image signals across a wide range in both spatial and frequency domains. A moir? pattern typically covers an entire image. The colour or thickness of the stripes or ripples in such patterns not only <ref type="bibr">Department</ref>   changes from image to image, but also is spatially varying within the same image. Thus, a moir? pattern could occupy a high-frequency range in one image region, but a low-frequency range in another region. Due to the complexity of moir? patterns in photos, little research has been dedicated to moir? pattern removal. Conventional image denoising <ref type="bibr" target="#b0">[1]</ref>, and texture removal techniques <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> are not well suited for this problem because these techniques typically assume noises and textures occupy a higher-frequency band than true image structures.</p><p>On the other hand, convolutional neural networks are leading a revolution in computer vision and image processing. After successes in image classification and recognition <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, they have also been proven highly effective in lowlevel vision and image processing tasks, including image super-resolution <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, demosaicking <ref type="bibr" target="#b7">[8]</ref>, denoising <ref type="bibr" target="#b8">[9]</ref>, and restoration <ref type="bibr" target="#b9">[10]</ref>.</p><p>In this paper, we introduce a novel multiresolution fully convolutional neural network for automatically removing moir? patterns from photos. Since a moir? pattern spans over a wide range of frequencies, to make the problem more tractable, our network first converts an input image into multiple feature maps at various different resolutions, which include different levels of details. Each feature map is then fed into a stack of cascaded convolutional layers that maintain the same input and output resolutions. These layers are responsible for the core task of canceling the moir? effect associated with a specific frequency band. The computed components at different resolutions are finally upsampled to the input resolution and fused together as the final output image.</p><p>To train and test our multiresolution network, we also create a dataset of 135,000 image pairs, each containing an image contaminated with moir? patterns and its corresponding uncontaminated reference image. The reference images are arXiv:1805.02996v1 [cs.CV] 8 May 2018 taken from the ImageNet dataset. The contaminated images have a wide variety of moir? effects. They are obtained by taking photos of reference images displayed on a computer screen using a mobile phone. To our knowledge, this is the first large-scale dataset for research on moir? pattern removal. The proposed network achieves state-of-the-art performance on this dataset, compared with existing learning architectures for image restoration problems.</p><p>We summarise our contributions in this paper as follows. <ref type="bibr" target="#b0">1</ref> We present a novel and highly effective learning architecture for restoring images contaminated with moir? patterns. <ref type="bibr" target="#b1">2</ref> We also create the first large-scale benchmark dataset for moir? pattern removal. This dataset contains 100, 000 + image pairs, and will be publicly released for research and evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Moir? Effect</head><p>When two similar, repetitive patterns of lines, circles, or dots overlap with imperfect alignment, a new dynamic pattern appears. This new pattern is called the moir? pattern, which can involve multiple colours. A moir? pattern changes the shape and frequency of its elements when the two original patterns move relative to each other ( <ref type="figure" target="#fig_1">Fig. 2)</ref>.</p><p>Moir? patterns are large-scale interference patterns. For such interference patterns to occur, the two original patterns must not be completely aligned. Moir? patterns magnify misalignments. The slightest misalignment between the two original patterns could give rise to a large-scale, easily visible moir? pattern. As the degree of misalignment increases, the frequency of the moir? pattern may also increase.</p><p>Moir? patterns often occur as an artefact of images generated by digital imaging or computer graphics techniques, such as when scanning a printed halftone picture or rendering a checkerboard pattern that extends toward the horizon <ref type="bibr" target="#b10">[11]</ref>. The latter is also a case of aliasing due to undersampling a fine regular pattern. a) Moir? Photos: Photographs of a computer or TV screen taken with a digital camera often exhibit moir? patterns. Examples are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. This is because a screen consists of a grid of pixels while the camera sensor is another grid of pixels. When one grid is mapped to another grid, pixels in these two grids do not line up exactly, giving rise to moir? patterns.</p><p>Similar to the formation of general moir? patterns, when the relative position between a screen and a digital camera changes, the moir? pattern in the image can change dramatically. It can be 1) of various types: stripes, dots or waves, 2) of various scales, 3) of various levels of intensity, 4) anisotropic or isotropic, and 5) uniform or non-uniform. Removing such moir? patterns with diverse properties is a challenging problem.</p><p>The occurrence of moir? patterns in photographs of computer or TV screens does not indicate a defect in the screen but is a result of a practical limitation in display technology. In order to completely eliminate moir? patterns, the dot or stripe pitch on the screen would have to be significantly smaller than the size of a pixel in the camera, which is generally not possible <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Related Work</head><p>Moir? Pattern Removal Several methods have been proposed to remove different types of moir? patterns. Sidorov and Kokaram <ref type="bibr" target="#b12">[13]</ref> presented a spectral model to suppress moir? patterns in film-to-video transfer using telecine devices. However, the moir? patterns they deal with are monotonous and monochrome. Thus, their method is unsuitable for eliminating the moir? patterns in our context. Observing that moir? patterns on textures are dissimilar while a texture is locally well-patterned, Liu et al. <ref type="bibr" target="#b13">[14]</ref> proposed a low-rank and sparse matrix decomposition method to remove moir? patterns on high-frequency textures. Because our moir? patterns occur on high-frequency textures as well as on low-frequency structures, the method in <ref type="bibr" target="#b13">[14]</ref> is unable to solve our problem. Taking advantage of frequency domain statistics, Sur and Gr?diac <ref type="bibr" target="#b14">[15]</ref> proposed to remove quasi-periodic noise. Different from our moir? patterns, quasi-periodic noise is simple and regular. Due to the complexity of our moir? patterns, aforementioned methods cannot remove the artefacts well while preserving the original image appearance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Descreening</head><p>In order to print continuous tone images, most electrophotographic printers take advantage of the halftoning techniques, which rely on local dot patterns to approximate continuous tones. Scanned copies of such printed images are commonly corrupted with screen-like highfrequency artefacts (moir? effect), exhibiting low aesthetic quality. Image descreening aims at reconstructing high-quality images from scanned versions of images printed using halftoning (such as scanned books), and has been well studied in the past decades. Various methods have been proposed, such as printer-end algorithms <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, image smoothing techniques <ref type="bibr" target="#b17">[18]</ref>, learning based methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, and advanced filters <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b22">[23]</ref>. Specialised methods have been proposed to process a specific subset of images, such as paper checks <ref type="bibr" target="#b23">[24]</ref>. Shou and Lin <ref type="bibr" target="#b19">[20]</ref> descreened images on the basis of a learning based pattern classification process. They found that it is sufficient to consider two classes of moir? patterns to produce satisfactory results. The reason is that halftoning typically involves binary colours, and that the viewing distance and angle during scanning are almost fixed. Such constraints make moir? patterns in the descreening problem regular, uniform, and local. Therefore, existing image descreening techniques are inadequate to deal with our complex moir? patterns.</p><p>Texture Removal Since moir? patterns in photos often have high-frequency and repetitive components, texture removal algorithms are a class of relevant techniques. Xu et al. <ref type="bibr" target="#b1">[2]</ref> introduced relative total variation to describe and identify textures. Karacan et al. <ref type="bibr" target="#b24">[25]</ref> took advantage of region covariances to separate texture from image structure. Ono et al. <ref type="bibr" target="#b25">[26]</ref> utilised block-wise low-rank texture characterisation to decompose images into texture and structure components. Cho et al. <ref type="bibr" target="#b2">[3]</ref> combined the bilateral filter with a "patch shift" texture range kernel to achieve a similar goal. Sun et al. <ref type="bibr" target="#b26">[27]</ref> took advantage of l 0 norm to retrieve structures from textured images. Ham et al. <ref type="bibr" target="#b27">[28]</ref> performed texture removal through image filtering with joint static and dynamic guidance. Stateof-the-art methods define a variety of local filters to remove high-frequency textures. However, moir? patterns in photos are not merely high-frequency artefacts but span a wide range of frequencies. In addition, moir? patterns also introduce colour distortions, which existing texture removal algorithms would not be able to remove.</p><p>Image Restoration Image restoration problems aim at removing noises or reconstructing high-frequency details. Recently, learning techniques have been successfully applied to image restoration tasks, including image super-resolution <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, denoising <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, and deblurring <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b28">[29]</ref>. These learning based methods have achieved state-of-the-art performance in image quality improvement. The problem we aim to solve in this paper can be considered as a special image restoration problem as well since it attempts to reconstruct the uncontaminated image by removing moir? artefacts. However, different from the uniformly distributed noises in the denoising task and the missing high-frequency details in the superresolution task, the moir? patterns in our problem can be anisotropic and non-uniform, and exhibit features across a wide range of frequencies. The models employed in traditional image restoration tasks are not specifically tailored for our problem and can only achieve suboptimal performance. Most recently, Gharbi et al. <ref type="bibr" target="#b7">[8]</ref> presented a learning-based method to demosaic and denoise images. However, demosaicking is also limited to removing high-frequency artefacts only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. MULTIRESOLUTION DEEP CNN FOR MOIR? PATTERN REMOVAL</head><p>By considering problem complexity, we choose CNNs to remove moir? patterns in photographs due to their recent impressive performance on image restoration tasks. In this section, we present a multiresolution fully convolutional neural network to tackle the problem. It exploits intrinsic correlations between moir? patterns and image components at different levels of a multiresolution pyramid. The training process of our network jointly optimises all parameters to minimize the loss function. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, once trained, our network can automatically remove moir? patterns in contaminated images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>Our network architecture is outlined in <ref type="figure" target="#fig_2">Fig. 3</ref>, which includes multiple parallel branches at different resolutions. The branch at the top processes feature maps at the original resolution of the input image while other branches process coarser and coarser feature maps. The first two convolutional layers in each branch form a group and are responsible for downsampling the feature maps from the immediate higherlevel branch by half if there is such a higher-level branch. Therefore the feature maps generated after the first two convolutional layers at all branches can be stacked together to form an upside-down pyramid, where any feature map has half of the resolution of the feature map at the next higher level. Interestingly, in contrast to traditional image pyramids computed using linear filters, our pyramid is computed using nonlinear "filters" (i.e. convolutional kernels + nonlinear activation functions). By converting the input image into multiple feature maps at various different resolutions, we aim to expose different levels of details in the input image.</p><p>Inside each branch, the output feature maps from the first two layers are fed into a sequence of cascaded convolutional layers. These convolutional layers maintain the same input and output resolutions, and do not perform any downsampling or pooling operations. They are responsible for the core task of canceling the moir? effect associated with the specific frequency band of that branch. Even with the above multiresolution analysis, this is still a hard task that involves sophisticated nonlinear transforms. Therefore, we place multiple convolutional layers (typically 5) each with 3 ? 3 kernels and 64 channels in this sequence.</p><p>To assemble the transformed results from all parallel branches together into a complete output image, we still need to increase the resolution of the feature map generated from the cascaded convolutional layers to the original resolution of the input image within each branch except for the first one. In the i-th branch from the top, we use a set of i ? 1 deconvolutional layers to achieve this goal. Each deconvolutional layer doubles the input resolution. There is an extra convolutional layer following the deconvolutional layers within each branch. This extra layer generates a feature map with 3 channels only. This feature map essentially cancels the component of the moir? pattern (in the input image) associated with the frequency band of that branch. At the end, the final 3-channel feature maps from all branches are simply summed together to produce the final output image with the moir? pattern removed.</p><p>In our network, whenever there is a need to reduce the resolution of a feature map by half, we use a kernel stride 2 instead of a pooling layer. Each layer is followed by a rectified linear unit (ReLU) and we pad zeros to ensure that the output of each layer is of desired size. The detailed configurations of the first two layers and last layers within all branches are given in <ref type="table" target="#tab_2">Table. I and Table.</ref>     <ref type="table" target="#tab_2">1  3x3  1x1  32  1  3x3  1x1  32  2  3x3  2x2  32  2  3x3  1x1  64  3  3x3  2x2  64  3  3x3  1x1  64  4  3x3  2x2  64  4  3x3  1x1  64  5  3x3  2x2  64  5  3x3  1x1  64   TABLE II  UPSAMPLING LAYERS   Scale  Type  Kernel  Stride Channels   1  conv  3x3  1x1  3  2  deconv  4x4  2x2  32  conv  3x3  1x1  3  3  deconv  4x4  2x2  64  deconv  4x4  2x2  32  conv  3x3  1x1  3  4  deconv  4x4  2x2  64  deconv  4x4  2x2  32  deconv  4x4  2x2  32  conv  3x3  1x1  3  5  deconv  4x4  2x2  64  deconv  4x4  2x2  32  deconv  4x4  2x2  32  deconv  4x4  2x2  32  conv  3x3  1x1  3</ref> components of the moir? pattern cannot be removed; if it deals with coarse-scale features only, high-frequency features of the moir? pattern cannot be removed. For these reasons, we perform a multiresolution analysis of the input image and remove the component of the moir? pattern within every frequency band separately. In <ref type="figure" target="#fig_2">Fig. 3</ref>, we illustrate how our network removes a moir? pattern from a contaminated image. The network branch for the original resolution (the finest scale) plays a dominant role because pixel colours in the final output image mostly come from this branch. We can see that moir? artefacts have not been completely removed in the 3-channel feature map produced from the last layer of the top branch ( <ref type="figure" target="#fig_2">Fig. 3(b)</ref>) though such artefacts have become much weaker than those in the original input ( <ref type="figure" target="#fig_2">Fig. 3(a)</ref>). Network branches for other coarser resolutions play a supporting role. The last layer of each coarser-resolution branch produces an image that aims to cancel the remaining moir? pattern (in the image produced from the last layer of the top branch) which falls into its frequency band ( <ref type="figure" target="#fig_2">Fig. 3(c)</ref>). When images from all the branches are summed together, the remaining artefacts in the image from the top branch can be successfully eliminated ( <ref type="figure" target="#fig_2">Fig. 3(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Training</head><p>We train our deep network using a dataset of images,</p><formula xml:id="formula_0">D = {(I i , O i )},</formula><p>where I i is an image contaminated with a moir? pattern and O i is its corresponding ground-truth uncontaminated image. The training process solves for weights w and biases b in our network via minimising the following l 2 loss defined on image patches of size p?p from the training set D in an end-to-end fashion:</p><formula xml:id="formula_1">L({w, b}) = 1 N N i=1 ||S i ? T i || 2 ,<label>(1)</label></formula><p>where N is the total number of image patch pairs and (S i , T i ) is a pair of patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATASET</head><p>We create a benchmark of 135, 000 image pairs, each containing an image contaminated with a moir? pattern and its corresponding uncontaminated reference image. The contaminated images have a wide variety of moir? effects <ref type="figure" target="#fig_3">(Fig.  4)</ref>. The uncontaminated reference images in our benchmark come from the 100,000 validation images and 50,000 testing images of the ImageNet ISVRC 2012 dataset. Of the 135,000 pairs of images, 90% are used as the training set and 10% are used for validation and testing. The pipeline to collect this data is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, which mainly consists of two steps: image capture and alignment. a) Image Capture: Each reference image is enhanced with a black border and displayed at the centre of a computer screen ( <ref type="figure" target="#fig_4">Fig. 5(a)</ref>). The reason to use black for the border is that we observe dark colours are least affected by the moir? effect. To increase the number of corner points that can be used during image alignment, we further extrude a black block from every edge of the black border. We then fill the rest of the screen outside the black border (and blocks) with pure white, which enables us to easily detect the black border in the captured images. We capture displayed images using a mobile phone ( <ref type="figure" target="#fig_4">Fig. 5(d)</ref>). During image acquisition, we randomly change the distance and angle between the mobile phone and the computer screen. Note that we require the black image borders to be always captured.</p><p>Detailed information of the phone models and the monitor screens is shown in <ref type="table" target="#tab_2">Table III and Table IV</ref>, respectively. For each combination of phone model and screen, we collected 15,000 pairs of images. Thus, we collected 15, 000 ? 9 = 135, 000 image pairs in total. Using different phone models as our capture devices ensures that moir? patterns are captured across different optical sensors while the diversity of display screens exhibits the difference in screen resolution.</p><p>b) Image Alignment: The prepared reference images and their corresponding captured images contaminated with moir? patterns have different resolutions and perspective distortions. To train our deep network in an end-to-end manner, we need to register them.  In practice, we rely on the corners along the black image border to accomplish image alignment. Since we use a flat computer screen, the four corners of a captured image (excluding the blocks extruded from the border) lie on a plane. So do the four corners of the prepared reference image. Therefore, corresponding points in both the captured image and reference image are associated via a homography, which can be represented with a 3?3 projective matrix with 8 degrees of freedom. The four black blocks we attached to the image border increase the number of non-collinear corresponding points from 4 to 20, which can improve the registration precision. We use these 20 corners to compute the projective matrix and further align every pair of images.</p><p>To detect the corners, we convert the images into binary images and search for corners along the outermost boundary of the black image border. Traditional corner detection methods, such as the Harris corner detector <ref type="bibr" target="#b29">[30]</ref>, can faithfully detect all corners in a target image ( <ref type="figure">Fig. 6(a)</ref>). However, because of the presence of moir? artefacts, they fail to robustly find the 20 corresponding corners in the source image ( <ref type="figure">Fig. 6(b)</ref>), where  certain edge pixels can be falsely detected as corners.</p><p>To eliminate such false corners, we check the ratio between the numbers of black pixels and white pixels in a square neighbourhood around each detected corner. Since each corner forms a right angle, ideally, the ratio between the numbers of black and white pixels should be either 3 or 1/3. According to this observation, we filter out false corners, where the ratio between the numbers of black and white pixels in a square neighbourhood is clearly different from 3 or 1/3. In practice, we set the neighbourhood size to 11 ? 11. To remove duplicate corners, we set a minimum distance between two distinct corners. When the pairwise distances among two or more detected corners fall below this threshold, we only keep one of them. As shown in <ref type="figure">Fig. 6(c)</ref>, these twenty corners can be successfully detected.</p><p>Finally, with the computed projective matrix, we can align every image pair. The registration results are demonstrated in <ref type="figure" target="#fig_4">Fig. 5(c)</ref> and 5(f). c) Automatic Verification: To automatically verify whether a registration result is correct or not, we measure the PSNR of the registered image pair and use a threshold ? to screen the PSNR value. In our experiments, we set ? = 12. we have found that even images with the most severe moir? artefacts achieve PSNR values higher than 12dB while false registrations produce PSNR values lower than 10dB. The quality distribution of moir? photos in our dataset is shown in <ref type="figure" target="#fig_6">Fig. 8</ref>.</p><p>However, note that PSNR cannot fully reflect the severity of the moir? effect. As shown in <ref type="figure">Fig. 7</ref>, an image corrupted by a visually more severe moir? pattern actually achieves a higher PSNR. This is perhaps because the colour bands in a moir? pattern do not significantly affect PSNR even though they are visually disturbing and easily noticeable. d) Setup: During image acquisition, images are displayed on the screen consecutively. Each reference image stays on the screen for 0.3 seconds. We use a mobile phone to record a video of the consecutively displayed images. Frames from the captured video are then retrieved as images contaminated with moir? patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. MODEL UNDERSTANDING AND IMPLEMENTATION A. Insights Behind Our Network Design</head><p>Moir? patterns span a wide range in both spatial and frequency domains. Therefore, we conceive a multi-resolution architecture, which has convolutional layers with multi-scale receptive fields, to tackle this problem. At the beginning, we experimented with U-Net <ref type="bibr" target="#b30">[31]</ref> with skip connections. Skip connections have been proven to be effective in highlevel vision tasks, such as image recognition and semantic segmentation. However, when tackling low-level vision problems, including super-resolution, denoising and deblurring, many approaches can produce state-of-the-art results without skip connections, such as VDSR, DnCNN and PyramidCNN. In high-level vision problems, the information from highresolution layers close to the input image is useful for the additional clues they introduce. Different from other tasks making use of networks with skip connections, moir? photos and their corresponding ground-truth images can differ dramatically, and thus, skip connections are not powerful enough to model such differences. In addition, the layer closer to the input image in a skip connection contains serious moir? artefacts, as shown in the top row of <ref type="figure" target="#fig_0">Fig. 10</ref>, while the feature maps produced by the deeper layer are relatively moir?-free. As a result, directly using high-frequency details from a layer closer to the input image would likely introduce artefacts in the final result.</p><p>PyramidCNN <ref type="bibr" target="#b28">[29]</ref> also adopts a multi-resolution architecture for deblurring. In their architecture, an input image is first downsampled to k resolutions linearly and then network  branches for different resolutions are trained simultaneously. For the task of deblurring, coarser level output guides the training process of finer level network branches. But for moir? pattern removal, the output from coarser levels is not completely free of moir? artefacts, which tend to make finer levels maintain such artefacts. To achieve better performance, we embed a multi-resolution pyramid in our network architecture. In contrast to traditional image pyramids built with linear filtering, the image pyramid in our architecture is actually built with nonlinear filtering because nonlinear activation always follows each convolutional layer. The nonlinearity in our pyramid allows the network to perform more effectively during downsampling. More importantly, in our network, each resolution is associated with a network branch with six stacked convolutional layers maintaining the same resolution. Such network branches are capable of performing sophisticated nonlinear transformations (such as removing moir? artefacts within a specific frequency band), and are more powerful than skip connections in U-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. A Detailed Study on Our Proposed Model</head><p>To show the advantage of the proposed model, we attempt to test different variants. Model specifications are given as follows: We will demonstrate later that although V Concate achieves a higher PSNR score on the test data, it produces worse visual results than our proposed network. Adding skip connections cannot further improve the performance of the proposed model while the other variants degrade the performance.</p><formula xml:id="formula_2">? V Concate (27.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Grayscale Moir? Artefacts</head><p>To verify that our model can remove moir? patterns rather than the unnatural colours, we convert the RGB dataset to a grayscale one and retrain the network. The average PSNR, SSIM and FSIM on the grayscale testing set are 27.26, 0.852, and 0.910, respectively, indicating that our model is able to deal with moir? patterns regardless of the colour information. Intermediate images produced from different branches on a test RGB image that is close to a grayscale one as well as those produced on its corresponding grayscale image are demonstrated in <ref type="figure" target="#fig_7">Fig. 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation</head><p>We have fully implemented our proposed deep multiresolution network using CAFFE on an NVIDIA Geforce 1080 GPU. The entire training process takes 3 days on average. We use a mini-batch size of 8, start with learning rate 0.0001, set the weight decay to 0.00001, and minimize the loss function using Adam <ref type="bibr" target="#b33">[34]</ref>. We have found that the training process could not converge properly with a higher learning rate. As the training process proceeds, we reduce the learning rate by a factor of 10 when the loss on a validation set stops decreasing. In all the experiments in this paper, we set the patch size p ? p to 256 ? 256. The network weights are randomly initialised using a Gaussian with a zero mean and a standard deviation equal to 0.01. The bias in each neuron is initialised to 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. COMPARISON AND DISCUSSION</head><p>In this section, we experimentally analyse our method's capability in improving image quality and removing moir? artefacts. Since we are not aware of any existing methods that solve exactly the same problem, we compare our method against state-of-the-art methods in related image restoration problems, including image denoising, deblurring, super-resolution and texture removal. We choose VDSR <ref type="bibr" target="#b6">[7]</ref> as a representative from image super-resolution algorithms, DnCNN <ref type="bibr" target="#b8">[9]</ref> and IRCNN <ref type="bibr" target="#b9">[10]</ref> from the latest image denoising methods, and RTV <ref type="bibr" target="#b1">[2]</ref> and SDF <ref type="bibr" target="#b27">[28]</ref> among texture removal techniques. For that a subset of the moir? photos in our dataset has a certain degree of blurriness and that deblurring techniques can reconstruct high-frequency details, we also add two latest learning based image deblurring techniques, multiscale pyramidCNN <ref type="bibr" target="#b28">[29]</ref> and IRCNN <ref type="bibr" target="#b9">[10]</ref>, for comparison. Moreover, since we adopt a hierarchical network architecture, we also compare our network with U-Net <ref type="bibr" target="#b30">[31]</ref>, an effective neural network for image segmentation.</p><p>To perform a fair comparison, we tune the parameters of the methods we compare against so that they reach the optimal performance on our dataset. When a method only has a small number of tuneable parameters, we tune those parameters to make the method achieve the lowest average error on our test set. When a method has a large number of parameters, such as learning based methods, we retrain the model in the method using our training set.</p><p>Even though descreening methods aim at removing a different and simpler moir? effect that occurs in scanned copies of printed documents and images, they are certainly relevant. Since such methods are relatively mature and have been integrated into commercial software, we choose to compare with the descreening function in Photoshop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Quantitative Comparison</head><p>In <ref type="figure" target="#fig_0">Fig. 11</ref> and <ref type="table" target="#tab_4">Table V</ref>, we demonstrate the quantitative performance of different methods on our test set. Since the contaminated image and the reference image within the same pair have different average intensity levels due to multiple reasons, including the brightness of the computer screen and the intensity response curve of the camera during image acquisition, that are mostly irrelevant to the moir? effect, we decided to factor out the differences in average intensity by adjusting the average intensity of the contaminated image to be the same as that of the reference image (Corrected Input). As shown, our method and the variant of our model, V Concate, outperform all other methods participating in the comparison on all performance measures, including PSNR, SSIM <ref type="bibr" target="#b31">[32]</ref> and FSIM <ref type="bibr" target="#b32">[33]</ref>. As the parameters for descreening in Photoshop have to be adjusted manually for each image, we cannot show the average performance on the entire test set. However, we will qualitatively compare it with our method in the next section.</p><p>Effective as a super-resolution method, VDSR <ref type="bibr" target="#b6">[7]</ref> delivers a reasonable performance but is unable to fully handle the complex moir? effect. Using a configuration with a large receptive field, the denoising network (DnCNN) in <ref type="bibr" target="#b8">[9]</ref> has a similar performance as VDSR <ref type="bibr" target="#b6">[7]</ref>. Both VDSR and DnCNN adopt , U-Net <ref type="bibr" target="#b30">[31]</ref>, VDSR <ref type="bibr" target="#b6">[7]</ref>, DnCNN <ref type="bibr" target="#b8">[9]</ref>, pyramidCNN <ref type="bibr" target="#b28">[29]</ref>, RTV <ref type="bibr" target="#b1">[2]</ref> and SDF <ref type="bibr" target="#b27">[28]</ref>.</p><p>a flat CNN architecture that maintains the same resolution across all layers. Nonetheless, both of them have been clearly outperformed by our multiresolution network.</p><p>By defining a denoising prior with dilated convolutions, IRCNN <ref type="bibr" target="#b9">[10]</ref> outperforms state-of-the-art methods in pixelwise image restoration tasks. However, it performs poorly on our dataset and its training process can hardly converge on our training set. After modifying IRCNN by interleaving ordinary convolutions and dilated convolutions, we obtain a revised model called IRCNN-IL. The convergence issue is resolved in the revised model but its performance is still not satisfactory. The PSNR, SSIM and FSIM achieved by IRCNN-IL are 21.55, 0.744, and 0.870, respectively. In theory, the noise IRCNN aims to deal with is completely different from the moir? patterns we attempt to remove. A noisy image is commonly modelled as the result of an additive process, which adds noise to the original signal, but a moir? pattern is a phenomenon caused by light interference, which is a different and much more complicated process. Dilated kernels can remove additive noises but might be insufficient to remove complex moir? patterns. Due to the different underlying mechanisms of image noises and moir? patterns, one cannot be certain that IRCNN is effective for restoring moir? photos.</p><p>Nah et al. <ref type="bibr" target="#b28">[29]</ref> deblur images bottom up using a multiresolution Gaussian pyramid. It first deblurs an image in 1/2 i resolution, then in 1/2 i?1 resolution and finally in the full resolution. The multiresolution architecture helps to produce acceptable results. However, unlike our multiresolution pyramid generated from trainable nonlinear filters (convolutional kernels), their pyramid is generated using the fixed Gaussian filter, which is linear. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref> and <ref type="table" target="#tab_4">Table V</ref>, our network architecture delivers clearly better performance.</p><p>Among all the methods, U-Net <ref type="bibr" target="#b30">[31]</ref> achieves a numerical performance closest to our method. However, we found that even though U-Net produces good statistics, it delivers relatively poor visual results, which will be demonstrated in visual comparisons. Likewise, V Concate produces the highest score on all metrics but its ability in visually removing moir? patterns is less than the original model.</p><p>Texture removal techniques, RTV <ref type="bibr" target="#b1">[2]</ref> and SDF <ref type="bibr" target="#b27">[28]</ref>, are useful in preserving important image structures while eliminating small repetitive textural details. But image features at a similar scale of texture elements would be removed as well.</p><p>In our context, these techniques are used for removing moir? patterns, and they give a poor performance on this task. The difficulty in setting an appropriate texture kernel size could be the main reason because a large smoothing and texture kernel would over-smooth the image while a small kernel would not be able to remove low-frequency large-scale moir? artefacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Visual Comparisons</head><p>We visually compare results from our method against those from other state-of-the-art methods in <ref type="figure" target="#fig_0">Fig. 12</ref>. Additional visual comparisons can be found in the supplemental materials. Note that the input images are all from the test set. From these comparisons, we have the following observations. RTV <ref type="bibr" target="#b1">[2]</ref> and SDF <ref type="bibr" target="#b27">[28]</ref> remove small-scale texture features which typically have higher frequencies than moir? patterns. Descreening in Photoshop over-smoothes the input image. Among deep learning based methods, IRCNN <ref type="bibr" target="#b9">[10]</ref> is unable to remove moir? patterns at all even though its network has been re-trained using our training set. Meanwhile, VDSR <ref type="bibr" target="#b6">[7]</ref>, PyramidCNN <ref type="bibr" target="#b28">[29]</ref>, and DnCNN <ref type="bibr" target="#b8">[9]</ref> have a better performance. However, colour distortion is still noticeable in their results.</p><p>Except for our methods, U-Net <ref type="bibr" target="#b30">[31]</ref> achieves the highest scores of all quality measures. But more moir? artefacts remain in its results than in the results of VDSR <ref type="bibr" target="#b6">[7]</ref> and DnCNN <ref type="bibr" target="#b8">[9]</ref>. As we have stated earlier, even though a quality measure, such as PSNR, can measure the overall image quality, it cannot precisely measure the effectiveness in moir? pattern removal. We show an example in <ref type="figure" target="#fig_0">Fig. 13</ref> and the supplemental materials that U-Net <ref type="bibr" target="#b30">[31]</ref> produces higher PSNRs but worse visual results. Our method has the most powerful network architecture and produces output images closest to the groundtruth reference images.</p><p>Additional visual results from our method are shown in <ref type="figure" target="#fig_0">Fig. 19</ref>, where the input images exhibit a variety of different moir? patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The Number of Variables</head><p>As shown in <ref type="table">Table.</ref> VI, the number of variables in our method is in the same order as U-Net and PyramidCNN while our proposed network outperforms both of them qualitatively and quantitatively. Variants of our model, V B15 and V C32, have a similar number of parameters as VDSR and DnCNN, however produce higher PSNR scores. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. User Study</head><p>Due to the limitation of image metrics in measuring moir? artefacts, we have also conducted a user study to compare different methods, which includes 20 questions. Each question consists of six randomly ordered results, generated by VDSR, DnCNN, PyramidCNN, U-Net, V Concate and our method, on a randomly selected test image. 60 participants have to choose 1 to 2 images that they perceive most appealing and comfortable. After averaging the votes from all the 20 questions, we obtain the statistics in <ref type="figure" target="#fig_0">Fig. 14</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. MODEL VERSATILITY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Cross-Data Evaluation</head><p>We quantitatively measure our model versatility by training and testing on data collected with different phone models or digital monitors. We perform three experiments, including testing on images taken with an iPhone on a Mac 2560 screen, with a SamSung S7 on a Dell 1920 monitor, and with a Sony Z5 on a Dell 1280 display, respectively. Note that in each experiment, the test data is excluded during training process. The performance is demonstrated in <ref type="table">Table.</ref> VII. Though the performance is not as good as before, our model can still produce reasonable results. We also observe that the quality improvement by our model is most noticeable when the input (moir?) images are in low quality, such as the images captured with the Sony Z5 on the DELL 1280 screen. a) Test on phone model HUAWEI P9: Though the camera sensors in different phone models are different, the underlying reason for the formation of moir? patterns is similar on different phones. To test the versatility of our network, we run our network directly on moir? photos captured by another phone model, that is not used in collecting our dataset, HUAWEI P9. Decent results have been achieved, as shown in <ref type="figure" target="#fig_0">Fig. 15</ref> and the supplemental materials. This indicates that our trained network can be used for removing moir? patterns in images captured by other phone models. B. Restore Partial Moir? Photos a) Synthesised moir? images: Moir? patterns on an image can be spatially varying, strong in a region and weak in another region. Under extreme conditions, moir? patterns can only appear in part of an image. In <ref type="figure" target="#fig_0">Fig. 16</ref>, we show our results on synthesised partial moir? images, where only a small portion of the image contains moir? artefacts. b) Real world moir? patterns not caused by display: When searching the Internet for "moir? photos", we find that moir? patterns most commonly appear on fine repetitive patterns, such as textile textures on clothes and buildings. In <ref type="figure" target="#fig_0">Fig. 17</ref>, we show the results of directly applying our trained model without fine-tuning on Internet images damaged by moir? artefacts. Though the moir? is caused by the repetition of the fine patterns rather than digital display, our model is able to reduce such moir? patterns as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. LIMITATIONS</head><p>When a moir? pattern exhibits very severe large-scale coloured bands, our method might not be able to infer the uncontaminated image correctly. We show a failure case in <ref type="figure" target="#fig_0">Fig. 18</ref>. Another limitation is that our model could not clearly reduce blurriness in the input images. Note that other baseline algorithms, including the image deblurring model PyramidCNN, are not able to resolve it either ( <ref type="figure" target="#fig_0">Fig. 12</ref>). We believe that such blurriness is introduced into a subset of acquired photos in our dataset because of multiple reasons, including motion blur due to the movement of the camera during image acquisition, the imperfect image alignment during pre-processing, and the damaged high-frequency components caused by highfrequency moir? patterns. Although our algorithm can faithfully detect all 20 corner points, moir? patterns can interfere with their exact localisation, giving rise to imperfect alignment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IX. CONCLUSION AND FUTURE WORK</head><p>To conclude, we presented a novel multiresolution fully convolutional network for automatically removing moir? patterns from photos as well as created a large-scale benchmark with 100, 000 + image pairs to evaluate moir? pattern removal algorithms. Although a moir? pattern can span over a wide range of frequencies, our proposed network is able to remove moir? artefacts within every frequency band thanks to the nonlinear multiresolution analysis of the moir? photos. We believe that people would like to use their mobile phones to record content on screens for more reasons than expected, such as convenience, simplicity, and efficiency. The proposed method and the collected large-scale benchmark together provide a decent solution to the moir? photo restoration problem.</p><p>In the future, we would like to explore different categories of moir? patterns and improve our method so that it can eliminate moir? artefacts according to their category labels. Moreover, it will be interesting to investigate the existence of an indicator that can better describe the level of moir? artefacts and guide the training process. We also plan to keep expanding our dataset by adding more examples under different shooting conditions and for different types of device screens. We believe that with a larger dataset, our method can produce even better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS.</head><p>This work was partially supported by Hong Kong Research Grants Council under General Research Funds (HKU17209714).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Given an image damaged by moir? patterns, our proposed network can remove the moir? artefacts automatically.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The mechanism underlying a general moir? pattern. The changing misalignment between two repetitive patterns produces varying moir? patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of our multiresolution fully convolutional network. The top row in (c) shows intermediate images produced from the second to fifth network branch, and the bottom row shows the same images with amplified intensity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Examples of image pairs from our dataset. From left to right: images are contaminated by stripe, dot and curved moir? patterns respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Reference image (T) (b) Sorted Corners of T (c) Registered T (d) Moir? Photo (S) (e) Sorted Corners of S (f) Registered S Image Acquisition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )Fig. 6 .Fig. 7 .</head><label>a67</label><figDesc>Detected corners of T (b) Detected corners of S (c) Cleaned corners of S Corner Detection and Clearance. PSNR cannot fully reflect the degree of moir? patterns. An image corrupted by visually more severe moir? patterns can have higher PSNR.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>The quality distribution of moir? photos in the entire dataset. The quality of a moir? photo with respect to its corresponding reference image is measured using PSNR (dB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9 .</head><label>9</label><figDesc>Visualisation of the 3-channel feature maps produced by different branches on a "grayscale-like" RGB image and its corresponding pure grayscale image. For each input, the top row in (c) shows the intermediate images produced from the second to the fifth network branch, and the bottom row shows the same images with amplified intensity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10 .</head><label>10</label><figDesc>Visualisation of U-Net feature maps. (Top) Feature maps produced by a layer A closer to the input image. (Bottom) Feature maps produced by a deeper layer B. Layer A is skipped connected to layer B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>12dB): replacing the sum operation with concatenation. To be specific, we concatenate the 32 feature maps from each scale, and append two convolutional layers after the concatenated feature maps. Each of these convolutional layers has 32 channels and 3 ? 3 kernels.? V Skip (26.36dB): in each scale, skip connecting the second downsampling layer to the last convolutional layer before the upsampling layers. ? V C32 (25.52dB): replacing all the 64-channel convolution filters with 32 channel convolutional filters. ? V B123 (25.28dB): using branch 1, 2 and 3 only. ? V B135 (26.04dB): using branch 1, 3 and 5 only. ? V B15 (25.52dB): using branch 1 and 5 only.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 .</head><label>11</label><figDesc>Average pixel-wise MSE error of various methods vs. the number of epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 .</head><label>12</label><figDesc>Comparison between our multiresolution deep network and other state-of-the-art methods for image restoration, including Photoshop Descreen, IRCNN<ref type="bibr" target="#b9">[10]</ref></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 .</head><label>13</label><figDesc>Another example in which U-Net<ref type="bibr" target="#b30">[31]</ref> produces a higher PSNR score but a worse moir? removal effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 15 .</head><label>15</label><figDesc>Restoration of moir? photos taken with HUAWEI P9. Our model is not fine-tuned for this phone model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 16 .</head><label>16</label><figDesc>Test on synthesised images contaminated by moir? patterns in a small region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 17 .</head><label>17</label><figDesc>Reduce moir? artefacts on Internet images without fine-tuning. Image courtesy @Fstoppers user Peter House and @Travel-Images.com user A.Bartel, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 18 .</head><label>18</label><figDesc>A failure example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 19 .</head><label>19</label><figDesc>Input images contaminated with different types of moir? patterns and their corresponding cleaned results from our proposed method. In this figure, we intentionally show some brighter images, where moir? patterns are more noticeable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>of Computer Science, The University of Hong Kong, Pokfulam Road, Hong Kong. E-mail: {yjsun, wenping}@cs.hku.hk, yizhouy@acm.org</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>II, respectively. a) Remarks.: Our deep network is designed on the basis of the key characteristics of moir? patterns, which exhibit features across a wide range of frequencies. A moir? pattern is typically spatially varying and spreads over an entire image. If a network deals with fine-scale features only, low-frequency</figDesc><table><row><cell>Input</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Scale 1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>? W</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Output</cell><cell>Target</cell></row><row><cell cols="2">Scale 2</cell><cell></cell><cell></cell></row><row><cell>2</cell><cell>? 2</cell><cell></cell><cell></cell></row><row><cell cols="2">Scale 3</cell><cell></cell><cell></cell><cell>Back</cell></row><row><cell>4</cell><cell>? 4</cell><cell></cell><cell></cell><cell>Prop</cell></row><row><cell cols="2">Scale 4</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>? 8</cell><cell></cell><cell></cell></row><row><cell cols="2">Scale 5</cell><cell></cell><cell></cell></row><row><cell>16</cell><cell>? 16</cell><cell>Downsampling Layers</cell><cell>Upsampling Layers</cell></row><row><cell>(a) Input</cell><cell></cell><cell>(b) Finest Scale</cell><cell>(c) Scales 2 to 5</cell><cell>(d) Output</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell cols="3">DOWNSAMPLING LAYERS</cell></row><row><cell>Scale</cell><cell>Kernel</cell><cell>Stride Channels</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell cols="4">PHONE MODEL SPECIFICATIONS</cell></row><row><cell cols="2">Manufacturer</cell><cell cols="2">Model</cell><cell>Camera</cell></row><row><cell>APPLE</cell><cell cols="3">iPhone 6</cell><cell>8MP</cell></row><row><cell>SAMSUNG</cell><cell cols="3">Galaxy S7 Edge</cell><cell>12MP</cell></row><row><cell>SONY</cell><cell cols="3">Xperia Z5 Premium Dual</cell><cell>23MP</cell></row><row><cell></cell><cell cols="2">TABLE IV</cell><cell></cell></row><row><cell cols="4">DISPLAY SCREEN SPECIFICATIONS</cell></row><row><cell>Manufacturer</cell><cell>Model</cell><cell></cell><cell>Resolution</cell><cell>Size (inch)</cell></row><row><cell>APPLE</cell><cell cols="3">Macbook Pro Retina 2560 ? 1600</cell><cell>13.3"</cell></row><row><cell>DELL</cell><cell>U2410 LCD</cell><cell></cell><cell>1920 ? 1200</cell><cell>24"</cell></row><row><cell>DELL</cell><cell cols="2">SE198WFP LCD</cell><cell>1280 ? 800</cell><cell>19"</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V A</head><label>V</label><figDesc>QUANTITATIVE COMPARISON AMONG PARTICIPATING METHODS ON OUR TEST SET WITH DIFFERENT METRICS. OUR METHOD CLEARLY OUTPERFORMS THE OTHER METHODS.</figDesc><table><row><cell></cell><cell cols="2">Corrected Input RTV [2]</cell><cell cols="2">SDF [28] IRCNN [10]</cell><cell cols="6">DnCNN [9] VDSR [7] PyramidCNN [29] U-Net [31] V Concate Our method</cell></row><row><cell>PSNR Mean (dB)</cell><cell>20.30</cell><cell>20.67</cell><cell>20.88</cell><cell>21.01</cell><cell>24.54</cell><cell>24.68</cell><cell>25.39</cell><cell>26.49</cell><cell>27.12</cell><cell>26.77</cell></row><row><cell>PSNR Gain (dB)</cell><cell>-</cell><cell>0.37</cell><cell>0.58</cell><cell>0.71</cell><cell>4.24</cell><cell>4.38</cell><cell>5.09</cell><cell>6.19</cell><cell>7.09</cell><cell>6.47</cell></row><row><cell>Ave Error (?10 ?3 )</cell><cell>34</cell><cell>31</cell><cell>30</cell><cell>28.32</cell><cell>5.82</cell><cell>5.74</cell><cell>4.83</cell><cell>3.81</cell><cell>3.36</cell><cell>3.62</cell></row><row><cell>SSIM [32]</cell><cell>0.738</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.834</cell><cell>0.837</cell><cell>0.859</cell><cell>0.864</cell><cell>0.878</cell><cell>0.871</cell></row><row><cell>FSIM [33]</cell><cell>0.869</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.901</cell><cell>0.902</cell><cell>0.909</cell><cell>0.912</cell><cell>0.922</cell><cell>0.914</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI THE</head><label>VI</label><figDesc>NUMBER OF VARIABLES IN LEARNING BASED APPROACHES (?10 5 ).</figDesc><table><row><cell></cell><cell>V B123</cell><cell>V B15</cell><cell>V C32</cell><cell>V Concate</cell><cell>Our method</cell></row><row><cell># var</cell><cell>9.28</cell><cell>7.42</cell><cell>4.11</cell><cell>16.14</cell><cell>15.44</cell></row><row><cell></cell><cell>IRCNN-IL</cell><cell>VDSR</cell><cell cols="2">DnCNN PyramidCNN</cell><cell>U-Net</cell></row><row><cell># var</cell><cell>3.35</cell><cell>6.67</cell><cell>7.04</cell><cell>14.15</cell><cell>24.62</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. It is clear that the proposed model is more preferable to the human visual system, although U-Net and V Concate achieve high scores under certain numerical image quality measures.Fig. 14. User study on moir? pattern restoration.</figDesc><table><row><cell></cell><cell cols="4">User Study on Moire Pattern Restoration</cell><cell></cell></row><row><cell>0.60</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>54.55%</cell></row><row><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.30</cell><cell></cell><cell></cell><cell></cell><cell>35%</cell><cell></cell></row><row><cell></cell><cell></cell><cell>25%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>13%</cell><cell>13.49%</cell><cell></cell><cell>14%</cell><cell></cell><cell></cell></row><row><cell>0.00</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VDSR</cell><cell>DnCNN</cell><cell>PyramidCNN</cell><cell>Unet</cell><cell>V_Concate</cell><cell>Our Method</cell></row><row><cell></cell><cell></cell><cell cols="3">Fundraiser Results by Salesperson</cell><cell></cell></row><row><cell></cell><cell>PARTICIPANT PARTICIPANT</cell><cell></cell><cell cols="2">UNITS SOLD</cell><cell></cell></row><row><cell></cell><cell>VDSR</cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DnCNN</cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>PyramidCNN</cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Unet</cell><cell></cell><cell>0</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII</head><label>VII</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">CROSS-DATA EVALUATION.</cell><cell></cell><cell></cell></row><row><cell>Test Data\Metrics</cell><cell cols="2">PSNR Input Result</cell><cell cols="2">SSIM Input Result</cell><cell cols="2">FSIM Input Result</cell></row><row><cell>iPhone Mac2560</cell><cell>23.09</cell><cell>25.18</cell><cell>0.840</cell><cell>0.862</cell><cell>0.914</cell><cell>0.930</cell></row><row><cell>SamSung Dell1920</cell><cell>18.34</cell><cell>20.84</cell><cell>0.594</cell><cell>0.636</cell><cell>0.833</cell><cell>0.870</cell></row><row><cell>Sony Dell1280</cell><cell>16.33</cell><cell>23.28</cell><cell>0.706</cell><cell>0.822</cell><cell>0.856</cell><cell>0.898</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast patch-based denoising using approximated patch geodesic paths</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1211" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Structure extraction from texture via relative total variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">139</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bilateral texture filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">128</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep joint demosaicking and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">191</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Moire pattern</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wikipedia</surname></persName>
		</author>
		<ptr target="https://en.wikipedia.org/wiki/Moir%C3%A9pattern" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Moire</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keohihdtv</surname></persName>
		</author>
		<ptr target="http://www.keohi.com/keohihdtv/learnabout/definitions/moire.html" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Suppression of moir? patterns via spectral analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sidorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kokaram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4671</biblScope>
			<biblScope unit="page">895</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Moir? pattern removal from texture images via low-rank and sparse matrix decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Communications and Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated removal of quasiperiodic noise using frequency domain statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grediac</surname></persName>
		</author>
		<idno>pp. 013 003-013 003</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive threshold modulation for error diffusion halftoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Damera-Venkata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="104" to="116" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Am/fm halftoning: digital halftoning through simultaneous modulation of dot size and dot density</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="286" to="302" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Inverse halftoning and kernel estimation for error diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="486" to="498" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Training-based descreening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="789" to="802" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Image descreening by ga-cnn-based texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems I: Regular Papers</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2287" to="2299" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A robust technique for image descreening based on the wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">De</forename><surname>Queiroz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1179" to="1184" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hardware-friendly descreening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Boutin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="746" to="757" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scanned image descreening with image redundancy and adaptive filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3698" to="3710" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Paper check image quality enhancement with moire reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Youn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">450</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure-preserving image smoothing via region covariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Karacan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cartoon-texture image decomposition using blockwise low-rank texture characterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yamada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1128" to="1142" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Image structure retrieval via l0 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust image filtering using joint static and dynamic guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey vision conference</title>
		<meeting><address><addrLine>UK</addrLine></address></meeting>
		<imprint>
			<publisher>Manchester</publisher>
			<date type="published" when="1988" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="10" to="5244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fsim: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
