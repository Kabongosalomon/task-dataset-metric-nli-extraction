<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Class-agnostic Object Detection with Multi-modal Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Maaz</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanoona</forename><surname>Rasheed</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Link?ping University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rao</forename><forename type="middle">Muhammad</forename><surname>Anwer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Aalto University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Mohamed bin Zayed</orgName>
								<orgName type="institution">University of AI</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Class-agnostic Object Detection with Multi-modal Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Object detection</term>
					<term>Class-agnostic</term>
					<term>Vision Transformers</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>What constitutes an object? This has been a long-standing question in computer vision. Towards this goal, numerous learning-free and learning-based approaches have been developed to score objectness. However, they generally do not scale well across new domains and novel objects. In this paper, we advocate that existing methods lack a topdown supervision signal governed by human-understandable semantics. For the first time in literature, we demonstrate that Multi-modal Vision Transformers (MViT) trained with aligned image-text pairs can effectively bridge this gap. Our extensive experiments across various domains and novel objects show the state-of-the-art performance of MViTs to localize generic objects in images. Based on the observation that existing MViTs do not include multi-scale feature processing and usually require longer training schedules, we develop an efficient MViT architecture using multi-scale deformable attention and late vision-language fusion. We show the significance of MViT proposals in a diverse range of applications including open-world object detection, salient and camouflage object detection, supervised and self-supervised detection tasks. Further, MViTs can adaptively generate proposals given a specific language query and thus offer enhanced interactability. Code: https://git.io/J1HPY.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The recent years have witnessed significant advances in object detection (OD) <ref type="bibr" target="#b41">[42]</ref> based on developments of large-scale annotated datasets and carefully designed deep learning models. Notably, efforts have been made to tackle more difficult cases such as universal OD <ref type="bibr" target="#b66">[67]</ref>, long-tailed object distribution modeling <ref type="bibr" target="#b18">[19]</ref>, open-vocabulary <ref type="bibr" target="#b77">[78]</ref> and open-world OD <ref type="bibr" target="#b27">[28]</ref>. In contrast, little progress has been made towards a seemingly simpler task of class-agnostic OD <ref type="bibr" target="#b0">[1]</ref> in recent years. In the era of fully trainable pipelines, class-agnostic OD is still often  <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>, satellite images <ref type="bibr" target="#b71">[72]</ref>, sketches, cartoons and paintings <ref type="bibr" target="#b25">[26]</ref> ( gray background ). The MViTs perform well on diverse datasets (with many classes e.g., LVIS, Object365) using intuitive natural language text queries (e.g., all objects). Further, class-agnostic detectors (MViTs) can be applied to several downstream applications ( pearl background ). In Open-world OD <ref type="bibr" target="#b27">[28]</ref>, unknown pseudo-labels generated using MDETR <ref type="bibr" target="#b28">[29]</ref> can improve novelty detection. For unsupervised object localization, replacing Selective Search proposals <ref type="bibr" target="#b63">[64]</ref> in DETReg <ref type="bibr" target="#b2">[3]</ref> pretraining with only top-30 MViT proposals leads to improved localization. For Salient and Camouflaged OD, task specific text queries can help perform competitively against fully supervised models without any task specific tuning. Overall, MViTs achieve the state-of-the-art results on various downstream applications. approached using typical bottom-up approaches such as Selective Search <ref type="bibr" target="#b63">[64]</ref>, EdgeBox <ref type="bibr">[84]</ref>, DeepMask <ref type="bibr" target="#b48">[49]</ref> and MCG <ref type="bibr" target="#b51">[52]</ref>.</p><p>Despite being an apparently simpler problem in terms of the two-way classification space, the class-agnostic OD task is indeed challenging from the representation learning perspective. The main challenge is to model the vast diversity of all valid object classes and delineate such a diverse group from the background class which itself has vague semantic definition <ref type="bibr" target="#b1">[2]</ref>. Our experiments indicate that this intrinsic complexity of the task makes it difficult to design fully trainable class-agnostic OD models that can work across domains and for novel unseen objects. Although the bottom-up approaches offer proposals for generic objects, they come at the cost of a prohibitively large number of candidate boxes, lowprecision, lack of semantic understanding and slow processing, making them less scalable to generic operation in the wild. More recently, self-supervised learning frameworks -based on both ViTs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b10">11]</ref> and CNNs <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b72">73]</ref> -have focused on promoting better localization of generic objects, however they still show modest performance on class-agnostic OD <ref type="bibr" target="#b2">[3]</ref>. Our intuition is that top-down supervisory signals are necessary to resolve the ambiguous nature of class-agnostic OD task, which is precisely what is missing from the aforementioned approaches.</p><p>In this paper, we bring out the capacity of recent Multi-modal Vision Transformers (MViTs) to propose generic class-agnostic OD across different domains. The high-level information provided by the language descriptions helps learn fairly generalizable properties of universal object categories. In turn, the MViTs perform exceptionally well compared to uni-modal object detectors trained for generic object detection as well as the typical bottom-up object proposal generation schemes. Due to the multi-modal nature of these models, we design language-driven queries to discover valid objects in a human-understandable format that can be adapted to explore varied aspects of the object semantic space. With the state-of-the-art performance, an ensuing question is to explore the root cause of such generalization for the 'concept of objects' embedded in MViTs. Through a series of systematic experiments, we find that it is the language skeleton/structure (rather than the lexicon itself) that defines this strong understanding of generic object definition within MViT models. As an interesting example, when the MViT is trained without actual captions, but just the bounding boxes corresponding to a natural language description, the model still demonstrates strong class-agnostic OD generalization. These insights on the interactive class-agnostic OD mechanism can be deployed in several downstream tasks such as novel object discovery, saliency detection, self-supervised learning and open-world detection. The main highlights of this work include:</p><p>-We demonstrate the state-of-the-art performance of pre-trained MViTs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref> towards class-agnostic OD via a set of human-understandable natural language queries. We also develop an efficient and flexible MViT model, Multiscale Attention ViT with Late fusion (MAVL), which performs better in locating generic objects as compared to existing MViTs (Secs. 2 and 3). -We benchmark generalization of MViT based OD models on diverse domains e.g., natural images, sketches, cartoons, satellite images, paintings and show their favorable performance compared to existing class-agnostic OD models (bottom-up approaches, CNN and ViT based uni-modal pipelines) (Sec. 3). -Our class-agnostic detectors can benefit various down-stream applications:</p><p>Open-world OD, Salient OD, Camouflaged OD and Self-supervised learning. Furthermore, when these proposals are combined with RPN proposals in two-stage detectors, it can lead to overall performance improvements due to their rich top-down semantic understanding of the image content (Sec. 4). -Through an extensive set of systematic experiments, we analyze the factors that majorly contribute to the improved performance of MViTs (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Multi-modal ViTs</head><p>In this work, we bring out the generalization capacity of Multi-modal ViTs (MViT) to tackle generic OD. The capability of relating natural language with visual features helps MViTs to generalize to novel concepts, achieving state-ofthe-art results on class-agnostic OD using human-understandable text queries  <ref type="bibr" target="#b19">[20]</ref>, MDETR <ref type="bibr" target="#b28">[29]</ref> and MAVL (ours). GPV-1 takes image along with a task description as input and outputs relevant region boxes and text. MDETR uses soft token prediction and contrastive alignment in latent space for cross-conceptualization using aligned image-text pairs. MAVL utilizes multi-scale image features with multi-scale deformable attention module (MSDA), and uses late-fusion strategy for vision-language fusion.</p><p>(e.g., 'all objects/entities'). Before a detailed analysis, we provide background on MViTs and propose Multiscale Attention ViT with Late fusion (MAVL).</p><p>(a) GPV: Gupta et al. proposed GPV-I <ref type="bibr" target="#b19">[20]</ref>, a unified architecture for multi-task learning, where the task is inferred from the text prompt. It takes an image and a task description as input and outputs text with the corresponding bounding boxes. This model uses pretrained BERT <ref type="bibr" target="#b11">[12]</ref> to encode the text, concatenates it with the region descriptors from DETR <ref type="bibr" target="#b4">[5]</ref> and passes it to ViLBERT <ref type="bibr" target="#b43">[44]</ref> coattention layers for cross-modal conceptualization. It predicts relevance scores for each predicted bounding box indicating the importance of the region for the prompted task. An output text decoder conditioned on the relevance scores is used for better cross-modal understanding ( <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>). GPV is trained on data from five different vision-language tasks.</p><p>(b) MDETR: Kamath et al. <ref type="bibr" target="#b28">[29]</ref> proposed a modulated transformer trained to detect objects in an image conditioned on a text query. In MDETR, visual and text features are extracted from a convolutional backbone (e.g., ResNet-101 <ref type="bibr" target="#b22">[23]</ref> or EfficientNet <ref type="bibr" target="#b62">[63]</ref>) and a language model (RoBERTa <ref type="bibr" target="#b42">[43]</ref>) respectively. These features are then concatenated and passed to the DETR [5] model for detection ( <ref type="figure" target="#fig_1">Fig. 2 (b)</ref>). MDETR uses soft token prediction and contrastive alignment in latent space for addressing text-conditioned object detection. In soft token prediction, a uniform probability distribution is predicted over all text tokens for each detected object. In contrastive alignment, the embedded object queries from decoder are aligned with the text representation from encoder. This multi-modal alignment makes the object embeddings closer to the corresponding text embeddings in feature space. The model is pre-trained with 1.3M image-text pairs and achieves the state-of-the-art results on various vision-language downstream tasks including VQA, referring expression and phrase grounding.   <ref type="bibr" target="#b28">[29]</ref> finds it challenging to scale to high-resolution feature maps due to a fixed self-attention design. Further, it operates on a specified spatial scale which can be sub-optimal for small objects. Our design calculates attention at multiple scales to incorporate better contextual information. However, multiple scales can increase the computational cost, therefore we use Deformable Attention proposed in [83] that employs multiscale feature processing and dynamically attends to relevant pixel locations for context aggregation. Specifically, it samples a small set of keys around a reference (query) image location. The sparse key sampling in MSDA achieves linear complexity with respect to the size of the image feature maps. ?Late Multi-modal Fusion. MSDA module utilizes the spatial structure of an image to sparsely sample keys for each query point. Following the MDETR strategy of concatenating text embeddings with flattened features would destroy the spatial structure of an image. Hence, we fuse text in MAVL model after the images are processed through the Def-DETR encoder-decoder architecture using a late fusion mechanism. Specifically, the object query representations from the deformable decoder are concatenated with the text embeddings, and passed through a series of six transformer self-attention (SA) blocks. This design choice is inspired by the recent vision-language fusion works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b60">61]</ref>. Using the training procedure of <ref type="bibr" target="#b4">[5]</ref>, the output head is applied after each SA block and the total loss is calculated by adding all auxiliary losses. We note that no explicit contrastive alignment of object query representation and encoded text is required in our approach. Our experiments show fast convergence (only half iterations) and competitive performance of MAVL against MDETR <ref type="table" target="#tab_1">(Tables 1, 2)</ref>. ?Implementation Details. Similar to MDETR <ref type="bibr" target="#b28">[29]</ref>, we train MAVL on approx. 1.3M aligned image-text pairs, using images from Flickr30k <ref type="bibr" target="#b50">[51]</ref>, MS-COCO (2014) <ref type="bibr" target="#b39">[40]</ref> and Visual Genome (VG) <ref type="bibr" target="#b31">[32]</ref>. The corresponding annotations are taken from Flickr entities, RefCOCO/+/g referring expression <ref type="bibr" target="#b29">[30]</ref>, VG regions and GQA <ref type="bibr" target="#b24">[25]</ref>. In the onward discussion, we refer to this dataset as Largescale Modulated Detection (LMDet)dataset. All MDETR and MAVL models are trained with ImageNet-1K <ref type="bibr" target="#b54">[55]</ref> pretrained ResNet-101 <ref type="bibr" target="#b22">[23]</ref>. Our MAVL converges in 20 epochs (MDETR requires 40 epochs) on LMDet using the same hyperparameters as in MDETR. See Appendix A.1 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Multi-modal ViTs as Generic Detectors</head><p>The class-agnostic OD seeks to differentiate between generic objects and background in images. This task involves learning the notion of objectness. Existing approaches typically explore low-level visual cues (i.e. superpixels, edges, etc.) or directly learn the mapping between images and generic object locations using fully trainable pipelines learned with bounding box annotations <ref type="bibr" target="#b63">[64,</ref><ref type="bibr">84,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b2">3]</ref>. We note that these procedures lack high-level semantic information necessary to relate objects across diverse scenes to derive a comprehensive and general notion of universal objects. In this work, We explore the class-agnostic OD capacity of MViTs trained using aligned image-text pairs (Sec. 2). We observe these models can produce high quality object proposals by using intuitive text queries like 'all objects' and 'all entities'. This demonstrates their capability to relate natural language with visual concepts to model generic objectness, enabling them to discover novel categories and generalize across different domains while offering human interaction with intelligible text queries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Class-agnostic Object Detection</head><p>Settings: <ref type="table" target="#tab_1">Table 1</ref> shows the object proposal generation performance of MViTs with the typical bottom-up approaches and the end-to-end supervised deep learning methods on five challenging natural image OD datasets (Pascal VOC <ref type="bibr" target="#b13">[14]</ref>, MS COCO <ref type="bibr" target="#b39">[40]</ref>, KITTI <ref type="bibr" target="#b16">[17]</ref>, Objects365 <ref type="bibr" target="#b55">[56]</ref> and LVIS <ref type="bibr" target="#b18">[19]</ref>  than the uni-modal approaches with the use of simple human understandable natural language text queries. This performance shows MViTs' strong understanding of language content obtained from the pretrained NLP model (BERT <ref type="bibr" target="#b11">[12]</ref>, RoBERTa <ref type="bibr" target="#b42">[43]</ref>) along with the aligned image-text pairs used in pretraining. For MViTs, interestingly a relatively small number of boxes match the quality achieved by a much larger proposal set from competing methods. <ref type="figure" target="#fig_3">Fig. 3a</ref> shows the recall obtained by varying the number of top object proposals for all methods on two datasets. MViTs achieve competitive recall with only top-10 proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">How well MViTs generalize?</head><p>Generalization to New Domains: We extend our analysis from natural image datasets (Sec. 3.1) to rule out if MViT representations are biased towards natural images, for which these models are originally trained on. To this end, we evaluate on universal OD datasets <ref type="bibr" target="#b66">[67]</ref> belonging to five different domains ( <ref type="table" target="#tab_3">Table 2</ref>). The studied domains include indoor kitchen scenes <ref type="bibr" target="#b17">[18]</ref>, cartoon images, watercolor drawings, clipart, comics <ref type="bibr" target="#b25">[26]</ref> and satellite/aerial images (DOTA dataset) <ref type="bibr" target="#b71">[72]</ref>. The experiments follow the same setting as in Sec. 3.1. These results indicate the generalization capability of MViTs in comparison to the best proposal generation methods earlier evaluated in <ref type="table" target="#tab_1">Table 1</ref> (RetinaNet trained for class-agnostic OD). Generalization to Rare/Novel Classes: With the notion of objectness, humans are capable of identifying novel and rare objects, although they may not recognize their specific category. Similarly, scalabiltiy to rare and novel classes is a desired quality of an object detector. To analyze this, the class-agnostic OD mechanism of MAVL is evaluated on rare categories from Open-Images <ref type="bibr" target="#b33">[34]</ref> versus frequent categories and compared with Deformable DETR and Deep Mask trained for class agnostic OD. <ref type="figure" target="#fig_3">Fig. 3b</ref> indicate state-of-the-art recall on rare categories such as lynx, humidifier, and armadillo with as few as zero training instance. Overall, we note the model generalizes well to rare/unseen categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Applications and Use-cases</head><p>The high-quality class-agnostic object proposals obtained from MViTs can be helpful towards several downstream applications, as we demonstrate next. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Enhanced Interactability</head><p>We have observed that MViTs can generate high quality object proposals with intuitive human understandable queries such as 'all objects'. This motivates us to explore the language semantic space of such models to construct a set of queries that can well capture the generic concept of objectness. We filter words from captions in LMDet that are semantically close to the word 'object' in the linguistic feature space. We then utilize these words to construct intuitive text queries such as 'all objects', 'all entities', 'all visible entities and objects', and 'all obscure entities and objects', for exploiting the class-agnostic OD performance of MViTs. The detections from the individual text queries are combined, filtered with classagnostic non-maximum suppression (NMS) to remove duplicate detections, and top-N boxes are selected for evaluation. We use N=50 in all of our experiments.</p><p>Task specific queries: The detection of small and irregular sized objects has remained a long-standing challenge. In our case, the flexible nature of MViTs facilitates using a range of human-understandable text queries. The queries can be chosen that best describe the special requirements needed in a given detection task. We demonstrate certain scenarios of how this feature can be exploited for better predictions. <ref type="figure">Fig. 4a</ref>  queries like 'all long objects' helps improve the detection of irregular shaped objects (without any dataset specific fine-tuning!).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Open-world Object Detection</head><p>The open-world setting assumes a realistic paradigm where a model can experience unknown objects during training and inference <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b27">28]</ref>. The goal is to identify unknowns and incrementally learn about them as and when new annotations are provided about a subset of unknowns. This stands in contrast to generic OD where models are trained to label unknown objects as background and only focus on the known objects. Here, we explore how a generic class-agnostic OD model can help with the open-world task to identify unknowns. As a case study, we apply our approach to a recent open-world detector (ORE) <ref type="bibr" target="#b27">[28]</ref>. ?ORE Setting: The authors distribute the 80 COCO <ref type="bibr" target="#b39">[40]</ref> classes in four incremental learning tasks where 20 classes have been added to the known categories in each subsequent task. At each stage, the model must learn from the given subset of 20 newly introduced known classes, should not forget the previous known classes and must be able to detect unknown classes whose labelled examples have not been provided so far as the unknowns. ORE uses Faster-RCNN <ref type="bibr" target="#b53">[54]</ref> as the base detector, with contrastive clustering in latent space and an energy-based classification head for unknown detection. It utilizes example-replay strategy <ref type="bibr" target="#b65">[66]</ref> for alleviating forgetting, when progressively learning the unknown categories once their labels become available. ?Unknown Pseudo-labels with MViTs: ORE exploits the two-stage mechanism of Faster-RCNN <ref type="bibr" target="#b53">[54]</ref> and uses proposals from the class-agnostic region proposal network (RPN) for pseudo-labelling of unknowns. The foreground object proposals with high objectness score which do not overlap with any ground-truth are labelled as unknowns. We note that since RPN is only trained on the objects of interest, its detections are overly sparse and lead to a low recall for unknowns. The pipeline therefore lacks a good proposal set that generalizes to novel objects. We propose a variant of ORE, by using class-agnostic proposals for unknown object categories obtained from MAVL. For a fair comparison, the MViT is trained on a filtered dataset, generated by explicitly removing all captions from LMDet that contain any unknown category, leaving 0.76M imagetext pairs (see Appendix A.4 for further details). The results in <ref type="table" target="#tab_6">Table 4</ref> and <ref type="figure">Fig. 4b</ref> indicate significant improvements in unknown detection. See <ref type="figure" target="#fig_0">Fig. 10</ref> in Appendix C for more qualitative results. The recent progress in self-supervised learning (SSL) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b78">79]</ref> has minimized the need for large labelled datasets to achieve good performance on downstream tasks. These techniques encode the global image representation and achieve competitive generalization on various downstream tasks. However, these methods are suboptimal for class-aware OD, where the classification needs to be performed at local image patches (i.e. bounding boxes). Several recent efforts have been reported to address this challenge. ReSim <ref type="bibr" target="#b72">[73]</ref> and DetCo <ref type="bibr" target="#b73">[74]</ref> only pretrain the backbone to encode local and global representations. Whereas, DETReg <ref type="bibr" target="#b2">[3]</ref> pretrains both the backbone and detection network using off-the-shelf proposals from selective search <ref type="bibr" target="#b63">[64]</ref> and achieves improvement over the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pretraining for Class-aware Object Detection</head><p>However, the proposals from heuristic selective search method, used in DE-TReg pretraining, are overly noisy and contain redundant boxes. We show that replacing these noisy pseudo-labels with MViT proposals can improve the downstream performance on OD task <ref type="table" target="#tab_7">(Table 5</ref>). Following DETReg, we select top-30 proposals from MAVL and pretrain the model for 50 epochs on ImageNet <ref type="bibr" target="#b54">[55]</ref> dataset, followed by fine-tuning on 10% and 100% data from Pascal VOC <ref type="bibr" target="#b13">[14]</ref> for 150 and 100 epochs respectively. The results show an absolute gain of ? 7 and ? 1 in AP in the two respective cases. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Salient Object Detection</head><p>Given the generalized class-agnostic performance of MViTs on multiple domains, we evaluate their ability to distinguish between salient and non-salient parts of an image. We exploit the interactive nature of MViTs by passing specific queries to detect the salient objects. To this end, MAVL proposals generated with queries like 'all salient objects' are compared with PoolNet <ref type="bibr" target="#b40">[41]</ref> and CPD <ref type="bibr" target="#b70">[71]</ref> models that are specifically trained for predicting saliency maps. We evaluate the models on the DUT-OMRON <ref type="bibr" target="#b76">[77]</ref> and ECSSD <ref type="bibr" target="#b56">[57]</ref> datasets. These datasets are only used for MViT evaluation and are not used during training. Since MViTs generate bounding boxes, we convert the saliency ground-truths and the saliency maps predicted by CPD and PoolNet to bounding boxes using connected components labelling <ref type="bibr" target="#b68">[69]</ref>. In the case of DUT-OMRON, the provided ground-truth bounding boxes are used by computing an average across the five human annotations. <ref type="table" target="#tab_8">Table 6a</ref> indicates the effectiveness of MAVL in detecting the foreground salient objects. It is also interesting to note how the task specific ? ? query (e.g., 'all salient/foreground objects') provides better prediction of salient parts of the image in comparison to a more generic ? query like 'all objects' <ref type="figure" target="#fig_4">(Fig. 5a</ref>). See Appendix D.5 and <ref type="figure" target="#fig_0">Fig. 11</ref> in Appendix C for additional details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Camouflaged Object Detection</head><p>Camouflaged object detection (COD) involves identifying objects that are seamlessly embedded in their background. The objects have a similar texture to their surroundings and are difficult to locate as compared to salient or generic objects. Here, we explore the interactive OD capacity of MViTs on COD task by evaluating the performance of MAVL against the state-of-the-art model (SINET-V2 <ref type="bibr" target="#b14">[15]</ref>) on CHAMELEON <ref type="bibr" target="#b58">[59]</ref>, CAMO <ref type="bibr" target="#b34">[35]</ref> and COD10K <ref type="bibr" target="#b15">[16]</ref> datasets <ref type="table" target="#tab_8">(Table 6b</ref>). Similar to salient OD setting, we convert camouflage ground-truth masks and masks predicted by SINET-V2 to bounding boxes using connected components labelling <ref type="bibr" target="#b68">[69]</ref>. However, the available bounding box ground-truths have been used for COD10K dataset. We note favorable performance of MAVL proposals, although the model is not specifically trained on camouflaged objects <ref type="figure" target="#fig_4">(Fig. 5a</ref>). This affirms the generality of MAVL proposals. See Appendix D.6 and <ref type="figure" target="#fig_0">Fig. 11</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Improving Two-stage Object Detection</head><p>The class-agnostic object proposals from MViTs have strong understanding of semantics and can be deployed along with the region proposal network (RPN) <ref type="bibr" target="#b53">[54]</ref>. We observe an improvement in accuracy when off-the-shelf MAVL proposals are combined with RPN proposals in Faster RCNN <ref type="bibr" target="#b53">[54]</ref> during inference <ref type="figure" target="#fig_4">(Fig. 5b)</ref>. This indicates the complimentary nature of these proposals that is based on a rich top-down perception of the image content. <ref type="figure" target="#fig_4">Fig. 5b</ref> shows the results of replacing RPN proposals in Faster RCNN with DETReg <ref type="bibr" target="#b2">[3]</ref> and MAVL proposals. The results indicate that the supervised proposal generation methods (RPN and MAVL) perform well compared to the unsupervised method (DETReg). However, off-the-shelf MAVL proposals show better performance than RPN when using a small proposal set (e.g., 10 proposals). Combining RPN and MAVL proposals improves the overall detection accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">What makes MViTs a Generic Detector?</head><p>Our empirical analysis shows the state-of-the-art performance of MViTs towards class-agnostic OD across different domains (Sec. 3) which positively impacts a number of downstream applications (Sec. 4). Having established this, we conduct a series of systematic experiments to explore the contributing factors for representational learning of the general 'objectness measure' in MViTs. Specifically, we identify the role of supervision and multi-modal learning as crucial factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">On the importance of supervision</head><p>We consider two recent unsupervised learning models, DETReg <ref type="bibr" target="#b2">[3]</ref> and UP-DETR <ref type="bibr" target="#b10">[11]</ref>. DETReg trains Deformable DETR [83] to localize objects in classagnostic fashion, with bounding box pseudo-labels from an off-the-shelf region trained on LMDet in class-agnostic fashion, to evaluate the performance contributed by language supervision. We note that the image-level supervision with only box labels improves the performance in comparison with unsupervised methods. However, the use of caption texts aligned with input images proves to be vital and improves the performance approximately by two times, highlighting the importance of multi-modal supervision. Given the importance of multi-modal supervision towards better performance, we find it pertinent to explore the benefit solely from the language supervision. We conduct an ablation study on MDETR and MAVL, by removing all textual inputs corresponding to captions, but keeping intact the structure introduced by language i.e., learning to localize boxes corresponding to a caption for each image in an iteration (without any language branch). Both MDETR and MAVL are trained on LMDet containing aligned image-text pairs. Here, the structure in which the information is fed during training is of high importance to us. Each image may have multiple captions, and hence it may be seen multiple times in the same iteration, but with varying contexts. The experimental setup removes all captions during training and evaluations, however keeps the described data loader structure intact, thus having approximately 1.3M iterations in an epoch. All models use ResNet-101 backbone and are evaluated after 10 epochs for ablation (instead of total 20 epochs). <ref type="table" target="#tab_10">Table 8</ref> indicate that visual branch plays a vital role, however the importance of language cannot be ruled out since the boxes related to a caption are still seen together. We analyze the importance of this implicit language structure next. Ablation on language structure: The above experimental results reveal that removal of textual information does not significantly affect model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">How much does language contribute?</head><p>However, a further ablation on the structure introduced by language is required for the completeness of this evaluation. As such, we conduct ablations at five levels using Deformable DETR [83], as shown in <ref type="table" target="#tab_11">Table 9</ref>. First, all the annotations in LMDet are combined at image level by concatenating the bounding boxes of all captions corresponding to an image (Setting-1). This removes any prior information introduced by the language structure. Then, class-agnostic NMS is applied at a threshold of 0.9 to filter boxes that have high overlaps (Setting-2). To imitate the repetitive pattern introduced during training, bounding box annotations corresponding to an image are randomly sampled and grouped (Setting-3). The number of samples in a combination is kept close to the average number of boxes in imagetext pairs in original MAVL training (?6 boxes). Finally, a longer training schedule is used in the same setting to replicate a scenario closer to the original MAVL training (Setting-4). These four settings are then compared with a model that is trained without any captions, but maintains the structure introduced by language (Setting-5, same as <ref type="table" target="#tab_10">Table 8</ref> last row). This analysis indicates that language structure has significant impact in learning a general notion of objectness. With the use of aligned image-text pairs, additional contextual information is provided to the model. As objects generally tend to co-occur with other objects and certain scenes, such contexual association can be exploited for visual understanding <ref type="bibr" target="#b46">[47]</ref>. Use of captions that describe a scene conveys such a notion of co-occurring objects and their mutual relationships, indicating that the structure introduced by language provides rich semantic and spatial context. Consistent with our findings, other recent efforts also indicate strong generalization achieved using the context encoded within natural language <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b81">82</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper demonstrates intriguing performance of MViTs, trained only on natural images, for generic OD across a diverse set of domains. We systematically study the main reasons for this generalization, and note that the language structure available in image-caption pairs used to train MViTs plays a key role. Based on these insights, we develop a more flexible and efficient MViT for offthe-shelf class-agnostic OD, that can be instantiated with different text queries to generate desired proposal sets. Furthermore, we show various use-cases where class-agnostic proposals can be used to improve performance e.g., open-world OD, camouflaged and salient OD, supervised and self-supervised OD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material</head><p>In this section, we provide additional information regarding,</p><formula xml:id="formula_0">-Implementation details (Appendix A) -Limitations (Appendix B) -Qualitative results (Appendix C) -Additional results (Appendix D) -Related works (Appendix E)</formula><p>A Implementation Details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 MAVL</head><p>Similar to MDETR <ref type="bibr" target="#b28">[29]</ref>, we train MAVL on LMDet dataset containing approximately 1.3M aligned image-text pairs. Unlike MDETR which converges in 40 epochs, our MAVL converges only in 20 epochs with overall better class-agnostic object detection (OD) accuracy. However, the inference for MAVL is approximately 30% slower (see <ref type="table" target="#tab_1">Table 10</ref>). MAVL is trained using a learning rate of 1e ?3 which decays by a factor of 10 after 16 epochs. The vision backbone (ResNet-101 <ref type="bibr" target="#b22">[23]</ref>) and language backbone (RoBERTa <ref type="bibr" target="#b42">[43]</ref>) use learning rates of 1e ?4 and 1e ?5 respectively. The number of object queries is set to 300. In the late-fusion transformer, a series of six selfattention blocks are used, where a detection head is applied after each block for calculating the individual auxiliary losses which are then summed up (see <ref type="figure" target="#fig_1">Fig. 2</ref> in the main paper).  <ref type="bibr" target="#b28">[29]</ref> and MAVL (ours) in terms of convergence epochs, parameters, inference speed and class-agnostic OD performance on COCO <ref type="bibr" target="#b39">[40]</ref> dataset. MAVL converges in half epochs with better accuracy at the cost of slightly slower inference. The frames per second (FPS) are measured on a Quadro RTX 6000 GPU by averaging the time for 1K inference passes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Epochs </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 MViTs as Class Agnostic Object Detectors</head><p>We explore the interactive nature of multi-modal vision transformers (MViTs) for class-agnostic OD task. We construct intuitive natural language text queries by exploring the semantic space of MViTs using an open-source natural language processing (NLP) library, spacy <ref type="bibr" target="#b23">[24]</ref>. Specifically, we find words closer to the keyword 'object' in the semantic space and construct multiple text queries for the class-agnostic OD task. The detected boxes from the multiple text queries are combined, a class-agnostic non-maximum suppression (NMS) at IoU threshold of 0.5 is applied and top-N boxes are selected. We use N=50 and report average precision and recall at IoU threshold of 0.5 in all experiments. For the salient and camouflaged object detection (SOD and COD) tasks, we only consider boxes with objectness scores greater than 0.7. For Pascal VOC <ref type="bibr" target="#b13">[14]</ref>, COCO <ref type="bibr" target="#b39">[40]</ref>, Objects365 <ref type="bibr" target="#b55">[56]</ref>, LVIS <ref type="bibr" target="#b18">[19]</ref>, Clipart, Comic and Watercolor <ref type="bibr" target="#b25">[26]</ref>, we use combined detections from queries 'all objects', 'all entities', 'all visible entities and objects', and 'all obscure entities and objects'. Additionally, 'all small objects' text query is included for the evaluation on KITTI <ref type="bibr" target="#b16">[17]</ref>, Kitchen <ref type="bibr" target="#b17">[18]</ref> and DOTA <ref type="bibr" target="#b71">[72]</ref> as these datasets have a larger number of small sized objects. Moreover, multi-scale evaluation is used for DOTA dataset due to the significant scale variations in the satellite imagery. Here the original image is split into 8 equal crops and the detections from the individual crops are combined. We observe the multi-scale inference improves the performance on DOTA as it contains more tiny objects as compared to other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Detection of Small Objects</head><p>We observe that the targeted queries like 'all small objects' and 'all little objects' can improve the detection accuracy of small objects as compared to a rather general text query 'all objects'. Quantitative and qualitative results are presented in <ref type="figure">Fig. 4a (main paper)</ref>. For quantitative comparison, all objects covering less than 5% of the image area are considered small, between 5% and 20% are considered medium and greater than 20% are considered large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Open-world Object Detection</head><p>The proposals from MAVL are used to generate the pseudo-labels for unknown categories in Open-world Object Detector (ORE) <ref type="bibr" target="#b27">[28]</ref> training. To avoid any data leakage, MAVL is trained on a subset of LMDet dataset, removing all the captions that contain any of the 60 unknown categories in ORE task-1. This filtering leaves us with a dataset having approximately 0.76M (out of 1.3M) image-text pairs. MAVL is trained from scratch on this filtered dataset for 20 epochs and then used to produce unknown pseudo-labels using class-agnostic object proposal generation.</p><p>To do so, firstly, proposals with objectness score less than 0.7 are discarded. Secondly, all proposals having an IoU greater than 0.5 with any ground-truth bounding box of a known category are removed. Rest of the proposals potentially belong to unknown categories and are used as pseudo-labels of unknowns in ORE training. All relevant scripts and annotations will be publicly released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Limitations</head><p>Although MViTs (GPV-1 <ref type="bibr" target="#b19">[20]</ref>, MDETR <ref type="bibr" target="#b28">[29]</ref> and MAVL) show state-of-the-art class-agnostic OD performance across various dataset domains, they cannot be directly adapted to specialized out-of-domain detection tasks such as in medical imaging. We evaluate the class-agnostic OD performance of MAVL on DeepLesion <ref type="bibr" target="#b75">[76]</ref> dataset <ref type="figure" target="#fig_5">(Fig. 6)</ref>. The groundtruth annotations represented by the green boxes in <ref type="figure" target="#fig_5">Fig. 6</ref>, indicate that the target lesions do not well represent the concept of an object, and require expert based supervision to identify the abnormalities. In medical domain, lesion detection task involves locating the congenital malformations in different types of medical images including X-rays, CT scans, MRI scans and Ultrasoud. These applications require specialized data along with expert supervision (obtained from well-trained domain specialists) to perform well. Hence, in most cases, the general class-agnostic OD methods (e.g., MViTs) cannot be direclty used. We observe that the generic class-agnostic detection mechanism of MViTs trained on out-of-domain natural images is not well-suited for generating proposals that can cater the need of specific medical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Qualitative Results</head><p>We present examples of class-agnostic predictions of MDETR and MAVL across natural image dataset Pascal VOC <ref type="bibr" target="#b13">[14]</ref>, COCO/LVIS <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b18">19]</ref>, autonomous driving dataset KITTI <ref type="bibr" target="#b16">[17]</ref> and indoor Kitchen dataset <ref type="bibr" target="#b17">[18]</ref> in <ref type="figure" target="#fig_6">Fig. 7</ref> and out-ofdomain datasets that include sketches, painting, cartoons <ref type="bibr" target="#b25">[26]</ref> and satellite images <ref type="bibr" target="#b71">[72]</ref> in <ref type="figure">Fig. 8</ref>. The detections are generated using the natural language text query, 'all objects'. In <ref type="figure">Fig. 9</ref>, we present some qualitative examples of classagnostic OD with DETReg <ref type="bibr" target="#b2">[3]</ref> trained using off-the-shelf proposals from Selective Search <ref type="bibr" target="#b63">[64]</ref> in comparison with DETReg trained using MAVL proposals. <ref type="figure" target="#fig_0">Fig. 10</ref> shows some examples of improved Open-world detector (ORE) trained with MAVL unknown pseudo-labels. The images on the left of each example correspond to the ORE trained with unknown pseudo-labels from RPN and on the right correspond to the ORE trained with unknown pseudo-labels from MAVL. The visualizations indicate that the improved model is better capable of detecting unknowns. Additionally, it reduces the misclassifications of unknown categories with other known categories. For example, the second sample in <ref type="figure" target="#fig_0">Fig. 10</ref> (top row -right side), corresponds to a sample in task 3 where 'laptop' belongs to the unknown categories set, was misclassified as 'TV', which is however correctly classified as an unknown with the improved model. This is advantageous as it (a) Pascal VOC <ref type="bibr" target="#b13">[14]</ref> (b) MS COCO <ref type="bibr" target="#b39">[40]</ref> / LVIS <ref type="bibr" target="#b18">[19]</ref> (c) Kitchen <ref type="bibr" target="#b17">[18]</ref> (d) KITTI <ref type="bibr" target="#b16">[17]</ref>  can better aid continual learning, i.e., the model can learn about the unknown categories when additional information about the unknowns are obtained via supervision. In <ref type="figure" target="#fig_0">Fig. 11</ref>, we present examples of qualitative results obtained for salient OD and camouflaged OD with specific queries, 'all salient objects' and 'all camouflaged objects' respectively, along with the bounding box annotations from the ground-truth masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Gains from MSDA in MAVL</head><p>We ablate the contribution of MSDA in <ref type="table" target="#tab_1">Table 11</ref> for our MAVL model. The class-agnostic OD results show the significance of MSDA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Impact of Late Fusion in MAVL</head><p>The late fusion is crucial to our MAVL since it enables an efficient MViT design while keeping the multi-scale spatial information intact. Notably, early fusion (as in MDETR) ignores the spatial structure of images which makes it infeasible <ref type="figure">Fig. 8</ref>: Class-agnostic detections of MViTs (MDETR <ref type="bibr" target="#b28">[29]</ref> and MAVL) on out-ofdomian datasets, Comic, Clipart, Watercolor and DOTA. <ref type="figure">Fig. 9</ref>: Class-agnostic OD performance of DETReg <ref type="bibr" target="#b2">[3]</ref> trained using Selective Search <ref type="bibr" target="#b63">[64]</ref> versus MAVL proposals. The images on the left side of each example correspond to DETReg trained with Selective search and the images on the right side correspond to the one trained with MAVL that results in better localized predictions to operate with MSDA (that requires spatial information for deformable attention). Thus, MAVL effectively combines MSDA with late vision-text fusion and provides gain over MDETR in class-agnostic OD benchmarks. Unlike MDETR, our MAVL does not rely on contrastive alignment and thus removing MSDA significantly affects the results <ref type="table" target="#tab_1">(Table 11</ref>).     <ref type="table" target="#tab_1">Table 12</ref>: Class-agnostic recall (R50) of MAVL on LVIS rare, common and frequent categories. MAVL-ORE is trained on a filtered dataset generated by removing all captions listing any of 60 unknown COCO categories evaluated in ORE <ref type="bibr" target="#b27">[28]</ref>. <ref type="table" target="#tab_1">Table 13</ref> shows the class-agnostic OD results of MAVL when queried using a general (e.g., combination of queries in <ref type="table" target="#tab_4">Table 3</ref>) versus combining detections from all category specific queries. Specifically, we use query 'every &lt; category name &gt;' for each category of a dataset and combine proposals using class-agnostic NMS. We note that MAVL generates better class-agnostic detections with general text queries.  <ref type="table" target="#tab_1">Table 13</ref>: Comparison of using general versus category-specific queries for classagnostic OD on three datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Querying All Class Names</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Salient Object Detection</head><p>A common formulation of deep learning based Salient Object Detection (SOD) approaches is to predict a saliency map for each input image. We evaluate MAVL against state-of-the art SOD approaches by converting the bounding box predictions of the the MViT model to masks using a COCO <ref type="bibr" target="#b39">[40]</ref> trained Mask-RCNN <ref type="bibr" target="#b21">[22]</ref> mask head. These converted masks are evaluated against the saliency predictions of PoolNet <ref type="bibr" target="#b40">[41]</ref> and CPD <ref type="bibr" target="#b70">[71]</ref> models on DUT-OMRON <ref type="bibr" target="#b76">[77]</ref> and ECSSD <ref type="bibr" target="#b56">[57]</ref> datasets <ref type="table" target="#tab_1">(Table 14a</ref>). Following <ref type="bibr" target="#b40">[41]</ref> and <ref type="bibr" target="#b70">[71]</ref>, F-measure (F b ) and mean absolute Error (MAE) are reported. <ref type="table" target="#tab_1">Table 14</ref>: Segmentation based evaluation of MAVL on salient and comouflaged object detection in comparison with the corresponding state-of-the art approaches. The MAVL proposals are converted to masks using COCO <ref type="bibr" target="#b39">[40]</ref> trained mask head of Mask-RCNN <ref type="bibr" target="#b21">[22]</ref>. </p><formula xml:id="formula_1">Dataset ? DUT-OMRON ECSSD Model MAE ? F-b ? MAE ? F-b</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Camouflaged Object Detection</head><p>In this section, we compare camouflaged masks predictions of SINET-V2 <ref type="bibr" target="#b14">[15]</ref> with MAVL. Similar to SOD task, the bounding box predictions from the MViT are converted to object masks using the mask head of COCO <ref type="bibr" target="#b39">[40]</ref>   <ref type="table" target="#tab_1">Table 15</ref> indicates that using a stronger backbone improves the class-agnostic OD accuracy across different dataset domains. The performance boost is significant for out of domain datasets, Kitchen <ref type="bibr" target="#b17">[18]</ref>, Clipart, Comic and Watercolor <ref type="bibr" target="#b25">[26]</ref>, indicating better generalization of MViT when trained using a stronger backbone model.  <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b80">81]</ref> generates binary features based on edge information for fast objectness estimation. DNNs have also been investigated for generating object proposals. DeepBox <ref type="bibr" target="#b32">[33]</ref> proposes a network that can be used to rerank any bottom-up proposals, e.g., the ones generated by EdgeBox <ref type="bibr">[84]</ref>. DeepMask <ref type="bibr" target="#b48">[49]</ref> generates rich object segmentations and an associated score of the likelihood of the patch to fully contain a centered object. A refinement of this method is proposed in SharpMask <ref type="bibr" target="#b49">[50]</ref>. Alternatively, Ren et al. proposed region proposal network (RPN) <ref type="bibr" target="#b53">[54]</ref> for generating object proposals, that identifies a set of regions that potentially contain objects along with corresponding objectness score. These are then refined for classification and localization for class-aware object detection. These are widely used in many two-stage objects detectors e.g., RCNN variants <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38]</ref>. Jaiswal et al. proposed an adversarial framework <ref type="bibr" target="#b26">[27]</ref> for class-agnostic object detection which replaces object type classification head with a binary classifier for class-agnostic detection. Another recent work proposes an Object Localization Network (OLN) <ref type="bibr" target="#b30">[31]</ref> that replaces the classifier head in Faster-RCNN <ref type="bibr" target="#b53">[54]</ref> with localization quality estimators such as centerness and IoU score for objectness estimation. Alternatively, Sim?oni et al. proposed a method <ref type="bibr" target="#b57">[58]</ref> that extracts features from a DINO <ref type="bibr" target="#b6">[7]</ref> self supervised pre-trained transformer that uses patch correlations in an image to propose object proposals.</p><p>Multi-modal Transformers: Multi-modal Vision Transformers (MViT) typically involve learning task agnostic vision-language (V+L) representations using millions of image-text pairs and then transferring the knowledge to downstream tasks <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b28">29</ref>]. Inspired from the success of BERT <ref type="bibr" target="#b11">[12]</ref> in natural language processing (NLP), VisualBERT <ref type="bibr" target="#b35">[36]</ref>, ViLBERT <ref type="bibr" target="#b43">[44]</ref> and LXMERT <ref type="bibr" target="#b61">[62]</ref> jointly learn V+L representations using image-caption pairs. They utilize a pretrained region proposal method <ref type="bibr" target="#b53">[54]</ref> and learn the V+L correlation using self-supervised tasks such as mask language modeling and sentence image alignment. In a concurrent work, VL-BERT <ref type="bibr" target="#b59">[60]</ref> performs pretraining on both text-only and visual-linguistic datasets and achieve an improved performance on multiple downstream visual comprehension tasks. UNITER <ref type="bibr" target="#b8">[9]</ref> introduces Word-Region Alignment (WRA) pretraining task using Optimal Transport (OT) <ref type="bibr" target="#b47">[48]</ref> which facilitates the alignment between text and image regions. It only masks one modality at a time while keeping the other modality intact which helps it to better capture the V+L relationships.</p><p>All these methods utilize an off-the-shelf region proposal method <ref type="bibr" target="#b53">[54]</ref> which usually produces noisy regions. OSCAR <ref type="bibr" target="#b36">[37]</ref> tries to mitigate this problem by using object detector tags for modeling V+L understanding. It relies on the fact that the salient objects in the image are easy to detect and are typically mentioned in the caption. Alternatively, MDETR <ref type="bibr" target="#b28">[29]</ref> leverages explicit alignment between text and ground-truth bounding boxes to learn visual-language alignment. It builds on-top-of recently proposed DETR <ref type="bibr" target="#b4">[5]</ref> model, generalizes to unseen concepts and outperforms the previous methods on many V+L downstream tasks. Going further, 12-in-1 <ref type="bibr" target="#b44">[45]</ref> utilizes the pretrained V+L representations and performs a joint training of a single model on 12 datasets. This learning paradigm improves the single task performance as compared to the typical taskwise training by achieving superior results on 11 out of 12 tasks. Gupta et al.</p><p>proposed GPV-I <ref type="bibr" target="#b19">[20]</ref>, a unified architecture for multi-task learning, where the task is inferred from the text prompt. It takes an image and a task description as input and outputs text with the corresponding bounding boxes. It is also based on DETR <ref type="bibr" target="#b4">[5]</ref>. We observe that these <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b19">20]</ref> multi-modal transformers, which are trained using aligned image-text pairs, produce high quality object proposals by using simple text queries e.g., 'all objects'.</p><p>Unsupervised Approaches: Recently, many unsupervised pretraining methods are proposed for the object detection task. Xiao et al. introduced ReSim [73] to encode both the region and global representations during self-supervised pretraining. In addition to the standard contrastive learning objective <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>, it slides a window in the overlapping region of the different views of an image and maximizes the feature similarity of the corresponding features across all convolutional layers. DetCo <ref type="bibr" target="#b73">[74]</ref> approaches this problem by generating both the global views and local patches from an image and defines hierarchical global-to-global, local-to-local and global-to-local contrastive objectives. UP-DETR <ref type="bibr" target="#b10">[11]</ref> proposes 'random query patch detection' pretext task for pretraining of DETR <ref type="bibr" target="#b4">[5]</ref>. The random patches from the image are generated and the model is trained on a large-scale dataset to locate these patches. DETReg <ref type="bibr" target="#b2">[3]</ref> argues that it is necessary to pre-train both the backbone and the detection network for learning good representations for object detection downstream tasks. It utilizes an off-the-shelf selective search <ref type="bibr" target="#b63">[64]</ref> proposal generation algorithm for acquiring pseudo-labels for localization and pretrained contrastive clustering based SwAV <ref type="bibr" target="#b5">[6]</ref> model for separating categories in the feature space. All these methods can be used for generating class-agnostic object proposals after the unsupervised pretraining. However, as shown in our analysis, the unsupervised approaches do not perform as well as the proposed class-agnostic OD framework based on supervised MViTs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*Fig. 1 :</head><label>1</label><figDesc>Equal contribution arXiv:2111.11430v6 [cs.CV] 19 Jul 2022 We show that Multi-modal Vision Transformers (MViTs) excel at Classagnostic OD across multiple domains: natural images</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Architecture overview of MViTs used in this work -GPV-1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>(</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>(a) Effect of using different number of top-ranked boxes on multiple classagnostic OD methods. The MViTs exhibits good recall even with only top-10 proposals. (b) MAVL class-agnostic OD performance on rarely and frequently occurring categories in LMDet. Rare categories are selected from Open Images [34]. The MAVL recall rates (represented by the bars) are compared with those of Def-DETR [83] and DeepMask [49] (represented by the lines). The numbers on top of the bars indicate the total occurrences of the category in LMDet captions. The MViT achieves good recall even for the classes with no or very few occurrences in the training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>(a) Qualitative results of Salient (Top) and Camouflaged OD (Bottom). The ground-truth masks and boxes are shown on top right of the images. (b) Complimentary effect of using off-the-shelf proposals from MAVL in Faster RCNN [54] trained on COCO [40], indicated as 'combined' (i.e., RPN + MAVL). The x-axis shows the number of proposals. MAVL generates good quality proposals, which perform well even with small proposal set sizes and demonstrate complimentary advantage to RPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Illustration of MAVL detections on the DeepLesion [76] dataset. The green boxes indicate the ground-truth bounding box enclosing the lesion on the CT images and the red boxes are the class-agnostic predictions. The samples indicate a failure case of class-agnostic detection of MViT's on lesion detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Class-agnostic detections of MViTs (MDETR [29] and MAVL) on natural image datasets, Pascal VOC, MS COCO/LVIS, Kitchen and KITTI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10 :</head><label>10</label><figDesc>Qualitative results of unknown detections in ORE<ref type="bibr" target="#b27">[28]</ref> when trained using RPN (left) versus MAVL (right) unknown pseudo-labels. Using proposals from MAVL as unknown pseudo-labels improves the prediction of unknowns. It reduces the misclassifications of unknown categories with other known categories. The second example (shown in top row -right side), corresponds to a sample in task 3 where 'laptop' belongs to the unknown categories set, was misclassified as 'TV', which is however correctly classified as an unknown with the improved model. This better aids in continual learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Top Rows: Qualitative results of MAVL for Salient OD. Bottom Rows: Camouflaged OD (right) tasks. The ground-truth masks along with the generated bounding boxes are shown on top right of the image D.3 Generalization Ability onto Novel/Rare Classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Class-agnostic OD results of MViTs in comparison with bottom-up ap-</figDesc><table><row><cell cols="11">proaches (row 3-5) and uni-modal detectors (row 6-8) trained to localize generic objects.</cell></row><row><cell cols="11">Bottom row shows gain of MAVL over the best uni-modal method. In general, MViTs</cell></row><row><cell cols="11">achieve state-of-the-art performance using intuitive text queries (details in Sec. 4.1).</cell></row><row><cell>Dataset ?</cell><cell cols="2">Pascal-VOC</cell><cell cols="2">COCO</cell><cell cols="2">KITTI</cell><cell cols="2">Objects365</cell><cell cols="2">LVIS</cell></row><row><cell>Model ?</cell><cell>AP50</cell><cell>R50</cell><cell>AP50</cell><cell>R50</cell><cell>AP50</cell><cell cols="5">R50 AP50 R50 AP50 R50</cell></row><row><cell>Edge Boxes</cell><cell>0.08</cell><cell>7.14</cell><cell>0.09</cell><cell>5.16</cell><cell>0.09</cell><cell>6.58</cell><cell>0.07</cell><cell>3.27</cell><cell>0.05</cell><cell>3.00</cell></row><row><cell>Selective Search</cell><cell>0.32</cell><cell>21.4</cell><cell>0.27</cell><cell>12.7</cell><cell>0.03</cell><cell>4.85</cell><cell>0.38</cell><cell>10.7</cell><cell>0.24</cell><cell>9.31</cell></row><row><cell>Deep Mask</cell><cell>5.92</cell><cell>40.4</cell><cell>2.16</cell><cell>19.2</cell><cell>1.33</cell><cell>15.5</cell><cell>1.31</cell><cell>14.5</cell><cell>0.51</cell><cell>8.17</cell></row><row><cell>Faster-RCNN</cell><cell>42.9</cell><cell>85.8</cell><cell>26.4</cell><cell>58.7</cell><cell>23.5</cell><cell>53.2</cell><cell>24.8</cell><cell>54.6</cell><cell>8.91</cell><cell>35.6</cell></row><row><cell>RetinaNet</cell><cell>43.2</cell><cell>86.6</cell><cell>24.6</cell><cell>59.1</cell><cell>30.4</cell><cell>57.6</cell><cell>24.3</cell><cell>54.8</cell><cell>8.57</cell><cell>35.7</cell></row><row><cell>Def-DETR</cell><cell>30.1</cell><cell>81.0</cell><cell>20.0</cell><cell>53.5</cell><cell>23.7</cell><cell>55.0</cell><cell>17.0</cell><cell>45.9</cell><cell>6.60</cell><cell>30.7</cell></row><row><cell>GPV-I</cell><cell>61.9</cell><cell>91.1</cell><cell>38.0</cell><cell>64.4</cell><cell>43.0</cell><cell>64.4</cell><cell>25.6</cell><cell>50.2</cell><cell>9.18</cell><cell>27.5</cell></row><row><cell>MDETR</cell><cell>66.0</cell><cell>90.1</cell><cell>40.7</cell><cell>62.2</cell><cell>46.7</cell><cell>67.2</cell><cell>30.4</cell><cell>54.0</cell><cell>10.7</cell><cell>32.8</cell></row><row><cell>MAVL (Ours)</cell><cell>68.6</cell><cell>91.3</cell><cell>43.6</cell><cell>65.0</cell><cell>48.2</cell><cell>63.5</cell><cell>33.2</cell><cell>57.9</cell><cell>11.7</cell><cell>37.0</cell></row><row><cell></cell><cell cols="6">+25.4 +4.7 +19.0 +5.9 +17.8 +5.9</cell><cell>+8.4</cell><cell>+3.1</cell><cell>+2.8</cell><cell>+1.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Class-agnostic OD performance of MViTs in comparison with RetinaNet[39] on several out-of-domain datasets. MViTs show consistently good results on all datasets. ? Proposals on DOTA<ref type="bibr" target="#b71">[72]</ref> are generated by multi-scale inference (see Sec. A.2).</figDesc><table><row><cell>Dataset ?</cell><cell cols="2">Kitchen</cell><cell cols="2">Clipart</cell><cell cols="2">Comic</cell><cell cols="2">Watercolor</cell><cell cols="2">DOTA  ?</cell></row><row><cell>Model ?</cell><cell cols="10">AP50 R50 AP50 R50 AP50 R50 AP50 R50 AP50 R50</cell></row><row><cell>RetinaNet</cell><cell>35.3</cell><cell>89.5</cell><cell>27.0</cell><cell>90.0</cell><cell>33.1</cell><cell>86.1</cell><cell>47.8</cell><cell>91.9</cell><cell>0.72</cell><cell>15.6</cell></row><row><cell>GPV-1</cell><cell>24.5</cell><cell>84.8</cell><cell>35.1</cell><cell>86.1</cell><cell>42.3</cell><cell>83.6</cell><cell>50.3</cell><cell>89.5</cell><cell>0.55</cell><cell>9.33</cell></row><row><cell>MDETR</cell><cell>38.4</cell><cell>91.4</cell><cell>44.9</cell><cell>90.7</cell><cell>55.8</cell><cell>89.5</cell><cell>63.6</cell><cell>94.3</cell><cell>1.94</cell><cell>21.8</cell></row><row><cell>MAVL (Ours)</cell><cell>45.4</cell><cell>91.0</cell><cell>50.6</cell><cell>92.9</cell><cell>57.7</cell><cell>89.2</cell><cell>63.8</cell><cell>95.6</cell><cell>2.86</cell><cell>24.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Using different intuitive text queries with MAVL. Combining detections from multiple queries captures varying aspects of objectness. 88.4 34.6 54.6 41.9 59.5 all visible entities &amp; objects 63.3 89.0 37.9 61.6 42.0 63.0 all obscure entities &amp; objects 59.5 86.6 35.2 59.1 42.4 63.5 all small objects 40.0 83.9 28.9 58.9 40.4 65.2</figDesc><table><row><cell>Dataset ?</cell><cell>Pascal-VOC</cell><cell>COCO</cell><cell>KITTI</cell></row><row><cell>Text Query ?</cell><cell cols="3">AP50 R50 AP50 R50 AP50 R50</cell></row><row><cell>all objects</cell><cell cols="3">51.3 85.5 33.3 58.4 40.2 64.0</cell></row><row><cell>all entities</cell><cell>65.2</cell><cell></cell><cell></cell></row></table><note>combined detections (CD) 63.7 91.0 42.0 65.0 48.2 63.5 CD w/o 'all small objects' 68.6 91.3 43.6 65.0 45.8 61.6</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>(left) shows an interesting case of how the text query 'all little objects' improves recall for small objects as compared to a rather general text query. Similarly,Fig. 4a (right)indicates how the use of special The use of specific query ('all little objects') increases recall of small objects across different datasets (left). Targeted detections by the relevant text queries (right).</figDesc><table><row><cell>(a)</cell><cell>(b)</cell></row><row><cell cols="2">Fig. 4: (a) MAVL recall for small (S), medium (M) and large (L) objects across three</cell></row><row><cell>datasets.</cell><cell></cell></row></table><note>(b) Visualizations of ORE [28] unknown detections when trained with RPN versus MAVL unknown pseudo-labels (top). Class-agnostic OD of DETReg [3] when trained using Selective Search (SS) [64] versus MAVL proposals (bottom).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>MViT proposals are used for pseudo-labelling of unknowns in ORE<ref type="bibr" target="#b27">[28]</ref>. MAVL represents the model trained on a filtered dataset generated by removing all captions from LMDet listing any of the 60 unknown categories evaluated in ORE. The results indicate a notable improvement in unknown detection.</figDesc><table><row><cell>Task ID</cell><cell cols="2">Task 1</cell><cell></cell><cell cols="2">Task 2</cell><cell></cell><cell></cell><cell cols="2">Task 3</cell><cell></cell><cell cols="2">Task 4</cell><cell></cell></row><row><cell>Pseudo-label for Unknown</cell><cell>mAP Current Known</cell><cell>R50 Unknown</cell><cell>Previous Known</cell><cell>mAP Current Known</cell><cell>Both</cell><cell>R50 Unknown</cell><cell>Previous Known</cell><cell>mAP Current Known</cell><cell>Both</cell><cell>R50 Unknown</cell><cell>Previous Known</cell><cell>mAP Current Known</cell><cell>Both</cell></row><row><cell>RPN</cell><cell>63.4</cell><cell>14.4</cell><cell>58.3</cell><cell cols="2">30.8 45.1</cell><cell>11.3</cell><cell>43.3</cell><cell cols="2">23.4 36.7</cell><cell>14.8</cell><cell>37.2</cell><cell cols="2">20.7 33.1</cell></row><row><cell>MAVL  *</cell><cell>64.0</cell><cell>50.1</cell><cell>61.6</cell><cell cols="2">30.8 46.2</cell><cell>49.5</cell><cell>43.8</cell><cell cols="2">22.7 36.8</cell><cell>50.9</cell><cell>36.2</cell><cell cols="2">20.6 32.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Effect of using MAVL proposals for pre-training of DETReg [3] instead of Selective Search [64] proposals. DETReg -MAVL 58.8 80.5 65.7 64.5 84.2 71.3</figDesc><table><row><cell>Dataset?</cell><cell>Pascal-VOC 10%</cell><cell>Pascal-VOC 100%</cell></row><row><cell>Model ?</cell><cell cols="2">AP AP50 AP75 AP AP50 AP75</cell></row><row><cell>DETReg -SS</cell><cell cols="2">51.4 72.2 56.6 63.5 83.3 70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Proposals from MAVL are evaluated against state-of-the-art SOD and COD approaches. The general ? represents 'all objects' text query.</figDesc><table><row><cell>Dataset ?</cell><cell></cell><cell cols="2">DUT-OMRON</cell><cell>ECSSD</cell><cell>Dataset ?</cell><cell cols="3">CHAMELEON CAMO</cell><cell>COD10K</cell></row><row><cell>Model ?</cell><cell>Text Query</cell><cell>AP50</cell><cell>R50</cell><cell>AP50 R50</cell><cell>Model ?</cell><cell cols="3">Text Query AP50 R50 AP50 R50 AP50 R50</cell></row><row><cell>CPD [71] PoolNet [41] MAVL MAVL</cell><cell cols="2">--General  ? Task specific  ? ? 75.5 64.5 66.5 67.0</cell><cell>77.4 78.8 89.1 93.3</cell><cell>87.1 92.7 87.4 93.1 84.5 95.7 85.7 96.1</cell><cell>SINET-V2 [15] MAVL MAVL</cell><cell>-General  ? Task specific  ? ? 36.2 67.3 30.2</cell><cell>76.7 53.3 61.1</cell><cell>56.5 77.2 44.4 66.6 46.5 75.4 39.6 67.8 48.0 78.3 42.0 69.1</cell></row><row><cell cols="5">(a) Salient OD (SOD). Here task specific  ? ? query combines proposals</cell><cell cols="4">(b) Camouflaged OD (COD) on three datasets. Here task specific  ? ? query combines proposals</cell></row><row><cell cols="5">from 'all salient objects' and 'all fore-</cell><cell cols="4">from 'all camouflaged objects' and 'all disguised</cell></row><row><cell cols="4">ground objects' text queries.</cell><cell></cell><cell cols="2">objects' text queries.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>MAVL proposals perform well com-</figDesc><table><row><cell>proposal method (Selective Search</cell><cell></cell><cell></cell></row><row><cell>[64]). Meanwhile, UP-DETR per-</cell><cell cols="3">pared to unsupervised methods (UP-DETR</cell></row><row><cell>forms unsupervised pretraining on</cell><cell cols="3">[11] and DETReg [3]) and supervised uni-</cell></row><row><cell>random query patches in an im-</cell><cell cols="3">modal method (Def-DETR [83]).</cell></row><row><cell>age for class-agnostic OD. Both</cell><cell></cell><cell></cell></row><row><cell>the unsupervised models, DETReg</cell><cell>Dataset ? Model ?</cell><cell cols="2">Pascal-VOC Supervision AP50 R50 AP50 R50 AP50 R50 COCO KITTI</cell></row><row><cell>and UP-DETR, are trained on uni-</cell><cell cols="3">UP-DETR unsupervised 0.56 16.6 0.19 6.56 0.01 0.65</cell></row><row><cell>modal (Deformable DETR [83])</cell><cell>DETReg</cell><cell cols="2">self-supervised 2.58 45.7 2.04 26.0 0.01 2.48</cell></row><row><cell></cell><cell>Def-DETR</cell><cell>box-level</cell><cell>30.1 81.0 20.0 53.5 23.7 55.0</cell></row><row><cell></cell><cell>MAVL</cell><cell>box + text</cell><cell>68.6 91.3 43.6 65.0 48.2 63.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Effect of removing language branch</figDesc><table><row><cell>Dataset?</cell><cell></cell><cell>Pascal-VOC</cell><cell>COCO</cell><cell>KITTI</cell></row><row><cell>Model ?</cell><cell cols="4">Lang. AP50 R50 AP50 R50 AP50 R50</cell></row><row><cell>MDETR</cell><cell>?</cell><cell cols="3">63.9 88.0 38.1 58.5 42.5 60.9</cell></row><row><cell>MAVL</cell><cell>?</cell><cell cols="3">65.0 89.1 39.3 62.0 39.0 61.0</cell></row><row><cell>MDETR</cell><cell>?</cell><cell cols="3">59.7 86.4 33.4 57.9 36.9 55.0</cell></row><row><cell>MAVL</cell><cell>?</cell><cell cols="3">61.6 86.7 34.4 58.3 36.5 58.9</cell></row></table><note>from MViTs keeping the data loader struc- ture intact. The performance is not affected largely as the language structure is still in- tact (boxes from caption are seen together).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Experimental analysis to explore the</figDesc><table><row><cell cols="4">contribution of language by removing all tex-</cell></row><row><cell cols="4">tual inputs, but maintaining the structure in-</cell></row><row><cell cols="4">troduced by captions. Experiments are per-</cell></row><row><cell cols="4">formed on Def-DETR [83] using LMDet.</cell></row><row><cell></cell><cell>Language</cell><cell>Pascal-VOC MSCOCO</cell><cell>KITTI</cell></row><row><cell>Experiment</cell><cell cols="3">Structure AP50 R50 AP50 R50 AP50 R50</cell></row><row><cell>Setting-1</cell><cell>?</cell><cell cols="2">16.2 74.5 10.7 47.0 19.4 57.3</cell></row><row><cell>Setting-2</cell><cell>?</cell><cell cols="2">30.1 81.0 20.0 53.5 23.7 55.0</cell></row><row><cell>Setting-3</cell><cell>?</cell><cell cols="2">33.8 82.5 19.3 55.8 21.2 52.7</cell></row><row><cell>Setting-4</cell><cell>?</cell><cell cols="2">35.1 82.7 21.2 56.3 21.5 58.5</cell></row><row><cell>Setting-5</cell><cell>?</cell><cell cols="2">61.6 86.7 34.4 58.3 36.5 58.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>83. Zhu, X., Su, W., Lu, L., Li, B., Wang, X.,Dai, J.: Deformable DETR: Deformable Transformers for End-to-End Object Detection. In: International Conference on Learning Representations (2021) 84. Zitnick, C.L., Doll?r, P.: Edge Boxes: Locating Object Proposals from Edges. In: The European Conference on Computer Vision. pp. 391-405. Springer (2014)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Comparison of MDETR</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12</head><label>12</label><figDesc>shows quantitative results on LVIS rare, common and frequent categories. (1) Similar to frequent and common, our MAVL provides good recall rates for rare LVIS categories, indicating its robust class-agnostic behavior. We note that most of the rare category instances in LVIS are of tiny size (area &lt;7?7 pixels) and have low recall (?19%) as compared to the medium/large instances</figDesc><table><row><cell></cell><cell>Pascal-VOC MSCOCO KITTI</cell></row><row><cell>Model</cell><cell>AP50 R50 AP50 R50 AP50 R50</cell></row><row><cell cols="2">MAVL w/o MSDA 59.9 82.4 33.3 51.6 33.2 50.1</cell></row><row><cell>MAVL</cell><cell>65.0 89.1 39.3 62.0 39.0 61.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>Effect of removing MSDA from MAVL. It decreases the class-agnostic OD performance, indicating the importance of MSDA. The models are evaluated after 10 epochs for ablation.</figDesc><table><row><cell cols="4">with much high recall (?86%). (2) MAVL-ORE is trained by removing 60/80</cell></row><row><cell cols="4">common COCO categories from LMDet leaving only 0.76M/1.3M image-text</cell></row><row><cell cols="4">pairs. This strict setting with much less training data also shows favorable rare</cell></row><row><cell>class recall.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Lang. Rare Common Frequent All</cell></row><row><cell>1:MAVL</cell><cell>? 30.0</cell><cell>31.6</cell><cell>32.4 32.1</cell></row><row><cell>2:MAVL</cell><cell>? 38.0</cell><cell>40.5</cell><cell>37.1 37.0</cell></row><row><cell cols="2">3:MAVL-ORE ? 33.4</cell><cell>36.7</cell><cell>33.2 33.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>We explore the class-agnostic OD performance of MViTs for different convolutional backbones. Following<ref type="bibr" target="#b28">[29]</ref>, we compare the ResNet-101<ref type="bibr" target="#b22">[23]</ref> taken from Torchvision with EfficientNet-E5<ref type="bibr" target="#b62">[63]</ref> taken from Timm Library<ref type="bibr" target="#b67">[68]</ref>. The ResNet model is trained on ImageNet<ref type="bibr" target="#b54">[55]</ref> and achieves 77.4% top-1 accuracy on ImageNet validation, while the EfficientNet model is trained using Noisy-Student<ref type="bibr" target="#b74">[75]</ref> on an additional 300M unlabelled images achieving 85.1% top-1 accuracy on ImageNet validation.</figDesc><table><row><cell>trained Mask-</cell></row><row><cell>RCNN [22] model. Following [16], S-measure (S ? ), E-measure (E ? ), weighted</cell></row><row><cell>F-measure (F w ? ) and MAE of mask predictions are reported in Table 14b.</cell></row><row><cell>D.7 Effect of Various Backbones</cell></row><row><cell>ResNet vs. EfficientNet:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 15 :</head><label>15</label><figDesc>Class-agnostic object detection performance of MDETR<ref type="bibr" target="#b28">[29]</ref> for different convolutional backbones. The results indicate that the use of strong backbone improves the results especially on the out-of-domain (Kitchen<ref type="bibr" target="#b17">[18]</ref>, Clipart, Comic, Watercolor [26]) datasets. 90.0 42.3 61.3 48.1 65.2 53.3 91.5 62.3 92.7 69.9 90.5 74.4 95.0 3.71 24.9The class-agnostic OD is relatively less studied compared to class-aware detection.However, many object proposal generation algorithms have been proposed, since it remains a critical step in many applications like recognition and detection. The proposal generation algorithms can be categorized into three categories: (a) bottom-up segmentation based, (b) edge information based and (c) data-driven approaches based on deep neural network (DNN) architectures. In the first category that uses segmentation to derive proposals, multiple pixel groupings (superpixels) are merged according to various heuristics. Alexe et al. proposed an objectness<ref type="bibr" target="#b1">[2]</ref> scoring method that combines various low-level features such as edges, color and superpixels to score object proposals. Selective Search<ref type="bibr" target="#b63">[64]</ref> uses multiple hierarchical segmentations based on superpixels for object proposals. Similarly, MCG<ref type="bibr" target="#b51">[52]</ref> uses segment hierarchy to group regions. Among the second category approaches, EdgeBoxes [84] scores bounding box proposals based on contours that the boxes enclose. BING algorithm</figDesc><table><row><cell>Dataset</cell><cell>Pascal VOC COCO</cell><cell>KITTI</cell><cell>Kitchen</cell><cell>Clipart</cell><cell>Comic Watercolor DOTA</cell></row><row><cell>Model</cell><cell cols="5">AP50 R-50 AP50 R-50 AP50 R-50 AP50 R-50 AP50 R-50 AP50 R-50 AP50 R-50 AP50 R-50</cell></row><row><cell cols="6">MDETR-R101 66.0 90.1 40.7 62.2 46.7 67.2 38.4 91.4 44.9 90.7 55.8 89.5 63.6 94.3 1.94 21.8</cell></row><row><cell cols="2">MDETR-E5 69.6 E Related work</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Class-Agnostic Detection:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. Ming-Hsuan Yang is supported by the NSF CAREER grant 1149783. Fahad Shahbaz Khan is supported by the VR starting grant (2016-05543).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="73" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring the Objectness of Image Windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2189" to="2202" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">DETReg: Unsupervised Pretraining with Region Priors for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards Open World Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bendale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1893" to="1902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-End Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<title level="m">Emerging Properties in Self-Supervised Vision Transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">UNITER: UNiversal Image-TExt Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>El Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BING: Binarized Normed Gradients for Objectness Estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">UP-DETR: Unsupervised Pre-training for Object Detection with Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1601" to="1610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Overlooked Elephant of Object Detection: Open Set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dhamija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gunther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1021" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The Pascal Visual Object Classes (VOC) Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Concealed Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Camouflaged Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2777" to="2787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? The KITTI vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Multiview RGB-D Dataset for Object Instance Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Georgakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Reza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ko?eck?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="426" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">LVIS: A Dataset for Large Vocabulary Instance Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards General Purpose Vision Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="16399" to="16409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<title level="m">spaCy: Industrial-strength Natural Language Processing in Python</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6700" to="6709" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cross-Domain Weakly-Supervised Object Detection Through Progressive Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Inoue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Furuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5001" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Class-agnostic Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaiswal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="919" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Towards Open World Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5830" to="5840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">MDETR-Modulated Detection for End-to-End Multi-Modal Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1780" to="1790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.06753</idno>
		<title level="m">Learning Open-World Object Proposals without Learning to Classify</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">DeepBox: Learning Objectness with Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2479" to="2487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The open images dataset v4</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1956" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Anabranch network for camouflaged object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sugimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page" from="45" to="56" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<title level="m">VisualBERT: A Simple and Performant Baseline for Vision and Language</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Feature Pyramid Networks for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common Objects in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Simple Pooling-Based Design for Real-Time Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep Learning for Generic Object Detection: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="261" to="318" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">12-in-1: Multi-Task Vision and Language Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10437" to="10446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The role of context in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="520" to="527" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Computational Optimal Transport</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to Segment Object Candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to Refine Object Segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2641" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="128" to="140" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning Transferable Visual Models From Natural Language Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hierarchical Image Saliency Detection on Extended CSSD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="717" to="729" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Localizing Objects with Self-Supervised Transformers and no Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sim?oni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roburin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Animal Camouflage Analysis: CHAMELEON Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Skurowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Abdulameer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Depta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kornacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kozie L</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Unpublished Manuscript</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">VideoBERT: A Joint Model for Video and Language Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<title level="m">LXMERT: Learning Cross-Modality Encoder Representations from Transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Selective Search for Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Unidentified Video Objects: A Benchmark for Dense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Open-World Segmentation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<title level="m">Frustratingly Simple Few-Shot Object Detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Towards Universal Object Detection by Domain Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7289" to="7298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wightman</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4414861</idno>
		<ptr target="https://doi.org/10.5281/zenodo.4414861" />
		<title level="m">PyTorch Image Models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Optimizing connected component labeling algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Otoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoshani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging 2005: Image Processing</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">5747</biblScope>
			<biblScope unit="page" from="1965" to="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Cascaded Partial Decoder for Fast and Accurate Salient Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">DOTA: A Large-scale Dataset for Object Detection in Aerial Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3974" to="3983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Region Similarity Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">DetCo: Unsupervised Contrastive Learning for Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="8392" to="8401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Self-training with Noisy Student improves ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical imaging</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36501</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Saliency Detection via Graph-Based Manifold Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Open-Vocabulary Object Detection Using Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14393" to="14402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Barlow Twins: Self-Supervised Learning via Redundancy Reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Putting visual object recognition in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12985" to="12994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">BING++: A Fast High Quality Object Proposal Generator at 100fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Saligrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1209" to="1223" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">UC2: Universal Cross-lingual Cross-modal Vision-and-Language Pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4155" to="4165" />
		</imprint>
	</monogr>
	<note>a) Comic [26] (b) Clipart [26</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
