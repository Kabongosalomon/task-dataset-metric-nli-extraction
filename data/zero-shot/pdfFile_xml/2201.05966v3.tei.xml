<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">UNIFIEDSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Henry</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Shi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
							<affiliation key="aff3">
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Scholak</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Sheng</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><forename type="middle">I</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengzu</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Boyle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ansong</forename><surname>Ni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Allen Institute for Artificial Intelligence 17</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">UNIFIEDSKG: Unifying and Multi-Tasking Structured Knowledge Grounding with Text-to-Text Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">5 ServiceNow Research 6 Stanford University 7 Salesforce Research 8 UIUC 9 Google Research 10 Facebook AI Research 11 University of Edinburgh 12 Shanghai AI Lab 13 Yale University 14 George Mason University 15 Penn State University</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests, such as semantic parsing over databases and question answering over knowledge bases. Since the inputs and outputs of SKG tasks are heterogeneous, they have been studied separately by different communities, which limits systematic and compatible research on SKG. In this paper, we overcome this limitation by proposing the UNIFIEDSKG framework, which unifies 21 SKG tasks into a text-to-text format, aiming to promote systematic SKG research, instead of being exclusive to a single task, domain, or dataset. We use UNIFIEDSKG to benchmark T5 with different sizes and show that T5, with simple modifications when necessary, achieves stateof-the-art performance on almost all of the 21 tasks. We further demonstrate that multi-task prefix-tuning improves the performance on most tasks, largely improving the overall performance. UNIFIEDSKG also facilitates the investigation of zero-shot and few-shot learning, and we show that T0, GPT-3, and Codex struggle in zero-shot and few-shot learning for SKG. We also use UNIFIEDSKG to conduct a series of controlled experiments on structured knowledge encoding variants across SKG tasks. UNIFIEDSKG is easily extensible to more tasks, and it is open-sourced at https: //github.com/hkunlp/unifiedskg.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Structured knowledge (e.g., web tables, knowledge graphs, and databases) stores large amounts of data in organized structures, forming a basis for a wide range of applications, e.g., medical diagnosis, personal assistants, and customer relations manage-ment. Accessing and searching data in structured knowledge typically requires mastering query languages through professional training. To promote the efficiency of data access, structured knowledge grounding (SKG) systems ground user requests in structured knowledge and produce various outputs, including computer programs (e.g., SQL and SPARQL), table cell values, and natural language responses <ref type="figure">(Figure 1</ref>). For example, semantic parsing <ref type="bibr">(Zelle and Mooney, 1996;</ref><ref type="bibr">Zettlemoyer and Collins, 2005)</ref> converts natural language questions into formal programs; knowledge-base question answering <ref type="bibr">(Berant et al., 2013)</ref> derives answers from tables or knowledge graphs.</p><p>SKG has attracted significant interest and has been studied through different tasks defined by different communities. Recent developments in tasks, models, and datasets for SKG have led to taskspecific modeling advances, making each task's progress seemingly unique and incompatible. A main reason is that SKG tasks are heterogeneous. Different types of structured knowledge, such as databases or knowledge graphs, lead to highly specialized encoders <ref type="bibr">(Lin et al., 2019;</ref><ref type="bibr">Herzig et al., 2020;</ref><ref type="bibr">Wang et al., 2020;</ref><ref type="bibr">Yasunaga et al., 2021)</ref>. Some SKG tasks, e.g., semantic parsing, use customized decoders to generate programs <ref type="bibr">(Yin and Neubig, 2018;</ref><ref type="bibr">Ren et al., 2021)</ref>. Therefore, instead of solving common challenges in SKG research, improvements in SKG have been prone to be exclusive to a single task, domain, or dataset.</p><p>In this paper, we propose the UNIFIEDSKG framework to advocate for a unifying view of 21 SKG tasks across six task families and multiple data domains <ref type="table" target="#tab_2">(Table 1)</ref>. UNIFIEDSKG standardizes datasets, models, code, experiments, and evaluation metrics into a single framework. By casting user requests, structured knowledge, and outputs Figure 1: Structured knowledge grounding (SKG) leverages structured knowledge to complete user requests. By casting inputs and outputs into the text-to-text format, UNIFIEDSKG standardizes datasets, models, code, experiments, and metrics for 21 SKG tasks.</p><p>into the text-to-text format <ref type="bibr">(Raffel et al., 2020)</ref>, it promotes model advances where new tasks can be framed with our standardized abstraction, and new models can be easily applied to diverse SKG tasks. While previous works also cast SKG tasks into the text-to-text format <ref type="bibr">(Hosseini-Asl et al., 2020;</ref><ref type="bibr">Shaw et al., 2021;</ref><ref type="bibr">Liu et al., 2021)</ref>, their independent choices of pretrained language models (PLMs), input-output formats, and frameworks make our unification non-trivial. UNIFIEDSKG is easily extensible to more SKG tasks, and it is open-sourced to promote community-wide progress.</p><p>Using UNIFIEDSKG as a benchmark, we show that finetuning T5 (with constrained decoding or reranking when necessary) on individual tasks achieves state-of-the-art (sota) results on almost all of the 21 tasks, establishing a powerful and reproducible starting point for SKG research. T5 performance also increases with size on most tasks. UNIFIEDSKG facilitates multi-task learning on SKG, enabling knowledge sharing and cross-task generalization. Although simple multi-task learning has mixed results, we show that multi-task learning with prefix-tuning <ref type="bibr">(Li and Liang, 2021)</ref> benefits most tasks and largely improves the overall performance, on both T5-base and T5-large.</p><p>UNIFIEDSKG is a challenging testbed for fewshot <ref type="bibr">(Brown et al., 2020;</ref><ref type="bibr">Ye et al., 2021a)</ref> and zero-shot learning <ref type="bibr" target="#b2">(Zhong et al., 2021;</ref><ref type="bibr">Wei et al., 2021;</ref><ref type="bibr">Sanh et al., 2021)</ref> with PLMs. Our experiments show that models like T0 <ref type="bibr">(Sanh et al., 2021)</ref> struggle in zero-shot learning on SKG tasks, and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> and <ref type="bibr">Codex (Chen et al., 2021a)</ref> struggle in few-shot learning on SKG tasks.</p><p>UNIFIEDSKG enables a series of controlled ex-periments on structured knowledge encoding. We find that T5 is sensitive to encoding variations, and the sensitivity varies across tasks. UNIFIEDSKG aims to facilitate more general and robust structured knowledge encoding methods. Finally, we conduct a comprehensive error analysis across SKG tasks. Although the errors made by PLMs decrease with the model size, T5-3B may still generate invalid outputs. In summary, we 1) unify and benchmark 21 SKG tasks under the UNIFIEDSKG framework to evaluate diverse grounding goals and structured knowledge sources, 2) demonstrate (near) sota performance of T5 on all the unified SKG tasks, using a single, general-purpose approach, 3) show the benefit of knowledge sharing across SKG tasks via multi-task prefix-tuning, and 4) analyze recent modeling contributions (zero-shot, few-shot, and structured knowledge encoding) on these tasks. We hope UNIFIEDSKG enables the design of new models and learning algorithms that generalize to diverse SKG tasks and to identify their challenges.</p><p>2 Related Work SKG with PLMs PLMs have been applied to several SKG tasks. To encode structured knowledge, prior work linearized the structured knowledge and concatenated it with the text <ref type="bibr">(Hwang et al., 2019;</ref><ref type="bibr">Liu et al., 2020;</ref><ref type="bibr">Hosseini-Asl et al., 2020;</ref><ref type="bibr">Liu et al., 2021)</ref>, which has been augmented by positional encoding (e.g., row/column embedding) <ref type="bibr">(Herzig et al., 2020;</ref><ref type="bibr">Yin et al., 2020a)</ref> and template-based linearization <ref type="bibr">(Chen et al., 2020a,b;</ref><ref type="bibr">Oguz et al., 2021)</ref>, and planning <ref type="bibr">(Su et al., 2021)</ref>. Recently, cell-column alignment is modeled by manipulating  <ref type="bibr">(Gu et al., 2021)</ref> Knowledge Graph Question s-Expression WebQSP <ref type="bibr">(Yih et al., 2016)</ref> Knowledge Graph Question s-Expression MTOP <ref type="bibr">(Li et al., 2021)</ref> API Calls Question TOP Representation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question Answering</head><p>WikiSQL <ref type="bibr" target="#b4">(Zhong et al., 2017)</ref>  <ref type="table">Table  Question</ref> Answer WikiTQ <ref type="bibr">(Pasupat and Liang, 2015)</ref> Table <ref type="table">Question</ref> Answer CompWebQ <ref type="bibr">(Talmor and Berant, 2018)</ref> Knowledge Graph Question Answer HybridQA <ref type="bibr">(Chen et al., 2020c)</ref>   unified program learning, understanding, and repair tasks into a graph-to-sequence format. In this paper, we focus on the text-to-text format <ref type="bibr">(Raffel et al., 2020)</ref> due to its flexibility. Different from unifying tasks that only take text as input, a core challenge in unifying SKG tasks into the text-to-text format is to linearize structured knowledge. Notably, Uni-fiedQA (Khashabi et al., 2020) unified QA tasks, while UNIFIEDSKG covers a broader scope of six task families for systematic exploration.</p><p>Cross-task generalization with PLMs Multitask learning and transfer learning go beyond task boundaries, view different tasks as related, and have been shown to outperform single-task learning <ref type="bibr" target="#b1">(Aghajanyan et al., 2021a;</ref><ref type="bibr">Vu et al., 2021)</ref>. Large PLMs show potential for zero-shot and few-shot learning, e.g., <ref type="bibr">GPT-2 (Radford et al., 2019)</ref> and <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, which can be improved by multi-task learning <ref type="bibr" target="#b2">(Zhong et al., 2021)</ref>, e.g., <ref type="bibr">FLAN (Wei et al., 2021)</ref>, <ref type="bibr">T0 (Sanh et al., 2021), and</ref><ref type="bibr">CrossFit (Ye et al., 2021a)</ref>. <ref type="bibr">ExT5 (Aribandi et al., 2021)</ref> shows that scaling up multi-task learning helps improve pretraining efficiency and downstream performances. UNIFIEDSKG facilitates the investigation of multi-task, zero-shot, and few-shot learning on SKG tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The UNIFIEDSKG Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Task Unification</head><p>The guiding principle of UNIFIEDSKG's task selection is diversity. We unify 21 SKG tasks across six task families and multiple domains <ref type="table" target="#tab_2">(Table 1)</ref>. Our task families include:</p><p>? Semantic parsing converts questions to logical forms <ref type="bibr">(Zelle and Mooney, 1996;</ref><ref type="bibr">Zettlemoyer and Collins, 2005)</ref>.</p><formula xml:id="formula_0">True Entailed False Refuted</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boolean</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural language</head><p>How many singers are there?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Formal language</head><p>select count (*) from singer  <ref type="figure">Figure 2</ref>: We unify SKG tasks with heterogeneous inputs and outputs into the text-to-text format.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Set of answers</head><p>? Question answering derives answers to natural language questions based on structured data <ref type="bibr">(Berant et al., 2013)</ref>.</p><p>? Data-to-text generation describes structured data in natural language <ref type="bibr">(Novikova et al., 2017)</ref>.</p><p>? Fact verification checks if a statement is true based on the structured data (Chen et al., 2020b).</p><p>? Conversational tasks require understanding of not only the user's last request but also the full interaction history between users and machines <ref type="bibr">(Budzianowski et al., 2018;</ref><ref type="bibr">Eric et al., 2019;</ref><ref type="bibr">Yu et al., 2019a)</ref>.</p><p>? Formal language to text translation describes formal language in natural language <ref type="bibr">(Chen et al., 2020d)</ref>.</p><p>All these tasks take as input x a user request, a structured knowledge input, and an optional (dialogue) context to predict an output y. <ref type="figure">Figure 2</ref> illustrates how we convert the input x to an input sequencex and the output y to an output sequence? by means of "linearization" <ref type="bibr">(Liu et al., 2021)</ref>, enabling the unification of diverse forms of structured knowledge. We provide more details, examples, and input length analysis in the Appendices F and G. Our code implementation uses Hugging Face's Transformers <ref type="bibr">(Wolf et al., 2020)</ref> and <ref type="bibr">Datasets (Lhoest et al., 2021)</ref> toolkits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Modeling</head><p>The simplest usage of UNIFIEDSKG is to train on individual tasks. In this case, we minimize the negative log-likelihood loss averaged over tokens in each batch. For decoding, we use beam search by default. UNIFIEDSKG also facilitates exploration of multi-task learning, few-shot, and zeroshot learning with PLMs, and details are presented in the corresponding parts in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Results on Individual Tasks</head><p>We apply T5 models (Raffel et al., 2020) on each individual task in UNIFIEDSKG. For model training, we set the maximum number of epochs as 50-200, depending on the dataset size. We use early stopping and model selection on the development set. More details are shown in Appendix D.1. For each task, we report one commonly used metric in <ref type="table" target="#tab_5">Table 2</ref>. See Appendix B for all metrics. Comparison with previous sota <ref type="table" target="#tab_5">Table 2</ref> shows that vanilla T5-3B outperforms most previous sota models not trained on extra unsupervised indomain data. Some semantic parsing sota models, denoted as + in <ref type="table" target="#tab_5">Table 2</ref>, are also T5 with constrained decoding <ref type="bibr">(Scholak et al., 2021)</ref> or reranking <ref type="bibr">(Ye et al., 2021b)</ref>. This shows that a generalist architecture like T5, when scaled up to a certain size, can be as good as task-specific architectures for SKG, suggesting the potential of larger PLMs. Model scalability In general, T5 performance increases with the model size, but this trend varies across task families. Semantic parsing, QA, and fact verification tasks get large benefits from increased sizes, while text generation does not. See Section 4.5 for a human evaluation for text generation tasks. Also, the gap between T5-base (220M) and T5-large (770M) is larger than the gap between T5-large (770M) and T5-3B (3B). Effect of pretraining on structured knowledge Some smaller models pretrained on structured knowledge <ref type="bibr">(Liu et al., 2021)</ref> show competitive performance as T5-3B, suggesting that pretraining with structured data is beneficial for SKG. This result calls for structured knowledge pretraining that generalizes to different SKG tasks across domains, which can be systematically explored using UNIFIEDSKG.   <ref type="table">Table 3</ref>: Comparison between T5-3B and T0-3B. T0-3B is initialized from LM-adapted T5 and further pretrained on a large number of non-SKG tasks. We finetune both models on individual tasks. T0-3B underperforms T5-3B on semantic parsing (Spider) and outperforms T5-3B on dialogue state tracking (MWoZ) and fact verification (TabFact). We report results on the dev. set.</p><p>Effect of pretraining on non-SKG tasks T0-3B (Sanh et al., 2021) is initialized from T5-3B and pretrained on multiple tasks that (in most cases) do not use structured knowledge as input (non-SKG tasks). Exploring the performance of T0-3B on SKG tasks helps us understand the relationship between SKG tasks and non-SKG tasks. <ref type="table">Table 3</ref> shows that T0-3B under-performs T5-3B on semantic parsing and outperforms T5-3B on dialogue state tracking and fact verification. We note that T0-3B is pretrained on dialogue QA, dialogue summarization, and NLI tasks; therefore, pretraining on non-SKG tasks might not be useful for SKG unless we add similar SKG tasks to pretraining.</p><p>2 For GrailQA and WebQSP, we run T5 and rerun the previous sota model (Ye et al., 2021b) using the gold entities. For</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-Task Learning</head><p>UNIFIEDSKG facilitates the exploration of multitask learning. In this part, we systematically study multi-task learning on all 21 unified tasks. We find that SKG benefits from multi-task prefix-tuning on both T5-base and T5-large, showing that the benefits from multi-task learning is scalable in terms of the model size. The baselines we use include: Single-task finetuning (ST-F), which is finetuning on individual tasks, same as Section 4.1. Single-task prefix-tuning (ST-P; Li and Liang, 2021), which learns lightweight task-specific pa-MultiModalQA and FEVEROUS, we report performance of T5 and the previous sota models on the dev. samples with at least one table (samples with image input are further excluded for MultiModalQA); The gold table and text candidates are used for both T5 and previous sota (for MultiModelQA, numbers are from <ref type="bibr">(Yoran et al., 2021)</ref>, and for FEVEROUS, we rerun the available model (Aly et al., 2021) on gold candidates to obtain the number). We use sacreBLEU to report all BLEU results. ? We use gold entity linking, but the previous sota does not, which makes the results not directly comparable; therefore, we do not bold any numbers for CompWebQ and HybridQA. * T5-base with the independent output scheme (Lee et al., 2021) achieves 56.66 on MWoZ2.1, higher than our sequence output scheme. For WebQSP, as the original dataset does not have a dev. set, we split the original train set into in-house train/dev. sets (90%/10%), following prior practice (e.g. <ref type="bibr">Ren et al. (2021)</ref>). Similarly, for CompWebQ, as the test set is not publicly available, we split the original dev. set into in-house dev./test sets (20%/80%). For GrailQA, we split the original dev. set into in-house dev./test sets (5%/95%). rameters while keeping the PLM fixed. We set the prefix length as 10. <ref type="bibr">Clive et al. (2021)</ref> also used prefix-tuning on T5 for data-to-text generation. Multi-task finetuning (MT-F), which combines the training data of all tasks with temperature mixing <ref type="bibr">(Raffel et al., 2020;</ref><ref type="bibr"></ref> after hyperparameter tuning with a few steps, we set the temperature as 2). We select model weights based on the average metric on all tasks' development set. <ref type="table">Table 4</ref> shows that ST-P is comparable to ST-F on nearly all tasks. However, we find that it takes about 5-10 times as many training steps (See Appendix E), which is similarly observed for prompttuning <ref type="bibr">(Lester et al., 2021)</ref>. We also observe that MT-F leads to mixed results. For many tasks, MT-F is even worse than ST-F. Multi-task prefix-tuning (MT-P) Our explanation for the mixed results of MT-F is that the inputs of SKG tasks contain different structured knowledge from diverse domains, making it difficult to learn shared parameters effectively. To address this challenge, we first pretrain a prefix on all tasks, freezing T5 and using the same temperature mixing as MT-F. In the second step, we initialize each task's prefix with this pretrained prefix and optimize the prefix while freezing T5. This initialization step is similar to the prompt transfer explored in <ref type="bibr">Vu et al. (2021)</ref>. Following ST-P, we set the prefix length as 10. <ref type="table">Table 4</ref> shows that multi-task prefix-tuning outperforms single-task finetuning and single-task prefix-tuning on most tasks, and it largely outperforms the naive multi-task learning baseline. It demonstrates that SKG tasks can be studied together to share data and knowledge. Exploring task knowledge transfer UNI-FIEDSKG facilitates studying knowledge transfer between SKG tasks. Given two tasks, task A and task B, we first train the model on task A and then continue training on task B. <ref type="table">Table 5</ref> shows that tasks benefit from other tasks with the same data source (e.g., tasks that all use Wikipedia tables as structured knowledge). We do not observe positive transfer between parallel tasks (e.g., semantic parsing tasks with different structured knowledge and different output) and subtask (e.g., question answering can be viewed as the execution semantic parses) when data sources are different. Compared to the positive results in <ref type="table">Table 4</ref>, results in this part indicate that manually selecting source and target tasks may not be efficient for multi-task learning.  <ref type="table">Table 4</ref>: Multi-task learning results. ST and MT stand for single-task and multi-task. F and P stand for finetuning and prefix-tuning. For total parameters, T and P are the numbers of T5 and prefix parameters (P T ). Multi-task learning with prefix improves the performance on most tasks, largely improving the overall performance. We report results on the dev. set.  <ref type="table">Table 5</ref>: Task knowledge transfer. We use T5-large here. B only means training the model on task B; A to B means to train the model on task A and then to finetune the model on task B. In both settings, we report task B's development set performance. We find that tasks benefit from other tasks with the same data source.</p><formula xml:id="formula_1">T5-base T5-large ST-F ST-P MT-F MT-P ST-F MT-P<label>Spider</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Zero-Shot and Few-Shot Learning</head><p>The text-to-text unification of UNIFIEDSKG enables us to investigate zero/few-shot learning on SKG with large PLMs. Zero-shot learning setting Zero-shot learning enables models to solve tasks with natural language descriptions without training samples. We follow T0 <ref type="bibr">(Sanh et al., 2021)</ref> to create similar natural language instructions for the unseen tasks. Our instructions are provided in Appendix D.3. Few-shot learning settings Brown et al. <ref type="bibr">(2020)</ref> showed that large PLMs could be few-shot learners  by encoding a few training samples as "context" to learn without gradient updates. We use <ref type="bibr">GPT-3 (Brown et al., 2020)</ref> and <ref type="bibr">Codex (Chen et al., 2021a)</ref> to explore such few-shot learning for SKG.</p><p>To stay within our budget, for GPT-3, we report the performance on 100 random dev. set samples. We explore two settings for few-shot learning.</p><p>In the first setting, we randomly sample few-shot examples from the training set; these examples are shared by all dev. set samples, denoted as random in <ref type="table" target="#tab_9">Table 6</ref>. For sequences that are too long for Codex (4096) and GPT-3 (2048), we use as many examples as possible and make sure that there is at least one example (truncated if needed).</p><p>In the second setting, we follow Gao et al. (2021) to select few-shot examples from the training set. We call this setting few-shot with example selection, denoted as select in <ref type="table" target="#tab_9">Table 6</ref>. We use the pretrained SBERT (Reimers and Gurevych, 2020) for sentence embeddings of the user request input (for tasks that only have structured input, we embed the linearized structured input) and sample five most similar examples measured by cosine similarity. Further details (e.g., prompts and task instructions) are provided in Appendix D.4. SKG is challenging for zero/few-shot learning. <ref type="table" target="#tab_9">Table 6</ref> shows that zero-shot performance is very poor on most tasks (Spider and MultiWoZ are even 0). It also shows a large gap between fewshot learning and finetuning for Spider, WikiTQ, MWoZ, and TabFact, while the gap is smaller for generation tasks. For few-shot learning, example selection based on similarity outperforms random selection, but the gap is usually smaller than 10 points out of 100. It is also interesting to compare the results between synthesis tasks (Spider), which requires predicting programs, and induction tasks   (WikiTQ and TabFact), where a model directly outputs answers <ref type="bibr">(Devlin et al., 2017)</ref>. We find that PLMs generally struggle more when adapting to induction tasks (e.g., close to random-guess on the binary classification task TabFact), reminiscent of recent attempts in program synthesis and induction using PLMs <ref type="bibr">(Austin et al., 2021)</ref>. For GPT-3 and Codex, better zero-shot performances can be expected by better prompt design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Structured Knowledge Encoding</head><p>Structured knowledge encoding has been widely explored <ref type="bibr">(Bogin et al., 2019;</ref><ref type="bibr">Lin et al., 2019;</ref><ref type="bibr" target="#b0">Agarwal et al., 2020;</ref><ref type="bibr">Saxena et al., 2020;</ref><ref type="bibr">Yasunaga and Liang, 2020;</ref><ref type="bibr">Yasunaga et al., 2022;</ref><ref type="bibr"></ref> and others detailed in Section 2). We hope that UNIFIEDSKG can promote systematic study of general structured knowledge encoding. To this end, this part focuses on the linearization of structured knowledge. Does the order of user input, structured knowledge, and context matter? To explore the effect of the order of user input, structured knowledge, and context, we rerun the single-task experiments while switching the order of these components in both the training and development set. <ref type="table" target="#tab_11">Table 7</ref> shows that placing the text before structured knowledge (rs) is better than the opposite (sr), which is consistent across SKG tasks. Our explanation is that the position of the text is relatively fixed in rs,  Is it beneficial to represent structured knowledge as natural language? SKG data is not typically used to pretrain PLMs. Given ample training data, PLMs adapt well to SKG tasks, as shown in <ref type="table" target="#tab_5">Table 2</ref>. However, under the low-resource setting, converting structured data to natural language might be helpful. For Spider, we use a shared template to convert structured data to natural language. For TabFact and WikiSQL, we randomly selected 236 tables shared by both datasets and manually labeled templates to convert each row into a sentence. Examples of the templates are shown in Appendix I. These templates produce about 1000 samples for each task, divided into training and test sets. We find that, in WikiSQL, the conversion to natural language stabilizes and accelerates the training process. <ref type="table" target="#tab_14">Table 9</ref> shows that conversion to natural language improves the performance on WikiSQL, has no significant influence on TabFact, and slightly degrades the performance on Spider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human Evaluation for Generation Tasks</head><p>For each generation task, we randomly sample 100 development set samples and ask human annotators to judge the correctness of each output, using a 0-1 score. Details are provided in Appendix D.5.  <ref type="table" target="#tab_2">Table 10</ref>: Automatic metrics and human evaluation on the development set of generation tasks. * p &lt; 0.05 for "the rank-1 model is better than the rank-2 model". ? p &lt; 0.05 for "the rank-2 model is better than the rank-3 model". Automatic metrics do not always reflect human evaluation. Larger models are not always better.</p><p>10 shows that automatic metrics do not always reflect human evaluation, calling for better automatic metrics to truly reflect the model's ability on generation tasks. Larger models are not always better, and detailed error analysis is provided below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Error Analysis</head><p>Error analysis based on output validity Unconstrained decoding from PLMs may generate invalid outputs. For semantic parsing, we divide wrong outputs into invalid outputs (i.e., not executable when the output is SQL, and not parse-able when the output is s-expression or TOP-representation) and valid but wrong answers. <ref type="figure">Figure 3</ref> shows that, for SQL semantic parsing, a large number of errors are caused by invalid outputs, and the number of invalid outputs gradually decreases with the increase of model size. This phenomenon is also observed by Scholak et al. <ref type="formula" target="#formula_2">(2021)</ref>, who used constrained decoding to improve the validity, largely improving the parsing performance. For s-expression semantic parsing, invalid outputs take up 30-50% of all wrong outputs, and increasing the model size does not reduce invalidity significantly. For fact verification tasks, valid outputs are "entailed" and "refuted". We observe that T5 always generates valid outputs. For question answering, we do not include the validity analysis since the validity check for an answer is non-trivial and could be imprecise.</p><p>Error analysis for text generation tasks For generation tasks, we consider four types of errors: missing information (required information is not Contra.</p><p>Halluc.</p><p>Ungram.</p><p>(d) ToTTo <ref type="figure">Figure 3</ref>: Error analysis. For semantic parsing, we plot the number of invalid/valid-but-wrong predictions.</p><p>For generation, we plot the proportion of missinginformation/contradiction/hallucination/ungrammatical errors among all predictions (one prediction may have multiple errors). Full visualization is in Appendix B.</p><p>shown in the output), contradiction (the output is contradictory to the input), 3) hallucination (the output contains information that cannot be verified by the input), and 4) ungrammatical. <ref type="figure">Figure 3</ref> shows that the proportion of ungrammatical outputs is generally less than 5%. Missing information and contradiction are common errors made by T5, and performance gains generally come from reducing contradiction. Hallucination is not a common error made by T5 except for the highlighted-table-to-text task (ToTTo), where T5 tends to output information of non-highlighted cell values.</p><p>Case study We summarize some interesting observations about the model output (more in Appendix H). Compared with T5-base and T5-large, T5-3B's outputs for text generation tasks tend to be more diverse and creative as shown in Appendix H.2 and H.7. Also, T5-3B sometimes leverages domain knowledge to summarize facts in some tasks such as DART (e.g., describing rating 5 out of 5 as low), while the other two copy the original expressions in the input, as shown in Appendix H.5 and H.6. However, this ability puts T5-3B in the risk of manipulating information and meaning of user request as shown in Appendix H.3.2 and H.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose the UNIFIEDSKG framework to promote systematic research on struc-tured knowledge grounding by unifying 21 SKG tasks. Using UNIFIEDSKG as a benchmark, we demonstrate that finetuning T5 on individual tasks achieves state-of-the-art results on almost all 21 tasks. We show that multi-task prefix-tuning benefits most SKG tasks, largely improving the overall performance. For structured knowledge encoding, we find that the effectiveness of encoding variations varies across tasks. Moreover, UNIFIEDSKG is a challenging testbed for zero-shot and few-shot learning, shown by the poor results of large PLMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitations</head><p>UNIFIEDSKG establishes a powerful and reproducible starting point for SKG research. New models can be easily applied to diverse SKG tasks, and new tasks can be easily framed based on our standardized abstraction. UNIFIEDSKG promotes a systematic study on more general and robust advances in structured knowledge encoding, multitask learning, zero-shot learning, and few-shot learning for SKG tasks. It also would be interesting to explore general pretraining methods within UNIFIEDSKG, which potentially benefit all the unified tasks. When the structured knowledge is too large for GPU memory, we truncate them based on heuristic rules, calling for future study on 1) incorporating retrieval component in SKG, 2) designing sparse attention in T5 for structured knowledge or other means to improve model efficiency.</p><p>UNIFIEDSKG currently provides the correct type of structured knowledge for each task. However, how a system searches for the correct structured knowledge resources, takes appropriate action, and integrates information and results from multiple structured sources given a user request is still underexplored, which are a prerequisite for building a unified multi-purpose SKG system.</p><p>Since we select popular tasks from each task family, we risk disproportionality in terms of the data language, domain and population, and we actively welcome diverse, multi-lingual tasks to be added into UNIFIEDSKG. Also, the error analysis of SKG can more fine-grained, and we hope our findings can promote future work on systematically studying and decomposing the behavior of PLMs on SKG tasks. Furthermore, training and evaluation data should reflect the intents and linguistic phenomena in the real world <ref type="bibr">(de Vries et al., 2020)</ref>, suggesting more realistic tasks to be added into UNIFIEDSKG.  <ref type="table" target="#tab_2">Table 11</ref>: Development set performance with full metrics. We do three experiments with different random seeds on representative task of each family and report their averages and standard variances format as avr var .</p><p>For the KVRET dataset, instead of the version used in our main tables, we re-run another more widely used pre-processed version <ref type="bibr">(Madotto et al., 2018;</ref><ref type="bibr">Wu et al., 2019;</ref><ref type="bibr">Qin et al., 2020)</ref> on T5-base, T5-large and T5-3b. Results are shown in <ref type="table" target="#tab_2">Table 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Input and Output Length Analysis</head><p>Linearization of large structured knowledge input (e.g., large tables and KGs) can be arbitrarily long, which needs to be truncated to fit in GPUs with a  <ref type="table" target="#tab_2">Table 12</ref>: Test set performance with full metrics (for tasks with a publicly available test set). We do three experiments with different random seeds on representative task of each family and report their averages and standard variances format as avr var .</p><p>limited size. The input and output are tokenized by T5Tokenizer in Huggingface's Transformers. <ref type="bibr">3</ref> We visualize the length distribution in <ref type="figure">Figure 5</ref>, and details are presented in <ref type="table" target="#tab_2">Table 14</ref>. Among the datasets with very long inputs, we choose Wik-iTableQuestion to study the impact of input length. We visualize the table length distribution and performances with different input truncation lengths in <ref type="figure">Figure 6</ref>. We observe that the accuracy increases as the input becomes longer, motivating future work to study how to effectively encode large structured input, e.g., leveraging sparse attention <ref type="bibr">(Zaheer et al., 2020)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contra.</head><p>Halluc.</p><p>Ungram.</p><p>(l) Logic2Text <ref type="figure">Figure 4</ref>: Error analysis. For semantic parsing, we show the number of invalid/valid-but-wrong predictions.</p><p>For generation tasks, we show the proportion of missing-information/contradiction/hallucination/ungrammatical predictions among all predictions (one prediction may have multiple errors).   <ref type="figure">Figure 5</ref>: Input token distribution(&lt;4096) in train set from different tasks. We exclude MTOP since it concentrates on a relatively small field which would make this figure unreadable. In general, 1024 is a good length for practice, and for most tasks, 2048 can hold all its inputs.</p><p>WikiSQL, and TabFact, for which for use batch size 128 because we found it to work significantly better. We use the Adafactor optimizer for T5-base and T5-large, and AdamW for T5-3b. We evaluate on the development set for each 500 steps and use the average development set metric for best check-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Metric Details</head><p>For most semantic parsing tasks, we report the exact match accuracy of logical forms, and for task has test suite <ref type="bibr" target="#b3">(Zhong et al., 2020)</ref>, we add test suite metric to represent model's performance; an exception is WebQSP, for which we follow previous work to execute the parses and report the F1 score. For QA, we report the exact match accuracy of answer sets. For data-to-text generation, we re-port sacre-BLEU (Post, 2018). <ref type="bibr">5</ref> We use each task's representative metric used by previous works. For fact verification, we report the accuracy. For highfidelity NLG, we report BLEC <ref type="figure">(Shu et al., 2021)</ref>, which is the exact match between keywords in the formal language and the natural language. Unless specified, we use T5-large and report the development set performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 T0 Zero-shot Experimental Details</head><p>For each task in UNIFIEDSKG we search Sanh et al.   Temperature For GPT3 and Codex, we set the decoding temperature to 0 (i.e., greedy decoding without sampling) for Spider, WikiTQ, MultiWoZ and TabFact. We observe a drop of 10% in the exact match metric when set the temperature to 1 by default in OpenAI. For Codex, we tune the temperature from 0 to 1 in a step of 0.1 for DART, SQL2Text, and no significant difference is observed. For GPT3, we do not tune on that to stay within our budget.</p><formula xml:id="formula_3">- - - - - - - - - - - CoSQL - - - - - - - - - - - - SQA</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max output length</head><p>We set max output length to 256 for Spider, WikiTQ, MultiWoZ and SQL2Text, while 4 for TabFact to contain more length in the input side(the concept of max length in GPT3 and Codex is the sum of input tokens length and output tokens length). We set "\n" as the stop token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4.2 Prompts</head><p>We use simple prompt words for each task to concatenate the request, linearized structured knowledge, and context together. For example, for each example in WikiTQ, we format it as "ex-amples\n\n[linearized table] || Write a answer for [request] \nThe answer is:", and make GPT3 and Codex make the completion as prediction. We do experiments on Spider with different format of forming structured knowledge (e.g., linearization, description), but get a similar result. Better us-age of GPT3 and Codex under the UNIFIEDSKG framework is an interesting direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Human Evaluation</head><p>Participants of our human evaluation are eight of the authors of this paper. They are familiar with the tasks being evaluated. The human evaluation guideline is shown below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Hyperparameters</head><p>Shown in <ref type="table" target="#tab_2">Table 17</ref>. For semantic parsing tasks, the decoding was done under the greedy search, where we set the beam size to 1 specially. For tasks with a long linearized sequence, we used 1024 as input length to hold the maximum of input; reasons are explained in App. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Training Details</head><p>Here we show comparisons of finetuning and prefix-tuning on aspect of training. For prefixtuning, we use random initialization as done by <ref type="bibr">Li and Liang (2021)</ref>. In general, prefix-tuning needs more steps than finetuning but has the ability to reach comparable results with continued training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Task Unification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Term Definition</head><p>Highlighted <ref type="table">tables A highlighted table contains  a table, table metadata (such as the title)</ref>, and a set of highlighted cells which entails the text description <ref type="bibr">(Parikh et al., 2020)</ref>.</p><p>Relation-triples Relation triples are a set of subject-predicate-object triples to capture rich relationships in the data. Many data-to-text tasks such as DART (Nan et al., 2021b) take these relation triples as inputs and generate natural language from them.</p><p>Knowledge Graph A knowledge graph is a multi-relational graph composed of entities (nodes) and relations (different types of edges). Each edge is represented as a triple of the form (head entity, relation, tail entity), also called a fact, indicating that two entities are connected by a specific relation <ref type="bibr">(Wang et al., 2017)</ref>.</p><p>Dialogue State and Ontology A dialogue state s t at any turn t in a dialogue comprises the summary of the dialogue history until turn t, such that s t contains all sufficient information for the system to choose the next action. <ref type="bibr">(Williams et al., 2016)</ref> Specifically, it captures the user goals in the conversation in the form of (slot, value) pairs. The set of possible slots is predefined in the ontology O, typically domain-dependent, while the values assumed by each slots are provided by the user as a dialogue goal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Request Input:</head><p>i am looking to book a train that is leaving from cambridge to bishops stortford on friday .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context:</head><p>booking was successful . reference number is : bmukptg6 . can i help you with anything else today ? | friday and can you book it for me and get a reference number ? | what day will you be staying ? | could you book the wartworth for 1 night , 1 person ? | if you would like something cheap , i recommend the allenbell . for something moderate -ly priced , i would recommend the warkworth house . | that does not matter as long as it has free wifi and parking . | i can help you with that . what is your price range ? | i need to book a hotel in the east that has 4 stars . | none Sequence Output: hotel pricerange none, hotel type none, hotel parking yes, hotel book day friday, hotel book people 1, hotel book stay 1, hotel area east, hotel stars 4, hotel internet yes, hotel name wartworth, train destination bishops stortford, train day friday, train departure cambridge, train arriveby none, train book people none, taxi destination none, taxi departure none, taxi leaveat none, train leaveat none, attraction area none, restaurant pricerange none, restaurant area none, restaurant food none, attraction name none, restaurant name none, attraction type none, restaurant book day none, restaurant book people none, restaurant book time none, taxi arriveby none when is the date and time of my dentist_appointment?</p><p>Context Input: None Sequence Output:</p><p>your dentist_appointment is at 10am on the_13th.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (correct):</head><p>you have two dentist_appointments scheduled. one on the_5th at 11am and another on the_13th at 10am.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (correct):</head><p>your dentist_appointment is on the_13th at 10 am.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-3B prediction (incorrect):</head><p>you have three dentist_appointments scheduled. one on the_5th at 11am, another on the_13th at 10am, and the last is on the_5th at 11am.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.4 SQL2Text case</head><p>Query:</p><p>SELECT company , main_industry FROM company WHERE headquarters != USA Sequence Out:</p><p>Show the company name and the main industry for all companies whose headquarters are not from USA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (incorrect):</head><p>What are the companies and main industries of companies whose headquarters are not "USA"?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (correct):</head><p>What are the companies and main industries of the companies whose headquarters are not located in the United States?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-3B prediction (incorrect):</head><p>What are the main industries of companies whose headquarters are not in the USA? <ref type="figure">Figure 7</ref>: Visualized highted table for ToTTo case 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (incorrect):</head><p>Alisson Perticheto placed 17th at the 2014 Four Continents and 16th at the 2015 Junior Worlds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-3B prediction (correct):</head><p>Alisson Perticheto finished 17th at the 2014 Four Continents, 16th at the 2015 Four Continents, and 18th at the 2013 Junior Worlds.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table +</head><label>+</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Text Passage</cell><cell>Question</cell><cell>Answer</cell></row><row><cell></cell><cell>MultiModalQA (Talmor et al., 2021)</cell><cell>Table + Text + Image</cell><cell>Question</cell><cell>Answer</cell></row><row><cell></cell><cell>FeTaQA (Nan et al., 2021a)</cell><cell>Table</cell><cell>Question</cell><cell>Free-Form Answer</cell></row><row><cell>Data-to-Text</cell><cell>DART (Nan et al., 2021b) ToTTo (Parikh et al., 2020)</cell><cell>Triple Highlighted Table</cell><cell>None None</cell><cell>Text Text</cell></row><row><cell></cell><cell>MultiWoZ (Budzianowski et al., 2018)</cell><cell>Ontology</cell><cell>Dialog</cell><cell>Dialog State</cell></row><row><cell></cell><cell>KVRET (Eric et al., 2017)</cell><cell>Table</cell><cell>Dialog</cell><cell>Response</cell></row><row><cell>Conversational</cell><cell>SParC (Yu et al., 2019b)</cell><cell>Database</cell><cell>Multi turn</cell><cell>SQL</cell></row><row><cell></cell><cell>CoSQL (Yu et al., 2019a)</cell><cell>Database</cell><cell>Dialog</cell><cell>SQL</cell></row><row><cell></cell><cell>SQA (Iyyer et al., 2017)</cell><cell>Table</cell><cell>Multi turn</cell><cell>Answer</cell></row><row><cell>Fact Verification</cell><cell>TabFact (Chen et al., 2020b) FEVEROUS (Aly et al., 2021)</cell><cell>Table Table + Text</cell><cell>Statement Statement</cell><cell>Boolean Boolean</cell></row><row><cell>Formal-Language-to-Text</cell><cell>SQL2Text (Shu et al., 2021) Logic2Text (Chen et al., 2020d)</cell><cell>Optional Database Table Schema</cell><cell>SQL Python-like program</cell><cell>Text Text</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>We unify 21 SKG tasks with different knowledge input, user input, and output, covering six task families.</figDesc><table><row><cell>SKG's</cell></row></table><note>the attention matrix of transformers (Zhang et al., 2020; Eisenschlos et al., 2021). Hierarchical encod- ing is another way to represent the structure, e.g., Wang et al. (2021b) used tree-based transformers to represent the structure of the tables; Iida et al. (2021) used transformers to encode row and col- umn representations; Chen et al. (2021b) used hier- archical transformers to encode KG triples.as textual entailment. PLUR (Chen et al., 2021c)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Test or development (dev.) set performance of models trained on individual tasks. Vanilla T5 or T5 with</cell></row><row><cell cols="3">simple modifications (e.g., + constrained decoding or reranking) achieve sota on nearly all tasks. The best result</cell></row><row><cell cols="3">without extra pretraining is shown in bold. More detailed results and result variances can be found in Tables 11</cell></row><row><cell cols="3">and 12 in Appendix. Human evaluation for generation tasks is in Section 4.5. w/ (w/o) extra means with (without)</cell></row><row><cell cols="3">extra pretraining on unsupervised structured data (e.g., web tables). 2</cell></row><row><cell cols="3">Spider WikiTQ DART MWoZ TabFact SQL2Text</cell></row><row><cell>T5-3B 71.76 50.65 50.38 58.46</cell><cell>83.97</cell><cell>92.71</cell></row><row><cell>T0-3B 68.09 50.62 50.16 60.20</cell><cell>85.51</cell><cell>92.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>18.333.78 40.72 43.234.16 WikiTQ 50.65 12.68 32.00 29.339.04 26.21 20.464.21 DART 50.38 23.42 40.23 34.214.50 42.13 36.541.67 49.673.79 50.97 51.581.59 SQL2Text 92.71 39.64 94.00 85.002.65 90.64 88.311.61</figDesc><table><row><cell>T5-3B</cell><cell>T0 3B</cell><cell cols="2">GPT-3 175B</cell><cell>Codex 175B</cell></row><row><cell cols="3">finetune zero-shot select</cell><cell>random</cell><cell>select</cell><cell>random</cell></row><row><cell cols="3">Spider 20.00 MWoZ 71.76 0.00 58.46 0.00 18.00</cell><cell cols="2">0.020.02 23.47</cell><cell>0.060.03</cell></row><row><cell>TabFact 83.97</cell><cell>52.45</cell><cell>51.00</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Zero-shot and few-shot learning for SKG. Subscripts show the standard deviation with three runs. select means to select the most similar training samples as few-shot examples, while random means to randomly select training samples as few-shot examples. T0 performs poorly on all the tasks in the zero-shot setting.</figDesc><table /><note>Codex outperforms GPT-3 on tasks that generate struc- tured programs (Spider and MultiWoZ).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ordering of inputs. Subscripts show the standard deviation with three runs. s, r, and c stand for the structured knowledge, request input, and context. Placing r before s is always better, and placing c between r and s is better for dialogue state tracking (Mul-tiWoZ2.1).</figDesc><table><row><cell></cell><cell>Spider</cell><cell>WikiTQ</cell><cell>DART</cell><cell>MultiWoZ2.1</cell></row><row><cell>Same Order</cell><cell>66.632.31</cell><cell>43.300.25</cell><cell>51.720.15</cell><cell>58.230.39</cell></row><row><cell>Reversed Order</cell><cell>64.80</cell><cell>37.80</cell><cell>48.47</cell><cell>13.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Order-sensitivity of structured knowledge. Subscripts show the standard deviation with three runs.</figDesc><table /><note>Same Order is the default benchmark setting. Re- versed Order means to reverse the structured knowl- edge ordering on the development set (but not the train- ing set). Tasks with cross-domain tables (in WikiTQ), databases (in Spider), and triples (in DART) are less order-sensitive, while pre-defined ontology (in Multi- WoZ2.1) is highly order-sensitive.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Converting structured knowledge into natural language for low-resource learning. A large improve-Table 8shows that tasks with cross-domain tables and databases are less order-sensitive, while models are very sensitive to the order of ontology. Other types of robustness (e.g., robustness to cell values irrelevant to the answer) remain an open question in UNIFIEDSKG.</figDesc><table><row><cell>ment is observed on question answering (WikiSQL),</cell></row><row><cell>but not on text2SQL semantic parsing (Spider) and fact</cell></row><row><cell>verification (TabFact).</cell></row><row><cell>helping the decoder to learn stable attention over</cell></row><row><cell>the text. Also, placing the context in between the</cell></row><row><cell>text and structured knowledge yields better results.</cell></row><row><cell>Is T5 sensitive to structured knowledge order-</cell></row><row><cell>ing? Order-insensitivity is common for most struc-</cell></row><row><cell>tured knowledge, e.g., permutation of columns in</cell></row><row><cell>a table preserves the meaning. To study this in-</cell></row><row><cell>sensitivity, we evaluate T5-large on a manipulated</cell></row><row><cell>development set where the order of schema (for</cell></row><row><cell>database), column (for table), or slots and values</cell></row><row><cell>(for ontology) is reversed.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table Metric</head><label>Metric</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">T5-base T5-large T5-3B</cell></row><row><cell>FeTaQA</cell><cell cols="2">BLEU Human  *  ? 36.0% 29.00</cell><cell>30.94 51.3%</cell><cell>31.73 57.3%</cell></row><row><cell>DART</cell><cell>BLEU Human</cell><cell>50.62 90.7%</cell><cell>51.72 91.7%</cell><cell>50.38 87.7%</cell></row><row><cell>ToTTo</cell><cell>BLEU Human</cell><cell>48.29 78.7%</cell><cell>48.95 80.0%</cell><cell>48.95 81.3%</cell></row><row><cell>KVRET</cell><cell>BLEU Human  ?</cell><cell>20.04 72.3%</cell><cell>18.84 66.3%</cell><cell>17.75 75.0%</cell></row><row><cell>SQL2Text</cell><cell>BLEC Human  *</cell><cell>93.69 83.7%</cell><cell>93.35 90.3%</cell><cell>92.71 84.7%</cell></row><row><cell>Logic2Text</cell><cell>BLEC Human  ?</cell><cell>92.15 77.2%</cell><cell>92.88 81.5%</cell><cell>91.69 84.2%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Zimin Chen, Vincent Josua Hellendoorn, Pascal Lamblin, Petros Maniatis, Pierre-Antoine Manzagol, Daniel Tarlow, and Subhodeep Moitra. 2021c. PLUR: A unifying, graph-based view of program learning, understanding, and repair. In Thirty-Fifth Conference on Neural Information Processing Systems. In Proceedings of the Web Conference 2021. Jonathan Herzig, P. Nowak, Thomas M?ller, Francesco Piccinno, and Julian Martin Eisenschlos. 2020. Tapas: Weakly supervised table parsing via pretraining. In Proceedings of ACL. Renusree Bandaru, Jacob Cunningham, Caiming Xiong, and Dragomir Radev. 2021a. Fetaqa: Freeform table question answering. TACL. Talmor and J. Berant. 2018. The web as a knowledge-base for answering complex questions. In North American Association for Computational Linguistics (NAACL). Sheng Wu, Richard Socher, and Caiming Xiong. 2019. Global-to-local memory pointer networks for task-oriented dialogue. In Proceedings of the International Conference on Learning Representations (ICLR). Tianbao Xie and Chen Henry Wu implemented the code base of the UNI-FIEDSKG framework and experiment pipeline. The code of PICARD and advice from Torsten Scholak sped up the implementation. 1.46 66.63 2.31 71.76 Exec 60.06 0.54 68.28 1.61 74.37 Test suite 56.22 0.73 64.12 1.28 68.38</figDesc><table><row><cell>Jordan Clive, Kris Cao, and Marek Rei. 2021. Control prefixes for parameter-efficient text generation. Yinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, and Xiaodan Zhu. 2021. Preview, attend and review: Schema-aware curriculum learn-ing for multi-domain dialogue state tracking. In Pro-ceedings ACL-IJCNLP 2021 (Volume 2: Short Pa-pers), pages 879-885, Online. Sylvain Gugger, Cl?ment Delangue, Th?o Matus-si?re, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, Fran?ois Lagu-nas, Alexander M. Rush, and Thomas Wolf. 2021. Datasets: A community library for natural language processing. Haoran Li, Abhinav Arora, Shuohui Chen, Anchit Gupta, Sonal Gupta, and Yashar Mehdad. 2021. MTOP: A comprehensive multilingual task-oriented semantic parsing benchmark. In Proceedings of the 16th Conference of the European Chapter of the As-sociation for Computational Linguistics: Main Vol-ume, pages 2950-2962, Online. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan-guage Processing (Volume 1: Long Papers), pages 4582-4597, Online. Bill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. Kagnet: Knowledge-aware graph net-works for commonsense reasoning. In Proceedings of EMNLP-IJCNLP. Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul A Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, and Rajen Subba. 2021. Leveraging slot descriptions for zero-shot cross-domain dialogue statetracking. In Proceed-ings of NAACL 2021, pages 5640-5648. Qian Liu, Bei Chen, Jiaqi Guo, Zeqi Lin, and Jian guang Lou. 2021. Tapex: Table pre-training via learning a neural sql executor. Weijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju, Haotang Deng, and Ping Wang. 2020. K-bert: Enabling language representation with knowledge graph. In AAAI. Andrea Madotto, Chien-Sheng Wu, and Pascale Fung. 2018. Mem2seq: Effectively incorporating knowl-edge bases into end-to-end task-oriented dialog sys-tems. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 1468-1478. Alana Marzoev, Samuel Madden, M Frans Kaashoek, Michael Cafarella, and Jacob Andreas. 2020. Unnat-ural language processing: Bridging the gap between synthetic and natural language data. arXiv preprint arXiv:2004.13645. Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2018. The natural language de-cathlon: Multitask learning as question answering. CoRR, abs/1806.08730. Nils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual us-ing knowledge distillation. In Proceedings of EMNLP 2020. Hongyu Ren, Hanjun Dai, Bo Dai, Xinyun Chen, Michihiro Yasunaga, Haitian Sun, Dale Schuurmans, Jure Leskovec, and Denny Zhou. 2021. Lego: La-tent execution-guided reasoning for multi-hop ques-tion answering on knowledge graphs. In Interna-tional Conference on Machine Learning (ICML). Ohad Rubin and Jonathan Berant. 2021. SmBoP: Semi-autoregressive bottom-up semantic parsing. In Pro-ceedings of NAACL 2021, pages 311-324, Online. Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Ab-heesht Sharma, Andrea Santilli, Thibault Fevry, Ja-son Alan Fries, Ryan Teehan, Stella Biderman, Leo Gao, Tali Bers, Thomas Wolf, and Alexander M. Rush. 2021. Multitask prompted training enables zero-shot task generalization. A Contributions B Results with Full Metrics Chien-Xiaoyu Yang, Feng Nie, Yufei Feng, Quan Liu, Zhi-gang Chen, and Xiaodan Zhu. 2020. Program en-hanced fact verification with verbalization and graph attention network. In Proceedings of EMNLP 2020, pages 7810-7825, Online. Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren, Xikun Zhang, Christopher D. Manning, Percy Liang, and Jure Leskovec. 2022. Deep bidirectional language-knowledge graph pretraining. In Neural Information Processing Systems (NeurIPS). Michihiro Yasunaga and Percy Liang. 2020. Graph-based, self-supervised program repair from diagnos-tic feedback. In International Conference on Ma-chine Learning (ICML). Metric T5-base T5-large T5-3B Spider Match 58.12 GrailQA Match 60.00 67.00 69.00 Code implementation Task unification Tianbao Xie, Peng Shi, Michi-WebQSP F1 72.50 73.96 75.97 hiro Yasunaga, Chen Henry Wu, and Ming Zhong implemented the 21 tasks into the text-to-text for-Match 83.89 84.70 84.88 MTOP Template 88.85 88.32 88.86 mat, adapted the metrics, and verified the perfor-WikiTQ Acc 36.94 0.19 43.30 0.25 50.65 mances. WikiSQL Acc 84.50 86.27 87.34 Paper writing Chen Henry Wu and Tianbao Xie finished most part of the paper. Michihiro Acc 66.71 68.85 70.27 CompWebQ F1 80.02 81.05 81.43 Hits@1 83.64 85.49 86.20 Yasunaga, Peng Shi, and Chengzu Li added re-sults and analysis for their corresponding parts. Acc 54.07 56.95 59.41 HybridQA F1 61.85 64.62 66.76 Peng Shi drafted related work on SKG with PLMs. Torsten Scholak, Pengcheng Yin, Rui Zhang, Ruiqi Acc 67.29 74.08 78.48 MMQA F1 75.51 81.84 82.28 Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Zhong, Victor Zhong, Michihiro Yasunaga, Connor FeTaQA BLEU 29.00 30.94 31.73 Percy Liang, and Jure Leskovec. 2021. QA-GNN: Reasoning with language models and knowledge graphs for question answering. In North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies (NAACL). Association for Computational Linguistics. Qinyuan Ye, Bill Yuchen Lin, and Xiang Ren. 2021a. Crossfit: A few-shot learning challenge for cross-Boyle, Chien-Sheng Wu, Sida Wang, Bailin Wang, DART BLEU 50.62 0.72 51.72 0.15 50.38 Ansong Ni, Ziyu Yao, Lingpeng Kong, Caiming ToTTo BLEU 48.29 48.95 48.95 Xiong, Dragomir Radev, Noah A. Smith, and Luke MultiWoZ2.1 Joint Acc 57.52 0.96 58.23 0.39 58.46 Zettlemoyer carefully reviewed the paper and gave KVRET BLEU 20.04 18.84 17.75 feedback for multiple rounds. Experiments Match 50.54 56.69 61.51 Chen Henry Wu, Tianbao Xie, and Chien-Sheng Wu conducted experiments on Exec 53.95 60.60 67.33 SParC Match (interact) 31.28 37.44 41.94 Apoorv Saxena, Aditay Tripathi, and Partha Taluk-dar. 2020. Improving multi-hop question answer-ing over knowledge graphs using knowledge base embeddings. In Association for Computational Lin-guistics (ACL). Torsten Scholak, Nathan Schucher, and Dzmitry Bah-danau. 2021. PICARD: Parsing incrementally for constrained auto-regressive decoding from language models. In Proceedings of EMNLP 2021, pages 9895-9901. Peter Shaw, Ming-Wei Chang, Panupong Pasupat, and Kristina Toutanova. 2021. Compositional general-ization and natural language variation: Can a seman-tic parsing approach handle both? In ACL/IJCNLP. Richard Shin, Christopher H Lin, Sam Thomson, Charles Chen, Subhro Roy, Emmanouil Antonios Platanios, Adam Pauls, Dan Klein, Jason Eisner, and Benjamin Van Durme. 2021. Constrained language models yield few-shot semantic parsers. arXiv preprint arXiv:2104.08768. Chang Shu, Yusen Zhang, Xiangyu Dong, Peng Shi, Tao Yu, and Rui Zhang. 2021. Logic-consistency text generation from semantic parses. In Findings of Radev, Richard Socher, and Caiming Xiong. 2020b. Universal natural language processing with limited annotations: Try few-shot textual entailment as a start. In Proceedings of EMNLP 2020, Online, vice. Torsten Scholak provided signals that prefix-Wang, and Lingpeng Kong actively provided ad-Yin, Victor Zhong, Peng Shi, Rui Zhang, Sida IJCNLP 2021. Wenpeng Yin, Nazneen Fatema Rajani, Dragomir R. them. Torsten Scholak, Ruiqi Zhong, Pengcheng the Association for Computational Linguistics: ACL-task generalization in nlp. In Proceedings of Exec (interact) 34.36 41.23 46.45 individual tasks and multi-task learning. Tian-EMNLP. Xi Ye, Semih Yavuz, Kazuma Hashimoto, Yingbo Zhou, and Caiming Xiong. 2021b. Rng-kbqa: Generation augmented iterative ranking for knowl-edge base question answering. arXiv preprint Match 42.30 48.26 54.08 bao conducted the zero-shot learning experiments. Chengzu Li and Tianbao Xie conducted the few-Exec 49.26 56.01 62.23 CoSQL Match (interact) 12.63 16.72 22.78 shot learning experiments. Tianbao Xie conducted Exec (interact) 16.04 20.14 26.16 experiments on the ordering of sequence inputs and SQA Overall Acc 49.49 59.12 60.93 arXiv:2109.08678. Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of se-order-sensitivity. Chengzu Li, Connor Boyle, and TabFact Acc 76.34 0.36 81.40 0.16 83.97 Peng Shi conducted the experiments on converting FEVEROUS Acc 75.05 79.81 82.40 structured knowledge into natural language. SQL2Text BLEC 93.69 0.29 93.35 0.29 92.71 mantic parse labeling for knowledge base question (Volume 2: Short Papers). Pengcheng Yin and Graham Neubig. 2018. TRANX: 2018: System Demonstrations, pages 7-12. Pengcheng Yin, Graham Neubig, Wen-tau Yih, and Proceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics. Association for Computational Linguistics. ings, and everyone in the project attended one of Discussion We had three separate weekly meet-study. joint understanding of textual and tabular data. In in the human annotation selected the cases for case Sebastian Riedel. 2020a. TaBERT: Pretraining for ing and generation tasks. Authors who participated and conducted the error analysis for semantic pars-Chen Henry Wu, and Michihiro Yasunaga designed semantic parsing and code generation. In EMNLP Error analysis and case study Tianbao Xie, A transition-based neural abstract syntax parser for participants. Shi, Tao Yu, and Chen Henry Wu were the human Chengzu Li, Connor Boyle, Tianbao Xie, Peng ing of the Association for Computational Linguistics the human evaluation. Torsten Scholak, Rui Zhang, answering. In Proceedings of the 54th Annual Meet-Human evaluation Chen Henry Wu organized Logic2Text BLEC 92.15 92.88 91.69</cell><cell>Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. In Pro-ceedings of Conference on Neural Information Pro-cessing Systems (NeurIPS). Binyuan Hui, Ruiying Geng, Qiyu Ren, Binhua Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, Pengfei Zhu, and Xiaodan Zhu. 2021. Dynamic hybrid re-lation network for cross-domain context-dependent semantic parsing. -tional Linguistics (Volume 1: Long Papers), pages 1821-1831, Vancouver, Canada. Mihir Kale and Abhinav Rastogi. 2020. Text-to-text pre-training for data-to-text tasks. In Proceedings of INLG 2020, Dublin, Ireland, December 15-18, 2020, pages 97-102. D. Khashabi, S. Min, T. Khot, A. Sabhwaral, O. Tafjord, P. Clark, and H. Hajishirzi. 2020. Uni-fiedqa: Crossing format boundaries with a single qa system. EMNLP -findings. Chia-Hsuan Lee, Hao Cheng, and Mari Ostendorf. 2021. Dialogue state tracking with a language model using schema-driven prompting. In Proceed-ings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4937-4949. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 3045-3059. Linyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-gru Tang, Aadit Vyas, Neha Verma, Pranav Kr-ishna, Yangxiaokang Liu, Nadia Irwanto, Jessica Pan, Faiaz Rahman, Ahmad Zaidi, Murori Mutuma, Yasin Tarabar, Ankit Gupta, Tao Yu, Yi Chern Tan, Xi Victoria Lin, Caiming Xiong, Richard Socher, and Nazneen Fatema Rajani. 2021b. Dart: Open-domain structured data record to text generation. In NAACL. Jekaterina Novikova, Ondrej Dusek, and Verena Rieser. 2017. The E2E dataset: New challenges for end-to-end generation. In SIGDial 2017, pages 201-206. Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Peshterliev, Dmytro Okhonko, Michael Schlichtkrull, Sonal Gupta, Yashar Mehdad, and Scott Yih. 2021. Unik-qa: Unified representations of structured and unstructured knowledge for open-domain question answering. arXiv preprint arXiv:2012.14610. Panupong Pasupat and Percy Liang. 2015. Composi-tional semantic parsing on semi-structured tables. In Proceedings of the 53rd Annual Meeting of the Asso-ciation for Computational Linguistics and the 7th In-ternational Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1470-1480, Beijing, China. Panupong Pasupat, Yuan Zhang, and Kelvin Guu. 2021. Controllable semantic parsing via retrieval augmen-tation. In Proceedings of EMNLP 2021, pages 7683-7698, Online and Punta Cana, Dominican Republic. Matt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186-191, Belgium, Brussels. Libo Qin, Xiao Xu, Wanxiang Che, Yue Zhang, and Ting Liu. 2020. Dynamic fusion network for multi-domain end-to-end task-oriented dialog. In Pro-ceedings of the 58th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 6344-6354, Online. Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Han-naneh Hajishirzi, and Jonathan Berant. 2021. Mul-timodal{qa}: complex question answering over text, tables and images. In International Conference on Learning Representations. Tu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and Daniel Cer. 2021. Spot: Better frozen model adaptation through soft prompt transfer. Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr Polozov, and Matthew Richardson. 2020. Rat-sql: Relation-aware schema encoding and linking for text-to-sql parsers. In ACL. Bailin Wang, Ivan Titov, and Mirella Lapata. 2019. Learning semantic parsers from denotations with la-tent structured alignments and abstract programs. In Proceedings of EMNLP-IJCNLP 2019, pages 3774-3785, Hong Kong, China. Quan Wang, Zhendong Mao, Bin Wang, and Li Guo. 2017. Knowledge graph embedding: A survey of approaches and applications. IEEE Transactions on Knowledge and Data Engineering, 29(12):2724-Ori Yoran, Alon Talmor, and Jonathan Berant. Zhong gave advice on analyzing the effect of model 2021. Turning tables: Generating examples from semi-structured tables for endowing language models with reasoning skills. arXiv preprint arXiv:2107.07261. Tao Yu, Rui Zhang, Heyang Er, Suyi Li, Eric Xue, Bo Pang, Xi Victoria Lin, Yi Chern Tan, Tianze Shi, Zihan Li, Youxuan Jiang, Michihiro Yasunaga, Sungrok Shim, Tao Chen, Alexander Fabbri, Zifan Li, Luyao Chen, Yuwen Zhang, Shreya Dixit, Vin-cent Zhang, Caiming Xiong, Richard Socher, Walter Lasecki, and Dragomir Radev. 2019a. CoSQL: A conversational text-to-SQL challenge towards cross-domain natural language interfaces to databases. In Proceedings of EMNLP 2019, pages 1962-1979, Hong Kong, China. Tao Yu, Rui Zhang, Oleksandr Polozov, Christo-pher Meek, and Ahmed Hassan Awadallah. 2021. SCoRE: Pre-training for context representation in size, Pengcheng Yin and Peng Shi gave advice on analysis on converting structured knowledge into natural language. Pengcheng Yin helped interpret experimental results. Ziyu Yao suggested that we report both sota (w/ extra) and sota (w/o extra) for a fair comparison. Victor Zhong and Bailin Wang gave valuable suggestions on multi-task learning and task transfer analysis. Luke Zettlemoyer, Noah A. Smith, Caiming Xiong, and Dragomir Radev gave valuable comments on research questions and experimental design. Computing resources We thank Salesforce Re-search, an Amazon Research Award, ServiceNow Research, and Yale NLP for providing computing resources generously. Tao Yu designed and led the research. conversational semantic parsing. In International Conference on Learning Representations. Tao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-sql task. In Proceedings of EMNLP 2018, Brussels, Belgium. Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language pro-cessing. In Proceedings of EMNLP 2020: System former. In Proceedings of EMNLP 2020, pages Table fact verification with structure-aware trans-Cao, Fuzheng Zhang, and Zhongyuan Wang. 2020. Hongzhi Zhang, Yingyao Wang, Sirui Wang, Xuezhi Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, 2743. Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. 2021a. Entailment as few-shot learner. CoRR, abs/2104.14690. Tao Yu, Rui Zhang, Michihiro Yasunaga, Yi Chern Tan, Xi Victoria Lin, Suyi Li, Irene Li Heyang Er, Bo Pang, Tao Chen, Emily Ji, Shreya Dixit, David Proctor, Sungrok Shim, Vincent Zhang Jonathan Kraft, Caiming Xiong, Richard Socher, and Dragomir Radev. 2019b. Sparc: Cross-domain Zhiruo Wang, Haoyu Dong, Ran Jia, Jia Li, Zhiyi Fu, Shi Han, and Dongmei Zhang. 2021b. Tuta: Tree-based transformers for generally structured table pre-training. In Proceedings of the 27th ACM SIGKDD semantic parsing in context. In Proceedings of the 57th Annual Meeting of the Association for Compu-tational Linguistics, Florence, Italy. Conference on Knowledge Discovery &amp; Data Min-ing, pages 1780-1790. Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, An-drew M. Dai, and Quoc V. Le. 2021. Finetuned lan-guage models are zero-shot learners. arXiv preprint. Jason D Williams, Antoine Raux, and Matthew Hen-derson. 2016. The dialog state tracking challenge series: A review. Dialogue &amp; Discourse, 7(3):4-33. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, R?mi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, UAI. classification with probabilistic categorial grammars. ing to map sentences to logical form: Structured Luke S. Zettlemoyer and Michael Collins. 2005. Learn-programming. In AAAI 1996, pages 1050-1055. ing to parse database queries using inductive logic John M. Zelle and Raymond J. Mooney. 1996. Learn-pages 17283-17297. tanon, Philip Pham, Anirudh Ravula, Qifan Wang, Neural Information Processing Systems, volume 33, formers for Longer Sequences. In Advances in Li Yang, and Amr Ahmed. 2020. Big Bird: Trans-Dubey, Joshua Ainslie, Chris Alberti, Santiago On-Manzil Zaheer, Guru Guruganesh, Kumar Avinava</cell></row><row><cell>November 16-20, 2020, pages 8229-8239. tuning would be comparable to fine-tuning. Ruiqi</cell><cell>Demonstrations, pages 38-45, Online. 1624-1629.</cell></row></table><note>Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez, Jay Yoon Lee, Lizhen Tan, Lazaros Polymenakos, and Andrew McCallum. 2021. Case-based reasoning for natural language queries over knowledge bases. In Proceedings of EMNLP 2021, pages 9594-9611, Online and Punta Cana, Dominican Republic. Harm de Vries, Dzmitry Bahdanau, and Christopher D. Manning. 2020. Towards ecologically valid re- search on language user interfaces. ArXiv. Jacob Devlin, Jonathan Uesato, Surya Bhupatiraju, Rishabh Singh, Abdel rahman Mohamed, and Push- meet Kohli. 2017. Robustfill: Neural program learn- ing under noisy i/o. In ICML. Julian Martin Eisenschlos, Maharshi Gor, Thomas M?ller, and William W Cohen. 2021. Mate: Multi- view attention for table transformer efficiency. arXiv preprint arXiv:2109.04312. Mihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyag Gao, and Dilek Hakkani- Tur. 2019. Multiwoz 2.1: Multi-domain dialogue state corrections and state tracking baselines. arXiv preprint arXiv:1907.01669. Mihail Eric, Lakshmi. Krishnan, Fran?ois Charette, and Christopher D. Manning. 2017. Key-value re- trieval networks for task-oriented dialogue. In SIG- DIAL Conference. Tianyu Gao, Adam Fisch, and Danqi Chen. 2021. Making pre-trained language models better few-shot learners. In Association for Computational Linguis- tics (ACL). Yanjie Gou, Yinjie Lei, Lingqiao Liu, Yong Dai, and Chunxu Shen. 2021. Contextualize knowledge bases with transformer for end-to-end task-oriented dialogue systems. In Proceedings of the EMNLP 2021, pages 4300-4310, Online and Punta Cana, Do- minican Republic. Yu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond iid: three levels of generalization for question answering on knowledge bases.Wonseok Hwang, Jinyeung Yim, Seunghyun Park, and Minjoon Seo. 2019. A comprehensive exploration on wikisql with table-aware word contextualization. ArXiv, abs/1902.01069. Hiroshi Iida, Dung Thai, Varun Manjunatha, and Mohit Iyyer. 2021. Tabbie: Pretrained representations of tabular data. arXiv preprint arXiv:2105.02584. Mohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequen- tial question answering. In Proceedings of the 55th Annual Meeting of the Association for ComputaQuentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario ?a?ko, Gun- jan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid,Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Vic- toria Lin, Neha Verma, Rui Zhang, Wojciech Kry?- ci?ski, Nick Schoelkopf, Riley Kong, Xiangru Tang, Murori Mutuma, Ben Rosand, Isabel Trindade,Ankur P Parikh, Xuezhi Wang, Sebastian Gehrmann, Manaal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipanjan Das. 2020. ToTTo: A controlled table-to- text generation dataset. In Proceedings of EMNLP.Yixuan Su, David Vandyke, Sihui Wang, Yimai Fang, and Nigel Collier. 2021. Plan-then-generate: Con- trolled data-to-text generation via planning. In EMNLP.A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table 13</head><label>13</label><figDesc></figDesc><table><row><cell></cell><cell>FRPSZHET</cell></row><row><cell></cell><cell>FRVTO</cell></row><row><cell></cell><cell>GDUW</cell></row><row><cell></cell><cell>IHWDTD</cell></row><row><cell></cell><cell>IHYHURXV</cell></row><row><cell></cell><cell>JUDLOTD</cell></row><row><cell></cell><cell>K\EULGTD</cell></row><row><cell></cell><cell>NYUHW</cell></row><row><cell></cell><cell>ORJLFWH[W</cell></row><row><cell>'HQVLW\</cell><cell>PPTD PXOWLZR] RWWTD VSDUF</cell></row><row><cell></cell><cell>VSLGHU</cell></row><row><cell></cell><cell>VTD</cell></row><row><cell></cell><cell>VTOWH[W</cell></row><row><cell></cell><cell>WDEBIDFW</cell></row><row><cell></cell><cell>WRWWR</cell></row><row><cell></cell><cell>ZHETVS</cell></row><row><cell></cell><cell>ZLNLVTO</cell></row><row><cell></cell><cell>ZLNLWT</cell></row><row><cell>LQSXWWRNHQVVWUXFWXUHLQSXWUHTXHVWLQSXW</cell><cell></cell></row><row><cell>: Baselines results are higher in pre-processed</cell><cell></cell></row><row><cell>KVRET dataset. It doesn't change our conclusion on</cell><cell></cell></row><row><cell>T5 with simple modification when necessary achieves</cell><cell></cell></row><row><cell>sota on almost all tasks.</cell><cell></cell></row><row><cell>D Experimental Setup</cell><cell></cell></row><row><cell>D.1 Implementation Details</cell><cell></cell></row><row><cell>We use T5 (Raffel et al., 2020) as our backbone</cell><cell></cell></row><row><cell>language model. Each experiment For T5-3B ex-</cell><cell></cell></row><row><cell>periments, we use Deepspeed 4 to save memory.</cell><cell></cell></row><row><cell>We use batch size 32 as default, except WikiTQ,</cell><cell></cell></row><row><cell>4 https://github.com/microsoft/DeepSpeed</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>Input and output length for each task's train set.</figDesc><table><row><cell></cell><cell cols="3">Structure Input Tokens</cell><cell></cell><cell></cell><cell>Text Input Tokens</cell><cell></cell><cell cols="4">Structure Input + Text Input Tokens</cell><cell cols="2">Sequence Output Tokens</cell></row><row><cell>Distribution(%)</cell><cell cols="3">[0, 512) [512, 1024) [1024,</cell><cell cols="4">8 ) [0, 512) [512, 1024) [1024,</cell><cell cols="3">8 ) [0, 512) [512, 1024) [1024,</cell><cell cols="3">8 ) [0, 128) [128, 256) [256,</cell><cell>8 )</cell></row><row><cell>Spider</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>99.23</cell><cell>0.77</cell><cell>0.00</cell></row><row><cell>GRAILQA</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>WebQsp</cell><cell>3.56</cell><cell>1.29</cell><cell cols="2">95.15</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>3.56</cell><cell>1.29</cell><cell cols="2">95.15</cell><cell>99.68</cell><cell>0.32</cell><cell>0.00</cell></row><row><cell>Russ</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>MTOP</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>WikiTableQuestions</cell><cell>49.56</cell><cell>28.65</cell><cell cols="2">21.79</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>48.60</cell><cell>29.11</cell><cell cols="2">22.29</cell><cell>99.93</cell><cell>0.07</cell><cell>0.00</cell></row><row><cell>WikiSQL</cell><cell>63.90</cell><cell>25.88</cell><cell cols="2">10.22</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>62.06</cell><cell>26.99</cell><cell cols="2">10.95</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ComWebQ</cell><cell>0.28</cell><cell>15.79</cell><cell cols="2">83.93</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.28</cell><cell>12.66</cell><cell cols="2">87.06</cell><cell>99.00</cell><cell>1.00</cell><cell>0.00</cell></row><row><cell>HybridQA</cell><cell>38.37</cell><cell>52.63</cell><cell>9.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>34.16</cell><cell>56.00</cell><cell>9.84</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>MultiModalQA</cell><cell>66.22</cell><cell>25.72</cell><cell>8.06</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>64.02</cell><cell>27.38</cell><cell>8.59</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>FeTaQA</cell><cell>67.03</cell><cell>27.47</cell><cell>5.49</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>64.84</cell><cell>29.57</cell><cell>5.59</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>DART</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ToTTo</cell><cell>95.82</cell><cell>2.92</cell><cell>1.26</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>95.82</cell><cell>2.92</cell><cell>1.26</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>MultiWoZ</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>99.16</cell><cell>0.84</cell><cell>0.00</cell><cell>25.07</cell><cell>74.68</cell><cell>0.24</cell><cell></cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell></row><row><cell>KVRET</cell><cell>65.76</cell><cell>34.24</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>65.76</cell><cell>34.24</cell><cell>0.00</cell><cell></cell><cell>99.79</cell><cell>0.21</cell><cell>0.00</cell></row><row><cell>SParC</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>99.26</cell><cell>0.74</cell><cell>0.00</cell></row><row><cell>CoSQL</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>99.62</cell><cell>0.38</cell><cell>0.00</cell><cell></cell><cell>99.23</cell><cell>0.77</cell><cell>0.00</cell></row><row><cell>SQA</cell><cell>60.09</cell><cell>33.38</cell><cell>6.53</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>56.91</cell><cell>36.42</cell><cell>6.67</cell><cell></cell><cell>94.17</cell><cell>5.39</cell><cell>0.44</cell></row><row><cell>TabFact</cell><cell>62.17</cell><cell>29.31</cell><cell>8.52</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>59.95</cell><cell>30.91</cell><cell>9.14</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>FEVEROUS</cell><cell>61.56</cell><cell>23.71</cell><cell cols="2">14.73</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>57.57</cell><cell>26.58</cell><cell cols="2">15.85</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SQL2Text</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Logic2Text</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>Input and output length for each task's development set.</figDesc><table><row><cell>point selection. For all tasks, we set learning rate</cell></row><row><cell>to 5e-5 and used linear learning rate decay. All</cell></row><row><cell>experiments are done on NVIDIA Tesla V100 and</cell></row><row><cell>NVIDIA Tesla A100.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>for the most similar instructions(if there is no one for use, we create one follow their writing</figDesc><table><row><cell></cell><cell cols="3">Structure Input Tokens</cell><cell></cell><cell></cell><cell>Text Input Tokens</cell><cell></cell><cell cols="4">Structure Input + Text Input Tokens</cell><cell cols="2">Sequence Output Tokens</cell></row><row><cell>Distribution(%)</cell><cell cols="3">[0, 512) [512, 1024) [1024,</cell><cell cols="4">8 ) [0, 512) [512, 1024) [1024,</cell><cell cols="3">8 ) [0, 512) [512, 1024) [1024,</cell><cell cols="3">8 ) [0, 128) [128, 256) [256,</cell><cell>8 )</cell></row><row><cell>Spider</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>GRAILQA</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>99.98</cell><cell>0.02</cell><cell>0.00</cell></row><row><cell>WebQsp</cell><cell>3.48</cell><cell>1.95</cell><cell cols="2">94.57</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>3.36</cell><cell>2.07</cell><cell cols="2">94.57</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>Russ</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>MTOP</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>WikiTableQuestions</cell><cell>48.00</cell><cell>31.15</cell><cell cols="2">20.86</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>47.08</cell><cell>31.70</cell><cell cols="2">21.22</cell><cell>99.98</cell><cell>0.02</cell><cell>0.00</cell></row><row><cell>WikiSQL</cell><cell>61.49</cell><cell>26.00</cell><cell cols="2">12.51</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>59.57</cell><cell>27.43</cell><cell cols="2">13.00</cell><cell>99.96</cell><cell>0.03</cell><cell>0.01</cell></row><row><cell>ComWebQ</cell><cell>0.85</cell><cell>16.02</cell><cell cols="2">83.13</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>0.85</cell><cell>13.07</cell><cell cols="2">86.08</cell><cell>99.43</cell><cell>0.57</cell><cell>0.00</cell></row><row><cell>HybridQA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FeTaQA</cell><cell>65.40</cell><cell>28.01</cell><cell>6.59</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>63.26</cell><cell>29.51</cell><cell>7.24</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>DART</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>ToTTo</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MultiWoZ</cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell></cell><cell>98.71</cell><cell>1.29</cell><cell>0.00</cell><cell>24.82</cell><cell>74.93</cell><cell>0.24</cell><cell></cell><cell>0.00</cell><cell>100.00</cell><cell>0.00</cell></row><row><cell>KVRET</cell><cell>66.14</cell><cell>33.86</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell><cell>66.14</cell><cell>33.86</cell><cell>0.00</cell><cell></cell><cell>100.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>SParC</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 16 :</head><label>16</label><figDesc>Input and output length for each task's test set.</figDesc><table><row><cell></cell><cell>0.0016</cell><cell cols="5">Length distribution &amp; model performance(T5 large)</cell><cell>wikitq</cell><cell>50</cell><cell>Paraphrase "[SQL]" to natural language:</cell></row><row><cell></cell><cell>0.0014</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>48</cell></row><row><cell></cell><cell>0.0012</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46</cell><cell>D.4 GPT3 and Codex Details</cell></row><row><cell>Density</cell><cell>0.0008 0.0010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>42 44</cell><cell>Acc.</cell><cell>D.4.1 Hyperparameter Settings</cell></row><row><cell></cell><cell>0.0006</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>40</cell></row><row><cell></cell><cell>0.0004</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>38</cell></row><row><cell></cell><cell>0.0002</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>36</cell></row><row><cell></cell><cell>0.0000</cell><cell>0</cell><cell>1000</cell><cell>2000</cell><cell>3000</cell><cell>4000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>input tokens</cell><cell></cell></row><row><cell cols="7">Figure 6: Length effect on WikiTableQuestion.</cell></row><row><cell cols="7">style), make our input in that format and directly</cell></row><row><cell cols="7">test on T0 3B. The specific instructions are shown</cell></row><row><cell cols="2">below.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Spider</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Given database schema "[linearized database</cell></row><row><cell cols="7">schema]". Can you tell me the SQL for "[request</cell></row><row><cell>]"?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WikiTQ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">I know that the answer to "[request]" is in "[</cell></row><row><cell cols="7">linearized table]". Can you tell me what it is?</cell></row><row><cell>DART</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Put the triples together to form a sentence: [</cell></row><row><cell cols="4">relation triples]</cell><cell></cell><cell></cell></row><row><cell cols="2">MultiWoZ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Known ontology "[ontology]", the dialogue state</cell></row><row><cell cols="7">when "[dialogue history and current request]" is</cell></row><row><cell cols="2">given</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TabFact</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">Suppose "[linearized table]" Can we infer that</cell></row><row><cell cols="3">"[statement]"?</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">SQL2Text</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table Description</head><label>Description</label><figDesc></figDesc><table /><note>Template:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>Table {table name} has column such as {column 1 name}, {column 2 name}, ...</figDesc><table><row><cell>Foreign Keys Description Template:</cell></row><row><cell>The {column1 name} of {table 1} is the foreign</cell></row><row><cell>key of {column2 name} of {table 2}</cell></row><row><cell>I.2 TabFact Template</cell></row><row><cell>Template Examples:</cell></row><row><cell>Table 1-24143253-5:</cell></row><row><cell>{name} lost his spouse {deceased spouse} to {</cell></row><row><cell>cause of death} on {date of spouses death} after</cell></row><row><cell>{length of marriage} of marriage; they had {</cell></row><row><cell>children together} together; he is currently {</cell></row><row><cell>current marital status}</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head>Table 2 -</head><label>2</label><figDesc>14978398-2: The {version} of song Comme j ai mal has a length of {length} in album {album} remixed by { remixed by} in year {year} Table 1-15187735-12: On {date} in 1936 VFL Season, the home team { home team} and away team {away team} had a game at venue {venue} with a crowd of {crowd}; the home team score is {home team score} and the away team score is {away team score} European Figure Skating, the home team {home team} and away team {away team} had a game at venue {venue} with a crowd of { crowd}; the home team score is {home team score} and the away team score is {away team score} Table 1-13740746-1: Episode number {ep no} of gerry anderson s new captain scarlet with a title of {title} is directed by {director} and written by {written by}; its original air date is {original air date }; the production number is {production no}</figDesc><table><row><cell>I.3 WikiSQL Template</cell></row><row><cell>Template Example:</cell></row><row><cell>Table 1-14240688-1:</cell></row><row><cell>in {year} were in division {division}, {league}</cell></row><row><cell>ranked {regular season}, made it to {playoffs}</cell></row><row><cell>of the playoffs, made it to &lt;{open cup}&gt; in the</cell></row><row><cell>open cup, and kept an average attendance of {avg</cell></row><row><cell>attendance}</cell></row><row><cell>Table 2-12997882-1:</cell></row><row><cell>On {date} in 2008</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Latest collections at https://unifiedskg.com.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://huggingface.co/t5-base/tree/main</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Signature: BLEU + case.lc + numrefs.1 + smooth.exp + tok.13a + version.1.4.0</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Yifei Min and Libo Qin for their earlystage discussion. We thank Panupong Pasupat and William W. Cohen for their valuable feedback on our initial draft. We thank Qian Liu for his TAPEX code and advice on question answering tasks. We thank wandb for free logging and OpenAI for free Codex usage.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? Relation-triples and knowledge graphs. <ref type="bibr">Following Nan et al. (2021b)</ref>, each relation-triple is linearized as "sub : rela : obj", and different triples are joined by " | ". The subgraph retrieved from the knowledge graph is treated as a list of relation-triples and we use the same formulation.</p><p>? Ontology. Following Hosseini-Asl et al. <ref type="bibr">(2020)</ref> and <ref type="bibr">Lin et al. (2021)</ref>, for each slot in ontology, each slot along with its all possible values is formatted as "slot : value 1 , ... value slotn ", different slot-values are joined by " | "</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Output Format</head><p>When the output is natural language or formal language we do not modify it because it is already in sequence format; a set of answers, we use a comma followed by a space to join the answers; a Boolean value, we map True to "entailed" and False to "refuted"; a dialogue state, we follow Hosseini-Asl et al. <ref type="formula">(2020)</ref>   <ref type="table">Spider  16500  100000  GrailQA  17000  78000  WebQSP  1500  8000  MTOP  30000  60000  WikiSQL  8500  80000  WikiTQ  1500  16000  CompWebQ  3500  27000  HybridQA  7000  30000  MultiModalQA  6000  40000  FeTaQA  11000  20000  DART  7000  250000  ToTTo  12000  &gt;250000  MultiWoZ2.1  6000  40000  KVRET  4000  40000  SParC  2000  6400  CoSQL  38000  100000  SQA  27000  &gt;250000  TabFact  8000  210000  FEVEROUS  1200  40000  SQL2Text  3000  10000  Logic2Text</ref> 3500 10000 Request Input:</p><p>what is the king of spain s name?</p><p>Sequence Output:  col : no. | title | narrator | aired between | original air date | us viewers row 1 : 1 | " magic is coming" | giancarlo esposito | "a land without magic" "broken" | september 30, 2012 (2012-09-30) | 6.04 row 2 : 2 | "the price of magic" | alan dale | "selfless, brave and true" "lacey" | april 14, 2013 (2013-04-14) | 5.17 row 3 : 3 | "journey to neverland" | alfred molina | "and straight on til morning" "the heart of the | (omitted to save space) | "dark swan rises: a once upon a time fan | howard parker | "operation mongoose, part 2" " the | september 27, 2015 (2015-09-27) | 3.20 row 8 : 8 | "evil reigns once more" | howard parker | "an untold story" "the savior" | september 25, 2016 (2016-09-25) | 2.86 row 9 : 9 | "the final battle begins" | howard parker | "the song in your heart" "the final battle" | may 14, 2017 (2017-05-14) | 2.82</p><p>Request Input:</p><p>list of once upon a time (debuted october 23, 2011) episodes consists of 10 specials, one of the narrators alan dale. Request Input:</p><p>Find the first name of students who have cat or dog pet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Output:</head><p>select distinct t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid join pets as t3 on t3.petid = t2.petid where t3.pettype = cat or t3.pettype = dog T5-base prediction (incorrect):</p><p>select t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid where t2.pettype = "cat" or t2.pettype = "dog"</p><p>T5-large prediction (incorrect):</p><p>select t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid where t2.pettype = "cat" or t2.pettype = "dog" T5-3B prediction (correct):</p><p>select t1.fname from student as t1 join has_pet as t2 on t1.stuid = t2.stuid join pets as t3 on t2.petid = t3.petid where t3.pettype = "cat" or t3.pettype = "dog"</p><p>H. Sequence Output:</p><p>votes for allen and fraser were 786 and 609 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (incorrect):</head><p>the te aroha election was won by william shepherd allen with a majority of 175.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (incorrect):</head><p>in the te aroha election, allen won with a majority of 175 votes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Request Input:</head><p>alhambra please.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context Input:</head><p>what city are you inquiring about? | is it going to be raining this weekend Sequence Output:</p><p>it will be raining on saturday and hailing on sunday in alhambra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (correct):</head><p>it will be raining on saturday and hailing on sunday in alhambra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (incorrect):</head><p>it will not rain in alhambra this weekend.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-3B prediction (incorrect):</head><p>there will be no rain this weekend in alhambra.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3.2 Case 2: Structured Input:</head><p>col : event | time | date | room | agenda | party row 1 : tennis activity | 5pm | wednesday | -| -| mother row 2 : taking medicine | 9am | the 3rd | -| -| sister row 3 : swimming activity | 11am | friday | -| -| father row 4 : conference | 6pm | the 8th | 100 conference room | go over budget | infrastructure team row 5 : meeting | 4pm | the 10th | conference room 102 | go over quarterly report | infrastructure team row 6 : dentist appointment | 10am | the 13 th | -| -| jeff row 7 : dentist appointment | 11am | the 5th | -| -| jeff row 8 : lab appointment | 2pm | the 4th | -| -| sister</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Output:</head><p>The Cotto is a highly rated English coffee shop in the city centre near The Portland Arms, and prices range more than [non utf-8 token]30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (correct):</head><p>Cotto is a coffee shop providing English food in the more than [non utf-8 token]30 price range. It is located in the city centre. It is near The Portland Arms. Its customer rating is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (correct):</head><p>Cotto is a coffee shop providing English food in the more than [non utf-8 token]30 price range. It is located in the city centre. It is near The Portland Arms. Its customer rating is high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-3B prediction (correct):</head><p>Cotto is a coffee shop located in the city centre near The Portland Arms. It serves English food with a price range of more than [non utf-8 token]30 and has a high customer rating. Sequence Output:</p><p>in the 2008 san diego chargers season , among the games that were played in qualcomm stadium , 3 of them started at 5:15 pm .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (incorrect):</head><p>in the 2008 san diego chargers season, when the game was at qualcomm stadium, there were three times the time was 5:15 pm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-large prediction (incorrect):</head><p>in the 2008 san diego chargers season, when the game was at qualcomm stadium, there were 3 times the time was 5:15 pm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-3B prediction (correct):</head><p>in the 2008 san diego chargers season, among the games played at qualcomm stadium, 3 of them started at 5:15 pm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.7 ToTTo case</head><p>Structured Input: See <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence Output:</head><p>Alisson Perticheto placed 18th at the 2013 Junior Worlds, 17th at the 2014 Four Continents and 16th at the 2015 Four Continents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T5-base prediction (incorrect):</head><p>Alisson Perticheto finished 18th at the Junior Worlds and 17th at the Four Continents.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Knowledge graph based synthetic corpus generation for knowledge-enhanced language model pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oshin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heming</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Shakeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.12688</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Muppet: Massive multi-task representations with pre-finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anchit</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshat</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2021</title>
		<meeting>EMNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5799" to="5811" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana, Dominican Republic</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting language models for zero-shot learning by meta-tuning on dataset and prompt collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristy</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of EMNLP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Semantic evaluation for text-to-sql with distilled test suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Seq2sql: Generating structured queries from natural language using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<idno>abs/1709.00103</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">moderate; restaurant-area: centre, east, north, south, west; restaurantfood: none; attraction-name: none; restaurantname: none; attraction-type: architecture, boat, church, cinema, college, concerthall, entertainment, hotspot, multiple sports, museum, nightclub, park, special, swimmingpool, theatre ; restaurant-book day: friday, monday, saturday, sunday, thursday, tuesday, wednesday</title>
	</analytic>
	<monogr>
		<title level="m">Structured Input: hotel-pricerange: cheap, dontcare, expensive, moderate; hotel-type: guesthouse, hotel; hotelparking: dontcare, free, no, yes; hotel-book day</title>
		<meeting><address><addrLine>1, 10, 15, 2, 3, 4, 5, 6, 7, 8, 9</addrLine></address></meeting>
		<imprint>
		</imprint>
	</monogr>
	<note>8; hotel-area: centre, dontcare, east, north, south, west; hotel-stars: 0, 1, 2, 3, 4, 5, dontcare; hotel-internet: dontcare, no, yes; hotel-name: none; train-destination: none; trainday: dontcare, friday, monday, saturday, sunday, thursday, tuesday, wednesday; train-departure: none; train-arriveby: none; train-book people: 0. restaurant-book people: 1, 2, 3, 4, 5, 6, 7, 8; restaurant-book time: none; taxi-arriveby: none</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<title level="m">DART case Structured Input: Cotto : eattype : coffee shop | Cotto : food : English | Cotto : pricerange : more than</title>
		<imprint/>
	</monogr>
	<note>30 | Cotto : customer rating : high | Cotto : area : city centre | Cotto : near : The Portland Arms</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
