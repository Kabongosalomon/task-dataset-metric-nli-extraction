<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghun</forename><surname>Jung</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungsoo</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daehoon</forename><surname>Gwak</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">KAIST AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">LG AI Research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
							<email>jchoo@kaist.ac.kr2shachoi@korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">KAIST AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Standardized Max Logits: A Simple yet Effective Approach for Identifying Unexpected Road Obstacles in Urban-Scene Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>s (a) Initial Prediction (b) Unexpected Detected (c) Final Prediction Input Image <ref type="figure">Figure 1</ref>: Results of our approach on identifying unexpected obstacles on roads. (a) Previous segmentation networks classify the unexpected obstacles (e.g., dogs) as one of the pre-defined classes (e.g., road) which may be detrimental from the safetycritical perspective. (b) Through our method, we detect the unexpected obstacles. (c) Finally, we can obtain the final prediction of segmentation labels with unexpected obstacles (cyan-colored objects) identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to ex-* indicates equal contribution isting pre-trained segmentation models. Such a straightforward approach achieves a new state-of-the-art performance on the publicly available Fishyscapes Lost &amp; Found leaderboard with a large margin. Our code is publicly available at this link 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>s (a) Initial Prediction (b) Unexpected Detected (c) Final Prediction Input Image</head> <ref type="figure">Figure 1</ref><p>: Results of our approach on identifying unexpected obstacles on roads. (a) Previous segmentation networks classify the unexpected obstacles (e.g., dogs) as one of the pre-defined classes (e.g., road) which may be detrimental from the safetycritical perspective. (b) Through our method, we detect the unexpected obstacles. (c) Finally, we can obtain the final prediction of segmentation labels with unexpected obstacles (cyan-colored objects) identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Identifying unexpected objects on roads in semantic segmentation (e.g., identifying dogs on roads) is crucial in safety-critical applications. Existing approaches use images of unexpected objects from external datasets or require additional training (e.g., retraining segmentation networks or training an extra network), which necessitate a non-trivial amount of labor intensity or lengthy inference time. One possible alternative is to use prediction scores of a pre-trained network such as the max logits (i.e., maximum values among classes before the final softmax layer) for detecting such objects. However, the distribution of max logits of each predicted class is significantly different from each other, which degrades the performance of identifying unexpected objects in urban-scene segmentation. To address this issue, we propose a simple yet effective approach that standardizes the max logits in order to align the different distributions and reflect the relative meanings of max logits within each predicted class. Moreover, we consider the local regions from two different perspectives based on the intuition that neighboring pixels share similar semantic information. In contrast to previous approaches, our method does not utilize any external datasets or require additional training, which makes our method widely applicable to ex-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12]</ref> in semantic segmentation focus on improving the segmentation performance on urban-scene images. Despite such recent advances, these approaches cannot identify unexpected objects (i.e., objects not included in the pre-defined classes during training), mainly because they predict all the pixels as one of the pre-defined classes. Addressing such an issue is critical especially for safety-critical applications such as autonomous driving. As shown in <ref type="figure">Fig. 1</ref>, wrongly predicting a dog (i.e., an unexpected object) on the road as the road does not stop the autonomous vehicle, which may lead to roadkill. In this safety-critical point of view, the dog should be detected as an unexpected object which works as the starting point of the autonomous vehicle to handle these objects differently (e.g., whether to stop the car or circumvent the dog).</p><p>Several studies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b13">14]</ref> tackle the problem of detecting such unexpected objects on roads. Some ap- <ref type="figure">Figure 2</ref>: Box plots of MSP, max logit, and standardized max logit in Fishyscapes Static. X-axis denotes the classes which are sorted by the occurrences of pixels in the training phase. Y-axis denotes the values of each method. Red and blue represent the distributions of values in in-distribution pixels and unexpected pixels, respectively. The lower and upper limits of each bar indicate the Q1 and Q3 while the dot represents the mean value of its predicted class. The gray indicates the overlapped regions of the two groups. The opacity of the gray region is proportional to the FPR at TPR 95%. Standardizing the max logits in a class-wise manner clearly reduces the FPR. proaches <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5]</ref> utilize external datasets <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24]</ref> as samples of unexpected objects while others <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> leverage image resynthesis models for erasing the regions of such objects. However, such approaches require a considerable amount of labor intensity or necessitate a lengthy inference time. On the other hand, simple approaches which leverage only a pre-trained model <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b19">20]</ref> are proposed for outof-distribution (OoD) detection in image classification, the task of detecting images from a different distribution compared to that of the train set. Based on the intuition that a correctly classified image generally has a higher maximum softmax probability (MSP) than an OoD image <ref type="bibr" target="#b17">[18]</ref>, MSP is used as the anomaly score (i.e., the value used for detecting OoD samples). Alternatively, utilizing the max logit <ref type="bibr" target="#b15">[16]</ref> (i.e., maximum values among classes before the final softmax layer) as the anomaly score is proposed, which outperforms using MSP for detecting anomalous objects in semantic segmentation. Note that high prediction scores (e.g., MSP and max logit) indicate low anomaly scores and vice versa.</p><p>However, directly using the MSP <ref type="bibr" target="#b17">[18]</ref> or the max logit <ref type="bibr" target="#b15">[16]</ref> as the anomaly score has the following limitations. Regarding the MSP <ref type="bibr" target="#b17">[18]</ref>, the softmax function has the fast-growing exponential property which produces highly confident predictions. Pre-trained networks may be highly confident with OoD samples which limits the performance of using MSPs for detecting the anomalous samples <ref type="bibr" target="#b22">[23]</ref>. In the case of the max logit <ref type="bibr" target="#b15">[16]</ref>, as shown in <ref type="figure">Fig. 2</ref>, the values of the max logit have their own ranges in each predicted class. Due to this fact, the max logits of the unexpected objects predicted as particular classes (e.g., road) exceed those of other classes (e.g., train) in the in-distribution objects. This can degrade the performance of detecting unexpected objects on evaluation metrics (e.g., AUROC and AUPRC) that use the same threshold for all classes.</p><p>In this work, inspired by this finding, we propose stan-dardizing the max logits in a class-wise manner, termed standardized max logits (SML). Standardizing the max logits aligns the distributions of max logits in each predicted class, so it enables to reflect the relative meanings of values within a class. This reduces the false positives (i.e., in-distribution objects detected as the unexpected objects, highlighted as gray regions in <ref type="figure">Fig. 2</ref>) when using a single threshold. Moreover, we further improve the performance of identifying unexpected obstacles using the local semantics from two different perspectives. First, we remove the false positives in boundary regions where predicted class changes from one to another. Due to the class changes, the boundary pixels tend to have low prediction scores (i.e., high anomaly scores) compared to the non-boundary pixels <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b0">1]</ref>. In this regard, we propose a novel iterative boundary suppression to remove such false positives by replacing the high anomaly scores of boundary regions with low anomaly scores of neighboring non-boundary pixels. Second, in order to remove the remaining false positives in both boundary and non-boundary regions, we smooth them using the neighboring pixels based on the intuition that local consistency exists among the pixels in a local region. We term this process as dilated smoothing.</p><p>The main contributions of our work are as follows:</p><p>? We propose a simple yet effective approach for identifying unexpected objects on roads in urban-scene semantic segmentation.</p><p>? Our proposed approach can easily be applied to various existing models since our method does not require additional training or external datasets. In-distribution unexpected <ref type="figure">Figure 3</ref>: Overview of our method. We obtain the max logits from a segmentation network and (a) standardize it using the statistics obtained from the training samples. (b) Then, we iteratively replace the standardized max logits of boundary regions with those of surrounding non-boundary pixels. (c) Finally, we apply dilated smoothing to consider local semantics in broad receptive fields.</p><p>margin and negligible computation overhead while not requiring additional training and OoD data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Semantic segmentation on urban driving scenes</head><p>Recent studies <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32]</ref> have strived to enhance the semantic segmentation performance on urban scenes. The studies <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b39">40]</ref> consider diverse scale changes in urban scenes or leverage the innate geometry and positional patterns found in urban-scene images <ref type="bibr" target="#b8">[9]</ref>. Moreover, several studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32]</ref> have proposed more efficient architectures to improve the inference time, which is critical for autonomous driving. Despite the advances, unexpected objects cannot be identified by these models, which is another important task for safety-critical applications. Regarding the importance of such a task from the safetycritical perspective, we focus on detecting unexpected obstacles in urban-scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Detecting unexpected objects in semantic segmentation</head><p>Several studies <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b2">3]</ref> utilize samples of unexpected objects from external datasets during the training phase. For example, by assuming that the objects cropped from the Im-ageNet dataset <ref type="bibr" target="#b35">[36]</ref> are anomalous objects, they are overlaid on original training images <ref type="bibr" target="#b1">[2]</ref> (e.g., Cityscapes) to provide samples of unexpected objects. Similarly, another previous work <ref type="bibr" target="#b4">[5]</ref> utilizes the objects from the COCO dataset <ref type="bibr" target="#b23">[24]</ref> as samples of unexpected objects. However, such methods require retraining the network by using the additional datasets, which hampers to utilize a given pre-trained segmentation network directly.</p><p>Other work <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33]</ref> exploits the image resynthesis (i.e., reconstructing images from segmentation predictions) for detecting unexpected objects. Based on the intuition that image resynthesis models fail to reconstruct the regions with unexpected objects, these studies use the discrepancy between an original image and the resynthesized image with such objects excluded. However, utilizing an extra image resynthesis model to detect unexpected objects necessitates a lengthy inference time that is critical in se-mantic segmentation. In the real-world application of semantic segmentation (e.g., autonomous driving), detecting unexpected objects should be finalized in real-time. Considering such issues, we propose a simple yet effective method that can be applied to a given segmentation model without requiring additional training or external datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>This section presents our approach for detecting unexpected road obstacles. We first present how we standardize the max logits in Section 3.2 and explain how we consider the local semantics in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Method Overview</head><p>As our method overview is illustrated in <ref type="figure">Fig. 3</ref>, we first obtain the max logits and standardize them, based on the finding that the max logits have their own ranges according to the predicted classes. These different ranges cause unexpected objects (pixels in blue boxes) predicted as a certain class to have higher max logit values (i.e., lower anomaly scores) than in-distribution pixels in other classes. This issue is addressed by standardizing the max logits in a classwise manner since it enables to reflect the relative meanings within each predicted class.</p><p>Then, we remove the false positives (pixels in green boxes) in boundary regions. Generally, false positives in boundary pixels have lower prediction scores than neighboring in-distribution pixels. We reduce such false positives by iteratively updating boundary pixels using anomaly scores of neighboring non-boundary pixels. Additionally, there exist a non-trivial number of pixels that have significantly different anomaly scores compared to their neighboring pixels, which we term as irregulars (pixels in yellow boxes). Based on the intuition that local consistency (i.e., neighboring pixels sharing similar semantics) exists among pixels in a local region, we apply the smoothing filter with broad receptive fields. Note that we use the negative value of the final SML as the anomaly score.</p><p>The following describes the process of how we obtain the max logit and the prediction at each pixel with a given image and the number of pre-defined classes. Let X ? R 3?H?W and C denote the input image and the number of pre-defined classes, where H and W are the image height, and width, respectively. The logit output F ? R C?H?W can be obtained from the segmentation network before the softmax layer. Then, the max logit L ? R H?W and predic-tion? ? R H?W at each location h, w are defined as</p><formula xml:id="formula_0">L h,w = max c F c,h,w (1) Y h,w = arg max c F c,h,w ,<label>(2)</label></formula><p>where c ? {1, ..., C}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Standardized Max Logits (SML)</head><p>As described in <ref type="figure">Fig. 2</ref>, standardizing the max logits aligns the distributions of max logits in a class-wise manner. For the standardization, we obtain the mean ? c and variance ? 2 c of class c from the training samples. With the max logit L h,w and the predicted class? h,w from the Eqs. <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_0">(2)</ref>, we compute the mean ? c and variance ? 2 c by</p><formula xml:id="formula_1">? c = i h,w 1(? (i) h,w = c) ? L (i) h,w i h,w 1(? (i) h,w = c) (3) ? 2 c = i h,w 1(? (i) h,w = c) ? (L (i) h,w ? ? c ) 2 i h,w 1(? (i) h,w = c) ,<label>(4)</label></formula><p>where i indicates the i-th training sample and 1(?) represents the indicator function. Next, we standardize the max logits by the obtained statistics. The SML S ? R H?W in a test image at each location h, w is defined as</p><formula xml:id="formula_2">S h,w = L h,w ? ?? h,w ?? h,w .<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Enhancing with Local Semantics</head><p>We explain how we apply iterative boundary suppression and dilated smoothing by utilizing the local semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Iterative boundary suppression</head><p>To address the problem of wrongly predicting the boundary regions as false positives and false negatives, we iteratively suppress the boundary regions. <ref type="figure">Fig. 4</ref> illustrates the process of iterative boundary suppression. We gradually propagate the SMLs of the neighboring non-boundary pixels to the boundary regions, starting from the outer areas of the boundary (green-colored pixels) to inner areas (graycolored pixels). To be specific, we assume the boundary width as a particular value and update the boundaries by iteratively reducing the boundary width at each iteration. This process is defined as follows. With a given boundary width r i at the i-th iteration and the semantic segmentation output  <ref type="figure">Figure 4</ref>: How iterative boundary suppression works. After standardizing the max logits, we apply average pooling by only using the SMLs of non-boundary pixels (i.e., boundary-aware average pooling) for several iterations. The boundary mask is obtained from a prediction output of a segmentation network. Y , we obtain the non-boundary mask M (i) ? R H?W at each pixel h, w as</p><formula xml:id="formula_3">M (i) h,w = 0, if ? h , w s.t.,? h,w =? h ,w 1, otherwise ,<label>(6)</label></formula><formula xml:id="formula_4">for ? h , w that satisfies |h ? h | + |w ? w | ? r i .</formula><p>Next, we apply the boundary-aware average pooling on the boundary pixels as shown in <ref type="figure">Fig. 4</ref>. This applies average pooling on a boundary pixel only with the SMLs of neighboring non-boundary pixels. With the boundary pixel b and its receptive field R, the boundary-aware average pooling (BAP) is defined as</p><formula xml:id="formula_5">BAP (S (i) R , M (i) R ) = h,w S (i) h,w ? M (i) h,w h,w M (i) h,w ,<label>(7)</label></formula><p>where S (i)</p><formula xml:id="formula_6">R and M (i)</formula><p>R denote the patch of receptive field R on S (i) and M (i) , and (h, w) ? R enumerates the pixels in R. Then, we replace the original value at the boundary pixel b using the newly obtained one. We iteratively apply this process for n times by reducing the boundary width by ?r = 2 at each iteration. We also set the size of receptive field R as 3 ? 3. In addition, we empirically set the number of iterations n and initial boundary width r 0 as 4 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Dilated smoothing</head><p>Since iterative boundary suppression only updates boundary pixels, the irregulars in the non-boundary regions are not addressed. Hence, we address these pixels by smoothing them using the neighboring pixels based on the intuition that the local consistency exists among the pixels in a local region. In addition, if the adjacent pixels used for iterative boundary suppression do not have sufficiently low or high anomaly scores, there may still exist boundary pixels that remain as false positives or false negatives even after the process. In this regard, we broaden the receptive fields of the smoothing filter using dilation <ref type="bibr" target="#b40">[41]</ref> to reflect the anomaly scores beyond boundary regions. For the smoothing filter, we leverage the Gaussian kernel since it is widely known that the Gaussian kernel removes noises <ref type="bibr" target="#b12">[13]</ref>. With a given standard deviation ? and convolution filter size k, the kernel weight K ? R k?k at location i, j is defined as</p><formula xml:id="formula_7">K i,j = 1 2?? 2 exp (? ?i 2 + ?j 2 2? 2 ),<label>(8)</label></formula><p>where ?i = i? (k?1) 2 and ?j = j ? (k?1) 2 are the displacements of location i, j from the center. In our setting, we set the kernel size k and ? to 7 and 1, respectively. Moreover, we empirically set the dilation rate as 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>This section describes the datasets, experimental setup, and quantitative and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Fishyscapes Lost &amp; Found <ref type="bibr" target="#b2">[3]</ref> is a high-quality image dataset containing real obstacles on the road. This dataset is based on the original Lost &amp; Found <ref type="bibr" target="#b34">[35]</ref> dataset. The original Lost &amp; Found is collected with the same setup as Cityscapes <ref type="bibr" target="#b9">[10]</ref>, which is a widely used dataset in urbanscene segmentation. It contains real urban images with 37 types of unexpected road obstacles and 13 different street scenarios (e.g., different road surface appearances, strong illumination changes, and etc). Fishyscapes Lost &amp; Found further provides the pixel-wise annotations for 1) unexpected objects, 2) objects with pre-defined classes of Cityscapes <ref type="bibr" target="#b9">[10]</ref>, and 3) void (i.e., objects neither in pre-defined classes nor unexpected objects) regions. This dataset includes a public validation set of 100 images and a hidden test set of 275 images for the benchmarking.</p><p>Fishyscapes Static <ref type="bibr" target="#b2">[3]</ref> is constructed based on the validation set of Cityscapes <ref type="bibr" target="#b9">[10]</ref>. Regarding the objects in the PASCAL VOC <ref type="bibr" target="#b10">[11]</ref> as unexpected objects, they are overlaid on the Cityscapes validation images by using various blending techniques to match the characteristics of Cityscapes. This dataset contains 30 publicly available validation samples and 1,000 test images that are hidden for benchmarking.</p><p>Road Anomaly <ref type="bibr" target="#b25">[26]</ref> contains images of unusual dangers which vehicles confront on roads. It consists of 60 webcollected images with anomalous objects (e.g., animals, rocks, and etc.) on roads with a resolution of 1280 ? 720. This dataset is challenging since it contains various driving circumstances such as diverse scales of anomalous objects and adverse road conditions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>Implementation Details We adopt DeepLabv3+ <ref type="bibr" target="#b6">[7]</ref> with ResNet101 <ref type="bibr" target="#b14">[15]</ref> backbone for our segmentation architecture with the output stride set to 8. We train our segmentation networks on Cityscapes <ref type="bibr" target="#b9">[10]</ref> which is one of the widely used datasets for urban-scene segmentation. We use the same pre-trained network for all experiments.</p><p>Evaluation Metrics For the quantitative results, we compare the performance by the area under receiver operating characteristics (AUROC) and average precision (AP). In addition, we measure the false positive rate at a true positive rate of 95% (FPR 95 ) since the rate of false positives in highrecall areas is crucial for safety-critical applications. For the qualitative analysis, we visualize the prediction results using the threshold at a true positive rate of 95% (TPR 95 ).</p><p>Baselines We compare ours with the various approaches reported in the Fishyscapes leaderboard. We also report results on the Fishyscapes validation sets and Road Anomaly with previous approaches that do not utilize external datasets or require additional training for fair comparisons. Additionally, we compare our method with approaches that are not reported in the Fishyscapes leaderboard. Thus, we  <ref type="table">Table 2</ref>: Comparison with other baselines in the Fishyscapes validation sets and the Road Anomaly dataset. ? denotes that the results are obtained from the official code with our pre-trained backbone and * denotes that the model requires additional learnable parameters. Note that the performance of kNN Embedding -Density is provided from the Fishyscapes <ref type="bibr" target="#b2">[3]</ref> team.</p><p>include the previous method using max logit <ref type="bibr" target="#b15">[16]</ref> and Syn-thCP <ref type="bibr" target="#b38">[39]</ref> that leverages an image resynthesis model for such comparison. Note that SynthCP requires training of additional networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation Results</head><p>This section provides the quantitative and qualitative results. We first show the results on Fishyscapes datasets and Road Anomaly, and then present the comparison results with various backbone networks. Additionally, we report the computational cost and the qualitative results by comparing with previous approaches. <ref type="table" target="#tab_1">Table 1</ref> shows the leaderboard result on the test sets of Fishyscapes Lost &amp; Found and Fishyscapes Static. The Fishyscapes Leaderboard categorizes approaches by checking whether they require retraining of segmentation networks or utilize OoD data. In this work, we add the Extra Network column under the Additional Training category. Extra networks refer to the extra learnable parameters that need to be trained using a particular objective function other than the one for the main segmentation task. Utilizing extra networks may require a lengthy inference time, which could be critical for real-time applications such as autonomous driving. Considering such importance, we add this category for the evaluation. <ref type="table" target="#tab_1">Table 1</ref>, we achieve a new state-of-the-art performance on the Fishyscapes Lost &amp; Found dataset with a large margin, compared to the previous models that do not require additional training of the segmentation network and external datasets. Additionally, we even outperform 6 previous approaches in Fishyscapes Lost &amp; Found and 5 models in Fishyscapes Static which fall into at least one of the two categories. Moreover, as discussed in the previous work <ref type="bibr" target="#b2">[3]</ref>, retraining the segmentation network with additional loss terms impair the original segmentation performance(i.e., mIoU) as can be shown in the cases of Bayesian Deeplab <ref type="bibr" target="#b30">[31]</ref>, Dirichlet Deeplab <ref type="bibr" target="#b28">[29]</ref>, and OoD Training with void class in <ref type="table" target="#tab_1">Table 1</ref>. This result is publicly available on the Fishyscapes benchmark website.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Comparison on Fishyscapes Leaderboard</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>As shown in</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Comparison on Fishyscapes validation sets and</head><p>Road Anomaly For a fair comparison, we compare our method on Fishyscapes validation sets and Road Anomaly with previous approaches which do not require additional training and OoD data. As shown in <ref type="table">Table 2</ref>, our method outperforms other previous methods in the three datasets with a large margin. Additionally, our method achieves a significantly lower FPR 95 compared to previous approaches. <ref type="figure">Fig. 5</ref> visualizes the pixels detected as unexpected objects (i.e., white regions) with the TPR at 95%. While previous approaches using MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref> require numerous in-distribution pixels to be detected as unexpected, our method does not. To be more specific, regions that are less confident (e.g., boundary pixels) are detected as unexpected in MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref>. However, our method clearly reduces such false positives which can be confirmed by the significantly reduced number of white regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we conduct an in-depth analysis on the effects of our proposed method along with the ablation studies.   <ref type="table" target="#tab_4">Table 3</ref> describes the effect of each proposed method in our work with the Fishyscapes Lost &amp; Found validation set. SML achieves a significant performance gain over using the max logit <ref type="bibr" target="#b15">[16]</ref>. Performing iterative boundary suppression on SMLs improves the overall performance (i.e., 4% increase in AP and 1% decrease in FPR 95 ). On the other hand, despite the increase in AP, performing dilated Ours Max Logit MSP Image <ref type="figure">Figure 5</ref>: Unexpected objects detected with TPR 95 . We compare our method with MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref>. White pixels indicate objects which are identified as unexpected objects. Our method significantly reduces the number of false positive pixels compared to the two approaches. smoothing on SMLs without iterative boundary suppression results in an unwanted slight increase in FPR 95 . The following is the possible reason for the result. When dilated smoothing is applied without iterative boundary suppression, the anomaly scores of non-boundary pixels may be updated with those of boundary pixels. Since the nonboundary pixels of in-distribution objects have low anomaly scores compared to the boundaries, it may increase false positives. Such an issue is addressed by performing iterative boundary suppression before applying dilated smoothing. After the boundary regions are updated with neighboring non-boundary regions, dilated smoothing increases the overall performance without such error propagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis</head><p>This section provides an in-depth analysis on the effects on segmentation performance, comparison with various backbones, and comparison on computational costs.   <ref type="table" target="#tab_6">Table 4</ref> shows the mIoU on the Cityscapes validation set with the detection threshold at TPR 95 . By applying the detection threshold, the segmentation model predicts a nontrivial amount of in-distribution pixels as the unexpected ones. Due to such false positives, the mIoU of all methods decreased from the original mIoU of 80.33%. To be more specific, using MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref> result in significant performance degradation. On the other hand, our approach maintains a reasonable performance of mIoU even with outstanding unexpected obstacle detection performance. This table again demonstrates the practicality of our work since it both shows reasonable performance in the segmentation task and the unexpected obstacle detection task.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Effects on the segmentation performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison with various backbones</head><p>Since our method does not require additional training or extra OoD datasets, our method can be adopted and used easily on any existing pre-trained segmentation networks.</p><p>To verify the wide applicability of our approach, we report the performance of identifying anomalous objects with various backbone networks including MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>, ShuffleNetV2 <ref type="bibr" target="#b27">[28]</ref>, and ResNet50 <ref type="bibr" target="#b27">[28]</ref>. As shown in <ref type="table" target="#tab_8">Table 5</ref>, our method significantly outperforms the other approaches <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16]</ref> using the same backbone network with a large improvement in AP. This result clearly demonstrates that our method is applicable widely regardless of the backbone network.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Comparison on computational cost</head><p>To demonstrate that our method requires a negligible amount of computation cost, we report GFLOPs (i.e., the number of floating-point operations used for computation) and the inference time. As shown in <ref type="table" target="#tab_10">Table 6</ref>, our method requires only a minimal amount of computation cost regarding both GFLOPs and the inference time compared to the original segmentation network, ResNet-101 <ref type="bibr" target="#b14">[15]</ref>. Also, among several studies which utilize additional networks, we compare with a recently proposed approach <ref type="bibr" target="#b38">[39]</ref> that leverages an image resynthesis model. Our approach requires substantially less amount of computation cost compared to SynthCP <ref type="bibr" target="#b38">[39]</ref>.  <ref type="table">Table 7</ref>: Comparison of metric gains after iterative boundary suppression and dilated smoothing on MSP, max logit, and SML. B Supp. and D. S refer to iterative boundary suppression and dilated smoothing, respectively. <ref type="table">Table 7</ref> describes how SML enables applying iterative boundary suppression and dilated smoothing. Applying iterative boundary suppression and dilated smoothing on other approaches does not improve the performance or even aggravates in the cases of MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref>. On the other hand, it significantly enhances the performance when applied to SML. The following are the possible reasons for such observation. As aforementioned, the overconfidence of the softmax layer elevates the MSPs of anomalous objects. Since the MSPs of anomalous objects and in-distribution objects are not distinguishable enough, applying iterative boundary suppression and dilated smoothing may not improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effects of Standardized Max Logit</head><p>Additionally, iterative boundary suppression and dilated smoothing require the values to be scaled since it performs certain computations with the values. In the case of using max logits, the values of each predicted class differ according to the predicted class. Performing the iterative boundary suppression and dilated smoothing in such a case aggravates the performance because the same max logit values in different classes represent different meanings according to their predicted class. SML aligns the differently formed distributions of max logits which enables to utilize the values of neighboring pixels with certain computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this work, we proposed a simple yet effective method for identifying unexpected obstacles on roads that do not require external datasets or additional training. Since max logits have their own ranges in each predicted class, we aligned them via standardization, which improves the performance of detecting anomalous objects. Additionally, based on the intuition that pixels in a local region share local semantics, we iteratively suppressed the boundary regions and removed irregular pixels that have distinct values compared to neighboring pixels via dilated smoothing. With such a straightforward approach, we achieved a new state-of-theart performance on Fishyscapes Lost &amp; Found benchmark. Additionally, extensive experiments with diverse datasets demonstrate the superiority of our method to other previous approaches. Through the visualizations and in-depth analysis, we verified our intuition and rationale that standardizing max logit and considering the local semantics of neighboring pixels indeed enhance the performance of identifying unexpected obstacles on roads. However, there still remains room for improvements; 1) dilated smoothing might remove unexpected obstacles that are as small as noises, and 2) the performance depends on the distribution of max logits obtained from the main segmentation networks.</p><p>We hope our work inspires the following researchers to investigate such practical methods for identifying anomalous objects in urban-scene segmentation which is crucial in safety-critical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>We deeply appreciate Hermann Blum and FishyScapes team for their sincere help in providing the baseline performances and helping our team to update our model on the FishyScapes Leaderboard. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><p>This supplementary presents the quantitative results on different architectures, hyper-parameter impacts, implementation details, and qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Effects on Different Architecture and Backbone</head><p>This section presents the quantitative results of different architecture and backbone (i.e., EfficientPS <ref type="bibr" target="#b29">[30]</ref> and ResNeSt <ref type="bibr" target="#b41">[42]</ref>) on the FishyScapes Lost &amp; Found validation set. As shown in <ref type="table" target="#tab_13">Table 8</ref>, our approach outperforms all other methods in both cases. However, the amount of performance increase is not strictly correlated with the downstream task performance, as also pointed out in the previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Analysis on Hyper-parameters</head><p>This section analyzes the impact of hyper-parameters in our proposed method through ablation studies on FishyScapes Lost&amp;Found validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of iterations n</head><p>We report the quantitative results according to the number of iterations n used in iterative boundary suppression, described in Section 3.3.1 of the main paper. Note that we set the initial boundary width r 0 to 2n so that ?r = r0 n equals 2 since we intend to reduce the width by 1 from each side of the boundary. As shown in Table 9, the performances in all metrics consistently increase as n increases up to n = 4. While AUROC and FPR 95 are improved at n = 5, AP rather aggravates. Since the number of in-distribution and unexpected pixels are unbalanced, we choose AP for our primary metric, which is invariant to the data imbalance, as done in Fishyscapes. Hence, we use n = 4 in our work.</p><p>Dilation rate d We present the quantitative results with respect to the dilation rate d used in dilated smoothing, described in Section 3.3.2 of the main paper. As shown in <ref type="table" target="#tab_1">Table 10</ref>, taking wider receptive fields improves the performance in AP up to d = 6. However, if the size of the   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Further Implementation Details</head><p>We adopt DeepLabV3+ <ref type="bibr" target="#b6">[7]</ref> as our segmentation network architecture and mainly use ResNet101 <ref type="bibr" target="#b14">[15]</ref> as the backbone for most of the experiments. Note that, as already shown in the main paper, our proposed method is model-agnostic and achieves the best performance with the MobileNetV2 <ref type="bibr" target="#b36">[37]</ref>, ShuffleNetV2 <ref type="bibr" target="#b27">[28]</ref>, and ResNet50 <ref type="bibr" target="#b14">[15]</ref> backbones compared to MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref>.</p><p>The model is trained with an output stride of 8 and the batch size of 8 for 60,000 iterations with an initial learning rate of 1e-2 and momentum of 0.9. In addition, we apply the polynomial learning rate scheduling <ref type="bibr" target="#b26">[27]</ref> with the power of 0.9 and the standard cross-entropy loss with the auxiliary loss proposed in PSPNet <ref type="bibr" target="#b20">[21]</ref>, where the auxiliary loss weight ? is set to 0.4. Moreover, in order to prevent the model from overfitting, we apply color and positional augmentations such as color jittering, Gaussian blur, random scaling with the range of [0.5, 2.0], random horizontal flipping, and random cropping. We adopt class-uniform sampling <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b3">4]</ref> with a rate 0.5.</p><p>As aforementioned, we set the number of boundary iterations n, the initial boundary width r 0 , and the dilation rate d as 4, 8, and 6, respectively. Additionally, we set the sizes of the boundary-aware average pooling kernel and the smoothing kernel size as 3 ? 3 and 7 ? 7, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Qualitative Results</head><p>This section presents the additional qualitative results. We first demonstrate the qualitative results of our methods and then their comparisons with other baselines. We use the threshold at T P R 95 and visualize the predicted indistribution and unexpected pixels as black and white, respectively. Our results <ref type="figure">Fig. 6</ref> presents the qualitative results of applying iterative boundary suppression to show the effectiveness of removing the false positives (i.e., in-distribution pixels detected as unexpected). We zoom in particular regions with the red boxes to show the changes in detail. After applying iterative boundary suppression, we significantly remove the false positives in boundary regions.</p><p>Additionally, <ref type="figure">Fig. 7</ref> describes the results of applying all of our methods. The false positives in the boundary regions (e.g., white pixels in the yellow boxes) are removed after applying iterative boundary suppression. Also, as shown in the green boxes, applying dilated smoothing effectively removes the false positives in the non-boundary regions. Comparison with other approaches We compare our method with MSP <ref type="bibr" target="#b17">[18]</ref> and max logit <ref type="bibr" target="#b15">[16]</ref> by showing qualitative results. <ref type="figure" target="#fig_2">Figs. 8 and 9</ref> show the results obtained from Fishyscapes Lost &amp; Found and Fishsyscapes Static, respectively. Since we visualize the images with the threshold at T P R 95 , most of the pixels in unexpected objects are identified. However, using MSP and max logit generate a substantial amount of false positives. In contrast, our method produces a negligible amount of false positives, which demonstrates our effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standardized</head><p>Max Logit Iteration 2 Iteration 4 <ref type="figure">Figure 6</ref>: Qualitative results of applying standardized max logit and iterative boundary suppression with iteration 2 and 4, respectively. We report the images of Fishyscapes Lost &amp; Found. The white pixels indicate the pixels predicted as unexpected. <ref type="figure">Figure 7</ref>: Qualitative results of applying standardized max logit, iterative boundary suppression, and dilated smoothing, respectively. We report the images of Fishyscapes Lost &amp; Found. Yellow boxes and green boxes show that the false positives are effectively removed by applying iterative boundary suppression and dilated smoothing, respectively. The white pixels indicate the pixels predicted as unexpected.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Standardized Max Logit</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Iterative Boundary Suppression Dilated Smoothing Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MSP Max Logit Ours</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image MSP Max Logit Ours</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>This work was supported by the Institute of Information &amp; communications Technology Planning &amp; Evaluation (IITP) grant funded by the Korean government(MSIT) (No. 2019-0-00075, Artificial Intelligence Graduate School Program(KAIST) and No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques) and the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (No. NRF-2019R1A2C4070420).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Comparison with MSP, max logit, and ours on Fishyscapes Lost &amp; Found dataset. The white pixels indicate the pixels predicted as unexpected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 9 :</head><label>9</label><figDesc>Comparison with MSP, max logit, and ours on Fishyscapes Static dataset. The white pixels indicate the pixels predicted as unexpected.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Max Logit Standardized Max Logit Suppressed Boundary Smoothed Result Input Image Pretrained Networks Proposed Methods Boundary Unexpected Irregular Unexpected class Road class Terrain class</head><label></label><figDesc></figDesc><table><row><cell>(a)</cell><cell>(b)</cell><cell>(c)</cell></row><row><cell></cell><cell></cell><cell>? We achieve a new state-of-the-art performance on the</cell></row><row><cell></cell><cell></cell><cell>publicly available Fishyscapes Lost &amp; Found Leader-</cell></row><row><cell></cell><cell></cell><cell>board 2 among the previous approaches with a large</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with previous approaches reported in Fishyscapes Leaderboard. Models are sorted by the AP scores in Fishyscapes Lost &amp; Found test set. We achieve a new state-of-the-art performance among the approaches that do not require additional training on the segmentation network or OoD data on Fishyscapes Lost &amp; Found dataset. Bold fonts indicate the highest performance in its evaluation metric among approaches that do not 1) retrain segmentation networks, 2) train extra networks, and 3) utilize OoD data.</figDesc><table><row><cell>Models</cell><cell>Additional training Seg. Network Extra Network</cell><cell>Utilizing OoD Data</cell><cell>mIoU</cell><cell cols="2">FS Lost &amp; Found AP ? FPR 95 ?</cell><cell>FS Static AP ? FPR 95 ?</cell></row><row><cell>MSP [18]</cell><cell></cell><cell></cell><cell>80.30</cell><cell>1.77</cell><cell>44.85</cell><cell>12.88</cell><cell>39.83</cell></row><row><cell>Entropy [18]</cell><cell></cell><cell></cell><cell>80.30</cell><cell>2.93</cell><cell>44.83</cell><cell>15.41</cell><cell>39.75</cell></row><row><cell>Density -Single-layer NLL [3]</cell><cell></cell><cell></cell><cell>80.30</cell><cell>3.01</cell><cell>32.90</cell><cell>40.86</cell><cell>21.29</cell></row><row><cell>kNN Embedding -density [3]</cell><cell></cell><cell></cell><cell>80.30</cell><cell>3.55</cell><cell>30.02</cell><cell>44.03</cell><cell>20.25</cell></row><row><cell>Density -Minimum NLL [3]</cell><cell></cell><cell></cell><cell>80.30</cell><cell>4.25</cell><cell>47.15</cell><cell>62.14</cell><cell>17.43</cell></row><row><cell>Density -Logistic Regression [3]</cell><cell></cell><cell></cell><cell>80.30</cell><cell>4.65</cell><cell>24.36</cell><cell>57.16</cell><cell>13.39</cell></row><row><cell>Image Resynthesis [26]</cell><cell></cell><cell></cell><cell>81.40</cell><cell>5.70</cell><cell>48.05</cell><cell>29.60</cell><cell>27.13</cell></row><row><cell>Bayesian Deeplab [31]</cell><cell></cell><cell></cell><cell>73.80</cell><cell>9.81</cell><cell>38.46</cell><cell>48.70</cell><cell>15.50</cell></row><row><cell>OoD Training -Void Class</cell><cell></cell><cell></cell><cell>70.40</cell><cell>10.29</cell><cell>22.11</cell><cell>45.00</cell><cell>19.40</cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell>80.33</cell><cell>31.05</cell><cell>21.52</cell><cell>53.11</cell><cell>19.64</cell></row><row><cell>Discriminative Outlier Detection Head [2]</cell><cell></cell><cell></cell><cell>79.57</cell><cell>31.31</cell><cell>19.02</cell><cell>96.76</cell><cell>0.29</cell></row><row><cell>Dirichlet Deeplab [29]</cell><cell></cell><cell></cell><cell>70.50</cell><cell>34.28</cell><cell>47.43</cell><cell>31.3</cell><cell>84.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on our proposed methods. B Supp. and D. Smoothing refer to iterative boundary suppression and dilated smoothing, respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>mIoU on the Cityscapes validation set with the unexpected obstacle detection threshold at TPR 95 on Fishyscapes Lost &amp; Found validation set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison with MSP and max logit on Fishyscapes Lost &amp; Found dataset. The backbone networks are trained with the output stride of 16.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparison of computational cost. Metrics are measured with the image size of 2048 ? 1024 on NVIDIA GeForce RTX 3090 GPU. The inference time is averaged over 100 trials.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Results of EfficientPS and DeeplabV3+ with ResNeSt backbone on Fishyscapes Lost &amp; Found validation set. ? denotes the results are obtained from the official code with their pre-trained networks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>Quantitative results with respect to n on Fishyscapes Lost &amp; Found. Results are obtained after standardizing the max logit, iterative boundary suppression, and dilated smoothing.</figDesc><table><row><cell cols="4">receptive field increases further (e.g., after d = 7), the per-</cell></row><row><cell cols="4">formance rather degrades, indicating that a proper size of a</cell></row><row><cell cols="4">receptive field is crucial in properly capturing the consistent</cell></row><row><cell>local patterns.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dilation</cell><cell>AUROC ?</cell><cell>AP ?</cell><cell>FPR 95 ?</cell></row><row><cell>d = 1</cell><cell>96.86</cell><cell>33.25</cell><cell>14.50</cell></row><row><cell>d = 2</cell><cell>96.90</cell><cell>34.61</cell><cell>14.36</cell></row><row><cell>d = 3</cell><cell>96.92</cell><cell>35.57</cell><cell>14.33</cell></row><row><cell>d = 4</cell><cell>96.93</cell><cell>36.15</cell><cell>14.39</cell></row><row><cell>d = 5</cell><cell>96.92</cell><cell>36.46</cell><cell>14.47</cell></row><row><cell>d = 6</cell><cell>96.89</cell><cell>36.55</cell><cell>14.53</cell></row><row><cell>d = 7</cell><cell>96.86</cell><cell>36.47</cell><cell>14.57</cell></row><row><cell>d = 8</cell><cell>96.81</cell><cell>36.28</cell><cell>14.66</cell></row><row><cell>d = 9</cell><cell>96.76</cell><cell>35.99</cell><cell>14.91</cell></row><row><cell>d = 10</cell><cell>96.70</cell><cell>35.64</cell><cell>15.31</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>Quantitative results according to the dilation rate d on Fishyscapes Lost &amp; Found. Results are obtained after standardizing the max logit, iterative boundary suppression, and dilated smoothing.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://fishyscapes.com/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3602" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Dense outlier detection and open-set recognition based on training with noisy negative images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Kre?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.09193</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The fishyscapes benchmark: Measuring blind spots in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samuel Rota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Bulo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Entropy maximization and meta classification for out-ofdistribution detection in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rottmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Gottschalk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.06575</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hardnet: A low memory traffic network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Yang</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shan</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chien-Hsiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youn-Long</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3552" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiwon</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="11580" to="11590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cars can&apos;t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungha</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanne</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaegul</forename><surname>Choo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9373" to="9383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mark Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Investigation on the effect of a gaussian blur in image filtering and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Estev?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murielle</forename><surname>Gedraite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Symposium on Electronics in Marine (ELMAR)</title>
		<meeting>of the International Symposium on Electronics in Marine (ELMAR)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="393" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Dense openset recognition with synthetic outliers generated by real nvp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Grci?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petra</forename><surname>Bevandi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sini?a?egvi?</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mohammadreza Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Basart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mantas</forename><surname>Mazeika</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11132</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Benchmarking neural network robustness to common corruptions and perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dietterich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12261</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A baseline for detecting misclassified and out-of-distribution examples in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations (ICLR</title>
		<meeting>of the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Pretrained transformers improve out-of-distribution robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Dziedzic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.06100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A simple unified framework for detecting out-of-distribution samples and adversarial attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7167" to="7177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pyramid attention network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanchao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the British Machine Vision Conference (BMVC)</title>
		<meeting>of the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">285</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Foveanet: Perspective-aware urban scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE international conference on computer vision (ICCV)</title>
		<meeting>of IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="784" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Enhancing the reliability of out-of-distribution image detection in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayadurgam</forename><surname>Srikant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations (ICLR)</title>
		<meeting>of the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><forename type="middle">Doll?r</forename><surname>Microsoft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.0312</idno>
		<title level="m">Common objects in context</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Honari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.13633</idno>
		<title level="m">Detecting road obstacles by erasing them</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting the unexpected via image resynthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Lis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Nakka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE international conference on computer vision (ICCV)</title>
		<meeting>of IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2151" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04579</idno>
		<title level="m">Parsenet: Looking wider to see better</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Shufflenet v2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="116" to="131" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>of the Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficientps: Efficient panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1551" to="1579" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Evaluating bayesian deep learning methods for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jishnu</forename><surname>Mukhoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12709</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bidirectional pyramid networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision</title>
		<meeting>the Asian Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Road obstacle detection method based on an autoencoder with semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiaki</forename><surname>Ohgushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masao</forename><surname>Yamanaka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>of the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="223" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient semantic segmentation with pyramidal fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marin</forename><surname>Or?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sini?a?egvi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107611</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Lost and found: detecting small road hazards for self-driving vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pinggera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Gehrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudolf</forename><surname>Mester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<meeting>of IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Active boundary loss for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunke</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaomiao</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuansong</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiwei</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.02696</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Synthesize then compare: Detecting failures and anomalies for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingda</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="145" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Learning Representations (ICLR)</title>
		<meeting>of the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongruo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manmatha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.08955</idno>
		<title level="m">Split-attention networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving semantic segmentation via video propagation and label relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>of IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8856" to="8865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengteng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IEEE international conference on computer vision (ICCV)</title>
		<meeting>of IEEE international conference on computer vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="593" to="602" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
