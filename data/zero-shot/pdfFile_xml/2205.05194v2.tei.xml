<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Student Collaboration Improves Self-Supervised Learning: Dual-Loss Adaptive Masked Autoencoder for Multiplexed Immunofluorescence Brain Images Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><forename type="middle">T</forename><surname>Ly</surname></persName>
							<email>stly@uh.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Q</forename><surname>Vo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragan</forename><surname>Maric</surname></persName>
							<email>maricd@ninds.nih.gov</email>
							<affiliation key="aff1">
								<orgName type="department">National Institute of Neurological Disorders and Stroke</orgName>
								<address>
									<settlement>Bethesda</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badri</forename><surname>Roysam</surname></persName>
							<email>broysam@central.uh.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hien</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
							<email>hvnguy35@central.uh.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">University of Houston</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Student Collaboration Improves Self-Supervised Learning: Dual-Loss Adaptive Masked Autoencoder for Multiplexed Immunofluorescence Brain Images Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (SSL) leverages the underlying data structure to generate supervisory signals for training deep networks. This approach offers a practical solution for learning with multiplexed immunofluorescence brain images where data are often more abundant than human expert annotations. SSL algorithms based on contrastive learning and image reconstruction have demonstrated impressive performances. Unfortunately, these methods were designed and validated mostly on natural images rather than biomedical images. A few recent works have applied SSL to analyzing cell images. However, none of these works studies SSL for multiplexed immunofluorescence brain images. These works also did not provide a clear theoretical justification for adopting a specific SSL method. Motivated by these limitations, our paper presents a self-supervised Dual-Loss Adaptive Masked Autoencoder (DAMA) algorithm developed from the information theory viewpoint. DAMA's objective function maximizes the mutual information by minimizing the conditional entropy in pixel-level reconstruction and feature-level regression. In addition, DAMA introduces a novel adaptive mask sampling strategy to maximize mutual information and effectively learn brain cell data contextual information. For the first time, we provide extensive comparisons of SSL algorithms on multiplexed immunofluorescence brain images. Our results demonstrate that DAMA is superior to other SSL approaches on cell classification and segmentation tasks. DAMA also achieves competitive accuracies on ImageNet-1k. The source code for DAMA is made publicly available at github.com/hula-ai/DAMA arXiv:2205.05194v2 [cs.CV]  </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Microscopic brain image analysis is critical for medical diagnosis and drug discovery <ref type="bibr" target="#b22">[23]</ref>. While collecting large amounts of brain imaging data using high-resolution multiplexed microscopes is efficient, annotating these images is time-consuming and labor-intensive. Each brain slice consists of several hundred thousand cells and dozens of cell types. Labeling these images requires highly skilled biology experts. For these reasons, the number of labels for this application is often limited, while the amount of unannotated data is enormous. Self-supervised learning (SSL) offers a practical solution to this situation. SSL methods have achieved impressive performance in natural language processing <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>, speech processing <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b31">32]</ref>, and computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. SSL aims to learn powerful data representations that are useful for the downstream tasks by using pretext tasks created without human supervision. Hence, this approach is ideal for biomedical applications with massive data and limited supervised information. However, there is no best selfsupervised method overall <ref type="bibr" target="#b13">[14]</ref>; thus, choosing the right SSL learning algorithm is not always straightforward.</p><p>A few recent works have applied SSL to cell data <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. For instance, <ref type="bibr" target="#b33">[34]</ref> reconstructs distorted input to learn representations for quantitative phase image cell segmentation. <ref type="bibr" target="#b11">[12]</ref> pre-trains 1M cancer cell images with convolutional autoencoder to classify the drug effects. Miscell <ref type="bibr" target="#b26">[27]</ref> utilizes contrastive learning for mining gene information from single-cell transcriptomes. None of these works studies SSL for multiplexed brain cell analysis. In addition, existing works mainly focus on the applications, while the theoretical analysis for adopting a specific SSL method is often unclear. In contrast, this study tries to bridge the gap between biomedical applications and theoretical moti-vation. We propose a novel SSL framework that optimizes both pixel <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>, and feature-level losses <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> for brain cell image analysis, called DAMA, see <ref type="figure" target="#fig_0">Fig. 1</ref>. Our dual loss is motivated by information theory and the observation that the context around cells is useful for analyzing them correctly. The method maximizes the mutual information between masked inputs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref> and self-supervised signals. Specifically, we first mask the original input and then map it to feature space from where the model learns to reconstruct the original unmasked input. Simultaneously, the exact representations also regress to the representations of a different masked version of the original input. Simultaneously learning pixel reconstruction and feature regression increases the consistency of different masked images from the same input. In addition, from the information standpoint and our observation, we further propose an adaptive masking strategy to enforce the networks to learn better representations.</p><p>The main contributions of our paper are as follows:</p><p>1. We present a novel SSL method for multiplexed biomedical data analysis, i.e., brain cells, motivated by the information theory perspective. Our method achieved superior performance compared to state-ofthe-art SSL methods and supervised.</p><p>2. We also propose an adaptive mask sampling strategy that considerably influences learning good representation. This could be the first adaptive masking method for self-supervised learning to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>2.1. Self-Supervised Learning.</p><p>Recently, self-supervised learning (SSL) has exhibited a very successful approach in computer vision <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. However, choosing the right SSL learning algorithm is not always straightforward <ref type="bibr" target="#b13">[14]</ref>. For example, one of the characteristics of multiplexed biomedical data is that the context also conveys crucial information about the cell. Learning self-supervised signals as multi-view augmented images with contrastive <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>, redundancy reduction <ref type="bibr" target="#b36">[37]</ref> or self-distillation <ref type="bibr" target="#b5">[6]</ref> objective would discard or unfocus on these context information. As an example, DINO <ref type="bibr" target="#b5">[6]</ref> visualizes the attention maps of Vision Transformers (ViT) <ref type="bibr" target="#b12">[13]</ref> after training, whose main focus is on the interesting objects and leaves contextual information unattended. MAE <ref type="bibr" target="#b16">[17]</ref> and SimMIM <ref type="bibr" target="#b35">[36]</ref> learn to reconstruct missing image patches from uncorrupted patches. Similarly, Data2Vec <ref type="bibr" target="#b1">[2]</ref> regresses the unmasked patches from masked patches in feature level. However, due to the high random masking ratio, MAE, SimMIM, and Data2Vec would not guarantee to focus on the context information in each iteration. MoCo-v3 <ref type="bibr" target="#b7">[8]</ref> learns to increase the mutual information of two augmented views of the same image, e.g., I(X 1 , X 2 ), due to the effect of augmentation transformations, MoCo-v3 would capture the cell body information and abandon the surrounding context information which distinct for each cell. Alternatively, based on ViT <ref type="bibr" target="#b12">[13]</ref> framework, we optimize the objective function on both pixel-level reconstruction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> and features-level regression <ref type="bibr" target="#b1">[2]</ref> to predict the content of masked regions. By doing so, the algorithm will concentrate on invariant features and the entire image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Masked Image Modeling (MIM).</head><p>Recent works built upon Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> framework, such as BeiT <ref type="bibr" target="#b3">[4]</ref>, MAE <ref type="bibr" target="#b16">[17]</ref>, SimMIM <ref type="bibr" target="#b35">[36]</ref> have shown potential of MIM in learning representations. Similar to our work, these prior studies propose masking out a random subset of image patches and encourage reconstructing the original pixel, but our work differs in that we also introduce regress feature representations of multiple ViT blocks <ref type="bibr" target="#b1">[2]</ref>. On the other hand, our method is also distinct from Data2Vec <ref type="bibr" target="#b1">[2]</ref> as they take the masked and unmasked patches as input and predict features produced from uncorrupted input. We, however, apply only to the visible patches and predict the feature also produced from the visible patches of the second network, i.e., teacher or momentum network. Another point of separating our work from others is that we introduce an adaptive masking strategy that can learn better representation and boost fine-tune performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-Supervised Learning on Biomedical Data</head><p>Available SSL methods are usually applied to specific applications with less novelty contribution in the biomedical field. For instance, <ref type="bibr" target="#b33">[34]</ref> reconstructs distorted input to better representations of quantitative phase image cell segmentation, <ref type="bibr" target="#b11">[12]</ref> pre-trains 1M cancer cell images with convolutional autoencoder to classify the drug effects. Miscell <ref type="bibr" target="#b26">[27]</ref> utilizes contrastive learning for mining gene information from single-cell transcriptomes. In addition, there are very few papers that study multiplexed biomedical data. While application-centric studies are acceptable, the theoretical analysis for adopting a specific SSL method is often unclear, missing out on potential approaches. In contrast, this study aims to bridge the gap between biomedical applications and theoretical motivation; and apply it to multiplexed biomedical data, e.g., brain cell data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-Supervised Learning from Information Theory Perspective</head><p>Notations. For the rest of this paper, we denote the input and self-supervised signal in general as X and S, respectively. S can be the augmented image <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18]</ref> or the target of image reconstruction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>. The deterministic mapping function F maps the input X to its representations Z X , i.e. Z X = F (X), and function G reconstructs the input as S = G(Z X ). Regarding the information, we use I(A, B), H(A), and H(A|B) to denote the mutual information, entropy, and conditional entropy of variables A and B, respectively.</p><p>As shown in <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, solid and dotted rectangles represent the information of input X and self-supervised signal S, respectively. From the information theory perspective, the mutual information between the representation Z X and S, denoted as I(Z X , S) (grey area), measures the amount of information obtained about one from the knowledge of the other. This mutual information can be expressed as the difference between two entropy terms:</p><formula xml:id="formula_0">I(ZX , S) = H(ZX ) ? H(ZX |S) = H(S) ? H(S|ZX ) (1)</formula><p>In the self-supervised learning context, one can directly maximize I(Z X , S) like in Completer <ref type="bibr" target="#b20">[21]</ref>. Alternatively, minimizing the conditional entropy H(S|Z X ) (green area) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> would also encourage S to be fully determined by X, indirectly maximizing I(Z X , S) and minimizing the irrelevant information between X and S. The Eq. (1) can be interpreted as I(Z X , S) minimize the uncommon information between X and S <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. Hence, if X and S are independent, then I(Z X , S) = 0, while if X and S are related, then I(Z X , S) will be greater than some lower bound. For this reason, S is usually the augmented images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref> or random masked images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>Augmentation could boost the performance of selfsupervised learning algorithm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. From the Information Bottleneck principle <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref>, augmented images could enforce the encoder F to estimate invariant information <ref type="bibr" target="#b34">[35]</ref>. However, augmentation is data-dependent, and finding the right transformation could sometimes be inconvenient. SimCLR <ref type="bibr" target="#b6">[7]</ref> conducts a resource-consuming experiment with the combination of only two transformations to find the most favorable combination for ImageNet-1k <ref type="bibr" target="#b9">[10]</ref>. Moreover, augmentation could remove contextual information, which is important for downstream biomedical tasks.</p><p>Motivated by <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>, our method aims to minimize the conditional entropy H(S|Z X ). <ref type="figure" target="#fig_0">Fig. 1</ref> (a) provides an illustration of our method. While <ref type="bibr" target="#b30">[31]</ref> employs forwardinverse predictive learning to boost the performance of contrastive objective, <ref type="bibr" target="#b20">[21]</ref> benefits from dual prediction and contrastive learning for recovering missing views. We, however, take a fundamentally different approach by not targeting to optimize the contrastive function <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref> but focusing on maximizing the mutual information between masked inputs and self-supervised signals at pixel-level reconstruction <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> and features-level regression <ref type="bibr" target="#b1">[2]</ref>. In addition, while most existing works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> utilize a random image masking strategy, our method uses adaptive sampling to more effectively minimize the conditional entropy H(S|Z X ) and learn better representations. To the best of our knowledge, our method is the first to use adaptive image masking for self-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Dual-loss Adaptive Masked Autoencoder</head><p>Motivated from the information theory, this section proposes a Dual-loss Adaptive Masked Autoencoder (DAMA) for self-supervised learning. Our method optimizes an objective function associated with pixel-and feature-level information masking. As illustrated in <ref type="figure" target="#fig_0">Fig. 1 (a)</ref>, it consists of a dual objective function:</p><formula xml:id="formula_1">L total = L p + ? L f<label>(2)</label></formula><p>where L p and L f are the losses associated with pixellevel reconstruction and feature-level regression, respectively. ? is a non-negative constant. From the information theory perspective, our method optimizes I(Z X , S) and H(S|Z X ). In addition, we present a novel adaptive masking strategy that is better than random masking in terms of performance and theory background. We first provide context information related to the method development and introduce theoretical details later. In our implementation, we fixed ? = 1 for all experiments. DAMA uses a Vision Transformer (ViT) <ref type="bibr" target="#b12">[13]</ref> as the backbone network. Given an multiplexed input image with seven channels x ? R H?W ?7 , we reshape it into small patches (x P ) N i=1 , where N = HW/P 2 patches and P is the resolution of each patch. We masked m% of the patches and denote them as M = {1, ..., N } m?N . Here, unlike BEiT <ref type="bibr" target="#b3">[4]</ref> and Data2Vec <ref type="bibr" target="#b1">[2]</ref> treat the masked patches and unmasked patches as input to ViT, i.e.</p><formula xml:id="formula_2">x P = {x P i : i / ? M} N i=1 ? {e P i : i ? M} N i=1</formula><p>, where e P is the learnable embedding replacing for masked patches, we feed only the unmasked patches</p><formula xml:id="formula_3">x P U = {x P i : i / ? M} N i=1 which similar to MAE [17].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pixel-level Reconstruction</head><p>Here, we present the theoretical background for the pixel-level loss L p in Eq. (2). In the context of pixel reconstruction, we regard the self-supervised signal S as the reconstruction target, i.e., the original input, denoted as S X in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>.</p><p>According to Eq. (1), minimizing the conditional entropy H(S|Z X ) (green area) would also encourage S to be fully determined by X, indirectly maximize I(Z X , S), and minimization the irrelevant information between X and S <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. To do so, the learned representation Z X is encouraged to reconstruct the self-supervised signal S, which leads to maximizing the log conditional likelihood: ?H(S|Z X ) = E P S,Z X [log P (S|Z X )]. However, directly inferring P (S|Z X ) = P (Z X |S)P (S)</p><formula xml:id="formula_4">P (Z X )</formula><p>would be intractable. A common approach to approximate this objective is to define a variational distribution Q(S|Z X ) and maximize the lower bound E P S,Z X [log Q(S|Z X )] using variational infor-mation maximization technique <ref type="bibr" target="#b0">[1]</ref> as:</p><formula xml:id="formula_5">I(Z X , S) = H(S) ? H(S|Z X ) = E P S,Z X [log P (S|Z X )] + H(S) = D KL (P (S|Z X ) || Q(S|Z X )) ? 0 + E P S,Z X [log Q(S|Z X )] + H(S) ? E P S,Z X [log Q(S|Z X )]<label>(3)</label></formula><p>Such Q(?|?) can be any type of Gaussian <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>, Laplacian [38], categorical <ref type="bibr" target="#b8">[9]</ref> distribution, or neural network <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b32">33]</ref>. We present the Q(S|Z X ) as Gaussian distribution with ?I as diagonal matrix, i.e., N (S|G(Z X ), ?I) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref>, where G(?) is parameterized as deterministic mapping model which map Z X to the sample space of S. After specifying the mapping functions G, the maximizing E P S,Z X [log Q(S|Z X )] objective functions in pixel-level is:</p><formula xml:id="formula_6">Lp = min n i=1 EP S,Z X m i G(Z X m i ) ? S 2 2 , i = 1, 2 (4) where Z X m i is the representation of masked input X m i , i.e. Z X m i = F (X m i )</formula><p>, and i = 1, 2 represents for two branches of models as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. From the above objective function, we can notice that when L p = 0, i.e., S can be fully determined by Z X m , mathematically, H(S|Z X m ) = 0. One common situation is that the G(F (?)) becomes the identical mapping I, and the network will learn nothing. For this reason, X m is usually the augmented images or random masking images from the same source to avoid the degenerate solution. Our DAMA employs the image masked autoencoder modeling approach similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref> instead of augmenting the input. To leverage the masking operation to contribute more than just generating random masks, we propose a novel adaptive masking strategy that can increase the mutual information I(Z X , S) and learn better representation, whose details will be explained in section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature-level Regression</head><p>To further encourage maximizing the mutual information I(Z X , S), DAMA also consist a feature-level regression objective L f in Eq. (2). In the context of feature-level regression, we prefer self-supervised signal S as the feature target produced by F Z (X m 2 ) and indicate as S z in <ref type="figure" target="#fig_0">Fig.  1</ref>. DAMA predicts feature representations of masked view X m 2 based on masked view X m 1 , i.e., F Z : X m 1 ? X m 2 , where F Z is the mapping function. This is different from Data2Vec <ref type="bibr" target="#b1">[2]</ref> which predicts feature representations of the original uncorrupted input X based on a masked view X m in a student-teacher setting, i.e., F Z : X m ? X. Furthermore, the masked patches of X m 2 are decided by adaptive sampling strategy.</p><p>Similar to the pixel-level loss, maximizing the mutual information I(Z X , S) leads to maximizing the log conditional likelihood E P S,Z X [log P (S|Z X )]. We can in-troduce a variational distribution Q(S|Z X ) and maximize the lower bound E P S,Z X [log Q(S|Z X )]. Let Q(S|Z X ) as Gaussian distribution with ?I as diagonal matrix, i.e., N (S|F Z (Z X ), ?I). The objective function is obtained as:</p><formula xml:id="formula_7">L f = min E P S,Z X m 1 F Z (Z X m 1 ) ? S 2 2 (5) where Z X m 1 is the representation of masked input X m 1 , i.e. Z X m 1 = F (X m 1 )</formula><p>. Note that F and F Z are two different mapping functions; refer to <ref type="figure" target="#fig_0">Fig. 1</ref> for visual illustration. Given that our application of interest is brain image analysis, where context provides critical information for classification and segmentation tasks, we adopt the smooth L1 loss presented in <ref type="bibr" target="#b1">[2]</ref>. Hence, the Eq. (5) becomes:</p><formula xml:id="formula_8">L f = 1 2 (F (Z X m 1 ) ? S) 2 /?, F (Z X m 1 ) ? S ? ? F (Z X m 1 ) ? S ? 1 2 ?, otherwise<label>(6)</label></formula><p>where ? is the smoothing from L2 to L1 loss term and depends on the difference between F Z (Z X m 1 ) and S. In addition, the self-supervised signal S is taken from the last K blocks of the second branch of the model before normalization to each block and then averaging similarly as in Data2Vec. In our implementation, K = 6 and ? = 2 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Adaptive Masking Strategy</head><p>Unlike other works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>, we propose an adaptive masking strategy to produce masked images X m 2 . This strategy helps increase the mutual information I(Z X , S) and learn better representations. The method is originated from our observation of the theoretical background presented in the pixel-level reconstruction section 4.1. The patches with the highest loss indicate the lowest mutual information I(Z X m 1 , S). See Algorithm 1. The proposed strategy takes the random binary mask of X m 1 and the patch reconstruction loss in Eq. (4) as inputs. It selects the patches with the highest loss, which indicate the lowest mutual information I(Z X m 1 , S) as masked patches for X m 2 . Regarding the unmasked patches in X m 1 , based on the overlap ratio, some will become the unmasked patches, and the rest will serve as masked patches in X m 2 . The overlap ratio is fixed at 50% for all experiments. This guarantees that the feature-pixel regression would not be too difficult to predict. One can think of the adaptive image masking strategy as a collaboration between two students, where the first student estimates the difficulties of reconstructing different patches, and the second student uses that information to select challenging patches to enhance the performance. Note that we develop DAMA upon ViT framework <ref type="bibr" target="#b12">[13]</ref>. Hence, we compute the reconstruction loss patch-wise, and unmasked patches are not considered in computing loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we validate our DAMA algorithm on the multiplexed immunofluorescence brain image dataset and compare its performance to supervised learning and stateof-the-art SSL approaches. See <ref type="table">Table 1, Table 2</ref>, <ref type="table">Table 3</ref>, and <ref type="figure" target="#fig_3">Fig. 2</ref>. See Appendix for extensive experiments and results in <ref type="table">Table 5</ref>, <ref type="figure">Fig. 6, 7, 8, 9</ref>, and 10. The cell images are collected from 5 major cell types from rat brain tissue sections: neurons, astrocytes, oligodendrocytes, microglia, and endothelial. Seven biomarkers are applied as the feature channels: DAPI, Histones, NeuN, S100, Olig 2, Iba1, and RECA1. DAPI and Histones are utilized to reveal the cells' locations, while other biomarkers are useful for identifying cell types. All models take 7-channel images as the input. No cell detection pre-processing has been applied to these images; thus, some patches may contain more than one cell, but only the central cell is what we are interested in, and all other cells are generally considered as the background for the cell type classification task, see the <ref type="figure" target="#fig_0">Fig. 11</ref> for example cell images. Since cell types are highly corresponded with specific biomarker, we only use affine transformations, such as rotating, translating, flipping, scaling, and no colorizing transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>For pretraining and evaluating the SSL methods on the classification task, we collected 1600 cells images for each cell type. In a total of 8000 images, our biologists manu- ally label 4000 images for model finetuning, and we keep the remaining 4000 images for pretraining purposes. For each finetune experiment, we randomly shuffle images and then split the finetune set into 60%/40%, i.e., 2400/1600, for training/validation sets. We validate each SSL method by repeating the finetune experiment ten (10) times and averaging the results. For the segmentation task, our biologists have manually collected and annotated 181 images of size 512 ? 512 ? 7. We split these images into the size of 128 ? 128 ? 7, totaling 2896 images. We further split these images into train/val sets for evaluating the segmentation task with a ratio of 60/40. We augmented the training set for pretraining SSL methods. The segmentation task requires segmenting the cell body from the background regardless of the cell type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We implemented DAMA using Pytorch. Unless stated otherwise, we trained on ViT-Base used Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with base learning rate of 0.00015 <ref type="bibr" target="#b7">[8]</ref>, batch size of 512, image size 128?128?7, ViT patch size 16. Regarding state-of-the-arts implementation, we take the official released code <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref> and conduct pre-training with our biomedical data, except for Data2Vec <ref type="bibr" target="#b1">[2]</ref>. We also use ViT-Base framework and similar parameters as above for these experiments. We report results of our DAMA and MAE <ref type="bibr" target="#b16">[17]</ref> with masking ratios 80% and 60% for Data2Vec <ref type="bibr" target="#b1">[2]</ref>. Training epochs and training times are listed along with the methods in result tables. All experiments were done on 4 GPUs of V100 32GB. Pre-training or finetune experiments of different methods on the same dataset have the same random seed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons with State-of-the-arts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Brain cell dataset.</head><p>In <ref type="table">Table 1</ref>, we compare the finetuning classification accuracy/error rate of randomly initialized and pretrained selfsupervised models. Our DAMA outperforms both state-ofthe-art SSL and randomly initialized methods.</p><p>Cell Classification. One can notice that DAMA is more accurate than image encoding methods, such as MAE <ref type="bibr" target="#b16">[17]</ref> and Data2Vec <ref type="bibr" target="#b1">[2]</ref>, even with random masking setting, denoted as DAMA-rand 500. This is expected since DAMA combines the strengths of both methods. MAE and Data2Vec were pretrained for 800 epochs while DAMA used 500 epochs. MAE reports high optimal masking ratios, ranging from 40%-90% on ImageNet-1K. For biomedical datasets, we only pretrained MAE with an 80% masking ratio for all datasets. Since Data2Vec has a similar approach to our feature-level regression, we implemented Data2Vec with a random patch masking ratio of 60%. Data2Vec does not produce good results since it only learns to regress from low dimension features. Compared to the contrastive learning method, MoCo-v3, our method reconstructs pixels and regresses features instead of optimizing the model through contrastive learning. Our DAMA significantly outperforms   Cell Segmentation. Pretrained and finetuned with the same ViT backbone architecture, except for SimMIM-Swin, and object detector frameworks Mask R-CNN <ref type="bibr" target="#b18">[19]</ref>, our DAMA achieves the best performances compared to other SSL methods, see <ref type="table">Table 2</ref> and <ref type="figure" target="#fig_3">Fig. 2</ref>. Cell segmentation is challenging as no single biomarker determines the whole cell body. DAMA significantly outperforms all baseline methods. We hypothesize that DAMA is more capable of utilizing contextual information around cells to resolve ambiguous cases where cells are dense and overlapping. Two observations can support this hypothesis. First, MoCo-v3's objective is to increase the mutual information of two augmented views coming from the same image, e.g., I(X 1 , X 2 ). The random augmentation operations encourage MoCo-v3 to focus on the cell body while discarding the contextual features since they are likely to decrease the mutual information. Second, MAE and Sim-MIM learn to reconstruct missing image patches from un-corrupted patches, e.g., I(X mask , X original ). However, the high random masking ratios in these two methods (e.g., 0.8) substantially reduce the contextual information. In contrast, our DAMA model emphasizes contextual information by adaptive masking strategy in each iteration and achieves the best segmentation performance. <ref type="figure" target="#fig_4">Fig. 3</ref> shows that DAMA can segment clusters of cells better than other methods. The results demonstrate DAMA's significant advantage in analyzing multiplexed brain cell data. More visualization results are in <ref type="figure">Fig. 6</ref>. <ref type="figure" target="#fig_3">Fig. 2</ref> shows the segmentation performances of DAMA and other methods. For DAMA, overall AP at IoU@.75 is 0.77, and perfect localization Loc (PR at IoU@.1) increases AP to 0.956. Since we only segment the cell body from the background regardless of the cell type, e.g., having only one class, the super categories Sim and classes confusion Oth are unchanged compared to perfect localization Loc. Removing background false positives BG would increase the performance by 0.014 (to 0.97 AP), and the rest of the errors are missed detections. In summary, DAMA's errors come from imperfect localization Loc and confusing background BG. Regarding other methods, the amount of Loc and BG errors are higher than those of DAMA. The meaning of errors is detailed on the COCO website cocodataset.org/#detection-eval. We also present the detection and segmentation errors in <ref type="figure">Fig. 7 and 8</ref>, the precisionrecall curves at different IoU threshold in <ref type="figure" target="#fig_0">Fig. 9, 10</ref>   <ref type="table">Table 4</ref>. Effect of sampling strategies, model settings, and masking ratios in term of classification accuracy. Bold are the highest score for each column, while underlined are highest score for each row. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">ImageNet-1k.</head><p>To demonstrate the potential of DAMA on other natural image type, we present DAMA's result on ImageNet-1k <ref type="bibr" target="#b9">[10]</ref>, see <ref type="table">Table 3</ref>. DAMA is competitive to other state-ofthe-art algorithms despite smaller numbers of pre-trained epochs and without any ablation experiment for searching optimal hyper-parameters. Due to the computational resource needed for training on such a large-scale dataset, we perform only a single pretraining/finetuning experiment on ImageNet-1K with the same configuration as for training on the brain image dataset, except for the image size and pretraining/finetuning batch size as 224 ? 224 ? 3, and 4096/1024, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head><p>In <ref type="figure" target="#fig_5">Fig. 4, Fig. 5</ref>, and <ref type="table">Table 4</ref>, we ablate our DAMA with different masking strategies, model strategies, and masking ratios. See Appendix for more results.</p><p>Masking Strategy and Masking Ratio. We compare <ref type="figure">Figure 5</ref>. Reconstruction effect of mask sampling strategy between MAE (b) and our (c-g) from (a) the original input. Note that, while MAE can not reconstruct the region in red box in a single iteration, our DAMA is able to do that with the adaptive sampling.</p><p>our DAMA with adaptive and random masking strategies and analyze how they affect the finetuning results. We vary the masking ratio in the range of (60%-90%). The masking ratio of 80% achieves a better result for all three masking settings. Similarly, MAE reports high masking ratios, ideally ranging in 60% ? 80% for good finetuning performance on ImageNet-1K. Primarily, adaptive masking produces better accuracy with different masking ratios in the same training condition. These experiments justify the effectiveness of our method. Overlapping, a step in the proposed adaptive masking, means some unmasked patches in X m 1 will also be unmasked in X m 2 . The overlapping ratio is set at 50% for all experiments. No overlapping between unmasked input X m 1 and X m 2 leads to more challenging optimization of L f as illustrated in <ref type="figure" target="#fig_5">Fig. 4</ref>. Specifically, the loss curves for none overlapping experiments are higher than others. These results demonstrate the effect of adaptive masking and feature-level regression on the overall objective function.</p><p>Student and Teacher Configurations. We perform ablation studies with different variants of student and teacher network configurations. Specifically, we compare three configurations: student-teacher, shared-weights (student1-student1), and student1-student2. student-teacher network is an exponential moving average (EMA) on the student weights <ref type="bibr" target="#b17">[18]</ref>, and the update rule is ? t ? ?? t + (1 ? ?)? s , where ? follows cosine scheduled from 0.996 to 1 during training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>. Both networks use the same set of parameters in the shared-weights setting. The student1-student2 setting consists of two independent networks, allowing more "freedom" in optimizing objective functions. <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_5">Fig. 4</ref> show that the student-teacher is less effective than shared-weights (student1-student1) and student1-student2 settings. In addition, utilizing adaptive sampling strategy can improve the performance with the same network configuration. This supports the advantage of our method.</p><p>Choice of ?. The choice of parameter ? usually is caseby-case determined. Following <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31]</ref>, we keep the scale of feature-level regression loss L f is 1 10 to 1 100 to the scale of the pixel-level reconstruction loss L p , see <ref type="figure" target="#fig_5">Fig. 4</ref>. We empirically found that setting ? = 1 yields consistently good performance. The paper <ref type="bibr" target="#b30">[31]</ref> shares a similar approach to our paper, except it has an additional contrastive loss, e.g. L total = ? 1 L p + ? 2 L f + ? 3 L CL . They construct controlled experiments to study the effect of this sensitive and dataset-dependent hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Adaptive Masking Strategy on Learning</head><p>Contextual Information. We show in <ref type="figure">Fig. 5</ref> the reconstruction results of (b) MAE and (c-g) our five settings on the same setting as in <ref type="table">Table 4</ref>. Except for (a), in (b-g), the upper row is the same random masking input X m 1 (left) and its reconstruction result G(Z X m 1 ) (right), the lower row is the applied mask strategy input X m 2 (left) and its reconstruction result G(Z X m 2 ) (right). From left to right, (a) original image is center cropped as input green box; (b) MAE's result, (c) our model-ema random sampling result; our adaptive sampling: (d) student-teacher without mask overlapping; (e) student-teacher with mask overlapping; (f) share-weights with mask overlapping; and (g) student1-student2 with mask overlapping. Without adaptive masking, (c) can not reconstruct properly. This could be explained as the teacher network is constrained by its student network and could not produce reasonable inference with another random mask. In contrast, (d) also uses the studentteacher setting, but with the adaptive mask, can reconstruct the cell reasonably well. Note that, while MAE can not reconstruct the region in red box which is part of another cell in a single iteration, our method can do that with adaptive sampling. This suggests MAE could leave out these finegrain details even with several epochs in a random high masking ratio setting, i.e., 80%. Conversely, our DAMA combines pixel-and feature-level optimization with adaptive masking to identify those details in every iteration with a high masking ratio. This supports the advantage of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper introduces DAMA as the first adaptive masking SSL for learning effective representations from multiplexed immunofluorescence brain images. DAMA leverages a dual loss consisting of a pixel-level reconstruction and a feature-level regression. Our experiments show DAMA's competitive performance on both cell classification and segmentation tasks. Our work demonstrate the importance of adaptive mask sampling and informationtheoretic dual loss function in SSL. DAMA also achieves competitive accuracy on ImageNet-1k dataset with a single experiment. Augmentation could generate unlimited data. However, the underline structure of data is likely to remain the same. To exam our method on noisy data, we first cropped the large image into many 1000?1000 images and performed morphological transformations, i.e., erosion. These images were then applied watershed segmentation to identify the cells' location. From cells' center locations, we cropped with the size of 100?100 to get the images. To be compatible with the manually collected set, we collected 30k random cell images regardless of the cell type as the second training set, called Noisy-30k brain cells dataset. In addition, we further constructed another Noisy-170k brain cells dataset as a large-scale dataset.</p><p>Our DAMA achieves competitive results better than MAE and Data2Vec and is more stable on three brain cell datasets. MoCo-v3 is influenced by a small pretraining dataset but improves on a large dataset. <ref type="table">Table 5</ref> presents the comparisons of DAMA and other methods on Noisy-30k and Noisy-170k. MoCo-v3 has the best performance compared to others. This suggests that a small pretraining set has a negative impact on MoCo-v3. The results also indicate larger pretraining data and longer training time. MoCo-v3 can outperform other methods. This is expected since MoCo-v3 learns to increase the mutual information of two augmented views of the same image, e.g., I(X 1 , X 2 ), capturing better cell body information that is invariant across the dataset. In addition, the classification is considered not a rigorous task by utilizing the one-to-one cdence between cell types and biomarkers. However, this is also a downside since MoCo-v3 would abandon other critical information, e.g., contextual information. Data2Vec does not produce good results since it only learns to regress from low dimension features. On the other hand, MAE and DAMA have better results and are comparable to those from the original dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Precision-Recall Curves</head><p>We present the overall-all-all precision recall curves for bounding box and segment mask in <ref type="figure">Fig. 7 and 8</ref>. DAMA's results are come from imperfect localization Loc and background confusions BG. Regarding other methods, the amount of Loc and BG errors are higher than those of DAMA. Note that, we has only one class, i.e., segment cell body from background regardless its type. The precision recall curves at different IoU threshold for bounding box and segment mask are shown in <ref type="figure">Fig. 9</ref>   <ref type="figure">Figure 6</ref>. Visualization segmentation validation set of DAMA and other methods at threshold IoU = 0.75. By focusing more on the contextual information, DAMA detects and segments cells better where cells are dense and overlap on each other, e.g., cluster of cell.   <ref type="figure" target="#fig_0">Figure 10</ref>. Segmentation mask Precision-Recall curve at different IoU threshold of DAMA and other SSL methods. DAMA has the best scores at the IoU from 0.1 : 0.75 and are competitive numbers at 0.8 : 0.9. <ref type="figure" target="#fig_0">Figure 11</ref>. Example of five cell types: microglia, neurons, oligodendrocytes, endothelial, and astrocytes correspond to five biomarkers: Iba1, NeuN, Olig2, RECA1, S100.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Overview of DAMA pipeline. During pre-training, random masked X m 1 input is mapped to the feature space wherein its representations Z m 1 learn to reconstruct the original unmasked input with patch-wise loss (Sec. 4.1). The adaptive masking strategy takes the reconstruct patch-wise loss and the binary mask of X m 1 and constructs the masked input X m 2 (Sec. 4.3) and continue the similar learning process as X m 1 . In addition, Z m 1 also try to predict Z m 2 (Sec. 4.2). The entire procedure encourages maximizing the mutual information between Z m 1 and the self-supervised signal in pixel-level SX and feature-level SZ which derive as the log conditional likelihood in thefigure.(b) Overview of our pipeline in information perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1</head><label>1</label><figDesc>Pytorch-like Adaptive Masking Pseudocode def adaptive_mask(m1, loss, mask_ratio, overlap_ratio): # mask_ratio: masking ratio in [0, 1] # m1, m2: binary masks; size[N, L] # overlap_ratio: overlap ratio between 2 masks # loss: patch reconstruction losses; size[N, L] # N: batch size # L: total number of patches in images len_keep = int(L * (1 -mask_ratio)) loss_len = int(L -len_keep * 2) overlap_len = int(len_keep * overlap_ratio) # get ids of high loss patches # discard losses of unmasked patches in m1 loss = loss * m1 loss_sorted = argsort(loss) loss_ids=loss_sorted[:,-(loss_len+overlap_len):] # m1(1) becomes m2(0) and m1(0) becomes m2(1) m2 = where(m1 == 1, 0, 1) # assign ids of highest loss m1 to m2 as masks m2[arange(m2.shape[0])[:,None], loss_ids] = 1 # overlap of unmasked patches of m1 and m2 m1_ids = argsort(m1) m1_ids = m1_ids[:, :overlap_len] m2[:, m1_ids] = 0 return m2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>5. 1 . 1</head><label>11</label><figDesc>Brain cell dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Segmentation mask error analysis: overall-all-all Precision-Recall curves. See more in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of segmentation results on validation set at IoU = 0.75. See more figures in Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Pre-training curves of different model settings on adaptive masking condition: Pixel-level reconstruction loss in (a) net-work1 and (b) network2, (c) feature-level regression loss, and (d) total loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .Figure 9 .</head><label>789</label><figDesc>Bounding box overall-all-all Precision-Recall curve of DAMA and other SSL methods. Segmentation mask overall-all-all Precision-Recall curve of DAMA and other SSL methods. Same as Fig. 2. Bounding box Precision-Recall curve at different IoU threshold of DAMA and other SSL methods. DAMA has the best scores at the IoU from 0.1 : 0.75 and are competitive numbers at 0.8 : 0.9.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2</head><label>12</label><figDesc>500 (3h) 94.69 94.19 94.81 95.81 94.50 94.00 94.88 94.69 95.25 94.81 94.76(+2.Comparisons of finetuning results with state-of-the-art SSL methods and randomly initialized in accuracy and error rate. Training epochs and training times are listed along with the methods.</figDesc><table><row><cell></cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Avg. ?</cell><cell>Err. ?</cell></row><row><cell>Random init.</cell><cell cols="8">91.75 91.19 92.75 92.69 92.56 92.31 91.44 91.06</cell><cell>93</cell><cell cols="2">91.06 91.98(+0.00)</cell><cell>8.02</cell></row><row><cell>Data2vec 800 (4h)</cell><cell cols="6">90.56 90.25 91.75 92.31 91.94 92.62</cell><cell>91</cell><cell>91.38</cell><cell>92.5</cell><cell cols="2">90.88 91.59(-0.39)</cell><cell>8.41</cell></row><row><cell>MOCO-v3 500 (6h)</cell><cell>90.94</cell><cell>91.5</cell><cell cols="9">92.38 92.38 92.56 92.12 91.25 90.94 92.69 90.75 91.75(-0.23)</cell><cell>8.25</cell></row><row><cell>MAE 800 (4h)</cell><cell cols="4">94.69 93.81 95.19 95.25</cell><cell>95</cell><cell cols="4">93.56 94.62 93.88 95.44</cell><cell>94</cell><cell>94.54(+2.56)</cell><cell>5.46</cell></row><row><cell cols="12">DAMA-rand 78)</cell><cell>5.24</cell></row><row><cell>DAMA 500 (5h)</cell><cell>95.5</cell><cell>94.5</cell><cell cols="9">95.69 96.25 95.56 95.44 95.62 94.94 95.69 95.25 95.47(+3.49)</cell><cell>4.53</cell></row><row><cell>Methods</cell><cell cols="11">Box mAP Box mAP@50 Box mAP@75 Mask mAP Mask mAP@50 Mask mAP@75</cell></row><row><cell>ViT random init.</cell><cell></cell><cell>63.4</cell><cell>90.8</cell><cell></cell><cell cols="2">73.9</cell><cell>66.7</cell><cell></cell><cell>90.9</cell><cell></cell><cell>76.1</cell></row><row><cell>Swin random init</cell><cell></cell><cell>63.2</cell><cell>90.6</cell><cell></cell><cell cols="2">73.7</cell><cell>66.3</cell><cell></cell><cell>90.5</cell><cell></cell><cell>76</cell></row><row><cell>MAE 800</cell><cell></cell><cell>63.7</cell><cell>90.8</cell><cell></cell><cell cols="2">74.8</cell><cell>67.1</cell><cell></cell><cell>91.4</cell><cell></cell><cell>76.9</cell></row><row><cell>MAE 1600</cell><cell></cell><cell>63.8</cell><cell>90</cell><cell></cell><cell cols="2">73.3</cell><cell>66.3</cell><cell></cell><cell>90.1</cell><cell></cell><cell>76.3</cell></row><row><cell>MOCO-v3 500</cell><cell></cell><cell>63.2</cell><cell>90.5</cell><cell></cell><cell cols="2">73.2</cell><cell>66.5</cell><cell></cell><cell>91</cell><cell></cell><cell>75.9</cell></row><row><cell>MOCO-v3 1000</cell><cell></cell><cell>63.1</cell><cell>90.2</cell><cell></cell><cell>73</cell><cell></cell><cell>66.1</cell><cell></cell><cell>90.8</cell><cell></cell><cell>75.2</cell></row><row><cell>SIMMIM-ViT 800</cell><cell></cell><cell>63.6</cell><cell>91.1</cell><cell></cell><cell cols="2">74.1</cell><cell>66.9</cell><cell></cell><cell>91.1</cell><cell></cell><cell>76.1</cell></row><row><cell cols="2">SIMMIM-Swin 800</cell><cell>64.2</cell><cell>91.3</cell><cell></cell><cell cols="2">75.1</cell><cell>67</cell><cell></cell><cell>91.2</cell><cell></cell><cell>77</cell></row><row><cell>DAMA 500</cell><cell></cell><cell>64.1</cell><cell>91.1</cell><cell></cell><cell cols="2">74.2</cell><cell>67.2</cell><cell></cell><cell>91.1</cell><cell></cell><cell>77</cell></row><row><cell>DAMA 1000</cell><cell></cell><cell>64.6</cell><cell>91.4</cell><cell></cell><cell cols="2">75.3</cell><cell>67.3</cell><cell></cell><cell>91.3</cell><cell></cell><cell>77</cell></row></table><note>. Comparisons of segmentation accuraccy with state-of-the-arts on brain cell dataset. Bold and underlined are the highest and second highest scores for each column.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>[ 38 ]</head><label>38</label><figDesc>Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223-2232, 2017. 4</figDesc><table><row><cell>A. Appendix</cell></row><row><cell>A.1. Noisy datasets</cell></row><row><cell>Real-30k and Real-170k Brain Cell Dataset.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>and 10. For both detection and segmentation, DAMA has the best scores at the IoU from 0.1 : 0.75 and are competitive at 0.8 : 0.9. MOCO-v3 1000 (12h) 94.44 93.62 94.38 95.19 94.69 95.25 94.12 94.38 95.25 94.31 94.56(+2.94.12 95.62 95.31 95.25 95.12 94.81 94.62 95.12 94.44 94.92(+2.94) 5.08Table 5. Comparisons of fine-tuning results with state-of-the-art SSL methods pretrained with on Noisy dataset and randomly initialized in accuracy and error rate. Our DAMA reports stable results over dataset settings compared with other state-of-the-arts. We report results of our DAMA and MAE<ref type="bibr" target="#b16">[17]</ref> with masking ratios 80% and 60% for Data2Vec<ref type="bibr" target="#b1">[2]</ref>. Training epochs and training times are listed along with the methods. Bold and underlined are the highest and second highest scores, respectively. Pretraining (left) and finetune (right) setting of our DAMA.</figDesc><table><row><cell>Folds</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>6</cell><cell>7</cell><cell>8</cell><cell>9</cell><cell>Avg. ?</cell><cell>Err. ?</cell></row><row><cell>Random init.</cell><cell cols="8">91.75 91.19 92.75 92.69 92.56 92.31 91.44 91.06</cell><cell>93</cell><cell cols="2">91.06 91.98(+0.00)</cell><cell>8.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Noisy-30k dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data2vec 800 (4h)</cell><cell>93.69</cell><cell>93</cell><cell cols="7">93.31 94.06 93.56 94.19 93.38 93.31 93.81</cell><cell>93.5</cell><cell>93.58(+1.60)</cell><cell>6.42</cell></row><row><cell>Data2vec 1600 (8h)</cell><cell>91.5</cell><cell cols="8">90.69 92.81 92.38 92.62 92.62 91.56 91.94 92.19</cell><cell>91.5</cell><cell>91.98(+0.00)</cell><cell>8.02</cell></row><row><cell>MOCO-v3 500 (6h)</cell><cell>95</cell><cell cols="2">94.12 95.81</cell><cell>96</cell><cell cols="4">95.75 95.19 95.12 94.44</cell><cell>95.5</cell><cell cols="2">95.19 95.21(+3.23)</cell><cell>4.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58)</cell><cell>5.44</cell></row><row><cell>MAE 800 (4h)</cell><cell cols="4">94.81 94.44 94.56 94.81</cell><cell>94</cell><cell>94</cell><cell>94.38</cell><cell>94</cell><cell cols="3">94.88 93.69 94.35(+2.37)</cell><cell>5.65</cell></row><row><cell>MAE 1600 (8h)</cell><cell cols="11">94.38 94.19 95.12 95.19 94.44 93.94 94.12 94.19 95.44 93.94 94.49(+2.51)</cell><cell>5.51</cell></row><row><cell>DAMA 500 (5h)</cell><cell>95</cell><cell cols="10">94.44 95.75 95.69 94.69 94.69 95.69 94.69 95.25 94.81 95.07(+3.09)</cell><cell>4.93</cell></row><row><cell>DAMA 1000 (10h)</cell><cell>94.5</cell><cell cols="10">94.06 95.75 95.44 94.69 94.44 94.44 94.38 95.25 94.25 94.72(+2.74)</cell><cell>5.28</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Noisy-170k dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data2vec 800 (29h)</cell><cell cols="2">92.25 91.06</cell><cell>92.5</cell><cell cols="4">92.69 91.94 91.56 92.88</cell><cell>91.5</cell><cell cols="3">92.25 91.31 91.99(+0.01)</cell><cell>8.01</cell></row><row><cell>MOCO-v3 500 (48h)</cell><cell cols="11">95.38 94.56 95.94 95.94 95.81 95.38 95.62 95.25 95.81 95.56 95.52(+3.54)</cell><cell>4.48</cell></row><row><cell>MAE 800 (34h)</cell><cell>94.81</cell><cell>93.5</cell><cell>95</cell><cell cols="8">94.38 94.88 93.69 93.94 93.81 94.88 93.69 94.25(+2.27)</cell><cell>5.75</cell></row><row><cell cols="2">DAMA 500 (35h) 94.88 Config</cell><cell></cell><cell>Value</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Config</cell><cell></cell><cell></cell><cell>Value</cell><cell></cell></row><row><cell>image size</cell><cell></cell><cell cols="2">128?128?7</cell><cell></cell><cell></cell><cell cols="2">image size</cell><cell></cell><cell></cell><cell></cell><cell>128?128?7</cell><cell></cell></row><row><cell>patch size</cell><cell></cell><cell cols="2">16?16</cell><cell></cell><cell></cell><cell cols="2">patch size</cell><cell></cell><cell></cell><cell></cell><cell>16?16</cell><cell></cell></row><row><cell>batch size</cell><cell></cell><cell></cell><cell>512</cell><cell></cell><cell></cell><cell cols="2">batch size</cell><cell></cell><cell></cell><cell></cell><cell>512</cell><cell></cell></row><row><cell>epochs</cell><cell></cell><cell></cell><cell>500</cell><cell></cell><cell></cell><cell cols="2">epochs</cell><cell></cell><cell></cell><cell></cell><cell>150</cell><cell></cell></row><row><cell>optimizer</cell><cell></cell><cell></cell><cell>Adam</cell><cell></cell><cell></cell><cell cols="2">optimizer</cell><cell></cell><cell></cell><cell></cell><cell>Adam</cell><cell></cell></row><row><cell>base learning rate</cell><cell></cell><cell cols="2">1.5e-04</cell><cell></cell><cell></cell><cell cols="3">Base learning rate</cell><cell></cell><cell></cell><cell>1e-02</cell><cell></cell></row><row><cell>min learning rate</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell cols="3">min learning rate</cell><cell></cell><cell></cell><cell>1e-5</cell><cell></cell></row><row><cell>weight decay</cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell cols="2">weight decay</cell><cell></cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell></row><row><cell>learning rate schedule</cell><cell></cell><cell cols="2">cosine decay</cell><cell></cell><cell></cell><cell cols="4">learning rate schedule</cell><cell></cell><cell>cosine decay</cell><cell></cell></row><row><cell>warmup epochs</cell><cell></cell><cell></cell><cell>40</cell><cell></cell><cell></cell><cell cols="3">warmup epochs</cell><cell></cell><cell></cell><cell>5</cell><cell></cell></row><row><cell>augmentation</cell><cell></cell><cell cols="3">RandomResizedCrop</cell><cell></cell><cell cols="2">augmentation</cell><cell></cell><cell></cell><cell cols="3">RandomResizedCrop</cell></row><row><cell>K-blocks/?</cell><cell></cell><cell></cell><cell>6/2</cell><cell></cell><cell></cell><cell cols="5">droppath/reprob/mixup/cutmix</cell><cell>0.1/0.25/0.8/1.0</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The im algorithm: A variational approach to information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">data2vec: A general framework for self-supervised learning in speech, vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiantong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.03555</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for self-supervised learning of speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12449" to="12460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hangbo</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08254</idno>
		<title level="m">Beit: Bert pre-training of image transformers</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="9650" to="9660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno>PMLR, 2020. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Self-supervised learning for analysis of temporal and morphological drug effects in cancer cell imaging data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Dmitrenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><forename type="middle">Miguel</forename><surname>Masiero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Zamboni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">How well do self-supervised models transfer?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Ericsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henry</forename><surname>Gouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5414" to="5423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.06377</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Completer: Incomplete multi-view clustering via contrastive prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijie</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanbiao</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Whole-brain tissue mapping toolkit using large-scale highly multiplexed immunofluorescence imaging and deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragan</forename><surname>Maric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jahandar</forename><surname>Jahanipour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Xiaoyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hien</forename><surname>Mobiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Van Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kedar</forename><surname>Sedlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badrinath</forename><surname>Grama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roysam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">wav2vec: Unsupervised pre-training for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05862</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Miscell: An efficient self-supervised learning approach for dissecting single-cell transcriptome</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyao</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiani</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Iscience</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">103200</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bialek</surname></persName>
		</author>
		<idno>physics/0004057, 2000. 3</idno>
		<title level="m">The information bottleneck method</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning and the information bottleneck principle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noga</forename><surname>Zaslavsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 ieee information theory workshop (itw)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno>PMLR, 2021. 6</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="page" from="10347" to="10357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Self-supervised learning from a multi-view perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Hung Hubert</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Representation learning with contrastive predictive coding. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised pretraining for transferable quantitative phase image cell segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Chmelik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Jakubicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larisa</forename><surname>Chmelikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaromir</forename><surname>Gumulec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Balvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Provaznik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radim</forename><surname>Kolar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical optics express</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6514" to="6528" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">On mutual information in contrastive learning for visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Mosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.13149,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simmim: A simple framework for masked image modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09886</idno>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Deny</surname></persName>
		</author>
		<idno>PMLR, 2021. 2</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
