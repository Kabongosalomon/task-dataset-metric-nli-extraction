<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Virtual View Selection for 3D Hand Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 Alibaba</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanguang</forename><surname>Wan</surname></persName>
							<email>wanyanguang17@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 Alibaba</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dexin</forename><surname>Zuo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<addrLine>3 Alibaba</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuixia</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Tan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Simon Fraser University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongan</forename><surname>Wang</surname></persName>
							<email>hongan@iscas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Software</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
							<email>yindaz@google.com</email>
							<affiliation key="aff3">
								<address>
									<settlement>Google</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Virtual View Selection for 3D Hand Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D hand pose estimation from single depth is a fundamental problem in computer vision, and has wide applications. However, the existing methods still can not achieve satisfactory hand pose estimation results due to view variation and occlusion of human hand. In this paper, we propose a new virtual view selection and fusion module for 3D hand pose estimation from single depth. We propose to automatically select multiple virtual viewpoints for pose estimation and fuse the results of all and find this empirically delivers accurate and robust pose estimation. In order to select most effective virtual views for pose fusion, we evaluate the virtual views based on the confidence of virtual views using a light-weight network via network distillation. Experiments on three main benchmark datasets including NYU, ICVL and Hands2019 demonstrate that our method outperforms the state-of-the-arts on NYU and ICVL, and achieves very competitive performance on Hands2019-Task1, and our proposed virtual view selection and fusion module is both effective for 3D hand pose estimation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Hand pose estimation plays a key role in many applications to support human computer interaction, such as autonomous driving, AR/VR, and robotics <ref type="bibr" target="#b2">(Erol et al. 2007)</ref>. Given an input image, the goal of hand pose estimation is to estimate the location of hand skeleton joints. While many works take color images as input, methods built upon depth images usually exhibit superior performance <ref type="bibr" target="#b15">(Sun et al. 2015)</ref>. Prior arts often use depth image and 2D CNN networks to regress 3D hand joints, or apply point-net based models <ref type="bibr" target="#b13">(Qi et al. 2017;</ref>) on point clouds converted from the depth using camera intrinsic parameters. Although great progress on hand pose estimation from depth has been made in the past decade, the existing methods still can not achieve satisfactory hand pose estimation results due to the severe viewpoint variations and occlusions caused by articulated hand pose.</p><p>To address the occlusion and viewpoint variation challenges, existing methods often rely on data alignment that transforms the input depth input into a canonical space. However, this process is either done in 2D space <ref type="bibr" target="#b15">(Sun et al. 2015;</ref><ref type="bibr" target="#b20">Ye, Yuan, and Kim 2016)</ref> that do not fully respect the 3D nature of the depth image, or in hand-crafted canonical space, e.g. via PCA  or axis-aligned bounding box <ref type="bibr" target="#b6">(Ge et al. 2016)</ref>, that are not engaged in a joint optimization with the full model and thus the performance may not be at its best. In contrast, learning based data alignment is more optimal as demonstrated in many previous work <ref type="bibr" target="#b10">(Jaderberg et al. 2015)</ref>, and back to the domain of pose estimation, this is mostly achieved in automated best viewpoint selection in multiple camera system <ref type="bibr" target="#b12">(Pirinen, G?rtner, and Sminchisescu 2019;</ref><ref type="bibr" target="#b4">G?rtner, Pirinen, and Sminchisescu 2020)</ref> via reinforcement learning. However, in the scenario when only one single depth is available, reinforcement learning does not typically perform well due to the limited inputs, and there is limited research study if viewpoint selection is necessary and possible with a single input depth.</p><p>We claim that viewpoint selection is very important for 3D hand pose estimation even from just a single depth. As a perspective projection of the 3D geometry, a depth image can be projected into the 3D space as a point cloud and rendered into depth maps from another viewpoints for 3D hand pose estimation. On one hand, the same amount of error on 2D perspective views may correspond to dramatically different errors in 3D space. One the other hand, machine learning models may favor some typical viewpoints than the others (See <ref type="figure" target="#fig_0">Fig. 1</ref> for an example). Therefore, finding the proper virtual viewpoint to re-project the given single input depth could be critical to further improve hand pose estimation, which has been well-studied in terms of architecture.</p><p>In this paper, we propose a novel virtual view selection and fusion module for 3D hand pose estimation methods from single depth, which can be easily integrated into the existing hand pose estimation models to enhance the performance. Instead of selecting just one best viewpoint, we propose to automatically select virtual viewpoints for pose estimation and fuse the results of all views, and find this empirically delivers accurate and robust pose estimation. To achieve this, we re-render the input depth into all candidate viewpoints, and train a viewpoint selection network to evaluate the confidence of each virtual viewpoint for pose estimation. We find this empirically works well but slows down the run-time speed when the viewpoint candidate pool is large. To alleviate the computation issue, we adopt network distillation and show that it is possible to predict the view confidence without explicitly re-project the input depth.</p><p>The contributions of our method can be summarized as follows: We propose a novel deep learning network to predict 3D hand pose estimation from single depth, which renders the point cloud of the input depth to virtual multi-view, and get 3D hand pose by fusing the 3D pose of each view. We then show that the view selection can be done efficiently without sacrificing run-time via network distillation. Extensive experiments on hand pose benchmarks demonstrate that our method achieves the state-of-the-art performance. The code is available in project webpage https://github.com/ iscas3dv/handpose-virtualview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The 3D hand pose estimation methods take depth images as input and estimate the locations of hand skeleton joints. Prior arts include <ref type="bibr" target="#b11">(Moon, Chang, and Lee 2018;</ref><ref type="bibr" target="#b7">Ge, Ren, and Yuan 2018;</ref><ref type="bibr" target="#b14">Rad, Oberweger, and Lepetit 2018;</ref><ref type="bibr" target="#b18">Wan et al. 2018;</ref><ref type="bibr" target="#b19">Xiong et al. 2019)</ref>. <ref type="bibr" target="#b7">(Ge, Ren, and Yuan 2018)</ref>, <ref type="bibr" target="#b1">(Deng et al. 2020)</ref> and <ref type="bibr" target="#b11">(Moon, Chang, and Lee 2018)</ref> use 3D representation of depth to estimate the hand pose, and (Wan et al. 2018), <ref type="bibr" target="#b14">(Rad, Oberweger, and Lepetit 2018)</ref> and <ref type="bibr" target="#b19">(Xiong et al. 2019</ref>) use 2D representation of depth to get the hand pose. Anchor-to-Joint Regression Network (A2J) <ref type="bibr" target="#b19">(Xiong et al. 2019)</ref> can predict accurate pose with efficiency, which sets up dense anchor points on the image and obtains the final joint locations by weighted joint voting of all anchor points. Although impressive results are obtained with these prior arts, these networks perform worse under occlusion or severe viewpoint variation.</p><p>The most related work to us is Ge et al. <ref type="bibr" target="#b6">(Ge et al. 2016</ref>), which shares the same thoughts with us that the input viewpoint may not be ideal and projects the input depth into the front, side and top view for pose estimation. However, the number of the selected virtual views is fixed (i.e. 3), and the view selection strategy is hand-crafted but not trained in an end-to-end manner. Different to them, we proposed a learnable virtual view selection and fusion module for 3D hand pose estimation, which can adaptively select informative virtual viewpoints for point cloud rendering, and fuse the estimated 3D poses of these views.</p><p>In the realm of pose estimation, viewpoint selection is also achieved in multiple camera system using reinforcement learning <ref type="bibr" target="#b12">(Pirinen, G?rtner, and Sminchisescu 2019;</ref><ref type="bibr" target="#b4">G?rtner, Pirinen, and Sminchisescu 2020)</ref>. These works use reinforcement learning methods to select a view sequence suitable for 3D pose estimation. However, they all require a multi-view capture setup, and cannot be used for a single input depth image. Moreover, these methods are timeconsuming, because views are selected in sequence, and thus reduce the inference efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview</head><p>In this section, we introduce our virtual view selection and fusion approach for 3D hand pose estimation from a single depth image <ref type="figure" target="#fig_1">(Fig. 2)</ref>.</p><p>We first convert a depth image into 3D point clouds, and uniformly set up candidate virtual views on the spherical surface centered in the hand point clouds. The point cloud is then rendered into candidate views as depth maps, which are then fed into a network to predict the confidence. A 3D pose estimation network then predicts the 3D hand pose from view with top-N confidence 1 , and finally fuses the pose with regard to the confidence to achieve accurate 3D hand pose results. To reduce the computational cost, we also design an efficient lightweight network by model distillation to predict the view confidence from the input depth itself, which saves the computation cost of point cloud rendering if the pool of candidate view is large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Virtual Multi-View Hand Pose Estimation</head><p>We first explain the idea of virtual multi-view hand pose estimation. Inspired by Ge et al. <ref type="bibr" target="#b6">(Ge et al. 2016</ref>) that the original camera view may not be optimal for hand pose estimation, we hypothesize that denser virtual view sampling should be more beneficial and propose to exploit rendered depth on multiple virtual views to estimate 3D hand pose ( <ref type="figure" target="#fig_2">Fig. 3</ref>). Candidate Virtual Views We first define a set of virtual camera views to re-render the input depth map. Specifically, we first convert the depth map of the hand into point clouds, and uniformly sample a series of virtual cameras on the spherical surface centered in the hand point clouds. Note that large camera rotation may cause severe occlusion issue for depth rendering, and thus we only keep the cameras close to the input camera view. In practice, we keep 25 virtual cameras uniformly sampled from the zenith angle in [??/3, ?/3] and the azimuth angle in [??/3, ?/3] on the sphere. Refer to <ref type="figure" target="#fig_5">Fig. 8</ref> in Sec. 4.3 for illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Joints Render</head><p>Virtual View Depth Map Rendering The point cloud from the input depth image can now be re-projected to each of the virtual cameras. Note that since the depth images does not provide a complete hand shape, the rendered depth may be partially incomplete or contain wrong occlusion. However, we found empirically that these does not confuse the pose estimation network when the virtual camera is not too far from the input. Therefore, we did not complete the rendered image as the face rendering methods <ref type="bibr" target="#b22">(Zhou et al. 2020;</ref><ref type="bibr" target="#b3">Fu et al. 2021)</ref>. We also implemented a parallel rendering process using CUDA to speed up the rendering process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Hand Pose Estimation from Single Depth</head><p>For each rendered depth image, we use the Anchor-to-Joint regression network (A2J) <ref type="bibr" target="#b19">(Xiong et al. 2019)</ref> as the backbone for 3D hand pose estimation for its great run-time efficiency and competitive accuracy. In practice, any other 3D hand pose estimation models from depth can be taken as the backbone.  <ref type="figure">Figure 4</ref>: Confidence network. The network uses CNN to extract features from the intermediate features of pose estimation net, and then uses multi-head attention to fuse the multi-views visual features. We use FC and Softmax to map the fused multi-view features to confidence of each view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Head Attention</head><p>We use the loss function from A2J and briefly explain here for self-contain. The loss function L A2J of A2J consists of the objective loss L obj and the informative anchor point surrounding loss L info . The objective loss L obj aims to minimizing the error between the predicted hand joints with the anchor points and the ground truth hand joints, and the informative anchor point surrounding loss L info aims to selecting informative anchor points located around the joints to enhance the generalization ability. The loss function L A2J can be formulated as follows</p><formula xml:id="formula_0">L A2J = ?L obj + L info (1)</formula><p>where ? is the loss weight to balance L obj and L info , and it is set to ? = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-View Pose Fusion</head><p>In the end, we run a fusion stage to combine the predicted 3D hand poses from virtual camera views. In the very basic version, we transform the 3D joints of each view to the original camera coordinate system with the camera extrinsic parameters, and get the final hand pose prediction by averaging the transformed hand poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Virtual View Selection</head><p>The method proposed in the previous section indeed significantly improves the hand pose estimation accuracy, however suffers from run-time efficiency issue. Essentially, the virtual multi-view baseline runs single view pose estimation network multiple times, thus needs linearly proportional computation power w.r.t the size of virtual camera pool size (e.g. 25 in our case). In another perspective, the pose prediction on some virtual views may not be great, e.g. due to sparsity or wrong occlusion, and may further hurt the overall performance when fused with an average. Therefore, we propose a view selection algorithm that can choose a small number of high impacting camera views without the necessity of running all of them into the pose estimation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Confidence Prediction</head><p>The key of our view selection algorithm is a confidence network that evaluates the importance of each candidate virtual view ( <ref type="figure">Fig. 4)</ref>. To save the computation cost, the confidence network takes the highlevel feature from A2J as input. It then extracts features using convolutional neural network, and then uses the multihead attention in <ref type="bibr" target="#b18">(Vaswani et al. 2017)</ref>   <ref type="figure">Figure 5</ref>: View selection with confidence. We use multi-view depth map data to train a "teacher" confidence network for view selection based on confidence (top), and we train a "student" lightweight confidence network for efficient view selection through network distillation (bottom).</p><p>features. The multi-head attention mechanism can direct the network to focus on the views which play more important role, e.g. provide more accurate pose or complementary information for fusion. Finally, we use a fully connected layer to map the feature of each view to a scalar, i.e. the confidence of each view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Virtual View Selection and Fusion</head><p>With the predicted confidence for each virtual camera viewpoint, we can perform view selection by picking the views with top-N confidence from all M candidate views (N &lt; M ). Empirically, larger N tends to produce more accurate pose (See Sec. 4.3) but needs more computation. In practice, we found that picking N = 3 in M = 25 virtual views can already beat most of the SoTA methods.</p><p>With the pose estimated from selected views, we can fuse hand poses and get final output? as:</p><formula xml:id="formula_1">J = N i=1 c i (R i?i + t i )<label>(2)</label></formula><p>where? i is the predicted hand joints of the i-th selected view, c i is the confidence of the i-th selected view after softmax, and [R i , t i ] is the known camera extrinsic parameters of the i-th selected view.</p><p>Joint Training During the training stage, we do not give direct supervision to the confidence of each view since they are simply unknown. Instead, we jointly train confidence network with the pose estimation network, which is supervised only on the final pose accuracy. The joint loss L J is formulated as:</p><formula xml:id="formula_2">L J = K i=1 L ? ( J i ?? i )<label>(3)</label></formula><p>where? i is the estimated i-th joint by multi-view pose fusion (See Eq. <ref type="formula" target="#formula_1">(2)</ref>), J i is the ground truth location of the ith joint, K is the number of hand joint, and L ? (?) is the smooth L1 loss function.</p><p>Note that the behavior of confidence network might be different with varying N since the predicted confidences have been used at both selection and fusion stage. For the selection stage, only the ranking of the confidence matters; and for the fusion stage, the precise value of confidences of the chosen views also matters. Therefore, for ideal performance, the confidence network should be trained for each specific N . In practice, however, a single model trained with N = M can still work for most of the varying N , with slightly worse but still reasonable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation for Efficient Confidence Prediction</head><p>With view selection, we are able to reduce the number of forward pass of the pose estimation network from the total number of virtual views M to a much smaller number of selected views N . This reduces enormous computation, but we still observe noticeable frame per second (FPS) drops mostly due to two reasons: 1) The input depth still needs to be rendered to all candidate camera views for confidence estimation. 2) The confidence network still needs to process all the rendered depth, though designed to be light-weighted but still costly to run the multi-head attention.</p><p>We resort to network distillation to alleviate this run-time issue. Specifically, we take our confidence network as the teacher network, and train an even more light-weighted student network (a ResNet-18 followed by a fully connected layer). More importantly, the student network takes only the original depth as the input and directly outputs confidence for all M = 25 views. This effectively removes the necessity of re-rendering the input depth to all virtual cameras.</p><p>To train the student network, we directly take the confi-dence predicted by teacher network as the ground truth. The loss is defined as</p><formula xml:id="formula_3">L light = M i=1 L ? (?(c i ?? i ))<label>(4)</label></formula><p>where ? is the scaling factor set to 100,? i is the confidence ground truth from teacher network, i.e. the multi-head attention network from M views, and c i is the student network output.</p><p>Once the training is done, the teacher network is no longer needed during the inference, and student network can support the view selection and fusion in an efficient way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>We train and evaluate our models on a workstation with two Intel Xeon Silver 4210R, 512GB of RAM and an Nvidia RTX3090 GPU. Our models are implemented within Py-Torch. Adam optimizer is used; the initial learning rate is set to 0.001 and is decayed by 0.9 per epoch. In order to conduct data augmentation, we randomly scale the cropped depth map, jitter the centroid of the point cloud, and randomly rotate the camera when rendering multi-view depth. For all smooth L1 loss, the switch point between quadratic and linear is set to 1.0.</p><p>Our network consists of the 3D pose estimation network (i.e. A2J), the teacher confidence network and the lightweight student confidence network. The network input is 176 ? 176 hand region cropped from the input depth, and we use ResNet-50 as the backbone of A2J. We first train the 3D pose estimation network and the teacher confidence network together, the loss can be formulated as:</p><p>L viewsel = L A2J + ?L J (5) where ? = 0.1 is the factor to balance the loss terms.</p><p>Then we fix the parameters of the two networks and train the lightweight student confidence network with L light .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metric</head><p>NYU Hand Pose Dataset (NYU) <ref type="bibr" target="#b17">(Tompson et al. 2014)</ref> contains 72,757 frames for training and 8,252 frames for testing. 36 hand joints are annotated, but we use only a subset of 14 hand joints for evaluations following the same evaluation protocol in <ref type="bibr" target="#b17">(Tompson et al. 2014)</ref>. Evaluation Metric We evaluate the hand pose estimation performance using standard metrics proposed in <ref type="bibr" target="#b16">(Tang et al. 2014)</ref>, i.e. mean joint error and the percentage of test examples that have all predicted joints within a given maximum distance from the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU ICVL</head><p>HandPointNet  10.54 6.93 DenseReg <ref type="bibr" target="#b18">(Wan et al. 2018)</ref> 10.21 7.24 P2P <ref type="bibr" target="#b7">(Ge, Ren, and Yuan 2018)</ref> 9.05 6.33 A2J <ref type="bibr" target="#b19">(Xiong et al. 2019)</ref> 8.61 6.46 V2V <ref type="bibr">(Moon, Chang, and Lee 2018) 8.42</ref> 6.28 AWR <ref type="bibr" target="#b9">(Huang et al. 2020)</ref> 7.37 5.98 Ours-1view 7.34 5.16 Ours-3views 6.82 4.86 Ours-9views 6.53 4.77 Ours-15views 6.41 4.76 Ours-25views 6.40 4.79 <ref type="table">Table 1</ref>: Comparison mean joint 3D error (mm) and ranking result with state-of-art methods on NYU dataset and ICVL dataset. "Ours-1view", "Ours-3views", "Ours-9views" and "Ours-15views" are the results of our method with selected 1, 3, 9 and 15 views from 25 uniformly sampled views, respectively. "Outs-25views" denotes the results of our method with 25 uniformly sampled views. In this experiment, we use our "student" lightweight confidence network for view selection and pose fusion.</p><p>We first compare our method with the state-of-the-art methods on NYU dataset and ICVL dataset. DenseReg (Wan et al. 2018), A2J <ref type="bibr" target="#b19">(Xiong et al. 2019</ref>) and AWR <ref type="bibr" target="#b9">(Huang et al. 2020</ref>) directly use the depth map for pose estimation. Hand-PointNet , P2P <ref type="bibr" target="#b7">(Ge, Ren, and Yuan 2018)</ref> and V2V <ref type="bibr" target="#b11">(Moon, Chang, and Lee 2018)</ref> <ref type="bibr" target="#b8">(Hr?z, Kanis, and Kr?oul 2021)</ref> use the 3D representation of the depth map for pose estimation. <ref type="table">Table 1</ref> shows the mean joint error. <ref type="figure" target="#fig_4">Fig. 6</ref> shows the percentage of success frames over different error thresholds and the error of each joint. We do not show AWR in <ref type="figure" target="#fig_4">Fig. 6</ref> because the best prediction results on NYU dataset are not released by AWR. Note that with just 1 view selected, we already outperform all the other methods. And when more view selected, the performance keeps improving. This clearly indicates that our view selection is effective in P2P V2V A2J 1 view 3 views 9 views 15 views 25 views Ground truth <ref type="figure">Figure 7</ref>: Comparison visualization results with state-of-art methods on NYU dataset. "1 view", "3 views", "9 views" and "15 views" are the results of our method with selected 1, 3, 9 and 15 views from 25 uniformly sampled views, respectively. "25 views" denotes the results of our method with 25 uniformly sampled views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mean error (mm)</head><p>V2V <ref type="bibr" target="#b11">(Moon, Chang, and Lee 2018)</ref> 15.57 AWR <ref type="bibr" target="#b9">(Huang et al. 2020)</ref> 13.76 A2J <ref type="bibr" target="#b19">(Xiong et al. 2019)</ref> 13.74 Rokid  13  <ref type="table">Table 2</ref>: Comparison with state-of-art methods on Hands19-Task1. We show the mean joint error on the test dataset split "Extrapolation", which aims to evaluating the model generalization performance and is the main evaluation metric on Hands19-Task1. "Ours-1view", "Ours-3views", "Ours-9views", "Ours-15views" and "Ours-25views" have the same meaning as shown in <ref type="table">Table 1</ref> finding a better virtual view for pose estimation, and more views are benefical through the confidence based fusion. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the qualitative comparison to other methods on a few testing examples of NYU dataset. Our method performs especially better on views with heavy occlusion and missing depth, e.g. the 3rd row. It is as expected since the rerender in a perpendicular virtual camera looking at the palm might be better to interpret the input depth.</p><p>We also compare our method with the state-of-the-art 3D hand pose estimation methods on Hands19-Task1. The details and results of state-of-the-art methods are cited from <ref type="bibr" target="#b0">(Armagan et al. 2020)</ref>. Rokid ) trains 2D CNN to regress joints with additional synthetic data. Compared to the used A2J backbone in our method, A2J <ref type="bibr" target="#b19">(Xiong et al. 2019)</ref> reports results using higher resolution depth as input (384 ? 384), deeper backbone (ResNet-152), and 10 backbone model ensemble. AWR <ref type="bibr" target="#b9">(Huang et al. 2020</ref>) also provides results with model ensemble. <ref type="table">Table 2</ref> shows the performance comparison. we can observe that our method with 3 selected views outperform other methods. Our method with 1 selected view performs slightly worse than AWR, A2J and Rokid, which can be due to using model ensemble in AWR and A2J and using additional synthetic training data in Rokid .</p><p>More experimental results can be found in the supplementary document. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of View Number</head><p>In order to investigate the effect of the number of virtual views with our method, we compare the hand pose performance on NYU dataset, ICVL dataset and Hands19-Task1 dataset by generating 3, 9, 15, 25 uniformly sampled virtual views ("UNIFOMR" column of <ref type="table">Table 3</ref>). We also visualize the results of our method using different numbers of views on NYU dataset in <ref type="figure" target="#fig_0">Fig. 13</ref> We observe that the hand pose estimation error decreases as the number of virtual views increases. Therefore, the virtual multi-view can boost the hand pose performance. The visualization results show that when using 25 views, the estimated wrist joints and joints in the missing depth value area are more accurate than using 3 views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of View Selection</head><p>In this section, we investigate if the confidences are effective for view selection. We use our method to select 3, 9, 15 views from 25 candidate views and  <ref type="table">Table 3</ref>: Comparison of mean joint error using uniform sampling and view selection on NYU, ICVL and Hands2019-Task1. "UNIFORM" denotes using uniformly sampled views. "SELECT" denotes selecting views from 25 uniformly sampled views with the "teacher" confidence network. "LIGHT" denotes selecting views from 25 uniformly sampled views with the "student" lightweight confidence network. compare to uniform sampling as illustrated in <ref type="figure" target="#fig_5">Fig. 8</ref>. Though simple, the uniform sampling is actually a very strong strategy since it roughly guarantee at least 1 views close to some good candidate. The results on NYU dataset, ICVL dataset and Hands19-Task1 dataset are shown in <ref type="table">Table 3</ref>. For each experiment, we show the performance using teacher and student confidence network ("SELECT": teacher network, "LIGHT": student network). Our view selection consistently outperforms uniform selection in all the settings, which indicates that the predicted confidence is effective for view selection. Though using student network results in slightly worse performance than the teacher network, the overall computation cost is significantly reduced. As shown in Table 4, using student network almost doubles FPS. On ICVL dataset, the student network performs better than the teacher network, which may be due to low annotation quality and small scale of ICVL. More comparisons to random sampled views can be found in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Different Multi-View Fusion</head><p>We now evaluate the confidence based fusion. In   pare to direct average without the confidence on NYU dataset, ICVL dataset and Hands2019-Task1 dataset. The confidence based fusion achieves better performance on 3 datasets than direct average, which shows that the confidence is also beneficial to guide the fusion stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a deep learning network to learn 3D hand pose estimation from single depth, which renders the point cloud to virtual multi-view, and get more accuracy 3D hand pose by fusing the 3D pose of each view. Meanwhile, the confidence network we built uses multi-head attention to calculate the confidence of each virtual view. The confidence is used to improve the accuracy of 3D reconstruction during fusion and view selection. In order to obtain the confidence of each view efficiently, we obtain a lightweight confidence network using network distillation. Experiments on the main benchmark datasets demonstrate the effectiveness of our proposed method. Our method can also inspire several related researches such as scene parsing and reconstruction from depth, etc.</p><p>In this section, we elaborate on the network architecture of the "teacher" confidence network in Section 3.3 and <ref type="figure">Fig. 4</ref> of our main submission. In order to obtain the confidence of each view, our method first uses a convolutional neural network to extract features, then uses multi-head attention <ref type="bibr" target="#b18">(Vaswani et al. 2017)</ref> to fuse multi-view features, and finally converts the fused multiview visual features into the confidence of each view using a fully connected layer. <ref type="figure" target="#fig_0">Fig. 10</ref> shows the details of network architecture of convolutional neural network for feature extraction. The network takes the low-level feature from A2J as input, and adopts 3 convolutional layers for feature encoder. The network extracts a 256-dimensional feature for each view, and then feeds the extracted features of multiple views to a multihead attention module <ref type="bibr" target="#b18">(Vaswani et al. 2017)</ref> for feature fusion. In the multi-head attention module, we set the number of heads n head to 1, d q , d k , d v are the dimensions of Q, K and V (defined as <ref type="bibr" target="#b18">(Vaswani et al. 2017)</ref>), and they are all set to 64. <ref type="bibr">(64,</ref><ref type="bibr">11,</ref><ref type="bibr">11)</ref> (64, 11, 11) (64, 5, 5) (128, 5, 5) <ref type="figure" target="#fig_0">Figure 10</ref>: Network details of the convolutional neural network for feature extraction. 'K' stands for 'kernel size', 'S' stands for 'stride', and 'P' stands for 'padding'.</p><formula xml:id="formula_4">MaxPool K=3 S=2 (128, 2, 2) Conv2D K=2 S=1 P=0 (256, 1, 1) Conv2D K=3 S=1 P=1 BatchNorm ReLU BatchNorm ReLU MaxPool K=3 S=2 Conv2D K=3 S=1 P=1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">More Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Comparison between Uniform Sampled Views and Random Sampled Views</head><p>In our paper, we use uniform sampling on a sphere to obtain the candidate virtual views. In order to investigate the effect of random sampled views on hand pose estimation, we compare the hand pose estimation performance with randomly sampled virtual views and uniformly sampled views on a sphere. For the experiments of randomly sampled views, the views are randomly sampled from the zenith angle in [??/3, ?/3] and the azimuth angle in [??/3, ?/3] on the sphere, and we adopt the same number of virtual cameras as uniformly sampled views. <ref type="table">Table 6</ref> compares the mean joint error of uniform sampling and random sampling on NYU, ICVL and Hands2019-Task1. We can observe that the performance with random sampled views is inferior to the performance with uniform sampled views. Conceptually, the patterns of random sampled views are more complex than those of uniform sampling, and make the confidence prediction of random sampled views very hard. Therefore, we choose to use uniform sampling to obtain the candidate virtual views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Detailed Results on the ICVL Dataset</head><p>More Comparisons with State-of-the-art Methods <ref type="figure" target="#fig_0">Fig. 11</ref> shows the error of each joint and the percentage of success frames over different error thresholds. Since the best prediction results by AWR on ICVL dataset are not released, we do not show AWR <ref type="bibr" target="#b9">(Huang et al. 2020)</ref> in <ref type="figure" target="#fig_0">Fig. 11</ref>. We can observe that our proposed method outperforms the other methods by a large margin on the ICVL dataset. <ref type="figure" target="#fig_0">Fig. 12</ref> shows the qualitative comparison to the SoTA methods on the ICVL dataset. Especially, our method performs better at the fingertips than the other methods, e.g. the 1st row, and also performs well under severe occlusions, e.g. the 2nd row. <ref type="figure" target="#fig_0">Fig. 13</ref> shows the results of our method using different numbers of views on the ICVL dataset. We can observe that using 25 uniformly sampled views can achieve better hand pose estimation performance than using 3 uniformly sampled views. Especially, the estimated fingertip joints using 25 uniformly sampled views are better. <ref type="table">Table 6</ref>: Comparison of mean joint error using uniform sampling and random sampling on NYU, ICVL and Hands2019-Task1. "UNIFORM" denotes using uniformly sampled views. "RANDOM" denotes using randomly sampled views. <ref type="figure" target="#fig_0">Figure 11</ref>: Comparison of our proposed method with state-of-the-art methods on the ICVL dataset. Left: 3D distance errors per hand joints. Right: the percentage of success frames over different error thresholds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of View Number</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P2P</head><p>V2V A2J 1 view 3 views 9 views 15 views 25 views Ground truth <ref type="figure" target="#fig_0">Figure 12</ref>: Comparison visualization results with state-of-art methods on ICVL dataset. "1 view", "3 views", "9 views" and "15 views" are the results of our method with selected 1, 3, 9 and 15 views from 25 uniformly sampled views, respectively. "25 views" denotes the results of our method with 25 uniformly sampled views.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of view selection for 3D hand pose estimation. The view of the original depth image may not be suitable for pose estimation. We select suitable views for pose estimation from the uniformly sampled virtual views. ? and ? represent the azimuth angle and the elevation angle of candidate virtual views, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of our virtual view selection and fusion pipeline for 3D hand pose estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Our virtual multi-view hand pose estimation baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>ICVL</head><label></label><figDesc>Hand Pose Dataset (ICVL) (Tang et al. 2014) contains 331,006 frames for training and 1,596 frames for testing. 16 hand joints are annotated. Task 1 of Hands19 Challenge Dataset (Hands19-Task1) (Armagan et al. 2020) contains 175,951 training depth images from 5 subjects and 124,999 testing depth images from 10 subjects, in which 5 subjects overlap with the training set. This dataset is very challenging because of its exhaustive coverage of viewpoints and articulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Comparison of our proposed method with stateof-the-art methods on NYU dataset. Left: mean joint error per hand joint. Right: the percentage of success frames over different error thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Uniform sampling schemes. 25 virtual cameras are set uniformly. Red cameras are candidate virtual views. The figures from left to right show how 3, 9, 15, and 25 candidate virtual views are uniformly sampled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Comparison the visualization results of our method using different numbers of views on NYU dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>, we com-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="4">: FPS comparison of uniform sampling and view</cell></row><row><cell cols="4">selection on NYU dataset. "UNIFORM", "SELECT",</cell></row><row><cell cols="4">"LIGHT" have the same meaning as shown in Table 3.</cell></row><row><cell>Component</cell><cell cols="3">NYU ICVL Hands2019-Task1</cell></row><row><cell>w confidence</cell><cell>6.40</cell><cell>4.79</cell><cell>12.55</cell></row><row><cell cols="2">w/o confidence 6.58</cell><cell>4.88</cell><cell>12.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison of mean 3D joint error using different multi-view fusions on 25 uniformly sampled views.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">N is a hyper-parameter that controls the run-time efficiency and pose accuracy.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">uniformly sampled views 25 uniformly sampled views Ground truth Figure 13: Qualitative comparison of our method using different numbers of views on the ICVL dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This work was supported by National Natural Science Foundation of China (No. 61473276), Beijing Natural Science Foundation (L182052), and Distinguished Young Researcher Program, ISCAS.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Network Details of Confidence Network</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Measuring generalisation to unseen viewpoints, articulations, shapes and objects for 3D hand pose estimation under hand-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Armagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hampali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="85" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised learning for single depth-based hand shape recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="532" to="545" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision-based hand pose estimation: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bebis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nicolescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Twombly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="52" to="73" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-fidelity face manipulation with extreme poses and expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="2218" to="2231" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning for Active Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10835" to="10844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hand PointNet: 3D Hand Pose Estimation using Point Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust 3D Hand Pose Estimation in Single Depth Images: from Single-View CNN to Multi-View CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Point-to-point regression pointnet for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hand Pose Estimation in the Task of Egocentric Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hr?z</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kr?oul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="10533" to="10547" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Awr: Adaptive weighting regression for 3d hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11061" to="11068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">V2v-posenet: Voxel-to-voxel prediction network for accurate 3d hand and human pose estimation from a single depth map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>G?rtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3912" to="3915" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature mapping for learning fast and accurate 3d pose inference from synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4663" to="4672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cascaded hand pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="824" to="832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Latent regression forest: Structured estimation of 3d articulated hand posture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3786" to="3793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Realtime continuous pose recovery of human hands using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Perlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">169</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dense 3d regression for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A2j: Anchor-to-joint regression network for 3d articulated pose estimation from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="793" to="802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatial attention deep net with partial PSO for hierarchical hybrid hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">HandAugment: A simple data augmentation method for depth-based 3D hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotate-and-render: Unsupervised photorealistic face rotation from single-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5911" to="5920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<title level="m">NYU ICVL Hands2019-Task1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">Number of views UNIFORM RANDOM UNIFORM RANDOM UNIFORM RANDOM</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
