<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-Supervised Semantic Segmentation with Visual Words Learning and Hybrid Pooling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixiang</forename><surname>Ru</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Sci-ence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution" key="instit1">National Engineering Research Center for Multimedia Soft-ware</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">JD Explore Academy</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>JD.com</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
							<email>dubo@whu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Institute of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Computer Sci-ence</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hubei Key Laboratory of Multimedia and Network Communication Engineering</orgName>
								<orgName type="institution" key="instit1">National Engineering Research Center for Multimedia Soft-ware</orgName>
								<orgName type="institution" key="instit2">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">JD Explore Academy</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>JD.com</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Zhan</surname></persName>
							<affiliation key="aff3">
								<orgName type="laboratory">LIESMARS</orgName>
								<orgName type="institution">Wuhan University</orgName>
								<address>
									<settlement>Wuhan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Du</surname></persName>
						</author>
						<title level="a" type="main">Weakly-Supervised Semantic Segmentation with Visual Words Learning and Hybrid Pooling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Weakly-Supervised Semantic Segmen- tation ? Visual Words Learning ? Hybrid Pooling ? Semantic Segmentation</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Weakly-Supervised Semantic Segmentation (WSSS) methods with image-level labels generally train a classification network to generate the Class Activation Maps (CAMs) as the initial coarse segmentation labels. However, current WSSS methods still perform far from satisfactorily because their adopted CAMs 1) typically focus on partial discriminative object regions and 2) usually contain useless background regions. These two problems are attributed to the sole image-level supervision and aggregation of global information when training the classification networks. In this work, we propose the visual words learning module and hybrid pooling approach, and incorporate them in the classification network to mitigate the above problems. In the visual words learning module, we counter the first problem by enforcing the classification network to learn fine-grained visual word labels so that more object extents could be discovered. Specifically, the visual words are learned with a codebook, which could be updated via two proposed strategies, i.e. learning-based strategy and memory-bank strategy. The second drawback of CAMs is alleviated with the proposed hybrid pooling, which incorporates the global average and local discriminative information to simultaneously ensure object completeness and reduce background regions. We evaluated our methods on PASCAL VOC 2012 and MS COCO 2014 datasets. Without any extra saliency prior, our method achieved 70.6% and 70.7% mIoU on the val and test set of PASCAL VOC dataset, respectively, and 36.2% mIoU on the val set of MS COCO dataset, which significantly surpassed the performance of state-of-theart WSSS methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic segmentation, aiming at assigning a specific label for each pixel in an image, is a fundamental and hot topic in computer vision. With the rapid development of deep learning, semantic segmentation based on deep neural networks has dominated the past decades <ref type="bibr" target="#b38">(Long et al., 2015;</ref><ref type="bibr" target="#b9">Chen et al., 2015;</ref><ref type="bibr" target="#b5">Badrinarayanan et al., 2017;</ref><ref type="bibr" target="#b9">Chen et al., 2017)</ref>. However, the datahungry nature of deep models determines that to obtain a segmentation model with fancy performance, a large number of images with well-annotated pixel-level labels are indispensable. Unfortunately, pixel-level labels are usually very costly in both time and money. The empirical statistics in  show that annotating the pixel-level label of an image in the PASCAL VOC dataset <ref type="bibr" target="#b11">(Everingham et al., 2010)</ref> needs about 4 minutes on average, meanwhile annotating the Cityscapes dataset <ref type="bibr">(Cordts et al., 2016)</ref> takes an even longer time, about 90 minutes per image.</p><p>To address the above problem, many researchers have dedicated to devising image segmentation models with weaker and cheaper labels, such as image-level labels <ref type="bibr" target="#b43">Pinheiro and Collobert,</ref> unexpected background discriminative regions Image Ours CAMs <ref type="figure">Fig. 1</ref>: Illustration of the drawbacks of CAMs. Typically, CAMs only discover partial discriminative object regions and adjacent background regions. We argue that these drawbacks are attributed to the sole image-level supervision and aggregation of global information. To mitigate them, in this work, we proposed the visual words learning and hybrid pooling module.</p><p>2015; <ref type="bibr" target="#b1">Ahn and Kwak, 2018;</ref><ref type="bibr" target="#b27">Lee et al., 2021a)</ref>. Prevailing WSSS methods with image-level labels usually adopt a multi-step framework. Specifically, these WSSS methods firstly train classification networks with only image-level labels and use the trained classification networks to generate initial coarse pixel-level labels by class activation mapping <ref type="bibr" target="#b64">(Zhou et al., 2016)</ref>. Then, the coarse pixel-level labels will be further refined by methods like dense CRF <ref type="bibr" target="#b25">(Kr?henb?hl and Koltun, 2011)</ref> and other pixel affinity-based approaches <ref type="bibr" target="#b1">(Ahn and Kwak, 2018;</ref><ref type="bibr" target="#b2">Ahn et al., 2019)</ref> to obtain the refined pseudo labels. Finally, the refined pseudo labels are used to train a regular semantic segmentation model to predict pixel-level labels of test images.</p><p>Prior works have demonstrated that the first step, i.e. generating initial coarse labels, is crucial to the training of segmentation models and the final segmentation performance <ref type="bibr" target="#b8">Chang et al., 2020b;</ref><ref type="bibr" target="#b27">Lee et al., 2021a)</ref>. As aforementioned, most methods train classification networks to produce Class Activation Maps (CAMs) <ref type="bibr" target="#b64">(Zhou et al., 2016)</ref> as the initial coarse labels. However, as illustrated in <ref type="figure">Fig. 1</ref>, there are two typical drawbacks of previous CAMs. Firstly, CAMs usually only discover partial discriminative regions of visual objects. The reason is that CAMs are derived from classification networks, whose purpose is to differentiate different semantic categories. Therefore, to attain discriminability, the classification network will shift attention to the most discriminative regions of visual objects instead of the integral object. Secondly, the activated regions of CAMs often include some un- desired background. This is attributed to that classification networks commonly use global average pooling (GAP) <ref type="bibr" target="#b34">(Lin et al., 2013)</ref> for feature aggregation, which averages information from both foreground objects and background, thus overestimating the size of objects <ref type="bibr" target="#b64">(Zhou et al., 2016)</ref>.</p><p>To tackle the first problem, as illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>, we argue that if the network could be supervised by more fine-grained labels, more object regions will be activated to provide sufficient information for differentiating different classes. Therefore, in this work, we propose the Visual Words Learning (VWL) module for WSSS task with image-level labels. The VWL module generates the visual word labels by using a codebook to encode the feature maps extracted by the CNN backbone. In the training process of the classification network, the network will be forced to jointly learn the image-level labels and visual word labels so that more object regions could be activated. To learn an effective codebook, based on the definition and solution of Bag of Visual Word models (BoVW) <ref type="bibr" target="#b3">(Arandjelovi? et al., 2017;</ref><ref type="bibr" target="#b41">Passalis and Tefas, 2017)</ref>, we devised two strategies for updating the codebook, i.e. learning-based strategy and memory-bank strategy. For the learning-based strategy, the codebook is set as a learnable parameter. By enforcing the encoded visual word features to learn image-level labels, the codebook could learn the latent visual word representations. In practice, we also notice that the learned representations in the codebook are often redundant, which affects the network training and the quality of CAMs. We tackle this problem by regularizing the codebook with DeCov loss <ref type="bibr" target="#b10">(Cogswell et al., 2017)</ref>, which reduces the redundancy of a matrix by minimizing its off-diagonal co-variance values. For the memory-bank strategy, we follow the classic BoVW models <ref type="bibr" target="#b36">(Liu et al., 2019;</ref><ref type="bibr">Gi-daris et al., 2020)</ref>, which take the clustering centroids of features as the codebook. Specifically, we decompose the clustering on the whole training set to each mini-batch iteration and leverage memory-bank strategy <ref type="bibr" target="#b57">(Wu et al., 2018;</ref><ref type="bibr" target="#b65">Zhuang et al., 2019)</ref> to gradually update the codebook. Our experimental results show that, after sufficient updates, the learning-based and memory-bank strategy could both learn codebooks with effective representations of visual words and achieve analogous performance.</p><p>To alleviate the second drawback, inspired by global max-pooling (GMP), which takes the maximum value in each feature map as outputs and tends to underestimate the object sizes <ref type="bibr" target="#b24">(Kolesnikov and Lampert, 2016)</ref>, we proposed a simple yet empirically effective pooling approach, named Hybrid Pooling (HP). Our major motivation of HP is to aggregate the local maximum values so that less background information is involved. In specific, the feature maps are partitioned into multiple bins from coarse to fine levels. For bins in the same level, we pool them separately via max pooling and average the aggregated features so that only local maximums are involved. The features from different levels and the output feature of GAP (to ensure the object completeness) are then averaged as the final outputs. On this account, more discriminative object extents and fewer background regions are preserved in feature maps, which could improve the accuracy of the generated CAMs.</p><p>We conducted extensive experiments to verify the effectiveness of our proposed methods. The experimental results showed that our method significantly improved the quality of generated CAMs. We refined the generated CAMs with IRNet <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref> and trained a DeepLabV2 segmentation network <ref type="bibr" target="#b9">(Chen et al., 2017)</ref> with the refined pseudo labels. Semantic segmentation results on two datasets, <ref type="bibr">PASCAL VOC 2012</ref><ref type="bibr" target="#b11">(Everingham et al., 2010</ref> and MS COCO 2014 <ref type="bibr" target="#b35">(Lin et al., 2014)</ref>, showed that our proposed method could outperform the state-of-the-art methods.</p><p>Overall, our contributions in this work are summarized as follows.</p><p>-We propose the Visual Words Learning (VWL) module. By jointly learning the visual word labels and image-level labels, the network is enforced to discover integral object extents. To encode the visual words, we devise two learning strategies to learn the codebook and empirically verify their efficacy. -We propose Hybrid Pooling (HP), a simple yet effective pooling approach, which incorporates local discriminative information and global information to aggregated less background and more object regions.</p><p>-By incorporating the proposed VWL and HP, we present a new classification network to generate CAMs with higher quality for the WSSS task. Our method achieves new state-of-the-art performance, i.e. 70.6% mIoU on PASCAL VOC 2012 val set and 36.2% mIoU on MS COCO 2014 val set.</p><p>This paper is an improved version of our preliminary work <ref type="bibr" target="#b45">(Ru et al., 2021)</ref>. Compared with the conference version, this work further improves the learning-based strategy and proposes the memory-bank strategy which could learn visual words better. The performance is remarkably improved with these improvements and surpasses the latest state-of-the-art methods. We also conduct further experiments on more datasets to verify the efficacy of our approach.</p><p>The rest of this paper is structured as follows. In Section 2, we briefly introduce some related works on WSSS with image-level labels and improvements on CAMs. The detailed methods are presented in Section 3. We present the experimental settings and results in Section 4. Section 5 concludes our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">WSSS with Image-level Labels</head><p>Weakly-Supervised Semantic Segmentation (WSSS) aims to develop semantic segmentation models with weak annotations, such as image-level labels <ref type="bibr" target="#b43">Pinheiro and Collobert, 2015;</ref><ref type="bibr" target="#b1">Ahn and Kwak, 2018;</ref><ref type="bibr" target="#b27">Lee et al., 2021a)</ref>, points <ref type="bibr" target="#b6">(Bearman et al., 2016)</ref>, bounding boxes <ref type="bibr" target="#b49">(Song et al., 2019;</ref><ref type="bibr" target="#b39">Oh et al., 2021;</ref><ref type="bibr" target="#b28">Lee et al., 2021b)</ref>, and scribbles <ref type="bibr" target="#b32">(Lin et al., 2016)</ref>. In this work, we focus on WSSS with only image-level labels. In the subsections below, we will introduce WSSS methods with image-level labels based on their motivations.</p><p>Growing Seed Regions with Constraints. <ref type="bibr" target="#b24">(Kolesnikov and Lampert, 2016)</ref> proposed SEC principle to expand the initial seed cues and coincide with the object shapes. This principle was adopted by subsequent works. For example, <ref type="bibr" target="#b44">(Roy and Todorovic, 2017)</ref> used CRF-CNN <ref type="bibr" target="#b63">(Zheng et al., 2015)</ref> to refine the initial labels with low-level pixel information to generate pseudo labels fitting object boundaries.  integrated seeded region growing <ref type="bibr" target="#b0">(Adams and Bischof, 1994)</ref> to expand the initial seed cues generated from classification networks and also adopted dense CRF <ref type="bibr" target="#b25">(Kr?henb?hl and Koltun, 2011)</ref> to refine pseudo labels.</p><p>Erasing. Based on the common observation that CAMs usually only captured the most discriminative regions, <ref type="bibr">Wei et al . proposed</ref> to adversarially erase the discriminative regions and progressively localize the integral object regions <ref type="bibr" target="#b55">(Wei et al., 2017)</ref>. Similarly, ACoL <ref type="bibr" target="#b62">(Zhang et al., 2018)</ref> used two parallel CNN to erase the feature maps in one branch with the discriminative regions derived from the other branch and fused the localized regions from both branches as outputs. To prevent the attention regions from shifting to non-object regions during erasing, SeeNet <ref type="bibr" target="#b18">(Hou et al., 2018)</ref> used saliency prior <ref type="bibr" target="#b17">(Hou et al., 2017)</ref> to suppress the attention in background regions.</p><p>Accumulating Attention. Another interesting observation is that classification networks tend to shift attention to the different regions of the object across the training process <ref type="bibr" target="#b20">(Jiang et al., 2019)</ref>. Motivated by this, Jiang et al . proposed OAA, which accumulated the activated regions during the different training stage. In <ref type="bibr" target="#b59">(Yao et al., 2021)</ref>, Yao et al . proposed a graph reasoning and non-salient region mining module to capture more object extents from non-salient regions, since the saliency prior used in OAA did not always correspond to the foreground objects. <ref type="bibr" target="#b23">Kim et al . in (Kim et al., 2021)</ref> combined the idea of erasing and accumulating to suppress the discriminative regions in training, which could assist in finding less discriminative object regions.</p><p>Mining Objects from Multiple Images. The works above mainly focused on mining semantic objects from single image. Some recent works also tried to leverage the semantic co-occurrence in two or more images <ref type="bibr" target="#b12">(Fan et al., 2020;</ref><ref type="bibr" target="#b30">Li et al., 2021a;</ref><ref type="bibr" target="#b50">Sun et al., 2020)</ref>. In <ref type="bibr" target="#b12">(Fan et al., 2020)</ref>, CIAN designed a cross image attention module to model the pixel-level affinity from different images with common semantics. In <ref type="bibr" target="#b50">(Sun et al., 2020)</ref>, in addition to the co-attention from image pairs, Sun et al . further proposed a contrastive attention module that could mine the unique semantic objects. <ref type="bibr" target="#b30">(Li et al., 2021a</ref>) used a graph neural network (GNN) <ref type="bibr" target="#b47">(Scarselli et al., 2008)</ref> based approach to reason and capture the integral object information from a group of input images.</p><p>Refining Seed Regions. Since CAMs only yield very coarse pixel labels, some pixel affinity based methods are proposed to refine CAMs and proved to work brilliantly.  proposed an EM framework, in which a unary network was used to predict the class score maps and a pairwise network was used to learn the pixel affinities. The learned pixel affinity would be used to refine the score maps and then supervise the training of the framework. PSA <ref type="bibr" target="#b1">(Ahn and Kwak, 2018)</ref> derived foreground and background regions with high confidence from coarse labels and utilized the reliable regions to learn an affinity network. For each input image, their coarse labels were refined via random walk propagation <ref type="bibr" target="#b52">(Vernaza and Chandraker, 2017)</ref> with the learned affinity matrix from the trained network. In further, IRNet <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref> proposed to derive instance labels from instance-agnostic CAMs via additionally learning semantic instance boundaries and propagating the initial CAMs with the learned pixel affinities and instance boundaries.</p><p>End-to-End Solutions. Though the majority of methods adopted a multi-step framework, some works also tried to devise elegant end-to-end models for WSSS with image-level labels. In , <ref type="bibr">Papandreou et al . proposed</ref> an EM framework that estimated the pseudo labels with a probabilistic model and utilized the pseudo labels to train the network at the maximization step. <ref type="bibr" target="#b60">(Zhang et al., 2020a</ref>) followed a similar framework of  but leveraged CRF to refine CAMs and jointly minimized the cross-entropy loss and low-level energy loss with the highly-confident pseudo labels. <ref type="bibr" target="#b4">(Araslanov and Roth, 2020)</ref> devised normalized global weighted pooling to aggregate classification scores from predicted score maps, which could improve the completeness of objects. The predicted score maps were then refined with a pixel affinity-based module to supervise the training process. To avoid the network degrading to trivial solutions, <ref type="bibr" target="#b4">(Araslanov and Roth, 2020)</ref> additionally proposed a stochastic gate that randomly transferred low-level features to high-level semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Improvements on CAMs</head><p>A prevailing series of WSSS with image-level labels is to derive better CAMs from the classification network. Many works have been proposed to produce better CAMs by encouraging more object extents to be discovered. In ), Wang et al . observed that CAMs of the same image with different scaling ratio usually differed largely in shape, while they were supposed to be the same since they consisted of the same objects. They proposed to regularize the classification network by minimizing the difference between the CAMs of different scales and achieved remarkable performance. In <ref type="bibr" target="#b8">(Chang et al., 2020b)</ref>, <ref type="bibr">Chang et al .</ref> proposed to cluster the original semantic categories to sub-categories and further leveraged the sub-category labels to supervise the training of the network, which could enforce the network to discover more object regions to distinguish sub-categories. <ref type="bibr" target="#b27">(Lee et al., 2021a)</ref> proposed an anti-adversarial attack approach to gradually pull image away from decision boundaries which helped to discover more object regions.  firstly introduced causal inference <ref type="bibr" target="#b46">(Rubin, 2019)</ref> to alleviate the confounding bias attributed by ambiguous boundaries. Some recent works showed that data augmentation techniques <ref type="bibr" target="#b7">(Chang et al., 2020a)</ref> and auxiliary self-supervised tasks <ref type="bibr" target="#b21">(Jo and Yu, 2021)</ref> could help to discover more object regions and thus improve the quality of CAMs. In this work, we also focus on deriving better CAMs from classification networks but from two aspects. Specifically, we propose the visual words learning module and hybrid pooling approach to counter the problem of partial discriminative object regions unexpected background regions in CAMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>This section expatiates our proposed methods, including the Visual Words Learning (VWL) module, Hybrid Pooling (HP) approach, and training process of our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Method Overview</head><p>The overall architecture of our method is presented in <ref type="figure">Fig. 3</ref>. For an input image, we firstly use a CNN backbone to extract the convolutional feature maps. In the visual words learning module, a predefined codebook is employed to encode the feature maps to visual word score maps, in which each element denotes the probability of a pixel belonging to each visual word. The visual word label of a given image is derived from the score maps and further used to supervise the network training to activate more object regions. To alleviate the problem of unexpected background regions, we use the proposed hybrid pooling, which aggregates local discriminative information and global average information, so that less background is preserved in the generated CAMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preliminaries</head><p>Currently, the majority of WSSS methods with imagelevel labels infer CAMs <ref type="bibr" target="#b64">(Zhou et al., 2016)</ref> from a trained classification network as the initial coarse labels. In this work, we follow the original way in <ref type="bibr" target="#b64">(Zhou et al., 2016)</ref> to directly produce CAMs with the feature maps of the last conv layer and the weight matrix W img in prediction layer. Specifically, CAMs for class c are given by  <ref type="figure">Fig. 3</ref>: Overview of the proposed method. To encourage more object extents to be discovered, we propose the Visual Words Learning (VWL) module, which utilizes a codebook to encode the feature maps extracted by CNN. The encoded visual word labels are then used to supervise the training process of the classification network. We also propose a novel feature aggregation method, i.e. Hybrid Pooling (HP), which incorporates GMP to reduce background information and GAP to ensure the object completeness in the generated CAMs.</p><p>weighting each feature map in F with its contribution to class c</p><formula xml:id="formula_0">M c = d i=1 (W img i,c F :,:,i ).<label>(1)</label></formula><p>M c is further passed through a relu layer to eliminate the negative values, denoted asM c . The generated M c will be used to produce pseudo segmentation labels with a background score threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Visual Words Learning</head><p>As aforementioned, classification networks guided by image-level labels usually only discover partial discriminative extents of objects. The reason is that focusing on partial discriminative extents of objects is more beneficial to recognize different semantic categories. To solve this problem, our motivation is that if the network could be supervised with more fine-grained labels in the training procedure, it will be enforced to activate more semantic regions so that the generated CAMs would be more accurate. To this end, we propose to jointly learn the visual words and image-level labels in the training process of classification networks.</p><p>Since only image-level labels are available in our task, in order to leverage visual word labels to guide the training of classification networks, we design an unsupervised visual words learning module. As shown in <ref type="figure">Fig. 3</ref>, in the visual words learning module, a matrix C ? R k?d is defined as the codebook, where k is the number of words and d denotes the feature dimension. C is utilized to encode the extracted convolutional feature map F ? R h?w?d to specific visual words. Here, we use cos distance to measure the similarity between the pixel at position i in F and the j-th word in C. The similarity matrix S is given as:</p><formula xml:id="formula_1">S ij = F i C j ||F i || 2 ||C j || 2 , 1 ? i ? hw, 1 ? j ? k.<label>(2)</label></formula><p>The obtained S will be normalized row-wisely using sof tmax to compute the probability of the i-th pixel in F belonging to j-th word in codebook C. The process is given as</p><formula xml:id="formula_2">P ij = exp(? ? S ij ) k n=1 exp(? ? S in ) ,<label>(3)</label></formula><p>where ? &gt;0 is a temperature parameter to control the smoothness of P. The visual word label Y i for F i is then given as the word with the maximum probability, i.e., the index of the maximum value in the i-th row of P ij , which is denoted as</p><formula xml:id="formula_3">Y i = arg max j P ij .<label>(4)</label></formula><p>For an input image X, its visual word labels are computed as a k-dimensional vector y word , where y word j = 1 if the j-th word exists in Y, and y word j = 0, otherwise. y word will be used to guide the training procedure of the classification network to enforce it to discover more discriminative extents.</p><p>Another problem is to ascertain the codebook for encoding visual word labels reasonably. In a classic BoVW model, the codebook is usually identified as the clustering centroids of the feature representations extracted from all local visual words <ref type="bibr" target="#b36">(Liu et al., 2019;</ref><ref type="bibr" target="#b14">Gidaris et al., 2020)</ref>. However, in our model, the feature representations for visual words are updated online as the training procedure. Therefore, the codebook C should also be updated online. To this end, as shown in <ref type="figure">Fig. 4</ref>, we devised two strategies to learn C, namely the Learningbased strategy and Memory-bank strategy.</p><p>Learning-based strategy. In the learning-based strategy, following <ref type="bibr" target="#b41">(Passalis and Tefas, 2017;</ref><ref type="bibr" target="#b3">Arandjelovi? et al., 2017)</ref>, we set the codebook as a trainable parameter to learn it from back-propagated gradients. In a BoVW model, the frequencies of visual words are collected as the feature descriptor to predict the image classes so as to learn the relations between visual words and semantic classes <ref type="bibr" target="#b36">(Liu et al., 2019)</ref>. However, this hard quantization approach will introduce noncontinuities and is proved to make the training process intractable <ref type="bibr" target="#b41">(Passalis and Tefas, 2017)</ref>. In this work, we compute the frequency of each word by accumulating the probabilities in P. Therefore, the soft frequency assignment of the j-th word is</p><formula xml:id="formula_4">f word j = 1 hw hw i=1 P ij ,<label>(5)</label></formula><p>where f word j denotes the frequency of the j-th word in F. As shown in <ref type="figure">Fig. 4 (a)</ref>, f word will be used to predict the image-level labels, i.e., modeling the relations between visual words and image-level labels, which encourages the codebook to learn latent visual word representations via gradients.</p><p>Memory-bank strategy. As aforementioned, a classic BoVW model usually takes the clustering centroids of image features as the codebook. However, clustering on the whole dataset with the network training is extremely time-consuming. Inspired by mini-batch K-Means <ref type="bibr" target="#b48">(Sculley, 2010)</ref>, which decomposes clustering on a large-scale dataset to mini-batch iterations, we propose the memory-bank strategy to gradually update the codebook with reconstructed codebook in each training step.</p><p>In Eq. (4), the visual word label Y for each pixel in F is computed. We firstly transform Y ? R hw to W ? R hw?k via one-hot encoding. The reconstructed codebook C is hereby given as averaging the representations that are encoded to the same visual word category.</p><formula xml:id="formula_5">C = D ?1 W F,<label>(6)</label></formula><p>where D is a degree matrix with D ii = 1/ j,k=i W jk and D ij = 0 for off-diagonal elements. Here, we also assume F is unfolded to R hw?d for simplicity. As shown in <ref type="figure">Fig. 4 (b)</ref>, the reconstructed codebook C is then used to update codebook with a momentum parameter ?. This process is given as</p><formula xml:id="formula_6">C t+1 ? ?C + (1 ? ?)C t ,<label>(7)</label></formula><p>where t denotes the training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Hybrid Pooling</head><p>To mitigate the aforementioned disadvantages of GAP and GMP, in this work, we present a simple yet empirically effective pooling method which aggregates local maximum and global average values of the feature maps.</p><p>Considering the output feature map F with size of h?w ?d of the last convolutional layer, we firstly partition F to multi-scale divisions. As illustrated in <ref type="figure">Fig</ref>  <ref type="figure">Fig. 4</ref>: Illustration of strategies for visual word codebook. For the learning-based strategy, we follow the original intention of BoVW models <ref type="bibr" target="#b36">(Liu et al., 2019;</ref><ref type="bibr" target="#b14">Gidaris et al., 2020)</ref>, i.e. using visual word frequencies to predict the image-level labels, which could enforce to learn the codebook from the back-propagated gradients. For the memory-bank strategy, inspired by mini-batch K-means <ref type="bibr" target="#b48">(Sculley, 2010)</ref>, we decompose the clustering on the whole dataset to each training step and update the codebook from reconstruction in the memory-bank mechanism. each division with size of h r ? w r ? d is pooled to a ddimensional vector via max pooling, where r ? {1, 2, 4} denotes the split size. F is thus aggregated to F max with size of r ? r ? d. It is conspicuous that F max only involves local maximum pixels so that less background is considered. We then pool F max for subsequent classification task. The pooled feature f max r with split size r is given by</p><formula xml:id="formula_7">f max r = 1 r 2 r i=1 r j=1 F max i,j,: .<label>(8)</label></formula><p>Note that f max r only preserves the maximum responses of local objects, which may corrupt the completeness of objects. To tackle this problem, in HP module, we also incorporate the results of GAP. Given the pooled feature of GAP layer, which is computed as</p><formula xml:id="formula_8">f gap = 1 hw h i=1 w j=1 F i,j,: ,<label>(9)</label></formula><p>the final output of hybrid pooling module is calculated by weighting the outputs in Eq. (8) and Eq. <ref type="formula" target="#formula_8">(9)</ref>, com-puted as</p><formula xml:id="formula_9">f hp = 1 ? + 3 ( r?{1,2,4} f max r + ?f gap ),<label>(10)</label></formula><p>where ? is a weight factor. Leveraging Eq. (10), more regions of foreground objects and less background are captured for classification, so that the generated CAMs could coincide better with object shapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Training</head><p>Since only image-level labels are available, the classification loss is indispensable to train the network. After calculating f hp via Eq. (10), the classification score for image label is computed with a classification layer (an 1 ? 1 conv layer in practice), denoted as p img = conv(f hp , W img ), where W img is the weight matrix of this layer. As a common practice <ref type="bibr" target="#b8">Chang et al., 2020b;</ref><ref type="bibr" target="#b4">Araslanov and Roth, 2020)</ref>, the multi-label soft margin loss <ref type="bibr" target="#b42">(Paszke et al., 2019)</ref> is employed to compute the classification loss</p><formula xml:id="formula_10">L cls (p img , y img ) = 1 L L i=1 [y img i log exp(p img i ) 1 + exp(p img i ) + (1 ? y img i ) log 1 1 + exp(p img i ) ],<label>(11)</label></formula><p>where y img denotes the ground-truth image label and L is the number of image classes.</p><p>To capture more semantic regions, the pooled feature is also used to predict the visual word label y word generated in previous steps. It is noted that y word is generated based on all pixels in feature map F. Therefore, we use GAP here instead of our HP to perform feature aggregation for predicting y word . The predicted visual word score is thus denoted as p word = conv(f gap , W word ), where W word is the weight matrix of the prediction layer. The classification loss for visual words is then denoted as L cls (p word , y word ), which is in the same form as Eq. (11). The overall loss function is the sum of L cls (p img , y img ) and L cls (p word , y word ), namely</p><formula xml:id="formula_11">L all = L cls (p img , y img ) + L cls (p word , y word ).<label>(12)</label></formula><p>Auxiliary Loss for Learning-based Strategy. Recall that in the learning-based strategy, to learn the codebook from gradients, we set the codebook as a trainable parameter and leverage the visual word representations to learn the image-level labels. Specifically, the visual word frequency f word acquired in Eq. <ref type="formula" target="#formula_4">(5)</ref> is projected into the class probability space with an 1?1 conv layer. The predicted score is denoted by p w2i so that the loss function is given as L cls (p w2i , y img ). We empirically found that learning with the loss L cls (p w2i , y img ) solely tended to make the learned visual word representations in codebook C redundant. To tackle this problem, we add a regularization term to minimize the correlation between different latent visual word representations. Here, we use the DeCov loss <ref type="bibr" target="#b10">(Cogswell et al., 2017)</ref>, which reduces the correlations between rows in a matrix by minimizing their covariance, i.e.</p><formula xml:id="formula_12">L decov = 1 2 ( ? 2 F ? diag(?) 2 2 ),<label>(13)</label></formula><p>where? is the covariance matrix of C, ? F denotes the Frobenius norm, and diag(?) extracts the main diagonal elements of a matrix to a vector. The auxiliary loss for the learning-based strategy is then given as the sum of aforementioned two losses L aux = L cls (p w2i , y img ) + L decov .</p><p>By regularizing Eq. (12) with Eq. <ref type="formula" target="#formula_0">(14)</ref>, we present the overall loss function to optimize the network under the learning-based strategy</p><formula xml:id="formula_14">L all = L cls (p img , y img ) + L cls (p word , y word ) + L aux .<label>(15)</label></formula><p>The overall training process with learning or memorybank strategy is summarized in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Dataset. We evaluated our method on the PASCAL VOC 2012 dataset <ref type="bibr" target="#b11">(Everingham et al., 2010)</ref>   <ref type="bibr" target="#b35">(Lin et al., 2014)</ref>. For all experiments, the mean Intersection-over-Union (mIoU) ratio was used as the evaluation criteria.</p><p>PASCAL VOC 2012 dataset <ref type="bibr" target="#b11">(Everingham et al., 2010)</ref> consists of 21 semantic categories, including 20 foreground object classes and the background class. Following the common practice <ref type="bibr" target="#b9">(Chen et al., 2017;</ref><ref type="bibr" target="#b54">Wang et al., 2020b;</ref><ref type="bibr" target="#b8">Chang et al., 2020b)</ref>, this dataset is augmented with SBD dataset <ref type="bibr" target="#b15">(Hariharan et al., 2011)</ref>. The train, val, and test set of the augmented dataset consist of 10,582, 1449, and 1456 images, respectively.</p><p>MS COCO 2014 dataset <ref type="bibr" target="#b35">(Lin et al., 2014)</ref> is a largescale dataset with 81 semantic categories, including the background class. After excluding the images without annotations <ref type="bibr" target="#b29">(Lee et al., 2021c)</ref>, the MS COCO dataset consists of 82,081 and 40,137 images in train and val set, respectively.</p><p>Classification Network. For the network to produce CAMs, we used ResNet101  pre-trained on ImageNet <ref type="bibr" target="#b26">(Krizhevsky et al., 2012)</ref> as the backbone to extract convolutional feature maps. For PASCAL VOC and MS COCO datasets, the classification network was trained for 6 epochs, with a batch size of 16. To optimize the network, we used the SGD optimizer with momentum mechanism and set the momentum coefficient as 0.9. The learning rate was initially set to 0.01 for the backbone parameters and 0.1 for the other parameters. All learning rates were decayed every iteration with a polynomial decay scheduler. Specifically, in each iteration, the learning rate was multiplied by (1 ? iter max iter ) power , with power = 0.9. As for the hyper-parameters, the number of visual words k and the weight factor ? in Eq. (10) were set to 256 and 2, respectively. The temperature parameter ? in Eq. (3) was empirically set to 1. For the memorybank strategy, we had an extra momentum coefficient ? in Eq. <ref type="formula" target="#formula_6">(7)</ref>, which was set to 0.001. More details and the impacts of these hyper-parameters are reported in Section 4.5. Our code is available at https://github. com/rulixiang/vwe/tree/master/v2. CAMs Refinement. The generated initial labels by directly segmenting CAMs with thresholds are usually very coarse <ref type="bibr" target="#b1">(Ahn and Kwak, 2018;</ref><ref type="bibr" target="#b2">Ahn et al., 2019)</ref>.</p><p>To improve the quality of pseudo labels and the semantic segmentation performance, we adopted IRNet <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref> as the refinement approach for processing the initial coarse labels generated from the classification network. In practice, we used the official implementation 1 without changing their settings.</p><p>Segmentation Network. For the semantic segmentation network, we used the DeepLabV2 <ref type="bibr" target="#b9">(Chen et al., 2017)</ref> system with ResNet101  as backbone, which is a prevailing choice for WSSS task <ref type="bibr" target="#b22">(Ke et al., 2021;</ref><ref type="bibr" target="#b27">Lee et al., 2021a;</ref><ref type="bibr" target="#b8">Chang et al., 2020b)</ref>. For experiments on PASCAL VOC 2012 dataset <ref type="bibr" target="#b11">(Everingham et al., 2010)</ref>, we followed the default settings of DeepLabV2 <ref type="bibr" target="#b9">(Chen et al., 2017)</ref>, i.e., the learning rate was initially set to 0.001 and decayed with a polynomial scheduler. The batch size and number of iterations were 10 and 20,000, respectively. We used a momentum optimizer with the momentum parameter of 0.9 and weights 1 https://github.com/jiwoon-ahn/irn  <ref type="bibr">AAAI'2021</ref> (2) 68.2 68.5 NSROM <ref type="bibr">CVPR'2021</ref> (2) 68.3 68.5 NSROM <ref type="bibr">CVPR'2021</ref> (2) ? 70.4 70.2 DRS <ref type="bibr">AAAI'2021</ref> (2) ? 70.4 70.7 EPS <ref type="bibr">CVPR'2021</ref> (2) ? 70.9 70.8 AuxSegNet <ref type="bibr">ICCV'2021</ref> (3) 69.0 68.6 EDAM <ref type="bibr">CVPR'2021</ref> (2) ? 70.9 70.6 (2) ? 70.6 70.7 5 * Accuracy obtained with our re-implementation. ? Backbone pre-trained on MS COCO dataset. decay rate of 0.0005. For a fair comparison with other WSSS works, we evaluated the DeepLabV2 initialized with ImageNet <ref type="bibr" target="#b26">(Krizhevsky et al., 2012)</ref> and MS COCO dataset <ref type="bibr" target="#b35">(Lin et al., 2014)</ref> pre-trained weights. For experiments on the MS COCO dataset <ref type="bibr" target="#b35">(Lin et al., 2014)</ref>, we followed the same settings as the experiments on the PASCAL VOC dataset. The only difference was that we trained the segmentation network for 60,000 iterations since MS COCO consisted of much more samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results on PASCAL VOC dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Ground Truth IRNet SEAM VWE Ours-M Ours-L <ref type="figure">Fig. 6</ref>: Visualization of the generated CAMs. Different colors denote the activated regions of different semantic categories. <ref type="table" target="#tab_1">Table 1</ref> reports the quantitative evaluation results of CAMs on the train set of the PASCAL VOC dataset. CRF denotes the generated CAMs are refined with dense CRF <ref type="bibr" target="#b25">(Kr?henb?hl and Koltun, 2011)</ref>. Ref. denotes the generated CAMs are refined with PSA <ref type="bibr" target="#b1">(Ahn and Kwak, 2018)</ref> or IRNet <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref>. The best results are highlighted in bold. We denote our methods with memory-bank strategy and learning-based strategy as Ours-M and Our-L, respectively. Ours results are compared with recent related works on improving the quality of CAMs, including AdvCAM <ref type="bibr" target="#b27">(Lee et al., 2021a)</ref>, SC-CAM <ref type="bibr" target="#b8">(Chang et al., 2020b)</ref>, CONTA , and SEAM ) etc. <ref type="table" target="#tab_1">Table 1</ref> shows that our methods with learning-based and memory-bank strategy could both remarkably surpass current state-of-the-art works. After further refinement with IRNet <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref>, Our-M and Ours-L achieve 71.1% and 71.4% mIoU on the pseudo labels, respectively, which also outperform the competitors.</p><p>In <ref type="figure">Fig. 6</ref>, we visualize the generated CAMs and compare them with the results of recent methods, including IRNet <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref>, SEAM , and VWE (our previous work with HP and simple visual words encoder) <ref type="bibr" target="#b45">(Ru et al., 2021)</ref>. The results of the learning-based strategy (Ours-L) and the memory-bank strategy (Ours-M) are both presented. It is observed that our results typically activate more object regions and less mis-activated background, which is owing to that the proposed visual words learning module encourages to discover more objects, while HP aggregates local discriminative information and thereby reduces background regions. It is also noticed that the results of Ours-L and Ours-M are very close visually <ref type="figure">(Fig. 6</ref>) and numerically <ref type="table" target="#tab_1">(Table 1)</ref>, which indicates that both the learning-based and memory-bank strategy could work finely.</p><p>We use the refined CAMs as the pseudo labels to train regular semantic segmentation networks and compare the results on the val and test set of PASCAL VOC dataset. The results are reported in <ref type="table" target="#tab_2">Table 2</ref>. For a fair comparison, we report the performance using DeepLabV2 with backbone pre-trained on ImageNet <ref type="bibr" target="#b9">(Chen et al., 2017)</ref> and MS COCO <ref type="bibr" target="#b35">(Lin et al., 2014)</ref>. By default, the presented results are obtained with dense CRF post-processing <ref type="bibr" target="#b25">(Kr?henb?hl and Koltun, 2011)</ref>. It is observed that, for the WSSS methods with only image-level labels, our method obtains the best performance. Specifically, Ours-L achieves 69.2% and 70.6% mIoU on the PASCAL VOC val set with DeepLabV2 initialized with ImageNet and MS COCO pre-trained weights, respectively, which recover 90.7% and 91.0% of the upper bound of their fully-supervised counterparts. Our methods also achieve comparable performance with recent state-of-the-art WSSS methods us-ing extra saliency maps, such as NSROM <ref type="bibr" target="#b59">(Yao et al., 2021)</ref>, DRS , EPS <ref type="bibr" target="#b29">(Lee et al., 2021c)</ref>, AuxSegNet <ref type="bibr" target="#b58">(Xu et al., 2021)</ref>, and EDAM . Our method also outperforms recent methods with superior backbone networks, such as PMM <ref type="bibr" target="#b31">(Li et al., 2021b)</ref>, which uses Res2Net101  as the backbone for semantic segmentation. Note that both Ours-M and Ours-L could surpass recent WSSS methods with only image-level supervision, which demonstrates the efficacy of our proposed learning-based and memory-bank strategies.</p><p>The qualitative results of our proposed method and some other methods' results, including LIID <ref type="bibr" target="#b37">(Liu et al., 2020)</ref> and VWE <ref type="bibr" target="#b45">(Ru et al., 2021)</ref>, are presented in <ref type="figure">Fig. 7</ref>. We could observe that our method significantly outperforms other WSSS methods and coincides better with the ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on MS COCO dataset</head><p>To further verify the efficacy of the proposed method, we conduct experiments on the MS COCO dataset <ref type="bibr" target="#b35">(Lin et al., 2014)</ref>, which consists of much more images and semantic categories than PASCAL VOC 2012 dataset. The quantitative evaluation results on the MS COCO dataset are presented in <ref type="table" target="#tab_4">Table 3</ref>. We observe that Ours-L and Ours-M achieve 36.2% and 36.1% mIoU on the MS COCO val dataset, respectively. Both of them could outperform other WSSS methods with only image-level labels. Besides, our results are also better than the results of recent state-of-the-art WSSS methods with image-level labels and extra saliency cues. The superiority of the performance on the MS COCO dataset also demonstrates the efficacy of our methods.</p><p>We present some predicted example images of MS COCO val dataset in <ref type="figure">Fig. 8</ref>. It is observed that our method could produce comparable results with groundtruth labels, though the MS COCO dataset is much more challenging. However, when the background is complex, as presented in the last two columns, the predicted results are clearly worse than the ground-truth labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study and Analysis</head><p>Quantitative Ablation Results. We conducted ablation experiments on the PASCAL VOC dataset to show the efficacy of the proposed methods. The quantitative evaluation results of the generated CAMs using baseline with different modules are presented in <ref type="table" target="#tab_5">Table 4</ref>. VWE denotes the visual words learning module without DeCov regularization in our preliminary work (Ru   <ref type="bibr">, 2021)</ref>. VWL-M and VWL-L denote the proposed visual words learning module with learning-based and memory-bank strategy, respectively. We observe that the both proposed HP and VWL module could improve the quality of the generated CAMs. Besides, our proposed memory-bank and learning-based strategy could further improve the mIoU on the train set to about 57%, which remarkably outperform recent state-of-theart methods presented in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>Visual Ablation Results. Our intention of the proposed VWL and HP is to encourage the network to activate more object extents and fewer background regions, respectively. Though <ref type="table" target="#tab_5">Table 4</ref> shows that the proposed methods could improve the quality of CAMs, we still want to explore their effects on the generated CAMs. Therefore, we further visualize the CAMs generated by baseline, baseline with only HP, baseline with only VWL and our method. The visualization results are presented in <ref type="figure" target="#fig_5">Fig. 9</ref>. It is observed that VWL typically discovers more object extents, while both of them tend to activate adjacent background around objects. HP could remarkably alleviate this drawback since it aggregates local discriminative information instead of the whole image. Our method, which combines VWL and HP, could jointly mine more object regions and reduce the unexpected background. <ref type="figure" target="#fig_5">Fig. 9</ref> also shows IR-Net <ref type="bibr" target="#b2">(Ahn et al., 2019)</ref> could further dampen the falsely activated regions and diffuse the object regions, so the CAMs can align better with the semantic boundaries.</p><p>Codebook Analysis. To verify whether the learningbased and memory-bank strategy could learn a reasonable codebook, we visualize the learned visual words represented in the codebook by extracting their corresponding regions in an image. The visualization results are presented in <ref type="figure">Fig. 10</ref>, where examples in each col-umn are sampled from the images of a specific visual word. We show that both the learning-based strategy and memory-bank strategy could effectively learn visual word representations from images. For example, on the MS COCO dataset, our learning-based strategy could successfully decompose person to face, body, hands and legs etc., which could be used to supervise the training of classification network and encourage more object extents to be discovered. Empirically, we also observe the learned visual words on MS COCO dataset typically consist of fewer noisy samples than the PAS-CAL VOC dataset, which indicates larger-scale dataset could benefit our visual words learning strategies. Codebook Initialization. In this work, the codebook is simply randomly initialized. We do not use any extra pre-training or warm-up strategy for the codebook.</p><p>To explore the impact of the initialization method, we present the performance of our method using random sample initialization (initializing the codebook with randomly sampled image features). The results are reported in Tab. 5. As shown in Tab. 5, the memory-bank strategy is not sensitive to the initialization method for the codebook, while the learning-based strategy achieves worse performance when using the random sample initialization. Technically, the codebook in the memorybank strategy does not straightly affect the optimization process. Therefore, the impact of the initialization method is trivial. However, the codebook in the learning-based strategy is a trainable parameter and directly impacts the update process of the network parameters, thus notably affecting the performance of the generated CAM. Learning-based versus Memory-bank. The learningbased strategy (VWL-L) and the memory-bank strategy (VWL-M) are both inspired by the simple Bag of Visual Words model. Specifically, in VWL-L, the visual word representation for an input image is automatically learned with the devised loss functions, while VWL-M extracts the visual word representations by online clustering. In other words, VWL-L and VWL-M model an image implicitly and explicitly, respectively. Therefore, we empirically find VWL-M could yield better visual words than VWL-L. Besides, as discussed in Section 4.4, compared to VWL-M, VWL-L is slightly sensitive to the initialization of the codebook. However, on the efficiency side, due to the online reconstruction operation, the training process of VWL-M takes a slightly longer time than VWL-L.</p><p>To better understand the quality of the generated visual words by the learning-based and memory-bank strategy, in <ref type="figure">Fig. 11</ref>, we visualize the extracted visual word features with t-sne (Van Der Maaten, 2014). The features for visualization are generated by averaging features of different visual word regions in each input image. As shown in <ref type="figure">Fig. 11</ref>, the clusters of VWL-L are more diverse than VWL-M's, i.e., VWL-M learns better visual words than VWL-L, which is attributed to the explicit modeling of visual words in VWL-M. We then use the extracted visual word frequencies of each image to predict the image-level labels. The classification accuracies are reported in Tab. 6. VWL-M is still superior to VWL-L, demonstrating the better visual words learning capacity. DeCov loss. In the loss function (Eq. <ref type="formula" target="#formula_0">(14)</ref>) for learning codebook in the learning-based strategy, we introduced the DeCov loss <ref type="bibr" target="#b10">(Cogswell et al., 2017)</ref> to reduce the redundancy of the learned visual word representations. In <ref type="table" target="#tab_5">Table 4</ref>, we show that learning visual words with DeCov loss could improve the mIoU of generated CAMs on PASCAL VOC train set from 55.1% to 57.3%. To further verify whether DeCov loss could eliminate the redundancy of the codebook, we visualized the similarity matrix of learned visual word representations. As presented in <ref type="figure" target="#fig_0">Fig. 12</ref>, when using DeCov loss regularization, the cosine similarity between two <ref type="figure">Fig. 10</ref>: Visualization of the visual words learned by learning strategy and memory-bank strategy. Each column denotes the example images sampled from a visual word category. different word representations is very close to 0. Taking the mIoU improvements in <ref type="table" target="#tab_5">Table 4</ref> into consideration, we demonstrate our regularization loss in Eq. (14) could successfully reduce codebook redundancy and improve CAMs quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours-L on VOC Ours-L on COCO Ours-M on VOC Ours-M on COCO</head><p>GAP in HP for Object Completeness. Some previous works <ref type="bibr" target="#b24">(Kolesnikov and Lampert, 2016;</ref><ref type="bibr" target="#b64">Zhou et al., 2016)</ref> show that GAP tends to overestimate the object size while GMP tends to underestimate it. In the design of our hybrid pooling, we incorporate GAP to ensure object completeness. To verify the efficacy of GAP in HP, we present the quantitative and visual results of the generated CAMs using HP with and without GAP. As shown in <ref type="figure" target="#fig_7">Fig. 13</ref>, using GAP in HP could bring a large mIoU improvement of about 16%. The qualitative results also show that HP without GAP tends to discover only incomplete object regions while incorporating GAP could remarkably alleviate this problem.</p><p>Parallel Branches in HP. In the Hybrid Pooling module (HP), the parallel branches are used to produce multi-scale local information via max-pooling with different split sizes. Empirically, max-pooling with a small split size r aggregates less foreground and background information, leading to more discriminative object activation in CAMs. On the contrary, max-pooling with a large r aggregates more foreground and unexpected background information. In this work, we balance the background and foreground information aggregation by averaging the features with the different split sizes, i.e., parallel branches in HP. We present the impact of the split sizes in Tab. 7. Tab. 7 shows that using maxpooling with the split size of {1, 2, 4} in HP can activate the background and foreground regions in CAMs well, and achieve the best performance. Comparison to GWRP and LSE. We compare the proposed HP with the global weighted rank pooling (GWRP) <ref type="bibr" target="#b24">(Kolesnikov and Lampert, 2016)</ref> and Log-Sum-Exp pooling (LSE) <ref type="bibr" target="#b43">(Pinheiro and Collobert, 2015)</ref>. The experiments are conducted with ResNet101  as the backbone (without visual word learning). As shown in Tab. 8, our method clearly outperforms GWRP and LSE. Besides, compared to GWRP and LSE, our HP is easier to implement since it only incorporates avg-pooling and max-pooling. GAP in Visual Word Learning. As illustrated in Section 3.5, we use GAP instead our HP for predicting the visual word labels. We conduct experiments using HP in visual word learning to explore its impact. As shown in Tab. 9, using HP in visual word learning (VWL-HP) also achieves notable improvements, demonstrating the effectiveness of VWL. Nevertheless, using GAP in visual word learning (VWL-GAP) could further outperform VWL-HP. We analyze the reason in Section 3.5: The pseudo visual words are generated based on all pixels of the feature maps. HP mainly considers partial discriminative information while GAP could aggregate all information. Therefore, we think GAP is the better pooling method for predicting visual words, which is also verified in Tab. 9. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Hyper-parameters</head><p>This subsection presents the quantitative evaluation results of the generated CAMs on the PASCAL VOC train and val set with different hyper-parameter settings. All the results are evaluated and reported in mIoU.</p><p>Number of visual words k. In <ref type="table" target="#tab_1">Table 10</ref> (a), we present the impact of the number of visual words k by setting it to {128, 256, 384, 512} and fixing other hyperparameters (the classification network to generate CAMs is trained with the learning-based strategy). As observed in <ref type="table" target="#tab_1">Table 10</ref> (a), our method with different k could consistently outperform the baseline in <ref type="table" target="#tab_5">Table 4</ref>, which demonstrates the effectiveness of our motivation. The best result is obtained with k = 256.</p><p>Weight factor ?. The effect of the weight factor ? in HP is presented in Table 10 (b), which is used to trade off the GAP and GMP features. We observe that ? = 2 works well, while the performance clearly decreases when ? = 1, 4 since the output feature will degrade to GMP or GAP when ? is too small or big.</p><p>Temperature parameter ? . In Eq.</p><p>(3), we use a temperature parameter ? to control the smoothness of the visual word probabilities. As presented in <ref type="table" target="#tab_1">Table 10</ref> (c), we empirically observe that ? = 1.0, 0.8 are proper values for generating CAMs with higher quality.</p><p>Momentum coefficient ?. For the memory-bank strategy, a momentum coefficient ? in Eq. (7) to manipulate the update rate of the codebook. In <ref type="table" target="#tab_1">Table 10</ref> (d), we show that ? = 1e ?3 works finely since a large ? makes the codebook dependent on the features from the current batch, while a smaller ? means a slower update rate and may make the codebook not adaptable to training iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Previous CAMs typically only cover partial discriminative object regions and some unexpected background.</p><p>To tackle the first problem, we propose the visual words learning module. By enforcing the network to learn auxiliary visual words, more object regions could be activated. To perform unsupervised learning of visual words with only image-level labels, we devise the learningbased and memory-bank strategies to update the codebook. To mitigate the second problem, we propose hybrid pooling, which aggregates local maximum and global average features to simultaneously reduce background regions in CAMs and ensure object completeness. We experimentally demonstrated the superiority of our proposed method by surpassing recent state-of-the-art performance on the PASCAL VOC 2012 and MS COCO 2014 dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the motivation of our visual words learning module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Illustration of the proposed hybrid pooling. avg: average pooling, max: max pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :Fig. 8 :</head><label>78</label><figDesc>Examples of the predicted segmentation from PASCAL VOC val and test set. Examples of the predicted labels from MS COCO val dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 :</head><label>9</label><figDesc>Visualization of the generated CAMs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>: t-sne visualization of the generated visual words. Different colors denote the different visual wordsFig. 12: Visualization of the similarity matrix between each visual words without (left) and with (right) DeCov loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 13 :</head><label>13</label><figDesc>Quantitative and visual results of hybrid pooling with (w/ ) and without (w/o) GAP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Image I, label y img ; Params: Backbone network E(?, ?), codebook C,</figDesc><table><row><cell>Algorithm 1: Training procedure of the pro-</cell></row><row><cell>posed network.</cell></row><row><cell>hyper-parameters {k, ?, ?, ?};</cell></row><row><cell>Initialize E(?, ?) and C;</cell></row><row><cell>while training do</cell></row><row><cell>Extract feature maps F = E(I, ?);</cell></row><row><cell>Compute visual word label y word via Eq. (2) to</cell></row><row><cell>Eq. (4);</cell></row><row><cell>Compute pooling features via Eq. (9) and</cell></row><row><cell>Eq. (10);</cell></row><row><cell>if Learning-based strategy then</cell></row><row><cell>Compute visual word feature via Eq. (5);</cell></row><row><cell>Compute loss via Eq. (15);</cell></row><row><cell>Optimize E(?, ?) and C;</cell></row><row><cell>else Memory-bank strategy</cell></row><row><cell>Compute loss via Eq. (12);</cell></row><row><cell>Optimize E(?, ?);</cell></row><row><cell>Reconstructing C via Eq. (6);</cell></row><row><cell>Update C ? ?C + (1 ? ?)C;</cell></row><row><cell>end</cell></row><row><cell>end</cell></row><row><cell>COCO 2014 dataset</cell></row><row><cell>and MS</cell></row></table><note>Input:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluation and comparison of the generated CAMs and pseudo labels in mIoU. The best results are highlighted in bold.</figDesc><table><row><cell>Method</cell><cell cols="2">CAMs +CRF</cell><cell>+Ref.</cell></row><row><cell cols="4">CAMs refined with PSA (Ahn and Kwak, 2018).</cell></row><row><cell>PSA CVPR'2018</cell><cell>48.0</cell><cell>-</cell><cell>61.0</cell></row><row><cell>Mixup CAM BMVC'2020</cell><cell>50.1</cell><cell>-</cell><cell>61.9</cell></row><row><cell>SC-CAM CVPR'2020</cell><cell>50.9</cell><cell>55.3</cell><cell>63.4</cell></row><row><cell>SEAM CVPR'2020</cell><cell>55.4</cell><cell>56.8</cell><cell>63.6</cell></row><row><cell>PuzzleCAM arXiv'2021</cell><cell>51.5</cell><cell>-</cell><cell>64.7</cell></row><row><cell>AdvCAM CVPR'2021</cell><cell>55.6</cell><cell>62.1</cell><cell>68.0</cell></row><row><cell cols="4">CAMs refined with IRNet (Ahn et al., 2019).</cell></row><row><cell>IRNet CVPR'2019</cell><cell>48.8</cell><cell>54.3</cell><cell>66.3</cell></row><row><cell>MBMNet ACM MM'2020</cell><cell>50.2</cell><cell>-</cell><cell>66.8</cell></row><row><cell>CDA arXiv'2021</cell><cell>50.8</cell><cell>-</cell><cell>67.7</cell></row><row><cell>VWE IJCAI'2021</cell><cell>55.1</cell><cell>60.9</cell><cell>69.5</cell></row><row><cell>CONTA NeurIPS'2020</cell><cell>56.2</cell><cell>-</cell><cell>67.9</cell></row><row><cell>AdvCAM CVPR'2021</cell><cell>55.6</cell><cell>62.1</cell><cell>69.9</cell></row><row><cell>Ours-M</cell><cell>56.9</cell><cell>62.6</cell><cell>71.1</cell></row><row><cell>Ours-L</cell><cell>57.3</cell><cell>63.0</cell><cell>71.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Semantic segmentation results on PASCAL VOC 2012 dataset. The best results are highlighted in bold. Sup. denotes supervision type. Seg. denotes segmentation network.</figDesc><table><row><cell>Method</cell><cell cols="2">Sup. Seg.</cell><cell>val</cell><cell>test</cell></row><row><cell>Full Supervision.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(1)  ? DeepLabV1  ? ICLR'2015</cell><cell></cell><cell>-</cell><cell>75.5</cell><cell>-</cell></row><row><cell>(2) DeepLabV2 TPAMI'2017</cell><cell></cell><cell>-</cell><cell>76.3  *</cell><cell>-</cell></row><row><cell>(2)  ? DeepLabV2  ? TPAMI'2017</cell><cell>F</cell><cell>-</cell><cell>77.6</cell><cell>79.7</cell></row><row><cell>(3) WideResNet38 PR'2019</cell><cell></cell><cell>-</cell><cell>80.8</cell><cell>82.5</cell></row><row><cell>(4) Res2Net101 TPAMI'2021</cell><cell></cell><cell>-</cell><cell>80.2</cell><cell>-</cell></row><row><cell cols="3">Image-level Supervision + Saliency Maps.</cell><cell></cell><cell></cell></row><row><cell>OAA+ ICCV'2019</cell><cell></cell><cell>(1)  ?</cell><cell>65.2</cell><cell>66.4</cell></row><row><cell>Li et al.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>I + S</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Semantic segmentation results on MS COCO dataset. The best results are highlighted in bold. Sup. denotes supervision type. Seg. denotes segmentation network.</figDesc><table><row><cell>Method</cell><cell>Sup.</cell><cell>Seg.</cell><cell>val</cell></row><row><cell cols="3">Image-level Supervision + Saliency Maps.</cell><cell></cell></row><row><cell>DSRG CVPR'2018</cell><cell></cell><cell>DeepLabV2</cell><cell>26.0</cell></row><row><cell>Li et al. AAAI'2020</cell><cell></cell><cell>DeepLabV2</cell><cell>28.4</cell></row><row><cell>ADL TPAMI'2020</cell><cell>I + S</cell><cell>DeepLabV2</cell><cell>30.8</cell></row><row><cell>EPS CVPR'2021</cell><cell></cell><cell>DeepLabV2</cell><cell>35.7</cell></row><row><cell>AuxSegNet ICCV'2021</cell><cell></cell><cell>WideResNet38</cell><cell>33.9</cell></row><row><cell cols="2">Image-level Supervision Only.</cell><cell></cell><cell></cell></row><row><cell>SEC ECCV'2016</cell><cell></cell><cell>DeepLabV2</cell><cell>22.4</cell></row><row><cell>Saleh et al. TPAMI'2017</cell><cell></cell><cell>DeepLabV2</cell><cell>20.4</cell></row><row><cell>IAL IJCV'2020</cell><cell></cell><cell>DeepLabV2</cell><cell>27.7</cell></row><row><cell>SEAM CVPR'2020</cell><cell></cell><cell>WideResNet38</cell><cell>31.9</cell></row><row><cell>CONTA NeurIPS'2020</cell><cell>I</cell><cell>WideResNet38</cell><cell>32.8</cell></row><row><cell>CDA ICCV'2021</cell><cell></cell><cell>WideResNet38</cell><cell>33.2</cell></row><row><cell>PMM ICCV'2021</cell><cell></cell><cell>Res2Net101</cell><cell>35.7</cell></row><row><cell>Ours-M</cell><cell></cell><cell>DeepLabV2</cell><cell>36.1</cell></row><row><cell>Ours-L</cell><cell></cell><cell>DeepLabV2</cell><cell>36.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies of our proposed methods on the train and val set. The best results are highlighted in bold.</figDesc><table><row><cell cols="2">Backbone HP VWE VWL-M VWL-L train</cell><cell>val</cell></row><row><cell>ResNet50</cell><cell>48.3</cell><cell>47.0</cell></row><row><cell>ResNet101</cell><cell>49.5</cell><cell>48.4</cell></row><row><cell></cell><cell cols="2">54.0 +4.5 53.1 +4.7</cell></row><row><cell>ResNet101</cell><cell cols="2">55.1 +5.6 54.8 +6.4 56.9 +7.4 56.4 +8.0</cell></row><row><cell></cell><cell cols="2">57.3 +7.8 56.9 +8.5</cell></row><row><cell>et al.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The performance of the generated CAMs with different initialization methods. The results are evaluated on the PASCAL VOC train set.</figDesc><table><row><cell></cell><cell cols="2">VWL-L VWL-M</cell></row><row><cell>Random initialization</cell><cell>57.3</cell><cell>56.9</cell></row><row><cell>Random sample initialization</cell><cell>55.8</cell><cell>56.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>The classification accuracies of the VWL-L and VWL-M. The performance is evaluated on the PASCAL VOC train set.</figDesc><table><row><cell></cell><cell cols="2">VWL-L VWL-M</cell></row><row><cell>Acc (%)</cell><cell>81.3</cell><cell>84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Impact of the set of split size in HP. The results are evaluated on the PASCAL VOC train set.</figDesc><table><row><cell>r</cell><cell>{1}</cell><cell cols="3">{1, 2} {1, 2, 4} {1, 2, 4, 8}</cell></row><row><cell cols="2">train 56.2</cell><cell>56.1</cell><cell>57.3</cell><cell>57.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Comparison of the pooling method on the PASCAL VOC train set.</figDesc><table><row><cell></cell><cell cols="4">GAP GWRP LSE Our HP</cell></row><row><cell>train</cell><cell>49.5</cell><cell>50.6</cell><cell>51.6</cell><cell>54.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Comparison of the pooling method in the visual word learning process on the PASCAL VOC train set.</figDesc><table><row><cell></cell><cell cols="3">without VWL VWL-HP VWL-GAP</cell></row><row><cell>train</cell><cell>54.0</cell><cell>55.5</cell><cell>57.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Impact of hyper-parameters.</figDesc><table><row><cell></cell><cell>k</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>128 256 384 512</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>0.6 0.8 1.0 1.2</cell><cell>1e ?4 1e ?3 1e ?2 1e ?1</cell></row><row><cell cols="2">train 54.8 57.3 55.6 55.4</cell><cell cols="4">55.2 57.3 56.6 55.2</cell><cell>56.9 57.1 57.3 56.7</cell><cell>55.6 56.9 56.3 55.9</cell></row><row><cell>val</cell><cell>54.5 56.9 55.1 54.9</cell><cell cols="4">54.5 56.9 55.8 54.3</cell><cell>56.3 56.4 56.9 55.9</cell><cell>55.0 56.4 55.6 55.3</cell></row><row><cell></cell><cell>(a) Number of visual words</cell><cell></cell><cell cols="3">(b) Weight factor.</cell><cell>(c) Temperature parameter.</cell><cell>(d) Momentum coefficient.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://host.robots.ox.ac.uk:8080/anonymous/ XJDOJG.html 3 http://host.robots.ox.ac.uk:8080/anonymous/ J00QBG.html 4 http://host.robots.ox.ac.uk:8080/anonymous/ Y0XECB.html 5 http://host.robots.ox.ac.uk:8080/anonymous/ 0QVYDO.html</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="641" to="647" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4981" to="4990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Weakly supervised learning of instance segmentation with inter-pixel relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2209" to="2218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1437" to="1451" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Single-stage semantic segmentation from image labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Araslanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4253" to="4262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="549" to="565" />
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Mixup-cam: Weakly-supervised semantic segmentation via uncertainty regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation via sub-category exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8991" to="9000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Chen LC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
	<note>Semantic image segmentation with deep convolutional nets and fully connected crfs</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Reducing overfitting in deep networks by decorrelating representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations Cordts M</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cian: Cross-image affinity net for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="10762" to="10769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Res2net: A new multi-scale backbone architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="652" to="662" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6928" to="6938" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3203" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-erasing network for integral object attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="549" to="559" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7014" to="7023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Integral object mining via online attention accumulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2070" to="2079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Puzzle-cam: Improved localization via matching partial and full features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="639" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Universal weakly supervised segmentation by pixel-to-segment contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discriminative region suppression for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1754" to="1761" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="695" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Anti-adversarially manipulated attributions for weakly and semisupervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4071" to="4080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Bbam: Bounding box attribution map for weakly supervised semantic and instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2643" to="2652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Railroad is not a train: Saliency as pseudo-pixel supervision for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5495" to="5505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Groupwise semantic mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1984" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pseudo-mask matters in weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Block annotation: Better image annotation with sub-image decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Upchurch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno>arXiv:13124400</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">From bow to cnn: Two decades of texture representation for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietik?inen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="109" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Leveraging instance-, image-and dataset-level information for weakly supervised instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Background-aware pooling and noise-aware loss for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6913" to="6922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning bag-of-features pooling for deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5766" to="5774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, highperformance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Combining bottom-up, topdown, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3529" to="3538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning visual words for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</title>
		<meeting>the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="982" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Essential concepts of causal inference: a remarkable history and an intriguing future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics &amp; Epidemiology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="140" to="155" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Web-scale k-means clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th international conference on World wide web</title>
		<meeting>the 19th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1177" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Boxdriven class-wise region masking and filling rate guided loss for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3136" to="3145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Mining cross-image semantics for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="347" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using treebased algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning randomwalk label propagation for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7158" to="7166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Weaklysupervised semantic segmentation by iterative affinity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1736" to="1749" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12275" to="12284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Embedded discriminative attention mechanism for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16765" to="16774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Leveraging auxiliary tasks with affinity learning for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6984" to="6993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Non-salient region object mining for weakly supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2623" to="2632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Reliability does matter: An end-to-end weakly supervised semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12765" to="12772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1325" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
	<note>Conditional random fields as recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
