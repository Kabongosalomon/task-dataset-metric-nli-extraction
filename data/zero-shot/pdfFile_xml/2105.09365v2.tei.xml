<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EXPLORING THE LIMITS OF DATA AUGMENTATION FOR RETINAL VESSEL SEGMENTATION A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Sadi</forename><surname>Uysal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">?afak</forename><surname>Bilici</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Selin</forename><surname>Zaza</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Yigit</forename><surname>?zgen?</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Onur</forename><surname>Boyar</surname></persName>
							<email>boyaronur@gmail.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Yildiz Technical Universit? Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Yildiz Technical Universit? Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Industrial Engineering Yildiz Technical Universit? Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Quantitative Methods Marmara Universit? Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Department of Computational Science and Engineering Bogazi?i</orgName>
								<orgName type="institution">Universit? Istanbul</orgName>
								<address>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EXPLORING THE LIMITS OF DATA AUGMENTATION FOR RETINAL VESSEL SEGMENTATION A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retinal Vessel Segmentation is important for the diagnosis of various diseases. The research on retinal vessel segmentation focuses mainly on the improvement of the segmentation model which is usually based on U-Net [1] architecture. In our study, we use the U-Net architecture and we rely on heavy data augmentation in order to achieve better performance. The success of the data augmentation relies on successfully addressing the problem of input images. By analyzing input images and performing the augmentation accordingly we show that the performance of the U-Net model can be increased dramatically. Results are reported using the most widely used retina dataset, DRIVE.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical images come in different forms, including X-ray, computed tomography (CT), magnetic resonance imaging (MRI), ultrasound imaging, fundoscopic images and more. The medical experts analyze the targeted area in these kinds of images to make a diagnosis. This process can be made much easier by performing segmentation to regions that are meaningful for the task at hand. The segmentation task is achieved by extracting the target class while ignoring the objects from other classes. There are various approaches to solve these segmentation problems. There are signal processing-based approaches <ref type="bibr" target="#b0">[1]</ref>, heuristic techniques <ref type="bibr" target="#b1">[2]</ref>, Support Vector Machine based applications <ref type="bibr" target="#b2">[3]</ref>, and there are also deep learning applications <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, to name a few. Deep Learning applications have been becoming more popular in recent years thanks to the increasingly available computing power, and medical image segmentation is one of the areas these applications are commonly used. It is known that the success of the deep learning models heavily relies on the input data volume. In supervised techniques, we also need the annotations of this input data. Getting the necessary annotated data is always costly. Moreover, when we are dealing with medical problems, a high level of expert knowledge is also required. For this reason, the lack of annotated data proves to be a much more challenging problem in medical image analysis compared with other imaging problems. This problem especially appears when working with retinal vessel images. These images are being used to diagnose several diseases like diabetes, cardiac diseases, migraine, cataract etc. To perform the correct diagnosis, successful segmentation of the vessels is important because obscured details in the fundoscopic images make the decision making process challenging and hard for the medical experts.</p><p>Performing the segmentation of the vessels from the fundoscopic image in order to let the medical expert to perform better diagnosis is also a challenging task. The edges of the vessels are extremely thin and quite hard to segment and the quality of the input images are also questionable. Additionally, input images might be noisy and key information can end up becoming very hard to extract in order to detect diseases. A mistake in this process may cause false positive or false negative diagnosis. Finally, it is important to underline that the quality of the input image can be affected by many things including illumination, sensor noises, incorrect angle, type of the filter used in retinal fundus cameras, etc.</p><p>Successful segmentation of the retinal vessel segmentation has been widely studied and it is still identified as one of the hot topics for research. Different architectures are tailored for this specific problem and numerous existing deep learning architectures are used in order to perform segmentation tasks. The scarcity of the annotated data pushed researchers to use data augmentation to a certain amount in order to avoid the overfitting problem. However, the usage of data augmentation is limited in these studies. This study argues that if data augmentation strategy can successfully address the problems of the input data, a successful segmentation model can be obtained. In our study, we are looking for the performance gains that can be obtained by the excessive data augmentation using U-Net architecture for retinal vessel segmentation problems. We use the DRIVE dataset 2 that has become one of the standard benchmarks in the retinal vessel segmentation studies. <ref type="bibr" target="#b2">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The Retinal Vessel Segmentation problem has gained much interest in the literature in the last few years. There are different types of segmentation strategies with varying complexities.</p><p>Medical Image Segmentation studies rely heavily on the U-Net architecture <ref type="bibr" target="#b5">[6]</ref>. In <ref type="bibr" target="#b6">[7]</ref>, MResU-Net is derived from U-Net architecture by replacing convolutional layers with residual blocks in order to achieve better accuracy and increase the depth of the model to infer more features. In <ref type="bibr" target="#b4">[5]</ref>, authors introduced a GAN [8] based approach to segment the retinal vessels. They take the segmentation problem as an image translation problem. RV-GAN architecture has reported the best accuracy and AUC metrics so far. In <ref type="bibr" target="#b8">[9]</ref>, a cross connected Convolutional Neural Network architecture is used to perform the segmentation. In <ref type="bibr" target="#b9">[10]</ref>, authors proposed an U-Net based model which is combined with a Residual U-Net. They argue that their proposed model is a good trade-off between the model accuracy and the training time. In <ref type="bibr" target="#b10">[11]</ref>, authors have also proposed a U-Net based model that is using Deformable Convolutional Networks, DU-Net. In order to train the model, they crop the input images into small batches. Cropping the input image into small batches is a strategy to overcome the problem of scarcity of the annotated training data.</p><p>Data augmentation techniques have been widely used in various problems. Not only in medical imaging segmentation, but also in a wide variety of computer vision problems and in problems which rely on the tabular data <ref type="bibr" target="#b11">[12]</ref>. For the purposes of this study, we limit our review in their usage in image segmentation problems. In <ref type="bibr" target="#b5">[6]</ref>, the U-Net architecture is proposed for the first time and its usage with data augmentation is also covered. In <ref type="bibr" target="#b12">[13]</ref>, authors investigate the performance gain as a result of using data augmentation in medical image segmentation and discuss the common problems of medical image segmentation due to the scarcity of the available annotated data. A comprehensive study of the data augmentation in deep learning models is reported in <ref type="bibr" target="#b13">[14]</ref>. Data Augmentation in retinal image segmentation problem is not a widely studied area and our study is one of the most comprehensive studies that explores the performance gains from data augmentation in retinal vessel segmentation. In <ref type="bibr" target="#b14">[15]</ref>, authors lay out the effect of data augmentation in retinal image segmentation. However, their study is limited to rotated augmentations. In <ref type="bibr" target="#b15">[16]</ref>, the authors focus on looking into data augmentation in the retinal image segmentation problem and a method that gives a robust segmentation model is proposed. <ref type="bibr" target="#b16">[17]</ref> studies the data augmentation techniques for brain tumor segmentation problem.</p><p>Other than elastic transformations and affine image transformations which are rotation, flipping, scaling and cropping etc., they also study the generative techniques in order to perform the data augmentation. Generative Adversarial Networks have been widely used in medical image segmentation problems for both data augmentation techniques and as a segmentation model itself <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dataset</head><p>In  <ref type="figure">Figure 2</ref>: Two examples for a combination of predictions and ground truths. Red pixels are predicted vessels, white pixels are ground truth pixels. By (a), it is observed that the model errors due to the minor vessels. The model performs well in segmenting thick vessels as it is pointed in the right hand side of (a). The model performs better at segmenting the vessels in (b). Minor vessels are segmented better than (a) in this example.</p><p>imaging problems, the problem at the hand becomes even more difficult. If one can identify the problems of the input images well, the quality of the segmentation model can be increased with the help of the data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Proposed Methodology</head><p>We propose that optimal data augmentation can be successful for Retinal Vessel Segmentation, using simple U-Net architecture <ref type="bibr" target="#b5">[6]</ref>. Data augmentation techniques are helpful for three reasons. First, they are helpful because the input data is very scarce. Data augmentation techniques increase the input image size and provide the model some extra information to learn. Second, through data augmentation we can recover some performance loss that occurred in the models due to the image quality. If the practitioner decides the data augmentation technique based on the problems of the input image the segmentation performance can be increased. Third, data augmentation will help with the segmentation model we used, which is the U-Net architecture that makes use of pooling operations. The model learns relatively lower from the corner and side parts of the input image.</p><p>Data augmentation strategies can address all these three problems. In order to address the third problem, we add rotated versions of the input images to the dataset using various angles. We use data augmentation techniques that rely on adding noise to the original image so that our model can learn more from the noisy images. Noise data come from the normal distribution with mean 0 and standard deviation . In our study we use augmentations with different epsilon values each greater than or equal to 1. Another technique we use is dropout, which targets input pixels. In dropout data augmentation technique, pixels of the input image are set to zero in a random fashion. The portion of the pixel values to be set to zero is a parameter that needs to be defined. It is well-known that minor vessels are one of the hardest regions to segment. <ref type="figure">Figure 2</ref> shows the predicted and ground truth vessels together. It can be seen that the majority of the incorrect segmentations occur at the minor vessels. An attempt to increase the success of the segmentation model might be to zoom to these regions and add zoomed images to the dataset. Randomly cropping the input image with the random sizes might be another strategy here. We use shifting and flipping of the input image which are also two successful data augmentation techniques. They are widely used with U-Net model training because U-Net uses convolutional filters. Convolutional filters miss the information in the edge of images. Shifting technique pushes the edges of images to the more central part of the image so that the U-Net model can learn the information in the edges of the original image from this augmented image. In order to address the image quality problems that occurred due to the brightness of the input image, we use gamma correction technique. Full list of augmentations are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation Details</head><p>For our method, we use the same architecture from U-Net <ref type="bibr" target="#b5">[6]</ref>. We train our models with the Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with learning rate of 1e-4, ? 1 = 0.9, ? 2 = 0.999. We don't use any learning rate scheduling algorithms. For experiments on DRIVE, we use mini-batches of size 3. We use a dropout probability of 0.1 on the fourth and fifth convolutional layers. The training is done by using binary cross-entropy loss. In our study, we experimented with dice loss and the combination of binary cross-entropy loss and dice loss <ref type="bibr" target="#b19">[20]</ref> as well. Nevertheless, it is observed that the best results are Year AUC Accuracy U-Net <ref type="bibr" target="#b10">[11]</ref> 2018 0.9830 0.9681 Residual UNet <ref type="bibr" target="#b6">[7]</ref> 2019 0.9779 -IterNet <ref type="bibr" target="#b21">[22]</ref> 2019 0.9816 0.9574 Wang et al. <ref type="bibr" target="#b14">[15]</ref> 2019 0.9814 0.9573 Sun et al. <ref type="bibr" target="#b15">[16]</ref> 2020 0.9788 0.9545 CcNet <ref type="bibr" target="#b22">[23]</ref> 2020 0.9678 0.9528 SUD-GAN <ref type="bibr" target="#b23">[24]</ref> 2020 0.9786 0.9560 RV-GAN <ref type="bibr" target="#b4">[5]</ref> 2020 0.9887 0.9790 This Paper 2021 0.9855 0.9712 obtained using binary cross-entropy loss.</p><p>We implement all our models in Keras <ref type="bibr" target="#b20">[21]</ref> and train them on a single RTX 2080. We train our models for 15 epochs, each epoch takes 10-15 minutes. We don't use pre-trained weights in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>As seen in <ref type="table" target="#tab_2">Table 2</ref>., our approach achieves a 0.9855 Area Under the Curve (AUC) and 0.9712 accuracy score. It outperforms most of the models that have more complex architectures in the literature. Our model also outperforms other data augmentation-based studies in the literature <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. It is observed that RV-GAN <ref type="bibr" target="#b5">[6]</ref> has the highest AUC and accuracy scores. However, the training time of their model takes up to 48 hours. Even though we train our model for about 3 hours, we are only 0.0032 AUC, 0.0078 accuracy points behind RV-GAN. Studies reported in <ref type="table" target="#tab_2">Table 2</ref> did not include the dice coefficient score of their models although it is a common metric in image segmentation tasks. Our method achieved a 0.8255 mean dice score on the DRIVE dataset.</p><p>The development of the model with the highest metrics is done by adding new augmentation techniques in each iteration. Different augmentation techniques provided the model a new information and therefore a performance gain. The first set of augmentation techniques that are included are augmentations obtained by rotating and flipping the input images. At each iteration, results of the model have evaluated and new augmentation techniques are added based on the areas that the model failed to perform well. The first observation was the lack of performance in thin vessels which lead us to include shifted, zoomed and cropped images to the model dataset. After gaining considerable amount of accuracy, in the next iteration the impact of white noise and elastic deformations is studied to perform better segmenting the noisy images. Another major problem about the DRIVE dataset is the brightness of some of the input images. To overcome this problem, new augmented images using gamma correction technique are added to model dataset. Finally, extra gain in performance is pursued by adding new augmented images using augmentation techniques like blurring, droput, histogram equalization and distortion techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a segmentation model that relies on the heavy usage of data augmentation techniques. We use the standard U-Net architecture and outperform most of the more complex architectures in the literature. The data augmentation strategy is governed by the problems about the input images caused by the fundus camera and the environment. Various types of data augmentation techniques are used individually and collectively. Data augmentation strategy also takes the drawbacks of U-Net architecture and makes use of various rotated versions of the augmented images into account in order not to lose valuable information due to pooling operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgements</head><p>This study is a part of the inzva AI Projects program. We would like to thank to inzva community for providing us a valuable research environment.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Training sample from DRIVE dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>List of augmentations we use. Affine Transformation Elastic Transformation Pixel-level Transformation</figDesc><table><row><cell>Rotation</cell><cell>Elastic Deformations</cell><cell>White Noise</cell></row><row><cell>Flipping</cell><cell>Grid Distortion</cell><cell>Gamma Correction</cell></row><row><cell>Zoom Out</cell><cell>Optical Distortion</cell><cell>Equalize Histogram</cell></row><row><cell>Random Cropping</cell><cell></cell><cell>Dropout</cell></row><row><cell>Shifting</cell><cell></cell><cell>Sharpening</cell></row><row><cell>Shearing</cell><cell></cell><cell>Blurring</cell></row><row><cell></cell><cell></cell><cell>Contrast</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Retinal Vessel Segmentation problem, the most widely used dataset is DRIVE 2 dataset. Each paper in recent years have reported their model performance metrics on this dataset. DRIVE dataset has 20 train and 20 test images. Each image in the training set addresses a different kind of disease. It is quite challenging to create a high-performance model with this amount of annotated data. We use DRIVE dataset to address these problems.The problem we are focusing on is the segmentation of retinal images with input data that has quality problems.</figDesc><table><row><cell>(a)</cell><cell>(b)</cell></row><row><cell>4 Experimental Setup</cell><cell></cell></row><row><cell>4.1 Problem Definition</cell><cell></cell></row></table><note>Moreover, the amount of the annotated data is very limited. Problems about the input data may occur due to various reasons such as illumination, sensor noise, filters of the retinal camera, the input image angle, and other noises. Using the data with such defects limits the capability of the segmentation model. Most of the time the model is unable to segment the regions with noise due to the fact that the model does not have enough information about the regions trying to be segmented. For example, in the DRIVE dataset, each image in the training set addresses a different disease. Each image may come with different quality problems. Combining this with the input image scarcity problem of the medical</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison on the DRIVE dataset. Paper</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://drive.grand-challenge.org/ 3 All models and methods are available at https://github.com/onurboyar/Retinal-Vessel-Segmentation</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Retinal vessel segmentation using the 2-D Gabor wavelet and supervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V B</forename><surname>Soares</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2006.879967</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1214" to="1222" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An effective retinal blood vessel segmentation method using multi-scale line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="703" to="715" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Retinal Blood Vessel Segmentation Using Line Operators and Support Vector Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renzo</forename><surname>Perfetti</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2007.898551</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="1357" to="1365" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2016</title>
		<editor>Sebastien Ourselin et al. Cham</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="978" to="981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharif Amit Kamran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00535</idno>
		<title level="m">Retinal Vessel Segmentation from Fundus Images using Multi-scale Generative Adversarial Networks. 2021</title>
		<imprint/>
	</monogr>
	<note>eess.IV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. U-Net</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04597</idno>
		<title level="m">Convolutional Networks for Biomedical Image Segmentation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Residual U-Net for Retinal Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICIP.2019.8803101</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1425" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
	</analytic>
	<monogr>
		<title level="j">Generative Adversarial Networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CC-NET: Image Complexity Guided Network Compression for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Mishra</surname></persName>
		</author>
		<idno type="DOI">10.1109/ISBI.2019.8759448</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="57" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chaining a U-Net With a Residual U-Net for Retinal Blood Vessels Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gendry</forename><surname>Alfonso</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.2975745</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Access PP</title>
		<imprint>
			<date type="published" when="2020-02" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">DUNet: A deformable network for retinal vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangguo</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.knosys.2019.04.025%7D</idno>
		<ptr target="http://dx.doi.org/10.1016/j.knosys.2019.04.025%7D" />
	</analytic>
	<monogr>
		<title level="m">Knowledge-Based Systems</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="page" from="149" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Differential Data Augmentation Techniques for Medical Imaging Classification Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeshan</forename><surname>Hussain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Symposium proceedings. AMIA Symposium</title>
		<imprint>
			<date type="published" when="2017-04" />
			<biblScope unit="page" from="979" to="984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improving Data Augmentation for Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Eaton-Rosen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on Image Data Augmentation for Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Shorten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="48" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data Augmentation is More Important Than Model Architectures for Retinal Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaolei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3348416.3348425</idno>
		<ptr target="https://doi.org/10.1145/3348416.3348425" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 International Conference on</title>
		<meeting>the 2019 International Conference on<address><addrLine>Ningbo, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="48" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Robust Retinal Vessel Segmentation from a Data Augmentation Perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.15883</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>eess.IV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data Augmentation for Brain-Tumor Segmentation: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Nalepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kawulok</surname></persName>
		</author>
		<idno type="DOI">https:/www.frontiersin.org/article/10.3389/fncom.2019.00083</idno>
		<idno>p. 83. ISSN: 1662-5188. DOI: 10.3389/ fncom.2019.00083</idno>
		<ptr target="https://www.frontiersin.org/article/10.3389/fncom.2019.00083" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Data Augmentation for Brain-Tumor Segmentation: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Nalepa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Kawulok</surname></persName>
		</author>
		<idno type="DOI">10.3389/fncom.2019.00083</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2019-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalised Dice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carole</forename><forename type="middle">H</forename><surname>Sudre</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-67558-9_28</idno>
		<idno>DOI: 10.1007/ 978-3-319-67558-9_28</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-67558-9_28" />
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="page" from="1611" to="3349" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras.2015" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">IterNet: Retinal Image Segmentation Utilizing Structural Redundancy in Vessel Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangzhi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.05763</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>eess.IV</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CcNet: A cross-connected convolutional network for segmenting retinal vessels using multi-scale features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouting</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2018.10.098</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0925231219304655" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">392</biblScope>
			<biblScope unit="page" from="925" to="2312" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SUD-GAN: Deep Convolution Generative Adversarial Network Combined with Short Connection and Dense Block for Retinal Vessel Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10278-020-00339-9</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Digital Imaging</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020-04" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
