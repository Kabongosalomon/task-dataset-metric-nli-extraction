<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandra</forename><surname>Shysheya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
							<email>kahofman@microsoft.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
							<email>nowozin@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contextual Squeeze-and-Excitation for Efficient Few-Shot Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent years have seen a growth in user-centric applications that require effective knowledge transfer across tasks in the low-data regime. An example is personalization, where a pretrained system is adapted by learning on small amounts of labeled data belonging to a specific user. This setting requires high accuracy under low computational complexity, therefore the Pareto frontier of accuracy vs. adaptation cost plays a crucial role. In this paper we push this Pareto frontier in the few-shot image classification setting with a key contribution: a new adaptive block called Contextual Squeeze-and-Excitation (CaSE) that adjusts a pretrained neural network on a new task to significantly improve performance with a single forward pass of the user data (context). We use meta-trained CaSE blocks to conditionally adapt the body of a network and a fine-tuning routine to adapt a linear head, defining a method called UpperCaSE. UpperCaSE achieves a new state-of-the-art accuracy relative to meta-learners on the 26 datasets of VTAB+MD and on a challenging real-world personalization benchmark (ORBIT), narrowing the gap with leading fine-tuning methods with the benefit of orders of magnitude lower adaptation cost.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, the growth of industrial applications based on recommendation systems <ref type="bibr" target="#b3">(Bennett et al., 2007)</ref>, speech recognition <ref type="bibr" target="#b44">(Xiong et al., 2018)</ref>, and personalization  has sparked an interest in machine learning techniques that are able to adapt a model on small amounts of data belonging to a specific user. A key factor in many of these applications is the Pareto frontier of accuracy vs. computational complexity (cost to adapt). For example, in a real-time classification task on a phone, a pretrained model must be personalized by exploiting small amounts of data on the user's device (context). In these applications the goal is twofold: maximize the classification accuracy on unseen data (target) while avoiding any latency and excessive use of computational resources.</p><p>Methods developed to face these challenges in the few-shot classification setting, can be grouped in two categories: meta-learning and fine-tuning. Meta-learning is based on the idea of learning-how-tolearn by improving the algorithm itself <ref type="bibr" target="#b34">(Schmidhuber, 1987;</ref><ref type="bibr" target="#b16">Hospedales et al., 2020)</ref>. Meta-learners are trained across multiple tasks to ingest a labeled context set, adapt the model, and predict the class membership of an unlabeled target point. Fine-tuning methods adjust the parameters of a pretrained neural network on the task at hand by iterative gradient-updates <ref type="bibr" target="#b7">(Chen et al., 2019;</ref><ref type="bibr" target="#b39">Triantafillou et al., 2019;</ref><ref type="bibr" target="#b38">Tian et al., 2020;</ref><ref type="bibr" target="#b19">Kolesnikov et al., 2020;</ref><ref type="bibr" target="#b9">Dumoulin et al., 2021)</ref>.</p><p>We can gain an insight on the differences between those two paradigms by comparing them in terms of accuracy and adaptation cost. <ref type="figure">Figure 1</ref> illustrates this comparison by showing on the vertical axis the average classification accuracy on the 18 datasets of the Visual Task Adaptation Benchmark (VTAB, <ref type="bibr" target="#b9">Dumoulin et al. 2021)</ref>, and on the horizontal axis the adaptation cost measured as the number of multiply-accumulate operations (MACs) required to adapt on a single task (see Appendix C.1 for details). Overall, fine-tuners achieve a higher classification accuracy than meta-learners but are more expensive to adapt. The comparison between two state-of-the-art methods for both categories, Big Transfer (BiT, <ref type="bibr" target="#b19">Kolesnikov et al. 2020</ref>) and LITE , shows a substantial performance gap of 14% in favor of the fine-tuner but at a much higher adaptation cost, with BiT requiring 526 ? 10 12 MACs and LITE only 0.2 ? 10 12 MACs. <ref type="figure">Figure 1</ref>: Accuracy and adaptation cost on VTAB for meta-learners (blue), finetuners (red), and hybrids (blue-red). Black dotted-line is the previous Pareto front across categories. UpperCaSE narrows the gap with the leading fine-tuning method and represents the best trade-off in terms of accuracy/adaptation-cost.</p><p>It is crucial to find solutions that retain the best of both worlds: the accuracy of fine-tuners and low adaptation cost of meta-learners. The main bottleneck that hampers the adaptation of fine-tuners is the need for multiple gradient adjustments over the entire set of network parameters. Restricting those adjustments to the last linear layer (head) significantly speeds up fine-tuning, but it harms performance (e.g. see experiments in Section 5.1). Finding a way to rapidly adapt the feature extractor (body) is therefore the main obstacle to bypass. In this paper we propose a hybrid solution to this issue, exploiting meta-learned adapters for rapidly adjusting the body and a fine-tuning routine for optimizing the head.</p><p>At the core of our approach is a novel extension of the popular Squeeze-and-Excitation block proposed by <ref type="bibr" target="#b17">Hu et al. (2018)</ref> to the meta-learning setting that we call Contextual Squeeze-and-Excitation (CaSE). We exploit CaSE as building block of a hybrid training protocol called UpperCaSE which is based on the idea of adjusting the body of the network in a single forward pass over the context, and reserving the use of expensive fine-tuning routines for the linear head, similarly to methods like MetaOptNet <ref type="bibr" target="#b20">(Lee et al., 2019)</ref>, R2D2 <ref type="bibr" target="#b4">(Bertinetto et al., 2018)</ref>, and ANIL <ref type="bibr" target="#b29">(Raghu et al., 2019)</ref>. <ref type="figure">Figure 1</ref> shows how UpperCaSE substantially improves the performance in the low-cost regime, outperforming meta-learners, fine-tuners such as MD-Transfer <ref type="bibr" target="#b39">(Triantafillou et al., 2019)</ref>, and reducing the gap with the current state of the art (BiT). When adaptation cost is critical, UpperCaSE is the best method currently available since it can provide substantial computation savings and compelling classification performance.</p><p>Our contributions can be summarized as follows:</p><p>1. We introduce a new adapter called Contextual Squeeze-and-Excitation (CaSE), based on the popular Squeeze-and-Excitation model proposed by <ref type="bibr" target="#b17">Hu et al. (2018)</ref>, that outperforms other adaptation mechanisms (e.g. the FiLM generators used in <ref type="bibr" target="#b5">Bronskill et al. 2021</ref>) in terms of parameter efficiency (a 75% reduction in the number of adaptation parameters) and classification accuracy (a 1.5% improvement on MetaDataset and VTAB). The code is released with an open-source license 1 . 2. We use CaSE adaptive blocks in conjuction with a fine-tuning routine for the linear head in a model called UpperCaSE, reporting an improved classification accuracy compared to the SOTA meta-learner  on the 8 datasets of MDv2 (+2.5% on average) and the 18 datasets of VTAB (+6.8% on average), narrowing the gap with BiT <ref type="bibr" target="#b19">(Kolesnikov et al., 2020)</ref> with the benefit of orders of magnitude lower adaptation cost. 3. We showcase the potential of UpperCaSE in a real-world personalization task on the ORBIT dataset , where it compares favorably with the leading methods in the challenging cross-domain setting (training on MDv2, testing on ORBIT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Contextual Squeeze-and-Excitation (CaSE)</head><p>Problem formulation In this paragraph we introduce the few-shot learning notation, as this will be used to describe the functioning of a CaSE adaptive block. Let us define a collection of meta-training tasks as D = {? 1 , . . . , ? D } where ? i = (C i , T i ) represents a generic task composed of a context set C i = {(x, y) 1 , . . . , (x, y) M } and a target set T i = {(x, y) 1 , . . . , (x, y) D } of input-output pairs. Following common practice we use the term shot to identify the number of samples per class (e.g. 5-shot is 5 samples per class) and the term way to identify the number of classes (e.g. 10-way is 10 classes per task). Given an evaluation task ? * = {C * , x * } the goal is to predict the true label y * of the unlabeled target point x * conditioned on the context set C * .</p><p>In fine-tuning methods, we are given a neural network f ? (?), with parameters ? estimated via standard supervised-learning on a large labeled dataset (e.g. ImageNet). Given a test task ? * adaptation consists of minimizing the loss L(?) via gradient updates to find the task-specific parameters</p><formula xml:id="formula_0">? ? * ? G( , L, ? * , f ? ),</formula><p>where is a learning rate, and G(?) is a functional representing an iterative routine that returns the adapted parameters ? ? * (used for prediction). This procedure is particularly effective because it can exploit efficient mini-batching, parallelization, and large pretrained models.</p><p>In meta-learning methods training and evaluation are performed episodically <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref>, with training tasks sampled from a meta-train dataset and evaluation tasks sampled from an unseen meta-test dataset. The distinction in tasks is exploited to define a hierarchy. The parameters are divided in two groups: ? task-common parameters shared across all tasks (top of the hierarchy), and ? ? task-specific parameters estimated on the task at hand as part of an adaptive mechanism (bottom of the hierarchy). The way ? and ? ? come into play is method dependent; they can be estimated via gradient updates (e.g. <ref type="bibr">MAML, Finn et al. 2017</ref>), learned metrics (e.g. ProtoNets, <ref type="bibr" target="#b36">Snell et al. 2017)</ref>, or Bayesian methods <ref type="bibr" target="#b13">(Gordon et al., 2018;</ref><ref type="bibr" target="#b27">Patacchiola et al., 2020;</ref><ref type="bibr" target="#b35">Sendera et al., 2021)</ref>.</p><p>Standard Squeeze-Excite (SE) We briefly introduce standard SE <ref type="bibr" target="#b17">(Hu et al., 2018)</ref>, as we are going to build on top of this work. SE is an adaptive layer used in the supervised learning setting to perform instance based channel-wise feature adaptation, which is trained following a supervised protocol together with the parameters of the neural network backbone. Given a convolutional neural network, consider a subset of L layers and associate to each one of them a Multi-Layer Perceptron (MLP), here represented as a function g ? (?). The number of hidden units in the MLP is defined by the number of inputs divided by a reduction factor. Given a mini-batch of B input images, each convolution produces an output of size B ? C ? H ? W where C is the number of channels, H the height, and W the width of the resulting tensor. For simplicity we split this tensor into sub-tensors that are grouped into a set {H 1 , . . . , H B } with H i ? R C?H?W . To avoid clutter, we suppress the layer indexing when possible. SE perform a spatial pooling that produces a tensor of shape B ? C ? 1 ? 1; this can be interpreted as a set of vectors {h 1 , . . . , h B } with h i ? R C . For each layer l, the set is passed to the associated MLP that will generate an individual scale vector</p><formula xml:id="formula_1">? i ? R C , where ? (l) 1 = g (l) ? h (l) 1 ? ? ? ? (l) B = g (l) ? h (l) B .<label>(1)</label></formula><p>An elementwise product is then performed between the scale vector and the original tensor</p><formula xml:id="formula_2">H (l) 1 = H (l) 1 * ? (l) 1 ? ? ?? (l) B = H (l) B * ? (l) B ,<label>(2)</label></formula><p>with the aim of modulating the activation along the channel dimension. This operation can be interpreted as a soft attention mechanism, with the MLP conditionally deciding which channel must be attended to. A graphical representation of SE is provided in <ref type="figure" target="#fig_0">Figure 2</ref> (left).</p><p>Contextual Squeeze-Excite (CaSE) Standard SE is an instance-based mechanism that is suited for i.i.d. data in the supervised setting. In a meta-learning setting we can exploit the distinction in tasks to define a new version of SE for task-based channel-wise feature adaptation. For a task ? = (C, T ), consider the N images from the context set C, and the tensors produced by each convolution in the layers of interest {H 1 , . . . , H N } with H i ? R C?H?W . As in standard SE, we first apply a spatial pooling to each tensor H i which produces N vectors {h 1 , . . . , h N } of shape h i ? R C . Then a context pooling is performed; this corresponds to an empirical mean over {h 1 , . . . , h N } (see Appendix A for more details about context pooling). The pooled representation is passed to the associated MLP to produce a single scale-vector for that layer which is then multiplied elementwise by the original tensor?</p><formula xml:id="formula_3">? (l) = g (l) ? h (l) withh (l) = 1 N h (l) 1 + ? ? ? + h (l) N ,<label>(3)</label></formula><formula xml:id="formula_4">H (l) 1 = H (l) 1 * ? (l) ? ? ?? (l) N = H (l) N * ? (l) .<label>(4)</label></formula><p>The scale vector is estimated in adaptive mode and transferred to the target points T in inference mode (no forward pass on the MLPs), as shown in the rightmost part of <ref type="figure" target="#fig_0">Figure 2</ref>. In synthesis, the three major differences between SE and CaSE are: (i) CaSE uses a contextual pooling with the aim of generating an adaptive vector per-task instead of per-instance as in SE; (ii) CaSE distinguishes between an adaptive mode and an inference mode that transfers the scale from context to target, while SE does not make such a distinction; and (iii) CaSE parameters are estimated via episodic meta-training while SE parameters via standard supervised-training. In Section 5.1 we show that those differences are fundamental to achieve superior performance in the few-shot setting. A representation of a CaSE block is reported in <ref type="figure" target="#fig_0">Figure 2</ref> (right), additional technical details are provided in Appendix A.</p><p>Comparison with other adapters Popular adaptation mechanisms for few-shot learning are based on Feature-wise Linear Modulation layers (FiLM, <ref type="bibr" target="#b28">Perez et al. 2018)</ref>. Those mechanisms perform adaptation using a separate convolutional set-encoder to produce a context embedding. The embedding is forwarded to local MLPs to produce the scale and shift vectors of the FiLM layers that modulate a pretrained model. Variations of this adapter have been used in several methods, such as TADAM <ref type="bibr" target="#b26">(Oreshkin et al., 2018)</ref>, CNAPs <ref type="bibr" target="#b33">(Requeima et al., 2019)</ref>, SimpleCNAPs <ref type="bibr" target="#b1">(Bateni et al., 2020)</ref>, CAVIA <ref type="bibr" target="#b45">(Zintgraf et al., 2019)</ref>, and LITE . We will use the generic term FiLM generator to refer to these adapters and the term FiLM to refer to the scale and shift vectors used to modulate the activations. There are two key differences between FiLM and CaSE: (i) CaSE exploits context pooling to aggregate the activations of the backbone instead of a separate set-encoder as in FilM generators (see Appendix A for details) which is more efficient in terms of parameter count and implementation overhead; and (ii) FiLM uses scale and shift to modulate the activations, CaSE only the scale, therefore 50% less parameters are stored in memory and transferred during inference. In Section 5.1 we compare CaSE and the FiLM generators used in a recent SOTA method <ref type="bibr">(LITE, Bronskill et al. 2021)</ref>, showing that CaSE is superior in terms of accuracy while using a fraction of the amortization parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">UpperCaSE: system description and optimization protocol</head><p>We exploit CaSE blocks as part of UpperCaSE, a hybrid training protocol based on Coordinate-Descent (CD). We call this protocol hybrid because it combines a meta-training procedure to optimize the CaSE parameters (body) with a fine-tuning routine to estimate the task-specific parameters (head).</p><p>Preliminaries We are given a feature extractor (body) pretrained with supervised learning on a large dataset (e.g. ImageNet), defined as b ? (?) where ? are the pretrained parameters. CaSE blocks, parameterized by ?, are added to the model at specific locations to give b ?,? (?) (see Appendix A for details about this step). We are interested in learning the CaSE parameters ? keeping constant the pretrained parameters ? (omitted from here to keep the notation uncluttered). At training time, we are given a series of tasks ? = {C, T } ? D, where D is the training set. The number of classes (way) is calculated from the context set and used to define a linear classification head h ? ? (?) parameterized by ? ? . The complete model is obtained by nesting the two functions as h ? ? (b ? (?)). We indicate a forward pass through the body over the context inputs with the shorthand b</p><formula xml:id="formula_5">? (C x ) ? {z 1 , . . . , z N },</formula><p>where z n is the context embedding for the input x n . All the context embeddings and the associated labels are stored in M = {(z n , y n )} N n=1 . Optimization challenges We have two sets of learnable parameters, ? the CaSE parameters, and ? ? the parameters of the linear head for the task ? . While ? is shared across all tasks (task-common), ? ? must be inferred on the task at hand (task-specific). In both cases, the objective is the minimization of a classification loss L. There are some challenges in optimizing the CaSE parameters in the body, as shown by the decomposition of the full gradient</p><formula xml:id="formula_6">dL d? = ? ?L ? ?? ? d? ? d? + ?L ? ?? .<label>(5)</label></formula><p>The first term ?L ? /?? ? (sensitivity of the loss w.r.t. the head) and the direct gradient ?L/?? (sensitivity of the loss w.r.t. the adaptation parameters with a fixed head) can be obtained with auto-differentiation as usual. The second term d? ? /d? (sensitivity of the head w.r.t. the adaptation parameters) is problematic because ? ? is obtained iteratively after a sequence of gradient updates.</p><p>Backpropagating the gradients to ? includes a backpropagation through all the gradient steps performed to obtain the task-specific ? ? . Previous work has showed that this produces instability, vanishing gradients, and high memory consumption <ref type="bibr" target="#b0">(Antoniou et al., 2018;</ref><ref type="bibr" target="#b30">Rajeswaran et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-training via Coordinate-Descent</head><p>A potential solution to these issues is the use of implicit gradients <ref type="bibr" target="#b8">(Chen et al., 2020;</ref><ref type="bibr" target="#b30">Rajeswaran et al., 2019;</ref><ref type="bibr" target="#b6">Chen et al., 2022)</ref>. The main problem with implicit gradients is the computation and inversion of the Hessian matrix as part of Cauchy's implicit function theorem, which is infeasible when the number of parameters in the linear head is large. Another possible solution is the use of an alternating-optimization scheme, similar to the one proposed in a number of recent methods such as MetaOptNet <ref type="bibr" target="#b20">(Lee et al., 2019)</ref>, R2D2 <ref type="bibr" target="#b4">(Bertinetto et al., 2018)</ref>, and ANIL <ref type="bibr" target="#b29">(Raghu et al., 2019)</ref>. These methods share the idea of inner-loop-head/outerloop-body meta-training, and they find the parameters of the linear head with closed form solutions or by stochastic optimization. Starting from similar assumptions we propose a simple yet effective alternating-optimization scheme, which we formalize using Coordinate-Descent (CD) <ref type="bibr" target="#b42">(Wright, 2015)</ref>. The idea behind CD is to consider the minimization of a complex multi-variate function as a set of simpler objectives that can be solved one at a time. In our case, we can consider the joined landscape w.r.t. ? and ? ? as composed of two separate sets of coordinates (block CD, Wright 2015). By minimizing ? ? first, we reach a local minimum where ?L ? /?? ? ? 0. Therefore CD induces a direct optimization objective w.r.t. ?, with Equation (5) reducing to ?L ? /?? (no red term). The time complexity of this method is only affected by the number of classes but is constant w.r.t. the number of training points due to the use of mini-batching, which scales well with large tasks (e.g. those in MetaDataset and VTAB). See Appendix B for more details.</p><p>In practice, at each training iteration we sample a task ? = (C, T ) ? D, perform a forward pass on the body (with CaSE in adaptive mode) to get</p><formula xml:id="formula_7">b ? (C x ) ? {z 1 , . . . , z N }.<label>(6)</label></formula><p>The context embeddings are temporarily stored in a buffer with their associated labels M = {(z n , y n )} N n=1 to avoid expensive calls to b ? (?). We then set the head parameters to zero, and solve the first minimization problem (inner-loop), obtaining the task-specific parameters ? ? via</p><formula xml:id="formula_8">? ? ? G ?, M, L, h ? ?<label>(7)</label></formula><p>where ? is a learning rate, and G(?) is a functional representing an iterative gradient-descent routine for parameter estimation (e.g. maximum likelihood estimation or maximum a posteriori estimation). Note that the iterative routine in Equation <ref type="formula" target="#formula_8">(7)</ref> only relies on the head h ? ? (?) and not on the body b ? (?), which is the primary source of memory savings and the crucial difference with common fine-tuning methods. Moreover, the inner-loop is agnostic to the choice of optimizer, it can handle many gradient steps without complications, exploit parallelization and efficient mini-batching.</p><p>We then turn our attention to the second coordinate: the task-common parameters of the CaSE blocks in the body. For a single task, the update consists of a single optimization step w.r.t. ? (outer-loop) given support/target points and the task-specific parameters ? ? identified previously. The final form of the equation depends on the optimizer, for a generic SGD the update is given by</p><formula xml:id="formula_9">? ? ? ? ?? ? L C y ? Q y , h ? ? , b ? ,<label>(8)</label></formula><p>where ? is a learning rate. CaSE blocks must be in adaptive mode to allow the backpropagation of the gradients to the MLPs. The process repeats, alternating the minimization along the two sets of coordinates. The pseudo-code for train and test is provided in Appendix B.</p><p>Inference on unseen tasks After the training phase, we are given an unseen task ? * = (C * , x * ) where x * is a single target input and y * the associate true label to estimate. Inference consists of three steps: (i) forward pass on the body for all the context inputs with CaSE set to adaptive mode as in Equation <ref type="formula" target="#formula_7">(6)</ref> and embeddings/labels stored in M, (ii) estimation of the task-specific parameters ? * via iterative updates as in Equation <ref type="formula" target="#formula_8">(7)</ref>, and (iii) inference of the target-point membership via a forward pass over body and head? * = h ? * (b ? (x * )) with CaSE in inference mode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related work</head><p>Meta-learning There has been a large volume of publications related to meta-learning. Here we focus on those methods that are the most related to our work, and refer the reader to a recent survey for additional details <ref type="bibr" target="#b16">(Hospedales et al., 2020)</ref>. LITE ) is a protocol for training meta-learners on large images, that achieved SOTA accuracy on VTAB+MD. LITE is particularly relevant in this work, as its best performing method is based on Simple CNAPs <ref type="bibr" target="#b1">(Bateni et al., 2020)</ref> that exploits FiLM for fast body adaptation. We compare against LITE in Section 5.2 showing that UpperCaSE is superior in terms of classification accuracy and parameter efficiency. Fine-tuning only the last linear layer can be effective <ref type="bibr" target="#b2">(Bauer et al., 2017;</ref><ref type="bibr" target="#b38">Tian et al., 2020)</ref>. We compare against this baseline in Section 5.1, showing that adapting the body via CaSE significantly boosts the performance.</p><p>Hybrids Hybrid methods are trained episodically like meta-learners but rely on fine-tuning routines for adaptation. Model Agnostic Meta-Learning (MAML, <ref type="bibr" target="#b11">Finn et al. 2017</ref>) finds a set of parameters that is a good starting point for adaptation towards new tasks in a few gradient steps. MAML has been the inspiration for a series of other models such as MAML++ <ref type="bibr" target="#b0">(Antoniou et al., 2018)</ref>, ProtoMAML <ref type="bibr" target="#b39">(Triantafillou et al., 2019)</ref>, and Reptile <ref type="bibr" target="#b23">(Nichol et al., 2018)</ref>.</p><p>Dynamic networks CaSE blocks belong to the wider family of dynamic networks, models that can adapt their structure or parameters to different inputs <ref type="bibr" target="#b15">(Han et al., 2021)</ref>. Adaptive components have been used in a variety of applications, such as neural compression <ref type="bibr" target="#b40">(Veit and Belongie, 2018;</ref><ref type="bibr" target="#b43">Wu et al., 2018)</ref>, generation of artistic styles <ref type="bibr" target="#b10">(Dumoulin et al., 2016;</ref><ref type="bibr" target="#b18">Huang and Belongie, 2017)</ref>, or routing <ref type="bibr" target="#b14">(Guo et al., 2019)</ref>. Residual adapters <ref type="bibr" target="#b31">(Rebuffi et al., 2017</ref><ref type="bibr" target="#b32">(Rebuffi et al., , 2018</ref> have been used in transfer learning (non few-shot) but they rely on fine-tuning routines which are significantly slow during adaptation. More recently, <ref type="bibr" target="#b21">Li et al. (2022)</ref> have used serial and residual adapters in the few-shot setting, with the task-specific weights being adapted from scratch on the context set. This approach has similar limitations, since it requires backpropagation to the task-specific weights in the body of the network which is costly. In <ref type="bibr" target="#b37">Sun et al. (2019)</ref> the authors introduce a Meta-Transfer Learning (MTL) method for the few-shot setting. In MTL a series of scale and shift parameters are meta-learned across tasks and then dynamically adapted during the test phase via fine-tuning. This method suffers of similar limitations, as the fine-tuning stage is expensive during adaptation. Moreover, MTL relies on scale and shift vectors to perform adaptation whereas CaSE only relies on a scale vector, meaning that it needs to store and transfer 50% less parameters at test time. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section we report on experiments on VTAB+MD  and ORBIT . VTAB+MD has become the standard evaluation protocol for few-shot approaches, and it includes a large number of datasets (8 test dataset for MD, 18 for VTAB). For a description of ORBIT, see Section 5.2. In all experiments we used the following pretrained (on ImageNet) backbones: EfficientNetB0 from the official Torchvision repository; ResNet50x1-S released with BiT <ref type="bibr" target="#b19">(Kolesnikov et al., 2020)</ref>. We used three workstations (CPU 6 cores, 110GB of RAM, and a Tesla V100 GPU), the meta-training protocol of <ref type="bibr" target="#b5">Bronskill et al. (2021)</ref> (10K training tasks, updates every 16 tasks), the Adam optimizer with a linearly-decayed learning rate in [10 ?3 , 10 ?5 ] for both the CaSE and linear-head. The head is updated 500 times using a random mini-batch of size 128. MD test results are averaged over 1200 tasks per-dataset (confidence intervals in appendix). We did not use data augmentation. Code to reproduce the experiments is available at https://github.com/mpatacchiola/contextual-squeeze-and-excitation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Analysis of CaSE blocks</head><p>In this sub-section we report empirical results related to CaSE blocks in three directions: 1) we compared standard SE <ref type="bibr" target="#b17">(Hu et al., 2018)</ref> and CaSE on MDv2 and VTAB, confirming that a) adaptation helps over not adapting, b) contextual adaptation (CaSE) outperforms instance based adaptation (SE); 2) we compare CaSE against a SOTA FiLM generator , showing that CaSE is significantly more efficient using 75% fewer parameters while boosting the classification accuracy on average by +1.5% on VTAB and MD2; and 3) we provide an insight on the effectiveness of CaSE blocks with a series of qualitative analysis.</p><p>Comparing SE vs. CaSE We compare standard SE and the proposed CaSE on VTAB and MD-v2. For a fair comparison we keep constant all factors of variation (backbone, training schedule, hyperparameters, etc.) and use the same reduction of 32 (0.8M adaptive parameters). In order to compare the results with the other experiments in this section, we use a Mahalanobis-distance head as in <ref type="bibr" target="#b5">Bronskill et al. (2021)</ref>, reporting results with a linear head in the appendix. We summarize the results in <ref type="figure">Figure 3</ref> (left) and add a full breakdown over all the 26 datasets in Appendix C.2. CaSE outperforms SE in all conditions, confirming that a contextual adaptation mechanism is fundamental to transfer knowledge effectively across tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparing adaptation mechanisms</head><p>We perform a comparison on VTAB+MD of CaSE against FiLM generators , and a baseline that uses a pretrained model but no adaptation of the body. Methods are compared in identical conditions, using a Mahalanobis-distance head, an EfficientNetB0 backbone, and same training schedule. We show a summary of the results in <ref type="figure">Figure 3</ref> (center) and provide a full breakdown in the appendix. CaSE is able to outperform FiLM generators in all conditions. In Appendix C.3 we report the results for CaSE with reduction 64 (0.4M parameters) showing that it is able to outperform FiLM generators (1.7M parameters) using a fraction of the parameters. The comparison with the baseline with no adaptation, shows that in all but one condition (VTAB specialized) adaptation is beneficial. This is likely due to the strong domain shift introduced by some of the specialized datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role of CaSE blocks</head><p>To examine the role of CaSE blocks we analyze the aggregated activations at different stages of the body for 800 tasks sampled from the MDv2 test set using an EfficientNetB0 trained with UpperCaSE on 224?224 images. In <ref type="figure">Figure 3 (right)</ref> we report the aggregated distribution as boxplots, and in Appendix C.5 we provide a per-dataset breakdown. Overall the median is close to 1.0 (identity) which is the expected behavior as on average we aim at exploiting the underlying pretrained model. The variance is small at early stages, indicating that CaSE has learned to take advantage of general-purpose filters that are useful across all tasks. In deeper layers the variance increases, showing a task-specific modulation effect. In Appendix C.5 we also include a plot with per-channel activations for all datasets at different depths, showing that the modulation is similar across datasets at early stages and it diverges later on. An ablation study of different factors (e.g. reduction, number of hidden layers, activation functions) is reported in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance evaluation of UpperCaSE</head><p>In this sub-section we analyze the performance of UpperCaSE in two settings: 1) comparison on the VTAB+MD benchmark against SOTA fine-tuners and meta-learners, where we show that UpperCaSE is able to outperform all the meta-learners, narrowing the gap with Big Transfer (BiT) on VTAB; 2) we show an application of UpperCaSE in a real-world personalization task on the challenging ORBIT dataset  for the cross-domain condition MDv2?ORBIT, where we achieve the best average-score in most metrics, although these improvements are within the error bars.  Comparison on VTAB+MD We compare UpperCaSE against fine-tuners, meta-learners, and hybrids on the 18 datasets of VTAB and the 8 datasets of MetaDataset-v2 (MDv2) and report the results  <ref type="table" target="#tab_1">Table 1</ref>) shows that the gap is narrower on the natural and specialized splits (+3.1% and +0.9%) but larger on structured (+16.9%).</p><p>The breakdown by dataset reported in Appendix C.6 shows that the major performance drops are on tasks that require localization and counting (e.g. dSprites, SmallNORB). Similar issues are encountered by methods such as LITE  which are based on FiLM generators, suggesting that those tasks may introduce a strong domain shift w.r.t. the meta-training set that is difficult to compensate without fine-tuning the body. It is not clear whether transfer learning is beneficial on these datasets in the first place. The results in terms of adaptation cost (see <ref type="table" target="#tab_1">Table 1</ref>) over a synthetic task (10-shot, 100 way) show that UpperCaSE is orders of magnitude more efficient (0.2 ? 10 12 MACs) than all fine-tuners, with BiT being the most expensive method overall (526.3 ? 10 12 MACs). The comparison against meta-learners in terms of number of adaptive parameters (see <ref type="table" target="#tab_2">Table 2</ref>) shows that UpperCaSE requires a fraction of the parameters (0.4 vs 1.7 millions for an EfficientNetB0) compared to LITE  which is based on FiLM generators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on ORBIT</head><p>We compare UpperCaSE to other methods on ORBIT , a real-world dataset for teachable object recognizers. ORBIT consists of 3822 videos of 486 objects recorded by 77 blind/low-vision people on their mobile phones. The dataset is challenging because objects are poorly framed, occluded, blurred, and in a wide variation of backgrounds and lighting. The dataset includes two sets of target videos, one for clean video evaluation (CLE-VE) with well-centered objects, and another for clutter video evaluation (CLU-VE) with objects in complex, cluttered environments. We consider a hard transfer-learning condition where classifiers are meta-trained on MetaDataset and tested on ORBIT.</p><p>Results are reported in <ref type="table" target="#tab_3">Table 3</ref>. UpperCaSE outperforms all other methods (on average) on most metrics, being within error bars with the two leading methods. Comparing UpperCaSE with FineTuner, the gap in favor of UpperCaSE is marginal on CLU-VE but substantial on CLE-VE (frame accuracy +2%, video accuracy +1.8%, and FTR ?2.7). Comparison in terms of adaptation cost (average MACs over all tasks) shows that UpperCaSE is orders of magnitude more efficient than FineTuner and close to the leading method (ProtoNet).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We have introduced a new adaptive block called CaSE, which is based on the popular Squeeze-and-Excitation (SE) block proposed by <ref type="bibr" target="#b17">Hu et al. (2018)</ref>. CaSE is effective at modulating a pretrained model in the few-shot setting, outperforming other adaptation mechanisms. Exploiting CaSE we have designed UpperCaSE, a hybrid method based on a Coordinate-Descent training protocol, that combines the performance of fine-tuners with the low adaptation cost of meta-learners. UpperCaSE achieves SOTA accuracy w.r.t. meta-learners on the 26 datasets of VTAB+MD and it compares favorably with leading methods in the ORBIT personalization benchmark.</p><p>Limitations There are two limitations that are worth mentioning: (i) UpperCaSE requires iterative gradient updates that are hardware-dependent and may be slow/unavailable in some portable devices; (ii) breakdown VTAB results per-dataset shows that the method falls short on structured datasets. This indicates that fine-tuning the body may be necessary for high accuracy when the shift w.r.t. the meta-training set is large.</p><p>Societal impact Applications based on CaSE and UpperCaSE could be deployed in few-shot classification settings that can have a positive impact such as: medical diagnosis, recommendation systems, object detection, etc. The efficiency of our method can reduce energy consumption and benefit the environment. Certain applications require careful consideration to avoid biases that can harm specific groups of people (e.g. surveillance, legal decision-making).</p><p>(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they were chosen)? [Yes] See Section 5 (c) Did you report error bars (e.g., with respect to the random seed after running experiments multiple times)? [Yes] Error bars have been reported for ORBIT and for the breakdonwn of the results on MetaDataset (MD) in the supplementary material. The aggregated results on MD in the paper do not report error bars as they are the average across many datasets. On VTAB error bars are not provided as the benchmark is only used for evaluation (this is in line with the original protocol). Standardization Empirically we have observed that standardizing the pooled representations before passing them to the MLP improves the training stability in CaSE (but not in SE). Standardization is performed by taking the pooled representation at layer l as showed in Equation <ref type="formula" target="#formula_3">(3)</ref>, that ish (l) ? R C , subtracting the mean and dividing by the standard deviation.</p><p>Activation function for the output layer Standard SE blocks usually rely on a sigmoid function in the last layer of the MLPs. This works well when the adaptive block is trained in parallel with the underlying neural network. However, in our case we use a pretrained model and learning can be speeded up considerably by enforcing the identity function as output of the MLPs. We achieve this by multiplying the output of the sigmoid by a constant scalar c = 2 which extends the range to [0, 2], and then set to zero the weights and bias of the layer. This has the effect of enforcing the identity function at the beginning of the training. We have also used a linear activation function instead of a sigmoid, with good results. When using a linear output the identity can be enforced by setting the weights of the last layer to zero, and the bias to one. An ablation over the activation function of SE and CaSE is provided in Appendix C.4 <ref type="table" target="#tab_8">(Table 6)</ref>.</p><p>CaSE location For the choice of CaSE location in the feature extractor, we followed the same principles used in <ref type="bibr" target="#b5">Bronskill et al. (2021)</ref> for FiLM generators. In EfficientNetB0 we place CaSE at the beginning of each hyperblock and the last layer (excluding the first layer). Differently from FiLM (placed after the BatchNorm) we place CaSE after the non-linearity (as done in standard SE) and before the Squeeze-and-Excitation block (included by default in EfficientNet):</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2d?BatchNorm2d?SiLU?CaSE?SqueezeExcitation?Conv2d?BatchNorm2d</head><p>This results in a total of 18 CaSE blocks for EfficientNetB0. Increasing the number of blocks did not provide a significant benefit. In ResNet18 we place two CaSE blocks per each basic block as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2d?BatchNorm2d?ReLU?CaSE?Conv2d?BatchNorm2d?ReLU?CaSE</head><p>Similarly we place two CaSE blocks inside a bottleneck block in ResNet50. See the code for more details.</p><p>Based on the qualitative analysis reported in Section 5 we hypothesize that adaptive blocks are not needed in the initial layers of the network, since at those stages their activity is minimal. Identifying which layer needs adapters and which layer does not, can reduce even more the parameter count of adaptive blocks. Additional work is needed to fully understand this factor.</p><p>CaSE reduction The number of parameters allocated to the CaSE blocks is regulated by a divider r that is used to compute the number of hidden units in the MLPs. Given the input size C (corresponding to the number of channels in that layer) the number of hidden units is given by C/r. We also use a clipping factor r min that prevents the number of units to fall under a given threshold. This prevents the allocation of a low number of units for layers with a small number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Context pooling</head><p>In this section we provide additional details about the context pooling operation performed in a CaSE adaptive block (described in Section 2).</p><p>Similarities with other methods Context pooling is a way to summarize a task with a permutationinvariant aggregation of the embeddings. A similar mechanism has been exploited in various meta-learning methods. For instance, in ProtoNets <ref type="bibr" target="#b36">(Snell et al., 2017)</ref> a prototype for a single class is computed by taking the average over all the context embeddings associated to the inputs for that class.</p><p>The embeddings are generated in the last layer of the feature extractor. In Simple-CNAPs <ref type="bibr" target="#b1">(Bateni et al., 2020)</ref> a prototype is estimated as in ProtoNets but it is used to define a Gaussian distribution instead of a mean vector. Neural latent variable models, such as those derived from the Neural Processes family <ref type="bibr" target="#b12">(Garnelo et al., 2018)</ref> also rely on similar permutation-invariant aggregations to define distributions over functions.</p><p>Global vs. local context-pooling Comparing CaSE with the FiLM generators of <ref type="bibr" target="#b5">Bronskill et al. (2021)</ref> it is possible to distinguish between two types of context pooling: global and local. The FiLM generators of <ref type="bibr" target="#b5">Bronskill et al. (2021)</ref> rely on a global pooling strategy, meaning that the aggregation is performed once-for-all by using a dedicated convolutional set encoder. More specifically, the encoder takes as input all the context images and produces embeddings for each one of them, followed by an average-pooling of those embeddings. The aggregated embedding is then passed to MLPs in each layer that generates a scale and shift parameter. Crucially, each MLP receives the same embedding.</p><p>CaSE exploits a local context-pooling at the layer level. The convolutional set encoder is discarded, and the feature maps produces by the backbone itself at each stage are used as context embeddings. Therefore, the MLPs responsible for generating the scale parameters receive a unique embedding. As showed in the experimental section (Section 5), local pooling improves performances and uses less parameters, as no convolutional encoder is needed. Additional details about the differences between CaSE and FiLM generators is also provided in the paper (Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Pytorch code for CaSE</head><p>Implementation of a CaSE adaptive block in Pytorch. The script is also available as case.py at https://github.com/mpatacchiola/contextual-squeeze-and-excitation. MACs counting MACs depend from the size of the task, size of the images, and number of classes. Therefore we can count MACs using synthetic tasks. In our case we used a synthetic task of 100-way, 10-shot with input images of size 224 ? 224 ? 3 generated via Gaussian noise (? = 0, ? = 1), and labels generated as random integers. We used a mini-batch of size 128 and 500 update steps for UpperCaSE and BiT with an EfficientNetB0 backbone for the first and a ResNet50-S for the second.</p><p>For MD-Transfer we used the same parameters reported in <ref type="bibr" target="#b9">Dumoulin et al. (2021)</ref>  For the evaluation phase we followed the instructions reported in <ref type="bibr" target="#b22">Massiceti et al. (2021)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 CaSE vs SE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Ablation studies</head><p>In this section we provide additional experimental results focusing on ablation studies of the CaSE adaptive block. The results can be summarized as follows:</p><p>? Ablation of the activation function for the output layer for both CaSE and SE. We have tested three activation funcitons: linear, sigmoid, sigmoid with multiplier. The sigmoid with multiplier uses a constant value set to 2 to center the sigmoid at 1 (this enforces the identity function). The empirical results reported in <ref type="table" target="#tab_8">Table 6</ref> show that the sigmoid with multiplier and the linear layer provide the best results.</p><p>? Ablation of the number of hidden units in the hidden layers of CaSE. The number of hidden units is controlled by the reduction and min-units parameters in the code and it depends on the number of inputs. See the paper for more details. The results reported in <ref type="table" target="#tab_10">Table 8</ref> show that blocks with more units provide marginal gains or no gains at all. This is probably due to overfitting issues affecting the models with more units.</p><p>? Ablation of the number of hidden layers of CaSE. The results reported in <ref type="table" target="#tab_9">Table 7</ref> show that the best performance is obtained with 1 and 2 layers. The performance worsen when there are 3 or more layers which is likely due to overfitting issues affecting the models with more parameters.</p><p>? Ablation of the activation function for the hidden layers. Results reported in <ref type="table" target="#tab_11">Table 9</ref> show that CaSE is quite robust against this factor when activations like ReLU and SiLU are used but the performance worsen with Tanh. We have chosen SiLU for the experiments as this is the same activation typically used in Squeeze-and-Excitation layers (e.g. in EfficientNet backbones).    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between the standard Squeeze-Excite (left) and the proposed Contextual Squeeze-Excite (right). Red frames highlight the two key differences between SE and CaSE: context pooling and scale transfer from context to target. B = mini-batch size, C = channels, H = height, W = width, N = context-set size, M = target-set size, * elementwise multiplication.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fine- tuning</head><label>tuning</label><figDesc><ref type="bibr" target="#b7">Chen et al. (2019)</ref> were the first to expose the potential of simple fine-tuning baselines for transfer learning. MD-Transfer has been proposed in<ref type="bibr" target="#b39">Triantafillou et al. (2019)</ref> as an effective fine-tuning baseline for the MetaDataset benchmark. More recently<ref type="bibr" target="#b19">Kolesnikov et al. (2020)</ref> have presented Big Transfer (BiT), showing that large models pretrained on ILSVRC-2012 ImageNet and the full ImageNet-21k are very effective at transfer learning. MD-Transfer and BiT differ in terms of classification head, learning schedule, normalization layers, and batching. It is only under the condition of strong class-imbalance that fine-tuners are outperformed(Ochal et al., 2021a,b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? [Yes] See setup description at beginning of Section 5. 4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... (a) If your work uses existing assets, did you cite the creators? [Yes] (b) Did you mention the license of the assets? [Yes] (c) Did you include any new assets either in the supplemental material or as a URL? [Yes] (d) Did you discuss whether and how consent was obtained from people whose data you're using/curating? [N/A] (e) Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Cost ? MDv2 ?</cell><cell>VTAB ?</cell></row></table><note>UpperCaSE outperforms fine-tuners on MDv2 and narrows the gap on VTAB with the leading method (BiT) with a much lower adaptation cost. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfficientNet. Img: image size. Param.: total parameters (no adapters) in millions. Cost: MACs to adapt on a task (10-shot, 100-way), in Teras. Best results in bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>UpperCaSE outperforms all meta-learning/hybrid methods and uses the lowest number of parameters per adaptive blocks. Average accuracy on the 26 datasets of VTAB+MD. RN=ResNet, EN=EfficientNet. Img: image size. Param.: total parameters (excluding adapters). Adapt.: total adaptive parameters in millions. Best results in bold.</figDesc><table><row><cell>Adapt. ? MDv2 ?</cell><cell>VTAB ?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>ORBIT: UpperCaSE obtains the best average-score in most metrics, being within error bars with leading methods. Average accuracy and 95% confidence interval for frames, videos, and frames to recognition (FTR). Cost: average MACs over all tasks (Teras). Results and setup from<ref type="bibr" target="#b22">Massiceti et al. (2021)</ref>: meta-train on MetaDataset and test on ORBIT, image-size 84 ? 84, ResNet18 backbone, 85 test tasks (17 test users, 5 tasks per user). Best results (within error bars) in bold.</figDesc><table><row><cell></cell><cell>Cost</cell><cell cols="3">Clean Video Evaluation (CLE-VE)</cell><cell cols="3">Clutter Video Evaluation (CLU-VE)</cell></row><row><cell>Method</cell><cell cols="2">MACs? frame acc.?</cell><cell>FTR?</cell><cell cols="2">video acc.? frame acc.?</cell><cell>FTR?</cell><cell>video acc.?</cell></row><row><cell>ProtoNet</cell><cell>3.2</cell><cell>59.0?2.2</cell><cell>11.5?1.8</cell><cell>69.2?3.0</cell><cell>47.0?1.8</cell><cell>20.4?1.7</cell><cell>52.8?2.5</cell></row><row><cell>CNAPs</cell><cell>3.5</cell><cell>51.9?2.5</cell><cell>20.8?2.3</cell><cell>60.8?3.2</cell><cell>41.6?1.9</cell><cell>30.7?2.1</cell><cell>43.0?2.5</cell></row><row><cell>MAML</cell><cell>95.3</cell><cell>42.5?2.7</cell><cell>37.3?3.0</cell><cell>47.0?3.2</cell><cell>24.3?1.8</cell><cell>62.3?2.3</cell><cell>25.7?2.2</cell></row><row><cell>FineTuner</cell><cell>317.7</cell><cell>61.0?2.2</cell><cell>11.5?1.8</cell><cell>72.6?2.9</cell><cell>48.4?1.9</cell><cell>19.1?1.7</cell><cell>54.1?2.5</cell></row><row><cell>UpperCaSE</cell><cell>3.5</cell><cell>63.0?2.2</cell><cell>8.8?1.6</cell><cell>74.4?2.8</cell><cell>48.1?1.8</cell><cell>18.2?1.7</cell><cell>54.5?2.5</cell></row></table><note>in Table 1 and Table 2. UpperCaSE outperforms all methods (including BiT) on MDv2 with an accuracy of 74.9% (ResNet50) and 76.1% (EfficientNetB0). On VTAB, UpperCaSE outperforms most methods, narrowing the gap with BiT. A closer look at the differences in performance on VTAB between UpperCaSE and BiT (see</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>with images of size 126 ? 126 ? 3 and ResNet18 backbone. For the ORBIT experiments we counted MACs by using the code in the original repository 2 and reporting the average MACs over all test tasks for both CLE-VE and CLU-VE using a ResNet18 backbone.VTAB+MD trainingWe follow the protocol reported in the original papers<ref type="bibr" target="#b39">(Triantafillou et al., 2019;</ref><ref type="bibr" target="#b9">Dumoulin et al., 2021)</ref> training UpperCaSE for 10K tasks on the training datasets and evaluating on the MD test set and on the VTAB datasets. At evaluation time we sample 1200 tasks from the MD test set, and report the mean and confidence intervals. On VTAB we report the results of a single run on the test data (data points are given in advance and do not change across seeds). In all experiments we used the MetaDataset-v2 (MDv2) which does not include ImageNet in the test set. We used a pretrained EfficientNetB0 from the official Torchvision repository 3 , and a pretrained ResNet50-S from the BiT repository 4 . We normalized the inputs using the values reported in the Torchvision documentation(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), for ResNet50-S we use theBiT normalization values (mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]).ORBIT training For the ORBIT experiments we trained UpperCaSE on MDv2 using a pretrained ResNet18 taken from the official Torchvision repository. We normalized the inputs using the values reported in the Torchvision documentation(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparing CaSE against standard Squeeze-and-Excitation (SE) on VTAB+MD using different adaptation heads. MD: Mahalanobis distance head. Linear: linear head trained with UpperCaSE. All adaptive blocks use a reduction of 32. Best results in bold.</figDesc><table><row><cell>Model</cell><cell cols="2">SE CaSE</cell><cell>SE</cell><cell>CaSE</cell></row><row><cell>Contextual pooling</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell></row><row><cell>Adaptation head</cell><cell>MD</cell><cell>MD</cell><cell cols="2">Linear Linear</cell></row><row><cell>Image size</cell><cell>84</cell><cell>84</cell><cell>224</cell><cell>224</cell></row><row><cell>MetaDataset (all)</cell><cell cols="2">67.8 69.6</cell><cell>74.6</cell><cell>76.2</cell></row><row><cell>VTAB (all)</cell><cell cols="2">43.6 45.3</cell><cell>56.6</cell><cell>58.2</cell></row><row><cell>VTAB (natural)</cell><cell cols="2">47.5 50.2</cell><cell>65.3</cell><cell>68.1</cell></row><row><cell cols="3">VTAB (specialized) 63.6 64.9</cell><cell>79.8</cell><cell>79.6</cell></row><row><cell>VTAB (structured)</cell><cell cols="2">30.6 31.8</cell><cell>38.6</cell><cell>40.1</cell></row><row><cell>C.3 CaSE vs other adapters</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Comparing CaSE adaptive blocks (with reduction 64, 32, 16) on VTAB+MD against the FiLM generators used in<ref type="bibr" target="#b5">Bronskill et al. (2021)</ref>, and a baseline with no body adaptation. CaSE blocks are more efficient in terms of adaptive and amortization parameters while providing higher classification accuracy. All models have been trained and tested on 84 ? 84 images, using a Mahalanobis distance head. Best results in bold.</figDesc><table><row><cell>Adaptation type</cell><cell cols="5">None FiLM CaSE64 CaSE32 CaSE16</cell></row><row><cell>Adaptive Params (M)</cell><cell>n/a</cell><cell>0.02</cell><cell>0.01</cell><cell>0.01</cell><cell>0.01</cell></row><row><cell>Amortiz. Params (M)</cell><cell>n/a</cell><cell>1.7</cell><cell>0.4</cell><cell>0.8</cell><cell>1.6</cell></row><row><cell>MetaDataset (all)</cell><cell>53.4</cell><cell>68.4</cell><cell>69.8</cell><cell>69.6</cell><cell>70.4</cell></row><row><cell>VTAB (all)</cell><cell>43.5</cell><cell>44.7</cell><cell>46.2</cell><cell>45.3</cell><cell>46.4</cell></row><row><cell>VTAB (natural)</cell><cell>45.4</cell><cell>49.5</cell><cell>52.1</cell><cell>50.2</cell><cell>52.6</cell></row><row><cell>VTAB (specialized)</cell><cell>69.4</cell><cell>63.8</cell><cell>66.3</cell><cell>64.9</cell><cell>65.5</cell></row><row><cell>VTAB (structured)</cell><cell>29.1</cell><cell>31.7</cell><cell>31.8</cell><cell>31.8</cell><cell>32.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Performance on VTAB+MD for various activation functions used in the last layer of SE and CaSE. Sigmoid-2 indicates that the output of a standard Sigmoid is multiplied by 2. Both SE and CaSE use a reduction factor of 32 with min-clipping of 32. All model have been trained using an EfficientNetB0 backbone with a linear head on images of size 224 ? 224. Results for SE with linear activation have not been reported because the training was unstable (loss rapidly diverging at the first iterations). Best results in bold.</figDesc><table><row><cell>Adaptive block</cell><cell>SE</cell><cell>SE</cell><cell>CaSE</cell><cell>CaSE</cell><cell>CaSE</cell></row><row><cell>Activation (output)</cell><cell cols="5">Sigmoid Sigmoid-2 Linear Sigmoid Sigmoid-2</cell></row><row><cell>MetaDataset (all)</cell><cell>74.2</cell><cell>74.6</cell><cell>75.8</cell><cell>74.9</cell><cell>76.2</cell></row><row><cell>VTAB (all)</cell><cell>56.8</cell><cell>56.6</cell><cell>58.4</cell><cell>56.8</cell><cell>58.2</cell></row><row><cell>VTAB (natural)</cell><cell>67.0</cell><cell>65.3</cell><cell>68.3</cell><cell>67.1</cell><cell>68.1</cell></row><row><cell>VTAB (specialized)</cell><cell>81.1</cell><cell>79.8</cell><cell>79.5</cell><cell>80.8</cell><cell>79.6</cell></row><row><cell>VTAB (structured)</cell><cell>36.9</cell><cell>38.6</cell><cell>40.3</cell><cell>37.1</cell><cell>40.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Comparing CaSE adaptive blocks with different number of hidden layers on VTAB+MD. All models have been trained and tested on 224 ? 224 images, using CaSE with reduction 64 and clip factor (min-units) 16, using UpperCaSE and EfficientNetB0 backbone. Best results in bold.</figDesc><table><row><cell># Hidden layers</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell></row><row><cell cols="5">Amortiz. Params (M) 0.420 0.426 0.432 0.438</cell></row><row><cell>MetaDataset (all)</cell><cell>76.0</cell><cell>76.1</cell><cell>75.5</cell><cell>75.2</cell></row><row><cell>VTAB (all)</cell><cell>58.2</cell><cell>58.4</cell><cell>58.2</cell><cell>58.0</cell></row><row><cell>VTAB (natural)</cell><cell>68.3</cell><cell>69.1</cell><cell>68.0</cell><cell>67.4</cell></row><row><cell>VTAB (specialized)</cell><cell>79.7</cell><cell>80.3</cell><cell>80.5</cell><cell>80.3</cell></row><row><cell>VTAB (structured)</cell><cell>40.0</cell><cell>39.4</cell><cell>39.7</cell><cell>39.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Comparing CaSE adaptive blocks with different number of hidden units on VTAB+MD. The number of hidden units depends on the input size and is defined by the reduction and the clip factor (min-units). All models have been trained and tested on 224 ? 224 images, using UpperCaSE and EfficientNetB0 backbone. Best results in bold.</figDesc><table><row><cell>Reduction factor</cell><cell>64</cell><cell>32</cell><cell>16</cell><cell>8</cell></row><row><cell>Clip factor</cell><cell>16</cell><cell>32</cell><cell>48</cell><cell>64</cell></row><row><cell cols="2">Amortiz. Params (M) 0.4</cell><cell>0.8</cell><cell>1.6</cell><cell>3.0</cell></row><row><cell>MetaDataset (all)</cell><cell cols="4">76.1 76.2 75.8 76.2</cell></row><row><cell>VTAB (all)</cell><cell cols="4">58.4 58.2 57.9 58.5</cell></row><row><cell>VTAB (natural)</cell><cell cols="4">69.1 68.1 67.9 68.3</cell></row><row><cell>VTAB (specialized)</cell><cell cols="4">80.3 79.6 79.4 79.0</cell></row><row><cell>VTAB (structured)</cell><cell cols="4">39.4 40.1 39.7 40.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparing CaSE adaptive blocks with different activation functions for the hidden layers on VTAB+MD. All models are based on a reduction factor of 64 and a clip factor of 16 (0.4M amortization parameters) and they have been trained and tested on 224 ? 224 images, using UpperCaSE and EfficientNetB0 backbone. Best results in bold. Role of CaSE blocksFigure 4: Boxplots for all the MDv2 test datasets (100 tasks per dataset) reporting the CaSE activation (vertical axis) at different stages of an EfficientNetB0 (horizontal axis, with early stages on the left). The box encloses first to third quartile, with the median represented by the orange line. The whiskers extend from the box by 1.5 the inter-quartile range. Outlier (point past the end of the whiskers) are represented with black circles.Figure 5: CaSE activation values (vertical axis) for all channels (horizontal axis) at different stages (top plots are early stages) in EfficientNetB0 for the MDv2 test dataset (one task per dataset). Values are similar and closer to one in the first stages but diverge in the latest. The magnitude tends to increase with depth.</figDesc><table><row><cell cols="4">Activation (hidden) SiLU ReLU Tanh</cell></row><row><cell>MetaDataset (all)</cell><cell>76.1</cell><cell>75.8</cell><cell>74.8</cell></row><row><cell>VTAB (all)</cell><cell>58.4</cell><cell>57.8</cell><cell>48.2</cell></row><row><cell>VTAB (natural)</cell><cell>69.1</cell><cell>69.8</cell><cell>67.0</cell></row><row><cell cols="2">VTAB (specialized) 80.3</cell><cell>79.7</cell><cell>80.8</cell></row><row><cell>VTAB (structured)</cell><cell>39.4</cell><cell>39.4</cell><cell>36.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/mpatacchiola/contextual-squeeze-and-excitation</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/microsoft/ORBIT-Dataset 3 https://pytorch.org/vision</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/google-research/big_transfer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>Funding in direct support of this work: Massimiliano Patacchiola, John Bronskill, Aliaksandra Shysheya, and Richard E. Turner are supported by an EPSRC Prosperity Partnership EP/T005386/1 between the EPSRC, Microsoft Research and the University of Cambridge. The authors would like to thank: anonymous reviewers for useful comments and suggestions; Aristeidis Panos and Daniela Massicetti for providing feedback on the preliminary version of the manuscript.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09502</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improved few-shot visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bateni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Masrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>?wi?tkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<title level="m">Discriminative k-shot learning using probabilistic models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
		<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08136</idno>
		<title level="m">Meta-learning with differentiable closed-form solvers</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory efficient meta-learning with large images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hern?ndez-Lobato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.02708</idno>
		<title level="m">Meta-learning feature representations for adaptive gaussian processes via implicit differentiation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04232</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modular meta-learning with shrinkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Friesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Behbahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Comparing transfer and meta learning approaches on a unified few-shot classification benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02638</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<title level="m">Neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09921</idno>
		<title level="m">Meta-learning probabilistic inference for prediction</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spottune: transfer learning through adaptive fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic neural networks: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in real-time with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Big transfer (bit): General visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Puigcerver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Cross-domain few-shot learning with task-specific adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Orbit: A real-world few-shot dataset for teachable object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theodorou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cutrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stumpf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">On first-order meta-learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Few-shot learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2101.02523</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">How sensitive are meta-learners to dataset imbalance?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05344</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez L?pez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Bayesian metalearning for the few-shot setting via deep kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>O&amp;apos;boyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Rapid learning or feature reuse? towards understanding the effectiveness of maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09157</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-learning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multi-domain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast and flexible multitask classification using conditional neural adaptive processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Requeima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning, or on learning how to learn: the meta-meta-... hook. PhD thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Non-gaussian gaussian processes for few-shot regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sendera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tabor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bedychaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Trzcinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Spurek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zieba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Meta-transfer learning for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking few-shot image classification: a good embedding is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional networks with adaptive inference graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<title level="m">Coordinate descent algorithms. Mathematical Programming</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">151</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Blockdrop: Dynamic inference paths in residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The microsoft 2017 conversational speech recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Alleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Droppo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stolcke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE international conference on acoustics, speech and signal processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICASSP</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shiarli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whiteson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">? ? ) CaSE in adaptive mode 12: end while Algorithm 2 UpperCaSE: test function for the few-shot classification setting. Require: ? * = (C * , x * ) unseen test task with target input x * an context C * . Require: b ? () pretrained feature extractor (body) with meta-learned CaSE blocks parameterized by ?. Require: step(): gradient-step function; L loss; ?: step-size hyperparameter for the optimizer. 1: Forward pass over context set b ? (C x * ) ? z1, . . . , zN CaSE in adaptive mode 2: Store context embeddings and associated labels M * = {(zn, yn)} N n=1 temporary memory buffer 3: Define a linear model for the head h ? ? * () and set ? ? * to zero 4: for total inner-steps do loop to estimate head params 5: Sample (with replacement) mini-batch of training pairs B * ? M * 6: Update the head parameters ? ? * ? step(?, L, B * , h ? ? * ) methods on the VTAB+MD benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Upperecase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Dumoulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">?D} training dataset Require: b ? () pretrained feature extractor (body) with CaSE blocks parameterized by ?. Require: step(): gradient-step function; L loss; ?, ?: step-size hyperparameters for the optimizer. 1: Set ? to random values optional: set ? to enforce identity in CaSE output 2: while not done do 3: Sample task ? = (C, T ) ? D 4: Forward pass over context set b ? (C x ) ? z1</title>
		<meeting><address><addrLine>, L, C, T , b ? , h</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>zN CaSE in adaptive mode 5: Store context embeddings and associated labels M = {(zn, yn)} N n=1 temporary memory buffer 6: Define a linear model for the head h ? ? () and set ? ? to zero 7: for total inner-steps do loop to estimate head params 8: Sample (with replacement) mini-batch of training pairs B ? M 9: Update the head parameters ? ? ? step(?, L, B, h ? ? ) 10: end for 11: Update the CaSE parameters ? ? step(?. UpperCaSE uses CaSE with reduction 64 (min-clip 16) for EfficientNetB0 and reduction 32 (min-clip 32) for ResNet50-S. Results for UpperCaSE on MD are the average over 1200 test tasks</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Overall UpperCaSE performs well on MD and the natural split of VTAB, this may be due to the fact that transfer learning is more beneficial on those datasets as they are more similar to those used during meta-training. The largest difference in performance between UpperCaSE and fine-tuning methods is on the structured split of VTAB, which includes tasks that require counting and pose estimation. This is likely due to the difference w.r.t. the meta-training set</title>
	</analytic>
	<monogr>
		<title level="m">Table 10 we report the results for UpperCaSE against fine-tuning methods (BiT, MD-Trasnfer, SUR) and in Table 11 the results for UpperCaSE against meta-learning and hybrid methods (ProtoNet, ProtoMAML, Cross Transformer CTX, LITE)</title>
		<imprint/>
	</monogr>
	<note>In this case, fine-tuning the entire network is more effective than body adaptation as the knowledge gap is wider and it requires more adjustments to the parameters. Table 10: Comparing UpperCaSE against fine-tuning methods. Best result in bold</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Comparing UpperCaSE against meta-learning and hybrid methods. Best result in bold</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
