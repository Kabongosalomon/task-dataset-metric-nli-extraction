<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as Reference</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Tu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE, Feng</roleName><forename type="first">Rongrong</forename><forename type="middle">Ji</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zheng</forename><forename type="middle">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Ling</forename><surname>Shao</surname></persName>
						</author>
						<title level="a" type="main">GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as Reference</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Computer Vision</term>
					<term>Semi-Supervised Learning</term>
					<term>Semantic Segmentation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semi-supervised learning is a challenging problem which aims to construct a model by learning from a limited number of labeled examples. Numerous methods have been proposed to tackle this problem, with most focusing on utilizing the predictions of unlabeled instances consistency alone to regularize networks. However, treating labeled and unlabeled data separately often leads to the discarding of mass prior knowledge learned from the labeled examples, and failure to mine the feature interaction between the labeled and unlabeled image pairs. In this paper, we propose a novel method for semi-supervised semantic segmentation named GuidedMix-Net, by leveraging labeled information to guide the learning of unlabeled instances. Specifically, we first introduce a feature alignment objective between labeled and unlabeled data to capture potentially similar image pairs and then generate mixed inputs from them. The proposed mutual information transfer (MITrans), based on the cluster assumption, is shown to be a powerful knowledge module for further progressive refining features of unlabeled data in the mixed data space. To take advantage of the labeled examples and guide unlabeled data learning, we further propose a mask generation module to generate high-quality pseudo masks for the unlabeled data. Along with supervised learning for labeled data, the prediction of unlabeled data is jointly learned with the generated pseudo masks from the mixed data. Extensive experiments on PASCAL VOC 2012, PASCAL-Context and Cityscapes demonstrate the effectiveness of our GuidedMix-Net, which achieves competitive segmentation accuracy and significantly improves the mIoU by +7% compared to previous state-of-the-art approaches. The source code and pre-trained models will release at https://github.com/ yh-pengtu/GuidedMix-Net.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>T HE past several years have witnessed the success of convolutional neural networks (CNNs) <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> for visual semantic segmentation. Although data-driven deep learning techniques have benefitted greatly from the availability of large-scale image datasets, they require dense and precise pixel-level annotations for parameter learning. Alternative learning strategies, such as semi-supervision, have thus emerged as promising approaches to reduce the need for annotations, requiring simpler or fewer labels for image classification <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>.</p><p>Recent semi-supervised methods for semantic segmentation, such as <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, exploit consistency by perturbing the unlabeled samples to regularize model training. Intuitively, the models are expected to manifest the invariance underlying any small perturbations while observing the natural properties of the data, especially for unlabeled data. To address this, numerous semi-supervised semantic segmentation methods have been proposed against the perturbations when leveraging unlabeled samples. For example, CCT <ref type="bibr" target="#b6">[7]</ref> introduces random manual perturbations by designing separated, unrelated decoders for each type of perturbation. DTC <ref type="bibr" target="#b7">[8]</ref> builds a tasklevel regularization rather than data-level perturbation. CutMix <ref type="bibr" target="#b8">[9]</ref> and ClassMix <ref type="bibr" target="#b9">[10]</ref> follow MixUp <ref type="bibr" target="#b10">[11]</ref> and achieve semisupervised segmentation by forcing the predictions for the augmented and original data to be consistent.</p><p>Although various approaches have been introduced over the years, a key bottleneck of semi-supervised segmentation models is that they typically treat the labeled and unlabeled samples separately during training. Existing methods focus on how to use unlabeled data alone under various manual perturbations. On the one hand, although low-level perturbations do improve the robustness slightly, the rich underlying intrinsic information of the unlabeled instances has not yet been fully explored. For instance, consistencybased methods rely primarily on the local information of the samples themselves by constraining the local smoothness. This strategy cannot comprehensively mine the structural information, especially for unlabeled data, which in turn causes the model to produce a suboptimal solution. Another clear weakness of consistency-based methods is that they require abundant and diverse perturbations, which are expensive and time-consuming to obtain. For instance, CCT <ref type="bibr" target="#b6">[7]</ref> incorporates almost thirty decoders to make the learned model adequately robust. The example results shown in <ref type="figure" target="#fig_0">Fig. 1 (c</ref>  The mass visualized in (c) and (d) are the pseudo masks, and our masks are decoupled from a mixed data. As can be seem, our model is better at representing details, such as the contours and semantic information of objects.</p><p>consistency training yields significant deviation in the contour and semantic understanding of objects. In other words, the massive amount of prior information learned from the labeled samples cannot be transferred to the unlabeled data. Current semi-supervised semantic segmentation methods provide inconsistent optimization objectives for labeled and unlabeled data in different training stages, where the labeled samples are used to improve the discriminative ability and the unlabeled samples enhance the smoothness of models. However, we should be able to train a uniform model by leveraging a large amount of unlabeled data under the guidance of labeled samples. This will enable the learned representations to be refine, thus facilitating mutual information interaction and transfer. We also note that humans can recognize unfamiliar objects subconsciously, by making inferences based on similar or recognizable objects. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>  <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>. The example shown in <ref type="figure" target="#fig_0">Fig. 1</ref> is a relatively simple scenario; natural images are usually far more complex with, for example, multiple occluded objects, making them even more challenging to segment. Another observation is that similar objects (e.g. intra-class objects) often contain common edges and textures. An intuitive way to improve the segmentation of unlabeled data is therefoce to refer to labeled images, as humans do. Motivated by these problems, we propose a novel semi-supervised method for semantic segmentation, named GuidedMix-Net. The proposed method is very effective, requiring much less memory than the current state-of-the-art approach (i.e. CCT). GuidedMix-Net allows knowledge to be transferred from the labeled images to the unlabeled samples, as occur in the human cognitive path. To learn from the unlabeled samples, GuidedMix-Net employs three processes, i.e., labeled-unlabeled image pair interpolation, mutual information transfer, and pseudo mask generation. Specifically, we feed pairs of labeled and unlabeled images as input into the model and carry out a linear interpolation of them to capture pairwise interactions. Then, we learn the uniform feature vectors from the mixed data to inherit different contexts from the image pairs. To incorporate non-local blocks <ref type="bibr" target="#b11">[12]</ref> into the mixed feature layer, long-range dependencies are explored both within images and between them to mine similar object patterns and learn semantic correlations. We further select objects with similar features to ensure that the cues will be similar for different image pairs. Feature selection improves the prediction and mask qualities of the unlabeled images by using the supervised information from the labeled images as reference. After that, we decouple the hybrid prediction to obtain a pseudo mask for the unlabeled image. As a result, the generated pseudo masks are more credible than the direct predictions of unlabeled samples. Finally, the pairs can be utilized for self-training to explore the rich underlying semantic structures provided by the unlabeled examples and further improve the performance of our model.</p><p>In summary, our main contributions are: ? To the best of our knowledge, GuidedMix-Net provides a new mechanism that adaptively captures similar cues from labeled-unlabeled image pairs, enabling it to transfer knowledge from the labeled objects to similar unlabeled ones. ? We introduce a simple, coherent and effective architecture that requires less computational memory during training, while at the same time bringing significant gain in performance. ? In addition to its simplicity and high speed, the proposed method establishes the new state-of-the-art on three popular benchmarks, i.e. PASCAL VOC 2012 <ref type="bibr" target="#b12">[13]</ref>, PASCAL Context <ref type="bibr" target="#b13">[14]</ref> and Cityscapes <ref type="bibr" target="#b14">[15]</ref>, for semi-supervised semantic segmentation. GuidedMix-Net obtains comparable results over other fully-supervised approaches, even in scenarios where only 1/4 of the labeled data is used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>This section will introduce the related applications of MixUp <ref type="bibr" target="#b10">[11]</ref> and the progress in semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Applications of MixUp</head><p>MixUp is a highly effective scheme, which linearly interpolates two random examples from the training set and their labels. MixUp has been widely applied for many tasks, including data augmentation <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, image classification <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, object detection <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, etc. In other words, it leverages unlabeled data to assist the production of labeled data. Deep neural networks trained with large amounts of unlabeled data usually provide incorrect yet extremely confident predictions since they do not have sufficient incentives to learn discriminative representations for the training data. To address this, Manifold-MixUp <ref type="bibr" target="#b19">[20]</ref> was proposed to train neural networks to interpolate the hidden representations. FocalMix <ref type="bibr" target="#b23">[24]</ref> leverages MixUp for 3D medical image detection and obtains excellent results with limited labeled data. In summary, MixUp and its variants impose certain "local linearity" constraints on the input data over the manifold <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b24">[25]</ref> and learn feature representations with fewer directions of variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Semi-Supervised Learning</head><p>In semi-supervised learning, where labels are not available for all training data, the aim is to utilize unlabeled data to improve the model performance. Recently, several semisupervised classification approaches have been proposed with remarkable success. However, few studies have focused on semi-supervised semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Semi-Supervised Classification</head><p>Semi-supervised classification methods <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> typically focus on achieving consistent training by combining a standard supervised loss (e.g. cross-entropy loss) and an unsupervised consistency loss to encourage consistent predictions for perturbations on the unlabeled samples. Randomness is essential for machine learning to either guarantee the generalization and robustness of the model or provides multiple different predictions for the same input. Based on this, Sajjadi et al. <ref type="bibr" target="#b25">[26]</ref> introduced an unsupervised loss function which leverages the stochastic property of randomized data augmentation, dropout and random max-pooling to minimize the difference between the predictions of multiple passes of a training sample through the network. Although these random augmentation techniques can improve the performance, they still remain difficulty on providing effective constraints for boundaries. Miyato et al. <ref type="bibr" target="#b4">[5]</ref> after proposed a virtual adversarial training scheme to achieve smooth regularization. Their method aims to perturb the decision boundary of the model via a virtual adversarial loss-based regularization to measure the local smoothness of the conditional label distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Semi-Supervised Semantic Segmentation</head><p>Semi-supervised semantic segmentation algorithms have achieved great success in recent years <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>. For example, EM-Fixed <ref type="bibr" target="#b27">[28]</ref> provides a novel online expectation-maximization method by training from either weakly annotated data such as bounding boxes, image-level labels, or a combination of a few strongly labeled and many weakly labeled images, sourced from different datasets. EM-Fixed benefits from the use of both a small amount of labeled and large amount of unlabeled data, achieving competitive results even against other fully supervised methods. From a certain point of view, EM-Fixed involves weak labels for training and hence is not a pure semi-supervised method. Spampinato et al. <ref type="bibr" target="#b29">[30]</ref> designed a semi-supervised semantic segmentation method using limited labeled data and abundant unlabeled data in a generative adversarial network (GAN). Their model uses discriminators to estimate the quality of predictions for unlabeled data. If the quality score is high, the pseudo-label generated from the prediction can be regarded as the ground-truth, and the model is optimized by calculating the cross-entropy loss. However, models trained on a limited amount of labeled data typically fail in the following ways: 1) they generate inaccurate low-level details; 2) they misinterpret high-level information. To address these problems, s4GAN-MLMT <ref type="bibr" target="#b32">[33]</ref> fuses a GAN-based branch and a classifier to discriminate the generated segmentation maps. However, considering the intrinsic training difficulty of GANs, some semi-supervised image classification approaches instead adopt a consistent training strategy to ensure similar outputs under small changes, since this is flexible and easy to implement. CCT <ref type="bibr" target="#b6">[7]</ref> employs such a training scheme for semisupervised semantic segmentation, where the invariance of the predictions is enforced over different perturbations applied to the outputs of the decoder. Specifically, a shared encoder and a main decoder are trained in a supervised manner using a few labeled examples. To leverage unlabeled data, CCT enforces consistency between the main decoder predictions and the other set of decoders (for each type of perturbations), and uses different perturbations output from the encoders as input to improve the representations. Unlike previous methods, which primarily focus on learning unlabeled data, we use the labeled images as references and transfer their knowledge to guide the learning of effective information from unlabeled data. As a result, the proposed method generate high-quality features from the labeled images, and refines features of unlabeled data via pairwise interaction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">GUIDEDMIX-NET</head><p>Assume that we have a limited number of labeled images S l ={x l , y l }, where y l is the ground-truth mask of the image x l , and a large amount of data without annotations S u ={x u }.</p><p>The image x ? R H?W has spatial dimensions of H ? W and the masks y ? R H?W ?C have C categories. Fully supervised methods aim to train a CNN ?(x; ?) that takes image x as input, where ? denotes the parameter of the model, and outputs the segmented mask? by minimizing the cross-entropy loss L ce as follows:</p><formula xml:id="formula_0">L ce (?, y) = ? i? i log(y i ),<label>(1)</label></formula><p>where i represents the i-th category. Generally, collecting large-scale labeled training data is time-consuming, costly and sometimes infeasible. In contrast, for some computer vision tasks, large amounts of unlabeled data can be collected relatively easily from either the web or through synthesis. However, for certain fields, such as visual inspection or medical imaging only very limited or even no examples can be gathered. In this case, fully supervised training scheme cannot achieve good performance when suffering in the presence of a minor data deficiency. To address this and employ unlabeled examples during training, we propose a novel framework, called GuidedMix-Net, to leverage the limited number of labeled samples to guide the learning of unlabeled data. The overall framework is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. GuidedMix-Net is based on U-Net <ref type="bibr" target="#b1">[2]</ref>, and it concatenates the feature maps of the same levels between the encoder and the decoder using skip connections. To leverage the labeled samples to guide the generation of credible pseudo masks for the unlabeled samples, we first perform a linear interpolation between the labeled-unlabeled image pairs. Then, a mutual information transfer (MITrans) block is utilized to transfer mutual information and associate similar intra-image cues (i.e., cues from the image itself) and inter-image cues (i.e., cues from different images). Finally, we introduce a decoupling module to separate the pseudo masks from the mixed data. The model can then be trained on the unlabeled samples and the generated pseudo masks. To guide the unlabeled samples, our GuidedMix-Net employs three operations: 1) interpolation of image pairs; 2) transfer of mutual information; 3) generation of pseudo masks, which will be introduced accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Labeled-Unlabeled Image Pair Interpolation</head><p>MixUp is a simple data augmentation technique that mixes two randomly selected samples and their labels to generate new training data for image classification. Labeled-unlabeled image pair interpolation (LUPI) applies MixUp to formulate a data mixing objective for current unlabeled instances with potentially similar labeled samples to guarantee the cues to be similar, and enable information to flow between them. Given a pair of samples (x i l , y i l ) and x k u , we can apply a mixup operation on the pair without mixing up their labels, as shown in Eq. 2. The output after mixup can be expressed as x mix . To learn unlabeled data x u over the labeled samples x l , we set ? ? min(?, 1 ? ?), where ? ? (0, 1) is a hyper-parameter sampled from the Beta(?, ?) distribution with ?. Mixup encourages the model to behave linearly in-between training samples for better generalization performance. Inspired by MixUp, we generate auxiliary data by linearly interpolating labeled and unlabeled samples, as follows:</p><formula xml:id="formula_1">x mix (x l , x u ) = ?x l + (1 ? ?)x u .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Similar Image Pair Selection</head><p>Although mixup associates similar cues between the labeled and unlabeled images, randomly selecting an image pair will not allow knowledge to be transferred from labeled to unlabeled data, since the individual pair may not contain enough similar objects. To overcome this problem, we instead construct similar image pairs in a batch for training. Specifically, we add a fully connected layer as a classifier after the encoder to enhance the semantics of the pooled features, and select image pairs with similar features. Note that the classifier is first trained using labeled data, so it has some recognition ability. We compare features between labeled and unlabeled data in a mini-batch according to the Euclidean distance, shown in Eq.  <ref type="bibr" target="#b33">[34]</ref>, we provide the comparison of mean activation over all convolutional layers between an unlabeled instance and a mixed image (mixing the labeled-unlabeled image pairs). The horizontal axis represents the different levels of convolutional layers in ResNet101 <ref type="bibr" target="#b34">[35]</ref>. Filter information in the CNN is represented by the activation value. The higher the value, the greater the effect of activation <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. We can see that the mixed data input has higher values of mean activation, which means more filters or neurons in the networks are activated. Therefore, with a high probability, more potential structures or correlations can be tapped to improve the segmentation decision.</p><p>3, to conduct image pairs, where ? enc is the encoder module of GuideMix-Net. This procedure allows the proposed model to capture the most similar labeled example for each unlabeled image.</p><formula xml:id="formula_2">d(x i l , x k u ) = H W (? enc (x i l ) ? ? enc (x k u )) 2 .<label>(3)</label></formula><p>It is worth noting that LUPI is quite different from the typical mixup for image classification, in which both images are associated with one-hot labels. Our method instead attempts to mix a pair of samples by fixing one of them as the labeled one, which brings two advantages: Firstly, the unlabeled image can be mixed with different labeled samples to obtain more diverse perturbations. The subsequent optimization of the pseudo mask supervision signal can encourage the model to focus on the object contour. Secondly, x mix is a mixture of x i l and x k u , containing complete contour and texture information of all similar objects in each pair. To recognize objects, CNNs commonly learn complex representations of object shapes. They combine low-level features (e.g. edges) to increase complex shapes like wheels and car windows until the object can be identified readily <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref>. As neurons interact through local connections, CNNs combine information in a growing perception field, where information is transmitted through successive filters, resulting in purified outputs <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>. GuidedMix-Net unifies the dimensional space of labeled-unlabeled image pairs through a specially developed LUPI. Further , it leverages the property of CNNs to refer the complete contour and texture information within a shortrange enabling it to implicitly refine and diversify the features of unlabeled data, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. The proposed method also prepares for the further mutual information transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mutual Information Transfer</head><p>After mixing the pair of samples, we associate similar cues to enhance the features and generate pseudo masks of the unlabeled samples. Generally, labeled data corresponds to the credible features, and unlabeled data are treated as poor-quality features, since no supervision signals are present to guide the gradient updates. This means that in a uniform mixed vector space, for a pair of similar objects from different sources, the poor features can use the credible ones as reference to improve their quality, whether the credible information is located in the short-range or long-range. Although LUPI enables the shortrange credible information to flow to the worse features, CNNs are ineffective in capturing information from distant spatial locations. We solve this problem by applying a non-local (NL) block <ref type="bibr" target="#b6">[7]</ref> to obtain long-range patches that are similar to a given local region of mixed data. After mixing labeledunlabeled images, we obtain a mutual image which contains all the information from the input pairs. The mixed data x mix is then fed to the encoder for segmentation ? until reaching layer j, providing an intermediate v j as follows:</p><formula xml:id="formula_3">v j = h j (x mix ).<label>(4)</label></formula><p>Another intermediate, v j?1 , is produced by the j ? 1 layer, where the spatial size of which is double that of v j . The nonlocal module collects contextual information from long-range features to enhance local feature representation. As shown in the dotted box of <ref type="figure" target="#fig_3">Fig. 4</ref>, the module uses two convolutional layers of 1?1 and filters v j to map and obtain two features Q and K. The spatial size is the same as for v j , but the channel number is half that of v j double for dimension reduction. Then, we generate a correlation matrix D by calculating the correlation between Q and K. A softmax layer is also utilized on D over the channel dimension to get the attention map A = f (Q, K). To obtain the adapted features of v j , another convolutional layer with a 1 ? 1 filter is used to generate V without size change. The long-range contextual information is captured by an aggregation operation, as follows:</p><formula xml:id="formula_4">v j,n = 1 C(x) ?m f (Q n , K m )V m + v j,n ,<label>(5)</label></formula><p>where n and m are the indices of the position in the variable space whose response needs to be computed and all other potential positions, respectively. Parameters v j,n denotes the feature vector in the final output features v j at position n, and v j,n is a feature vector in v j at position n. The function f is used to represent relationships such as the affinity between Q n and all K m . Finally, C(x) is a normalization factor. After that, several convolutional layers are combined with the PixelShuffle layer [43] to fuse the low-level features and restore the spatial information (shown in <ref type="figure" target="#fig_3">Fig. 4)</ref>. The model is first trained on the labeled samples before mixing. Although the number of training samples is small, they provide some recognition ability. The non-local blocks use the features of the labeled samples as the novel training signals to correct the feature generation of the unlabeled samples in the mixed data (i.e. inter-images). Unlike previous methods, which primarily focus on intra-image information, the "mutual information transfer" module addresses the semantic relations within for images comprehensive object pattern mining. The proposed module captures semantic similarities for image themselves to build a mutual information transformation mode, and thus improves the prediction of the unlabeled samples.</p><p>Note that the non-local blocks in MITrans are used to associate similar patches from the labeled and unlabeled features. Therefore, they can leverage supervision signals for unlabeled data training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Pseudo Mask Generation</head><p>To effectively learn from the unlabeled samples, we need to generate pseudo masks for them. In this section, we will introduce a novel method to decouple the masks from the mixed data x mix .</p><p>According to the translation equivariance <ref type="bibr" target="#b38">[39]</ref> of the convolution operator, the translation operated on the input image is still detectable on the output features with the corresponding translation. The translation equivariance can be reflected in the mixed data as well, as shown in <ref type="figure" target="#fig_4">Fig. 5 (a)</ref>, (b), (c), where the activations for the spatial locations of the interesting object, e.g., bus and person, are invariant. Further, the predicted layer of the segmentation network assigns these activated features to certain category channels, separately. We focus on this and propose a pseudo mask generation (PMG) to generate the masks for the unlabeled instances by conducting subtraction between the predictions to constrain foreground separation, which comes from labeled and unlabeled data, respectively. We then train both the labeled and unlabeled data jointly.</p><p>In general, semantic segmentation can be regarded as seeking a mapping function ?, such that the output M = ?(x) is the desired mask which is close to the ground-truth. For a pair of labeled and unlabeled images (x l , x u ), the predictions are M l = ?(x l ) and M u = ?(x u ), respectively. After feeding x mix from Eq. 2 into the segmentation network ?, we can obtain the predicted mask M mix = ?(x mix ), which can be treated as the approximation of directly mixing the masks M l and M u :</p><formula xml:id="formula_5">M mix = ?(x mix ) ? M l + M u .<label>(6)</label></formula><p>We decouple the pseudo masks of x u from M mix , and then leverage them as the target to calculate the mean squared error loss (with the direct output of x u from the main decoder). This procedure ensures the model to be robust and less sensitive to small perturbations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Hard Decoupling</head><p>The goal of mask decoupling is to eliminate M l from the mixed data and then generate pseudo masks for the unlabeled samples. Considering that the ground-truth of labeled data are provided for the model in the early training stage, the prediction M l has higher probability of being close to the real mask. Once M l is obtained, we can directly decouple the unlabeled data mask M u?dec using Eq. 7, which we refer to as hard decoupling:</p><formula xml:id="formula_6">M u?dec = M mix ? M l .<label>(7)</label></formula><p>Hard decoupling is reasonable since the neural network has the ability to separate the corresponding category channels (an example is shown in <ref type="figure" target="#fig_4">Fig. 5</ref>). Directly subtracting between final predictions can separate and obtain more refined results for the unlabeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Soft Decoupling</head><p>The proposed hard decoupling directly performs a subtraction between the prediction of the mixed image and the labeled image, which may counteract the prediction of the overlapping region of objects in the mixed image. To overcome this problem, we propose a soft decoupling for the pseudo mask generation as follows:</p><formula xml:id="formula_7">M u?dec = M mix ? ?M l ,<label>(8)</label></formula><p>where ? is the parameter from the Beta(?, ?) distribution. Soft decoupling retains the details of the overlapping region by weakening the intensity of M l in M mix . As shown in <ref type="table" target="#tab_5">Table  2</ref>, soft decoupling is better than hard decoupling. GuidedMix-Net pays attention to the outline of objects in a complex environment by first transferring knowledge from the mixed labeled-unlabeled pairs, and then decoupling their predictions, to understand the complete semantic information of objects. As shown in <ref type="figure" target="#fig_5">Fig. 6</ref>, GuidedMix-Net is complements object contours and semantic understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss Function</head><p>We develop an overall loss function L for our consistency based semi-supervised learning (SSL) as follows:</p><formula xml:id="formula_8">L = L sup + ? usup L usup ,<label>(9)</label></formula><p>where ? usup is an unsupervised loss weight, such as <ref type="bibr" target="#b43">[44]</ref>, that controls the balance between the two losses. On the one hand, L usup in Eq. 10 is an unsupervised mean squared error (MSE) loss to calculate the difference between the decoupling mask M u?dec and the direct prediction M u :</p><formula xml:id="formula_9">L usup = 1 H * W H W (M u?dec ? M u ) 2 .<label>(10)</label></formula><p>On the other hand, for supervised training, the loss L sup consists of three terms to optimize the model as follows:</p><formula xml:id="formula_10">L sup = L ce (M l , y l ) + L dec + L cla ,<label>(11)</label></formula><p>where L ce (M l , y l ) is the same as in Eq. 1, and L cla is the classifier loss term for image-level annotations. For L dec , we first select a samplex l for a seed x l according to the matching rules in Sec. 3.1.1, where ? follows Sec. 3.1. We then denot? y l andM l (y l and M l ) as the corresponding ground-truth and prediction ofx l (the seed x l ), respectively. In addition, a mixup operation can be conducted on both labeled sample x l andx l following Eq. 2 to obtain a mixed sample x l mix =?x l + (1 ? ?)x l . A consistency loss between decoupled masks M dec = M mix ? M l and the predictionM l can be defined as</p><formula xml:id="formula_11">L dec = 1 H * W H W (M dec ? M l ) 2 .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Analysis of the Proposed Method</head><p>Edges and textures of images can be interactively combined through layer-by-layer convolutions, and various combinations can be obtained at the higher level of a neural network <ref type="bibr" target="#b44">[45]</ref>. Inspired by this, we propose LUPI for map labeled-unlabeled image pairs in the same dimension, enabling similar cues to be interacted in the hidden states. To refine the unlabeled features by learning relevant features from labeled data, we propose MITrans to correct the local errors generated from unlabeled data by capturing the similar long-range cues from the labeled instances. <ref type="figure" target="#fig_0">Fig. 1, 4</ref> show the results of CCT (direct prediction) and our GuidedMix-Net for an unlabeled image.</p><p>We can see that the proposed method encourages knowledge transfer from the labeled instances to the unlabeled samples, while other approach cannot achieve. We also observe that, although x mix seems to overlap between objects, the neural network can easily separate objects in the hidden layer (shown in <ref type="figure" target="#fig_4">Fig. 5</ref>). In addition, PMG can be used to generate the pseudo masks for the unlabeled samples by performing subtraction after the prediction layer. In summary, the proposed approach greatly encourages the segmentation model to mine the rich underlying semantic structures provided by the unlabeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we mainly evaluate our GuidedMix-Net on the semi-supervised semantic segmentation task, and provide the performances of the proposed approach and other state-of-theart methods on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Evaluation Metrics</head><p>PASCAL VOC 2012. This dataset is widely used for semantic segmentation and object detection. It consists of 21 classes including background. We use 1,464 training images and 1,449 validation images from the original PASCAL dataset, and also leverage the augmented annotation dataset (involving 9,118 images) <ref type="bibr" target="#b45">[46]</ref> like <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architecture and Training Details</head><p>Encoder. The encoder is based on ResNet <ref type="bibr" target="#b34">[35]</ref> pretrained on ImageNet <ref type="bibr" target="#b48">[49]</ref>, and also includes the PSP module <ref type="bibr" target="#b47">[48]</ref> after the last layer. Decoder. Inspired by MixUp, GuidedMix-Net combines labeled and unlabeled data, setting the new SOTA, for semisupervised semantic segmentation. To avoid the disintegrates of details for the mixed pairs, we employ a skip connection in the decoder, as done in U-Net <ref type="bibr" target="#b49">[50]</ref>. A pixel shuffle layer <ref type="bibr" target="#b42">[43]</ref> is also utilized to restore the spatial resolution of features. Efficient Training. The proposed method is encouraged to recognize complex objects from the mixed data and decoupled unlabeled data masks by using single decoder. In contrast, CCT <ref type="bibr" target="#b6">[7]</ref> is computationally expensive due to its vast number of decoders, making it difficult to train an efficient model. Our method, in contrast, is encouraged to recognize complex objects from the mixed data and decoupled unlabeled data masks using a single decoder. Therefore, has s much lower time cost during training was shown in <ref type="table" target="#tab_7">Table 4</ref>) and requires only half the memory compared with CCT. Training Details. Similar to <ref type="bibr" target="#b2">[3]</ref>, we use a "poly" learning rate policy, where the base learning rate is multiplied by ((1 ? iter maxier )power) and power = 0.9. Our segmentation network is optimized using the stochastic gradient descent (SGD) optimizer with a base learning rate of 1e-3, momentum of 0.9 and a weight decay of 1e-4. The model is trained over 40,000 iterations for all datasets, and the batch-size is set to 12 for PASCAL VOC 2012, and 8 for Cityscapes and PASCAL Context. We conduct all our experiments on a Tesla V-100s GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on Pascal VOC 2012</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Ablation Studies</head><p>Our ablation studies examine the effect of different values of ? and the impact of different components in our framework.</p><p>Different Values of ?. The results under different values of ? are reported in <ref type="table" target="#tab_4">Table 1</ref>. An can be see, changing ? used in Sec. 3.1 impacts the results, because ? controls the intensity of pixels in the mixed input data. As shown in <ref type="table" target="#tab_4">Table 1</ref>, too high or too low a ? is not conducive to model optimization. A high ? value leads to the labeled information being discarded, while a low ? value results in the unlabeled data being covered. When ? &lt; 0.5, GuidedMix-Net provides the best performance on PASCAL VOC 2012. We thus select ? &lt; 0.5 for the remaining experiments on PASCAL VOC 2012.</p><p>Different Components of GuidedMix-Net. We evaluate the influence of different components of GuidedMix-Net by  Firstly, we investigate different strategies for constructing image pairs, i.e., random selection of similar pairs. i) The first strategy randomly selects a labeled image for each unlabeled image in a mini-batch. ii) To seek similar pairs of labeled and unlabeled images, we add a classifier after the encoder model. We match the most similar unlabeled images for each labeled sample according to the Euclidean distance between the features. As shown in the third row of <ref type="table" target="#tab_5">Table 2</ref>, the similar pairs bring a 2.5% mIoU gain over the plain random selection (71.9% vs. 73.7%). The construction of similar pairs provides context for the target objects in the subsequent segmentation task, and assists GuidedMix-Net in transferring knowledge from the labeled images to the unlabeled samples, with little increasing complexity.</p><p>Secondly, we explore whether MITrans is useful for knowledge transfer. The results provided in the fourth row of <ref type="table" target="#tab_5">Table 2</ref> clearly show that MITrans achieves a significant mIoU gain of 1.4% (72.7% vs. 73.7%) by explicitly referencing similar and high-confidence non-local feature patches to refine the coarse features of the unlabeled samples.</p><p>Finally, as shown in the fifth row <ref type="table" target="#tab_5">Table 2</ref>, the performance of soft decoupling is 3.2% better than hard decoupling (71.4% vs. 73.7%), since soft decoupling considers the overlapping that occurs in the mixed data and tends to preserve local details. The various components used in our GuidedMix-Net are beneficial alone, and therefore combining them leads to significantly improved optimization and better performance than even fully supervised learning, with an increase of over 5.7% in mIoU (as shown in the second row of <ref type="table" target="#tab_5">Table 2</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Semi-Supervised Semantic Segmentation</head><p>The experimental setting of CCT <ref type="bibr" target="#b6">[7]</ref> is different from other semi-supervised semantic segmentation methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>. Here, we provide detailed comparison experiments.</p><p>Comparing with Other State of the Arts. We explore the performance using the deeper backbone of ResNet101 for the semi-supervised semantic segmentation task. The settings and training images are the same as above <ref type="bibr" target="#b9">(10,</ref><ref type="bibr">582)</ref>. Following GCT <ref type="bibr" target="#b51">[52]</ref>, we divide the training images into 1/16 labels, 1/8 labels, 1/4 labels, 1/2 labels, and show the results in <ref type="table" target="#tab_8">Table 5</ref>. GuidedMix-Net outperforms the current methods for semi-supervised image segmentation by 3.7%, 5.3%, and 3.6% on 1/8 labels, 1/4 labels, 1/2 labels, respectively. The significant performance gains on different ratios of labeled data demonstrate that GuidedMix-Net is a generally efficient and effective semi-supervised semantic segmentation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on Cityscapes</head><p>Cityscapes has 2,975 training images. In our experiments, we divide them into 1/8 labels and 1/4 labels, while the remaining data are treated as unlabeled. We use ResNet101 as the backbone to train the models. Since the optimal value of ? varies with the training dataset, we conduct experiments on Cityscapes leveraging 1/8 labeled images as the training data to explore the impact of ? on this dataset, and show the Compared to CCT, our method is able to predict more complete contours and correct semantics. The visual results represent for tiny object parts (the predictions of airplane tails and bicycle handlebars provide more complete contours) and objects occluded by the foreground (shown in the second row). In terms of category semantics, results show that CCT easing misinterprets the semantic information of objects under complex environments, while our method achieves better results (an example is shown in the last row).</p><p>results in <ref type="table" target="#tab_9">Table 6</ref>. For Cityscapes, when ? &lt; 0.3, GuidedMix-Net achieves 65.8 mIoU on the validation dataset, which is better than other selected value ranges. We thus fix the value of ? to be less than 0.3, and verify the gap between GuidedMix-Net and other approaches. Relevant results are presented in <ref type="table" target="#tab_10">Table 7</ref>. GuidedMix-Net yields considerable improvements on Cityscapes over other semi-supervised semantic segmentation methods, i.e., mIoU increases of 5.1%, 7.2%, 6.1% and 5.3% for the 100, 1/8, 1/4 and 1/2 labels, respectively. The distribution of different classes on Cityscapes is highly  imbalanced. The vast majority of classes are present in almost every image, and the few remaining classes occur scarcely. As such, inserting a classifier after the encoder to semantically enhance features and assist in matching similar images is unhelpful. Thus, we use the mixture of randomly selected image pairs in GuidedMix-Net. We also provide the visul results on Cityscapes in <ref type="figure" target="#fig_6">Fig.  7</ref> using only 1/8 labeled images. The visual differences are subtle, and therefore we follow s4GAN <ref type="bibr" target="#b52">[53]</ref> to add a zoomedin view of informative areas.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on PASCAL Context</head><p>In addition to Cityscapes, GuidedMix-Net also successfully generalizes to whole scene parsing on PASCAL Context. <ref type="table" target="#tab_11">Table  8</ref> shows the impact of different ? values on PASCAL Context when training with the limited 1/8 labeled images. We select ? &lt; 0.4 for our two experiments on PASCAL Context. Results are shown in <ref type="table" target="#tab_12">Table 9</ref>. PASCAL Context is smaller, but more difficult than PASCAL VOC 2012. The proposed method also achieves great improvements over other semi-supervised semantic segmentation approaches with mIoU gains of 14.2% and 10.3% for 1/8 and 1/4 labeled data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Gap Between GuidedMix-Net and Fully Supervised Image Segmentation Methods</head><p>The availability of large amounts of data is crucial for deep learning approaches. However, labeling such data is expensive and time-consuming. Half or even weak annotations, in the form of bounding boxes or image-level labels, are far easier to collect than detailed pixel-level annotations. Thus, there is growing interest in semi-supervised learning, which improves performance under low-data conditions. However, with the increase of labeled data, semi-supervised training often move towards from "available" to "unavailable". This is because most semi-supervised methods do not allow the structural information of unlabeled data to be mined, resulting in accumulated errors. To the best of our knowledge, GuidedMix-Net is the first algorithm to explicitly mine the structural information of unlabeled data, by using labeled data as the guidance to model the unlabeled samples learning. GuidedMix-Net generates high-quality pseudo masks, in which the distribution of the generation is close to the mask of the labeled samples.</p><p>Our experimental results show that the proposed approach has exceeds state-of-the-art results on different datasets. We also explore the gap between GuidedMix-Net and various fully supervised image segmentation methods, e.g., FCN [1], DeepLabV3 <ref type="bibr" target="#b53">[54]</ref>, and DeepLabV3+ <ref type="bibr" target="#b54">[55]</ref>, on PASCAL VOC 2012.</p><p>For the experimental settings, it is common to randomly crop 321?321 patches for semi-supervised learning, and 512?512 patches for supervised learning. This will widen the performance gap between fully supervised and semisupervised methods, and therefore we set the same input size for GuidedMix-Net training. As shown in <ref type="table" target="#tab_4">Table 10</ref>, when using an input size of 512?512 for training, GuidedMix-Net outperforms fully supervised methods, obtaining mIoUs of 75.2, 76.5 and 77.1 for varying proportions of limited labels of 1/8, 1/4, and 1/2. The fully supervised methods training with the available labels, for example, FCN <ref type="bibr" target="#b0">[1]</ref> only achieved 69.9 mIoU, while GuidedMix-Net demonstrated excellent results with 1/8 labels (69.9 vs. 75.2), which outperforms FCN by 7.7%. On the other hand, the performance of GuidedMix-Net relatively lower than DeepLabV3 <ref type="bibr" target="#b2">[3]</ref>, ANNet <ref type="bibr" target="#b55">[56]</ref>, and DeepLabV3+ <ref type="bibr" target="#b54">[55]</ref> when using 1/8 labels. We can see that, with an increasing amount of labels, our method generates comparable results by training with 1/2 labels (77.1 vs. 78.6). To boost the performance of GuidedMix-Net, we apply an external data augmentation process to the validation set. The results are shown in the last row of <ref type="table" target="#tab_4">Table 10</ref>. The data augmentation techniques include multi-scale (MS) resize (0.5x, 0.75x, 1.0x, 1.25x, 1.5x, 1.75x), and flipping. These tricks enhance the performance of GuidedMix-Net without adding more labeled data, and obtained 76.4, 77.8, and 78.2 mIoU for 1/8, 1/4, 1/2 labels. We note that DeepLabV3+ presents 78.6 mIoU, and GuidedMix-Net provides 78.2 mIoU while using only 1/2 labels. GuidedMix-Net is thus a promising alternative to fully supervised methods, using less labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper has presented a novel semi-supervised learning method for semantic segmentation, called GuidedMix-Net, to generate pseudo masks of unlabeled data under the guidance of labeled data by mining the structural information. Our experimental results clearly showed that GuidedMix-Net achieves SOTA performance on three benchmark datasets: PASCAL VOC 2012, PASCAL Context and Cityscapes. In the future, we will investigate the use of unlabeled data in other related areas, such as medical imaging. We will continue improving the learning mechanism of the unlabeled samples guided by labeled data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Upper (dotted box): The mutual information transfer module selects similar features to transfer knowledge from the labeled samples to unlabeled images. Lower: Examples of ground-truths (GTs), the pseudo mask of CCT and ours.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of our proposed semi-supervised segmentation approach. GuidedMix-Net follows the basic architecture of U-Net, consisting of an encoder-decoder architecture. The main decoder is constructed by ResNet, while the decoder is incorporated by our MITrans modules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Similar to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Construction details of the MITrans module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Visualization of PMG processes. Blue images are network features of the input, and the bright area represent the foreground objects. (a) Labeled data: the first row is the image with the corresponding ground-truth, both of which are used for training. In the labeled image, the interested object is a bus, where the channel represents different categories of buses, and the most highlighted area is the location of the bus. (b) Unlabeled data: the first row shows an image and its ground-truth, where only the image itself participates in the training. (c) Mixed data: the first row is the mixed data and its prediction, where the interested objects from (a) and (b) are highlighted. (d) Decoupled data: objects will be activated in their corresponding category channels, which are shown in (a), (b) and (c). Decoupling the pseudo masks by subtraction using PGM is effective for separate and obtaining the required valuable information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Experimental eesults on Pascal VOC 2012.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Visual results on Cityscapes using 1/8 labeled examples. The proposed semi-supervised approach generates improved results compared to ClassMix [10]. (a) Input image. (b) Ground-truth. (c) Segmentation results of ClassMix. (d) Segmentation results of GuidedMix-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?</head><label></label><figDesc>Feng Zheng and Peng Tu are with the Department of Computer Science and Technology, Southern University of Science and Technology, Shenzhen 518055, China (e-mail: f.zheng@ieee.org; yh.peng.tu@gmail.com). ? Yawen Huang is with MalongLLC, Wilmington, DE 19805 USA (e-mail: rrji@xmu.edu.cn).</figDesc><table /><note>? Rongrong Ji is with the Department of Information Science and Engineer- ing, Xiamen University, Xiamen 361005, China (e-mail: f.zheng@ieee.org; yh.peng.tu@gmail.com).? Ling Shao is with the Mohamed bin Zayed University of Artificial Intel- ligence, Abu Dhabi, United Arab Emirates, and also with the Inception Institute of Artificial Intelligence, Abu Dhabi, United Arab Emirates (e- mail: ling.shao@ieee.org). ? * Equal contribution. ? Corresponding author.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>) also confirm that generating pseudo masks using simple perturbation arXiv:2106.15064v2 [cs.CV] 30 Jun 2021</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Cityscapes. We use Cityscapes to further evaluate our model. For semantic segmentation, 59 semantic classes and 1 background class are used in training and validation, respectively. Evaluation Metric. Common data augmentation methods are used during our training procedure, which including random resizing (scale: 0.5?2.0), cropping (321?321 for PASCAL VOC 2012, 513?513 for Cityscapes, 480?480 for PAS-CAL Context), horizontal flipping and slight rotation. We evaluate different methods by measuring the averaged pixel intersection-over-union (IoU).</figDesc><table><row><cell>This dataset provides different driving scenes distributed in 19</cell></row><row><cell>classes, with 2,975, 500, 1,525 densely annotated images for</cell></row><row><cell>training, validation and testing.</cell></row></table><note>PASCAL Context. Compared with the above datasets, PAS- CAL Context is more difficult to implement. We use it to demonstrate that the proposed GuidedMix-Net can generalize to various scenes. Pascal Context provides a set of additional annotations for PASCAL VOC 2010, with labels for more than 400 categories. There are 4,998 images for training and 5,105 images for validation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 1 :</head><label>1</label><figDesc>The impact of different lambda values on the experimental results of PASCAL VOC 2012.</figDesc><table><row><cell>?</cell><cell>backbone</cell><cell cols="2">Data labels unlabels</cell><cell>mIoU</cell></row><row><cell>&lt; 0.1</cell><cell></cell><cell></cell><cell></cell><cell>67.9</cell></row><row><cell>&lt; 0.2</cell><cell></cell><cell></cell><cell></cell><cell>69.5</cell></row><row><cell>&lt; 0.3</cell><cell>ResNet50</cell><cell>1464</cell><cell>9118</cell><cell>70.7</cell></row><row><cell>&lt; 0.4</cell><cell></cell><cell></cell><cell></cell><cell>71.4</cell></row><row><cell>&lt; 0.5</cell><cell></cell><cell></cell><cell></cell><cell>73.7</cell></row><row><cell cols="5">training with 1,464 labeled images and 9,118 unlabeled sam-</cell></row><row><cell cols="5">ples. For fair comparison, we evaluate one component per</cell></row><row><cell cols="3">experiment and freeze the others.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 2 :</head><label>2</label><figDesc>Ablation studies of using similar image pairs, MITrans, and hard &amp; soft decoupling modules in GuidedMix-Net. We train the models on ResNet50 and test them on the validation set of PASCAL VOC 2012.</figDesc><table><row><cell>Method</cell><cell>used</cell><cell cols="2">Data labels unlabels</cell><cell>mIoU</cell></row><row><cell>FCN [1]</cell><cell></cell><cell>1464</cell><cell>None</cell><cell>69.9</cell></row><row><cell>Similar Pair</cell><cell>? ?</cell><cell></cell><cell></cell><cell>71.9 73.7</cell></row><row><cell>MITrans Hard Decoupling Soft Decoupling</cell><cell>? ? ? ?</cell><cell>1464</cell><cell>9118</cell><cell>72.7 73.7 71.4 73.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 3 :</head><label>3</label><figDesc>Results on different classes of the PASCAL VOC 2012 validation set. The models are trained on 1464 labeled images and use ResNet50 as their backbone. GuidedMix-Net outperforms CCT in almost all classes, with a performance increase of over 4.3%.</figDesc><table><row><cell>Method</cell><cell>mIoU</cell><cell>aero</cell><cell>bicycle</cell><cell>bird</cell><cell>boat</cell></row><row><cell>CCT</cell><cell>69.4</cell><cell>87.0</cell><cell>58.1</cell><cell>83.2</cell><cell>63.4</cell></row><row><cell>Ours</cell><cell>73.7</cell><cell>89.9</cell><cell>60.3</cell><cell>83.4</cell><cell>71.1</cell></row><row><cell>?</cell><cell>4.3?</cell><cell>2.9?</cell><cell>2.2?</cell><cell>0.2?</cell><cell>7.7?</cell></row><row><cell>Method</cell><cell>mIoU</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell></row><row><cell>CCT</cell><cell>-</cell><cell>64.4</cell><cell>89.6</cell><cell>81.0</cell><cell>83.5</cell></row><row><cell>Ours</cell><cell>-</cell><cell>74.4</cell><cell>88.1</cell><cell>83.2</cell><cell>86.9</cell></row><row><cell>?</cell><cell>-</cell><cell>10.0?</cell><cell>1.5?</cell><cell>2.2?</cell><cell>3.4?</cell></row><row><cell>Method</cell><cell>mIoU</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell></row><row><cell>CCT</cell><cell>-</cell><cell>28.9</cell><cell>76.1</cell><cell>46.9</cell><cell>74.1</cell></row><row><cell>Ours</cell><cell>-</cell><cell>30.5</cell><cell>74.5</cell><cell>57.8</cell><cell>84.6</cell></row><row><cell>?</cell><cell>-</cell><cell>1.6?</cell><cell>1.6?</cell><cell>10.9?</cell><cell>10.5?</cell></row><row><cell>Method</cell><cell>mIoU</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>plan</cell></row><row><cell>CCT</cell><cell>-</cell><cell>69.5</cell><cell>78.5</cell><cell>78.4</cell><cell>48.2</cell></row><row><cell>Ours</cell><cell>-</cell><cell>78.1</cell><cell>81.6</cell><cell>83.5</cell><cell>53.2</cell></row><row><cell>?</cell><cell>-</cell><cell>8.6?</cell><cell>3.1?</cell><cell>5.1?</cell><cell>5.0?</cell></row><row><cell>Method</cell><cell>mIoU</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell></row><row><cell>CCT</cell><cell>-</cell><cell>72.7</cell><cell>42.4</cell><cell>77.4</cell><cell>61.7</cell></row><row><cell>Ours</cell><cell>-</cell><cell>79.1</cell><cell>48.7</cell><cell>77.6</cell><cell>67.3</cell></row><row><cell>?</cell><cell>-</cell><cell>6.4?</cell><cell>6.3?</cell><cell>0.2?</cell><cell>5.6?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 :</head><label>4</label><figDesc>Comparison with CCT using fewer labeled samples.</figDesc><table><row><cell>SSL</cell><cell>500</cell><cell>1000</cell><cell>1464</cell><cell>Memory Size</cell></row><row><cell cols="4">Methods labels labels labels</cell><cell>in Training</cell></row><row><cell>MT [27]</cell><cell>51.3</cell><cell>59.4</cell><cell>-</cell><cell>-</cell></row><row><cell>VAT [5]</cell><cell>50.0</cell><cell>57.9</cell><cell>-</cell><cell>-</cell></row><row><cell>CCT [7]</cell><cell>58.6</cell><cell>64.4</cell><cell>69.4</cell><cell>24kM</cell></row><row><cell>Ours</cell><cell>65.4</cell><cell>68.1</cell><cell>73.7</cell><cell>15kM</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 5 :</head><label>5</label><figDesc>Comparison with other state-of-the-art semisupervised semantic segmentation methods under different ratios of labeled data on PASCAL VOC 2012.</figDesc><table><row><cell cols="4">SSL Methods 1/8 labels 1/4 labels 1/2 labels</cell></row><row><cell>MT [27]</cell><cell>68.9</cell><cell>70.9</cell><cell>73.2</cell></row><row><cell>AdvSSL [32]</cell><cell>68.4</cell><cell>70.8</cell><cell>73.3</cell></row><row><cell>S4L [51]</cell><cell>67.2</cell><cell>68.4</cell><cell>72.0</cell></row><row><cell>GCT [52]</cell><cell>70.7</cell><cell>72.8</cell><cell>74.0</cell></row><row><cell>CutMix [9]</cell><cell>70.8</cell><cell>71.7</cell><cell>73.9</cell></row><row><cell>Ours</cell><cell>73.4</cell><cell>75.5</cell><cell>76.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 6 :</head><label>6</label><figDesc>Influence of different lambda values on the experimental results of Cityscapes.</figDesc><table><row><cell>?</cell><cell>backbone</cell><cell cols="2">Data labels unlabels</cell><cell>mIoU</cell></row><row><cell>&lt; 0.1</cell><cell></cell><cell></cell><cell></cell><cell>64.3</cell></row><row><cell>&lt; 0.2</cell><cell></cell><cell></cell><cell></cell><cell>64.0</cell></row><row><cell>&lt; 0.3</cell><cell>ResNet101</cell><cell>1/8</cell><cell>7/8</cell><cell>65.8</cell></row><row><cell>&lt; 0.4</cell><cell></cell><cell></cell><cell></cell><cell>65.5</cell></row><row><cell>&lt; 0.5</cell><cell></cell><cell></cell><cell></cell><cell>65.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 7 :</head><label>7</label><figDesc>Comparison with other semi-supervised semantic segmentation methods under different ratios of labeled data on Cityscapes.</figDesc><table><row><cell>SSL</cell><cell>100</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell></row><row><cell>Methods</cell><cell cols="4">labels labels labels labels</cell></row><row><cell>AdvSSL [32]</cell><cell>-</cell><cell>57.1</cell><cell>60.5</cell><cell>-</cell></row><row><cell>s4GAN [53]</cell><cell>-</cell><cell>59.3</cell><cell>61.9</cell><cell>-</cell></row><row><cell>CutMix [9]</cell><cell>51.2</cell><cell>60.3</cell><cell>63.9</cell><cell>-</cell></row><row><cell>ClassMix [9]</cell><cell>54.1</cell><cell>61.4</cell><cell>63.6</cell><cell>66.3</cell></row><row><cell>Ours</cell><cell>56.9</cell><cell>65.8</cell><cell>67.5</cell><cell>69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8 :</head><label>8</label><figDesc>Influence of different lambda values on the experimental results of PASCAL Context.</figDesc><table><row><cell>?</cell><cell>backbone</cell><cell cols="2">Data labels unlabels</cell><cell>mIoU</cell></row><row><cell>&lt; 0.1</cell><cell></cell><cell></cell><cell></cell><cell>37.6</cell></row><row><cell>&lt; 0.2</cell><cell></cell><cell></cell><cell></cell><cell>39.4</cell></row><row><cell>&lt; 0.3</cell><cell>ResNet101</cell><cell>1/8</cell><cell>7/8</cell><cell>39.8</cell></row><row><cell>&lt; 0.4</cell><cell></cell><cell></cell><cell></cell><cell>40.3</cell></row><row><cell>&lt; 0.5</cell><cell></cell><cell></cell><cell></cell><cell>40.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 9 :</head><label>9</label><figDesc>Comparison with other semi-supervised semantic segmentation methods under different ratios of labeled data on PASCAL Context.</figDesc><table><row><cell>SSL Methods</cell><cell>backbone</cell><cell cols="2">1/8 labels labels 1/4</cell></row><row><cell>AdvSSL [32]</cell><cell></cell><cell>32.8</cell><cell>34.8</cell></row><row><cell>s4GAN [53]</cell><cell>ResNet101</cell><cell>35.3</cell><cell>37.8</cell></row><row><cell>Ours</cell><cell></cell><cell>40.3</cell><cell>41.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 10 :</head><label>10</label><figDesc>Comparison with fully-supervised methods on PASCAL VOC 2012 by used ResNet101.</figDesc><table><row><cell>SSL</cell><cell>1/8</cell><cell>1/4</cell><cell>1/2</cell><cell>Full</cell></row><row><cell>Methods</cell><cell cols="4">labels labels labels labels</cell></row><row><cell>FCN [1]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>69.9</cell></row><row><cell>DeepLabV3 [54]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>77.9</cell></row><row><cell>ANNet [56]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.7</cell></row><row><cell>DeepLabV3+ [55]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>78.6</cell></row><row><cell>GuidedMix-Net</cell><cell>75.2</cell><cell>76.5</cell><cell>77.1</cell><cell>-</cell></row><row><cell>+MS and Flip</cell><cell>76.4</cell><cell>77.8</cell><cell>78.2</cell><cell>-</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: A regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takeru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shin-Ichi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Masanori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semisupervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with cross-consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ouali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hudelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semisupervised medical image segmentation through dual-task consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.04448</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Semisupervised semantic segmentation needs strong, varied perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>French</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mackiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Finlayson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Classmix: Segmentation-based data augmentation for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tranheden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1369" to="1378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Autoaugment: Learning augmentation policies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Manifold mixup: Better representations by interpolating hidden states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Interpolation consistency training for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Data augmentation by pairing samples for images classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02929</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bag of freebies for training object detection neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04103</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Focalmix: Semisupervised learning for 3d medical image detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3951" to="3960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3714" to="3722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Regularization with stochastic transformations and perturbations for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Javanmardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tasdizen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Semi supervised semantic segmentation using generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Souly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Spampinato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Ficklenet: Weakly and semi-supervised semantic image segmentation using stochastic inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faramarzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Badrinaaraayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chandar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Patchup: A regularization technique for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks with a non-polynomial activation function can approximate any function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leshno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schocken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Notes on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bouvrie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep neural networks: A new framework for modeling biological vision and brain information processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kriegeskorte</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="417" to="446" />
		</imprint>
	</monogr>
	<note>Annual review of vision science</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Deep learning with Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">361</biblScope>
			<pubPlace>Manning New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Understanding neural networks through deep visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fuchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.06579</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Not just a black box: Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<idno>abs/1605.01713</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Real-time single image and video superresolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Feature visualization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schubert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Distill</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic segmentation network with deep seeded region growing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">S4l: Self-supervised semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1476" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Guided collaborative training for pixel-wise semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Di Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Semi-supervised semantic segmentation with high-and low-level consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Encoderdecoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1802.02611</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Asymmetric non-local neural networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.07678" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
