<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DehazeNet: An End-to-End System for Single Image Haze Removal</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Xiangmin</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Kui</forename><surname>Jia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Chunmei</forename><surname>Qing</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
						</author>
						<title level="a" type="main">DehazeNet: An End-to-End System for Single Image Haze Removal</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Dehaze</term>
					<term>image restoration</term>
					<term>deep CNN</term>
					<term>BReLU</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Single image haze removal is a challenging ill-posed problem. Existing methods use various constraints/priors to get plausible dehazing solutions. The key to achieve haze removal is to estimate a medium transmission map for an input hazy image. In this paper, we propose a trainable end-to-end system called DehazeNet, for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover a haze-free image via atmospheric scattering model. DehazeNet adopts Convolutional Neural Networks (CNN) based deep architecture, whose layers are specially designed to embody the established assumptions/priors in image dehazing. Specifically, layers of Maxout units are used for feature extraction, which can generate almost all haze-relevant features. We also propose a novel nonlinear activation function in DehazeNet, called Bilateral Rectified Linear Unit (BReLU), which is able to improve the quality of recovered haze-free image. We establish connections between components of the proposed DehazeNet and those used in existing methods. Experiments on benchmark images show that DehazeNet achieves superior performance over existing methods, yet keeps efficient and easy to use.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H AZE is a traditional atmospheric phenomenon where dust, smoke and other dry particles obscure the clarity of the atmosphere. Haze causes issues in the area of terrestrial photography, where the light penetration of dense atmosphere may be necessary to image distant subjects. This results in the visual effect of a loss of contrast in the subject, due to the effect of light scattering through the haze particles. For these reasons, haze removal is desired in both consumer photography and computer vision applications.</p><p>Haze removal is a challenging problem because the haze transmission depends on the unknown depth which varies at different positions. Various techniques of image enhancement have been applied to the problem of removing haze from a single image, including histogram-based <ref type="bibr" target="#b0">[1]</ref>, contrast-based <ref type="bibr" target="#b1">[2]</ref> and saturation-based <ref type="bibr" target="#b2">[3]</ref>. In addition, methods using multiple images or depth information have also been proposed. For example, polarization based methods <ref type="bibr" target="#b3">[4]</ref> remove the haze effect through multiple images taken with different degrees of polarization. In <ref type="bibr" target="#b4">[5]</ref>, multi-constraint based methods are applied to multiple images capturing the same scene under different weather conditions. Depth-based methods <ref type="bibr" target="#b5">[6]</ref> require some depth information from user inputs or known 3D models. In practice, depth information or multiple hazy images are not always available.</p><p>Single image haze removal has made significant progresses recently, due to the use of better assumptions and priors. Specifically, under the assumption that the local contrast of the haze-free image is much higher than that of hazy image, a local contrast maximizing method <ref type="bibr" target="#b6">[7]</ref> based on Markov Random Field (MRF) is proposed for haze removal. Although contrast maximizing approach is able to achieve impressive results, it tends to produce over-saturated images. In <ref type="bibr" target="#b7">[8]</ref>, Independent Component Analysis (ICA) based on minimal input is proposed to remove the haze from color images, but the approach is time-consuming and cannot be used to deal with dense-haze images. Inspired by dark-object subtraction technique, Dark Channel Prior (DCP) <ref type="bibr" target="#b8">[9]</ref> is discovered based on empirical statistics of experiments on haze-free images, which shows at least one color channel has some pixels with very low intensities in most of non-haze patches. With dark channel prior, the thickness of haze is estimated and removed by the atmospheric scattering model. However, DCP loses dehazing quality in the sky images and is computationally intensive. Some improved algorithms are proposed to overcome these limitations. To improve dehazing quality, Kratz and Nishino et al. <ref type="bibr" target="#b9">[10]</ref> model the image with a factorial MRF to estimate the scene radiance more accurately; Meng et al. <ref type="bibr" target="#b10">[11]</ref> propose an effective regularization dehazing method to restore the hazefree image by exploring the inherent boundary constraint. To improve computational efficiency, standard median filtering <ref type="bibr" target="#b11">[12]</ref>, median of median filter <ref type="bibr" target="#b12">[13]</ref>, guided joint bilateral filtering <ref type="bibr" target="#b13">[14]</ref> and guided image filter <ref type="bibr" target="#b14">[15]</ref> are used to replace the time-consuming soft matting <ref type="bibr" target="#b15">[16]</ref>. In recent years, hazerelevant priors are investigated in machine learning framework. Tang et al. <ref type="bibr" target="#b16">[17]</ref> combine four types of haze-relevant features with Random Forests to estimate the transmission. Zhu et al. <ref type="bibr" target="#b17">[18]</ref> create a linear model for estimating the scene depth of the hazy image under color attenuation prior and learns the parameters of the model with a supervised method. Despite the remarkable progress, these state-of-the-art methods are limited by the very same haze-relevant priors or heuristic cues -they are often less effective for some images.</p><p>Haze removal from a single image is a difficult vision task. In contrast, the human brain can quickly identify the hazy area from the natural scenery without any additional information. One might be tempted to propose biologically inspired models for image dehazing, by following the success of bio-inspired CNNs for high-level vision tasks such as image classification <ref type="bibr" target="#b18">[19]</ref>, face recognition <ref type="bibr" target="#b19">[20]</ref> and object detection <ref type="bibr" target="#b20">[21]</ref>. In fact, there have been a few (convolutional) neural network based deep learning methods that are recently proposed for low-level vision tasks of image restoration/reconstruction <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. However, these methods cannot be directly applied to single image haze removal.</p><p>Note that apart from estimation of a global atmospheric light magnitude, the key to achieve haze removal is to recover an accurate medium transmission map. To this end, we propose DehazeNet, a trainable CNN based end-to-end system for medium transmission estimation. DehazeNet takes a hazy image as input, and outputs its medium transmission map that is subsequently used to recover the haze-free image by a simple pixel-wise operation. Design of DehazeNet borrows ideas from established assumptions/principles in image dehazing, while parameters of all its layers can be automatically learned from training hazy images. Experiments on benchmark images show that DehazeNet gives superior performance over existing methods, yet keeps efficient and easy to use. Our main contributions are summarized as follows.</p><p>1) DehazeNet is an end-to-end system. It directly learns and estimates the mapping relations between hazy image patches and their medium transmissions. This is achieved by special design of its deep architecture to embody established image dehazing principles. 2) We propose a novel nonlinear activation function in DehazeNet, called Bilateral Rectified Linear Unit 1 (BReLU). BReLU extends Rectified Linear Unit (ReLU) and demonstrates its significance in obtaining accurate image restoration. Technically, BReLU uses the bilateral restraint to reduce search space and improve convergence. 3) We establish connections between components of De-hazeNet and those assumptions/priors used in existing dehazing methods, and explain that DehazeNet improves over these methods by automatically learning all these components from end to end. The remainder of this paper is organized as follows. In Section II, we review the atmospheric scattering model and haze-relevant features, which provides background knowledge to understand the design of DehazeNet. In Section III, we present details of the proposed DehazeNet, and discuss how it relates to existing methods. Experiments are presented in <ref type="bibr" target="#b0">1</ref> During the preparation of this manuscript (in December, 2015), we find that a nonlinear activation function called adjustable bounded rectifier is proposed in <ref type="bibr" target="#b24">[25]</ref> (arXived in November, 2015), which is almost identical to BReLU. Adjustable bounded rectifier is motivated to achieve the objective of image recognition. In contrast, BReLU is proposed here to improve image restoration accuracy. It is interesting that we come to the same activation function from completely different initial objectives. This may also suggest the general usefulness of the proposed BReLU. Section IV, before conclusion is drawn in Section V.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head><p>Many image dehazing methods have been proposed in the literature. In this section, we briefly review some important ones, paying attention to those proposing the atmospheric scattering model, which is the basic underlying model of image dehazing, and those proposing useful assumptions for computing haze-relevant features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Atmospheric Scattering Model</head><p>To describe the formation of a hazy image, the atmospheric scattering model is first proposed by McCartney <ref type="bibr" target="#b25">[26]</ref>, which is further developed by Narasimhan and Nayar <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>. The atmospheric scattering model can be formally written as</p><formula xml:id="formula_0">I (x) = J (x) t (x) + ? (1 ? t (x)) ,<label>(1)</label></formula><p>where I (x) is the observed hazy image, J (x) is the real scene to be recovered, t (x) is the medium transmission, ? is the global atmospheric light, and x indexes pixels in the observed hazy image I. <ref type="figure" target="#fig_2">Fig. 1</ref> gives an illustration. There are three unknowns in equation <ref type="formula" target="#formula_0">(1)</ref>, and the real scene J (x) can be recovered after ? and t (x) are estimated. The medium transmission map t (x) describes the light portion that is not scattered and reaches the camera. t (x) is defined as</p><formula xml:id="formula_1">t (x) = e ??d(x) ,<label>(2)</label></formula><p>where d (x) is the distance from the scene point to the camera, and ? is the scattering coefficient of the atmosphere. Equation <ref type="bibr" target="#b1">(2)</ref> suggests that when d (x) goes to infinity, t (x) approaches zero. Together with equation (1) we have</p><formula xml:id="formula_2">? = I (x) , d (x) ? inf<label>(3)</label></formula><p>In practical imaging of a distance view, d (x) cannot be infinity, but rather be a long distance that gives a very low transmission t 0 . Instead of relying on equation <ref type="formula" target="#formula_2">(3)</ref> to get the global atmospheric light ?, it is more stably estimated based on the following rule</p><formula xml:id="formula_3">? = max y?{x|t(x)?t0} I (y)<label>(4)</label></formula><p>The discussion above suggests that to recover a clean scene (i.e., to achieve haze removal), it is the key to estimate an accurate medium transmission map.   The dark channel prior is based on the wide observation on outdoor haze-free images. In most of the haze-free patches, at least one color channel has some pixels whose intensity values are very low and even close to zero. The dark channel <ref type="bibr" target="#b8">[9]</ref> is defined as the minimum of all pixel colors in a local patch:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Haze-relevant features</head><formula xml:id="formula_4">D (x) = min y??r(x) min c?{r,g,b} I c (y) ,<label>(5)</label></formula><p>where I c is a RGB color channel of I and ? r (x) is a local patch centered at x with the size of r ? r. The dark channel feature has a high correlation to the amount of haze in the image, and is used to estimate the medium transmission t (x) ? 1 ? D (x) directly.</p><p>2) Maximum Contrast: According to the atmospheric scattering, the contrast of the image is reduced by the haze transmission as x ?I (x) = t x ?J (x) ? x ?J (x) . Based on this observation, the local contrast <ref type="bibr" target="#b6">[7]</ref> as the variance of pixel intensities in a s ? s local patch ? s with respect to the center pixel, and the local maximum of local contrast values in a r ? r region ? r is defined as:</p><formula xml:id="formula_5">C (x) = max y??r(x) 1 |? s (y)| z??s(y) I (z) ? I (y) 2 ,<label>(6)</label></formula><p>where |? s (y)| is the cardinality of the local neighborhood. The correlation between the contrast feature and the medium transmission t is visually obvious, so the visibility of the image is enhanced by maximizing the local contrast showed as <ref type="bibr" target="#b5">(6)</ref>.</p><p>3) Color Attenuation: The saturation I s (x) of the patch decreases sharply while the color of the scene fades under the influence of the haze, and the brightness value I v (x) increases at the same time producing a high value for the difference. According to the above color attenuation prior <ref type="bibr" target="#b17">[18]</ref>, the difference between the brightness and the saturation is utilized to estimate the concentration of the haze:</p><formula xml:id="formula_6">A (x) = I v (x) ? I s (x) ,<label>(7)</label></formula><p>where I v (x) and I h (x) can be expressed in the HSV color space as</p><formula xml:id="formula_7">I v (x) = max c?{r,b,g} I c (x) and I s (x) = max c?{r,b,g} I c (x) ? min c?{r,b,g} I c (x) max c?{r,b,g} I c (x).</formula><p>The color attenuation feature is proportional to the scene depth d (x) ? A (x), and is used for transmission estimation easily. 4) Hue Disparity: Hue disparity between the original image I (x) and its semi-inverse image,</p><formula xml:id="formula_8">I si (x) = max [I c (x) , 1 ? I c (x)] with c ? {r, g, b},</formula><p>has been used to detect the haze. For haze-free images, pixel values in the three channels of their semi-inverse images will not all flip, resulting in large hue changes between I si (x) and I (x). In <ref type="bibr" target="#b28">[29]</ref>, the hue disparity feature is defined:</p><formula xml:id="formula_9">H (x) = I h si (x) ? I h (x) ,<label>(8)</label></formula><p>where the superscript "h" denotes the hue channel of the image in HSV color space. According to <ref type="bibr" target="#b7">(8)</ref>, the medium transmission t (x) is in inverse propagation to H (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. THE PROPOSED DEHAZENET</head><p>The atmospheric scattering model in Section II-A suggests that estimation of the medium transmission map is the most important step to recover a haze-free image. To this end, we propose DehazeNet, a trainable end-to-end system that explicitly learns the mapping relations between raw hazy images and their associated medium transmission maps. In this section, we present layer designs of DehazeNet, and discuss how these designs are related to ideas in existing image dehazing methods. The final pixel-wise operation to get a recovered haze-free image from the estimated medium transmission map will be presented in Section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Layer Designs of DehazeNet</head><p>The proposed DehazeNet consists of cascaded convolutional and pooling layers, with appropriate nonlinear activation functions employed after some of these layers. <ref type="figure" target="#fig_3">Fig. 2</ref> shows the architecture of DehazeNet. Layers and nonlinear activations of DehazeNet are designed to implement four sequential operations for medium transmission estimation, namely, feature extraction, multi-scale mapping, local extremum, and nonlinear regression. We detail these designs as follows.</p><p>1) Feature Extraction: To address the ill-posed nature of image dehazing problem, existing methods propose various assumptions and based on these assumptions, they are able to extract haze-relevant features (e.g., dark channel, hue disparity, and color attenuation) densely over the image domain. Note that densely extracting these haze-relevant features is equivalent to convolving an input hazy image with appropriate filters, followed by nonlinear mappings. Inspired by extremum processing in color channels of those haze-relevant features, an unusual activation function called Maxout unit <ref type="bibr" target="#b29">[30]</ref> is selected as the non-linear mapping for dimension reduction. Maxout unit is a simple feed-forward nonlinear activation function used in multi-layer perceptron or CNNs. When used in CNNs, it generates a new feature map by taking a pixel-wise maximization operation over k affine feature maps. Based on Maxout unit, we design the first layer of DehazeNet as follows</p><formula xml:id="formula_10">F i 1 (x) = max j?[1,k] f i,j 1 (x) , f i,j 1 = W i,j 1 * I + B i,j 1 ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_11">W 1 = {W i,j 1 } (n1,k) (i,j)=(1,1) and B 1 = {B i,j 1 } (n1,k) (i,j)=(1,1)</formula><p>represent the filters and biases respectively, and * denotes the convolution operation. Here, there are n 1 output feature maps in the first layer. W i,j 1 ? R 3?f1?f1 is one of the total k ? n 1 convolution filters, where 3 is the number of channels in the input image I (x), and f 1 is the spatial size of a filter (detailed in <ref type="table" target="#tab_0">Table I</ref>). Maxout unit maps each of kn 1 -dimensional vectors into an n 1 -dimensional one, and extracts the haze-relevant features by automatic learning rather than heuristic ways in existing methods.</p><p>2) Multi-scale Mapping: In <ref type="bibr" target="#b16">[17]</ref>, multi-scale features have been proven effective for haze removal, which densely compute features of an input image at multiple spatial scales. Multi-scale feature extraction is also effective to achieve scale invariance. For example, the inception architecture in GoogLeNet <ref type="bibr" target="#b30">[31]</ref> uses parallel convolutions with varying filter sizes, and better addresses the issue of aligning objects in input images, resulting in state-of-the-art performance in ILSVRC14 <ref type="bibr" target="#b31">[32]</ref>. Motivated by these successes of multi-scale feature extraction, we choose to use parallel convolutional operations in the second layer of DehazeNet, where size of any convolution filter is among 3 ? 3, 5 ? 5 and 7 ? 7, and we use the same number of filters for these three scales. Formally, the output of the second layer is written as</p><formula xml:id="formula_12">F i 2 = W i/3 ,(i\3) 2 * F 1 + B 2 i/3 ,(i\3) ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_13">W 2 = {W p,q 2 } (3,n2/3) (p,q)=(1,1) and B 2 = {B p,q 2 } (3,n2/3) (p,q)=(1,1)</formula><p>contain n 2 pairs of parameters that is break up into 3 groups. n 2 is the output dimension of the second layer, and i ? [1, n 2 ] indexes the output feature maps. takes the integer upwardly and \ denotes the remainder operation.</p><p>3) Local Extremum: To achieve spatial invariance, the cortical complex cells in the visual cortex receive responses from the simple cells for linear feature integration. Ilan et al. <ref type="bibr" target="#b32">[33]</ref> proposed that spatial integration properties of complex cells can be described by a series of pooling operations. According to the classical architecture of CNNs <ref type="bibr" target="#b33">[34]</ref>, the neighborhood maximum is considered under each pixel to overcome local sensitivity. In addition, the local extremum is in accordance with the assumption that the medium transmission is locally constant, and it is commonly to overcome the noise of transmission estimation. Therefore, we use a local extremum operation in the third layer of DehazeNet.</p><formula xml:id="formula_14">F i 3 (x) = max y??(x) F i 2 (y) ,<label>(11)</label></formula><p>where ? (x) is an f 3 ? f 3 neighborhood centered at x, and the output dimension of the third layer n 3 = n 2 . In contrast to max-pooling in CNNs, which usually reduce resolutions of feature maps, the local extremum operation here is densely applied to every feature map pixel, and is able to preserve resolution for use of image restoration. 4) Non-linear Regression: Standard choices of nonlinear activation functions in deep networks include Sigmoid <ref type="bibr" target="#b34">[35]</ref> and Rectified Linear Unit (ReLU). The former one is easier to suffer from vanishing gradient, which may lead to slow convergence or poor local optima in networks training. To overcome the problem of vanishing gradient, ReLU is proposed <ref type="bibr" target="#b35">[36]</ref> which offers sparse representations. However, ReLU is designed for classification problems and not perfectly suitable for the regression problems such as image restoration. In particular, ReLU inhibits values only when they are less than zero. It might lead to response overflow especially in the last layer, because for image restoration, the output values of the last layer are supposed to be both lower and upper bounded in a small range. To this end, we propose a Bilateral Rectified Linear Unit (BReLU) activation function, shown in <ref type="figure" target="#fig_4">Fig. 3</ref>, to overcome this limitation. Inspired by Sigmoid and ReLU, BReLU as a novel linear unit keeps bilateral restraint and local linearity. Based on the proposed BReLU, the feature map of the fourth layer is defined as</p><formula xml:id="formula_15">F 4 = min (t max , max (t min , W 4 * F 3 + B 4 ))<label>(12)</label></formula><p>Here W 4 = {W 4 } contains a filter with the size of n 3 ?f 4 ?f 4 , B 4 = {B 4 } contains a bias, and t min,max is the marginal value of BReLU (t min = 0 and t max = 1 in this paper). According to <ref type="bibr" target="#b11">(12)</ref>, the gradient of this activation function can be shown as</p><formula xml:id="formula_16">?F 4 (x) ?F 3 = ? ? ? ?F 4 (x) ?F 3 , t min ? F 4 (x) &lt; t max 0, otherwise<label>(13)</label></formula><p>The above four layers are cascaded together to form a CNN based trainable end-to-end system, where filters and biases associated with convolutional layers are network parameters to be learned. We note that designs of these layers can be connected with expertise in existing image dehazing methods, which we specify in the subsequent section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Connections with Traditional Dehazing Methods</head><p>The first layer feature F 1 in DehazeNet is designed for hazerelevant feature extraction. Take dark channel feature <ref type="bibr" target="#b8">[9]</ref> as an example. If the weight W 1 is an opposite filter (sparse matrices with the value of -1 at the center of one channel, as in <ref type="figure" target="#fig_5">Fig.  4(a)</ref>) and B 1 is a unit bias, then the maximum output of the feature map is equivalent to the minimum of color channels, which is similar to dark channel <ref type="bibr" target="#b8">[9]</ref> (see Equation <ref type="formula" target="#formula_4">(5)</ref>). In the same way, when the weight is a round filter as <ref type="figure" target="#fig_5">Fig. 4(c)</ref>, F 1 is similar to the maximum contrast <ref type="bibr" target="#b6">[7]</ref> (see Equation <ref type="formula" target="#formula_5">(6)</ref>); when W 1 includes all-pass filters and opposite filters, F 1 is similar to the maximum and minimum feature maps, which are atomic operations of the color space transformation from RGB to HSV, then the color attenuation <ref type="bibr" target="#b17">[18]</ref> (see Equation <ref type="formula" target="#formula_6">(7)</ref>) and hue disparity <ref type="bibr" target="#b28">[29]</ref> (see Equation <ref type="formula" target="#formula_9">(8)</ref>) features are extracted. In conclusion, upon success of filter learning shown in <ref type="figure" target="#fig_5">Fig. 4(e)</ref>, almost all haze-relevant features can be potentially extracted from the first layer of DehazeNet. On the other hand, Maxout activation functions can be considered as piece-wise linear approximations to arbitrary convex functions. In this paper, we choose the maximum across four feature maps (k = 4) to approximate an arbitrary convex function, as shown in <ref type="figure" target="#fig_5">Fig.  4(d)</ref>.</p><p>White-colored objects in an image are similar to heavy haze scenes that are usually with high values of brightness and low values of saturation. Therefore, almost all the haze estimation models tend to consider the white-colored scene objects as being distant, resulting in inaccurate estimation of the medium transmission. Based on the assumption that the scene depth is locally constant, local extremum filter is commonly to overcome this problem <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In DehazeNet, local maximum filters of the third layer operation remove the local estimation error. Thus the direct attenuation term J (x) t (x) can be very close to zero when the transmission t (x) is close to zero. The directly recovered scene radiance J (x) is prone to noise. In DehazeNet, we propose BReLU to restrict the values of transmission between t min and t max , thus alleviating the noise problem. Note that BReLU is equivalent to the boundary constraints used in traditional methods <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training of DehazeNet</head><p>1) Training Data: It is in general costly to collect a vast amount of labelled data for training deep models <ref type="bibr" target="#b18">[19]</ref>. For <ref type="figure">Fig. 5</ref>. Example haze-free training images collected from the Internet training of DehazeNet, it is even more difficult as the pairs of hazy and haze-free images of natural scenes (or the pairs of hazy images and their associated medium transmission maps) are not massively available. Instead, we resort to synthesized training data based on the physical haze formation model <ref type="bibr" target="#b16">[17]</ref>.</p><p>More specifically, we synthesize training pairs of hazy and haze-free image patches based on two assumptions <ref type="bibr" target="#b16">[17]</ref>: first, image content is independent of medium transmission (the same image content can appear at any depths of scenes); second, medium transmission is locally constant (image pixels in a small patch tend to have similar depths). These assumptions suggest that we can assume an arbitrary transmission for an individual image patch. Given a haze-free patch J P (x), the atmospheric light ?, and a random transmission t ? (0, 1), a hazy patch is synthesized as</p><formula xml:id="formula_17">I P (x) = J P (x) t + ? (1 ? t).</formula><p>To reduce the uncertainty in variable learning, atmospheric light ? is set to 1.</p><p>In this work, we collect haze-free images from the Internet, and randomly sample from them patches of size 16 ? 16. Different from <ref type="bibr" target="#b16">[17]</ref>, these haze-free images include not only those capturing people's daily life, but also those of natural and city landscapes, since we believe that this variety of training samples can be learned into the filters of DehazeNet. <ref type="figure">Fig. 5</ref> shows examples of our collected haze-free images.</p><p>2) Training Method: In the DehazeNet, supervised learning requires the mapping relationship F between RGB value and medium transmission. Network parameters ? = {W 1 , W 2 , W 4 , B 1 , B 2 , B 4 } are achieved through minimizing the loss function between the training patch I P (x) and the corresponding ground truth medium transmission t. Given a set of hazy image patches and their corresponding medium transmissions, where hazy patches are synthesized from hazefree patches as described above, we use Mean Squared Error (MSE) as the loss function:</p><formula xml:id="formula_18">L (?) = 1 N N i=1 F I P i ; ? ? t i 2<label>(14)</label></formula><p>Stochastic gradient descent (SGD) is used to train De-hazeNet. We implement our model using the Caffe package <ref type="bibr" target="#b36">[37]</ref>. Detailed configurations and parameter settings of our proposed DehazeNet (as shown in <ref type="figure" target="#fig_3">Fig. 2</ref>) are summarized in <ref type="table" target="#tab_0">Table I</ref>, which includes 3 convolutional layers and 1 max-pooling layer, with Maxout and BReLU activations used respectively after the first and last convolutional operations. IV. EXPERIMENTS To verify the architecture of DehazeNet, we analyze its convergence and compare it with the state-of-art methods, including FVR <ref type="bibr" target="#b37">[38]</ref>, DCP <ref type="bibr" target="#b8">[9]</ref>, BCCR <ref type="bibr" target="#b10">[11]</ref>, ATM <ref type="bibr" target="#b38">[39]</ref>, RF <ref type="bibr" target="#b16">[17]</ref>, BPNN <ref type="bibr" target="#b39">[40]</ref>, RF <ref type="bibr" target="#b16">[17]</ref> and CAP <ref type="bibr" target="#b17">[18]</ref>.</p><p>Regarding the training data, 10,000 haze-free patches are sampled randomly from the images collected from the Internet. For each patch, we uniformly sample 10 random transmissions t ? (0, 1) to generate 10 hazy patches. Therefore, a total of 100,000 synthetic patches are generated for DehazeNet training. In DehazeNet, the filter weights of each layer are initialized by drawing randomly from a Gaussian distribution (with mean value ? = 0 and standard deviation ? = 0.001), and the biases are set to 0. The learning rate decreases by half from 0.005 to 3.125e-4 every 100,000 iterations. Based on the parameters above, DehazeNet is trained (in 500,000 iterations with a batch-size of 128) on a PC with Nvidia GeForce GTX 780 GPU.</p><p>Based on the transmission estimated by DehazeNet and the atmospheric scattering model, haze-free images are restored as traditional methods. Because of the local extremum in the third layer, the blocking artifacts appear in the transmission map obtained from DehazeNet. To refine the transmission map, guided image filtering <ref type="bibr" target="#b14">[15]</ref> is used to smooth the image. Referring to Equation (4), the boundary value of 0.1 percent intensity is chosen as t 0 in the transmission map, and we select the highest intensity pixel in the corresponding hazy image I (x) among x ? {y|t (y) ? t 0 } as the atmospheric light ?. Given the medium transmission t (x) and the atmospheric light ?, the haze-free image J (x) is recovered easily. For convenience, Equation <ref type="formula" target="#formula_0">(1)</ref> is rewritten as follows:</p><formula xml:id="formula_19">J (x) = I (x) ? ? (1 ? t (x)) t (x)<label>(15)</label></formula><p>Although DehazeNet is based on CNNs, the lightened network can effectively guarantee the realtime performance, and runs without GPUs. The entire dehazing framework is tested in MATLAB 2014A only with a CPU (Intel i7 3770, 3.4GHz), and it processes a 640 ? 480 image with approximately 1.5 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model and performance</head><p>In DehazeNet, there are two important layers with special design for transmission estimation, feature extraction F 1 and non-linear regression F 4 . To proof the effectiveness of De-hazeNet, two traditional CNNs (SRCNN <ref type="bibr" target="#b40">[41]</ref> and CNN-L [23]) 1) Maxout unit in feature extraction F 1 : The activation unit in F 1 is a non-linear dimension reduction to approximate traditional haze-relevant features extraction. In the field of image processing, low-dimensional mapping is a core procedure for discovering principal attributes and for reducing pattern noise. For example, PCA <ref type="bibr" target="#b41">[42]</ref> and LDA <ref type="bibr" target="#b42">[43]</ref> as classical linear lowdimensional mappings are widely used in computer vision and data mining. In <ref type="bibr" target="#b22">[23]</ref>, a non-linear sparse low-dimensional mapping with ReLU is used for high-resolution reconstruction. As an unusual low-dimensional mapping, Maxout unit maximizes feature maps to discover the prior knowledge of the hazy images. Therefore, the following experiment is designed to confirm the validity of Maxout unit. According to <ref type="bibr" target="#b22">[23]</ref>, linear unit maps a 16-dimensional vector into a 4-dimensional vector, which is equivalent to applying 4 filters with a size of 16?1?1. In addition, the sparse low-dimensional mapping is connecting ReLU to the linear unit. <ref type="figure" target="#fig_6">Fig. 6</ref> presents the training process of DehazeNet with Maxout unit, compared with ReLU and linear unit. We observe in <ref type="figure" target="#fig_6">Fig. 6</ref> that the speed of convergence for Maxout network is faster than that for ReLU and linear unit. In addition, the values in the bracket present the convergent result, and the performance of Maxout is improved by approximately 0.30e-2 compared with ReLU and linear unit. The reason is that Maxout unit provides the equivalent function of almost all of haze-relevant features, and alleviates the disadvantage of the simple piecewise functions such as ReLU.</p><p>2) BReLU in non-linear regression F 4 : BReLU is a novel activation function that is useful for image restoration and reconstruction. Inspired by ReLU and Sigmoid, BReLU is designed with bilateral restraint and local linearity. The bilateral restraint applies a priori constraint to reduce the solution space scale; the local linearity overcomes the gradient vanishing to gain better precision. In the contrast experiment, ReLU and Sigmoid are used to take the place of BReLU in the nonlinear regression layer. For ReLU, F 4 can be rewritten as F 4 = max (0, W 4 * F 3 + B 4 ), and for Sigmoid, it can be rewritten as F 4 = 1/(1 + exp (?W 4 * F 3 ? B 4 )).   <ref type="figure" target="#fig_7">Fig. 7</ref> shows the training process using different activation functions in F 4 . BReLU has a better convergence rate than ReLU and Sigmoid, especially during the first 50,000 iterations. The convergent precisions show that the performance of BReLU is improved approximately 0.05e-2 compared with ReLU and by 0.20e-2 compared with Sigmoid. <ref type="figure">Fig. 8</ref> plots the predicted transmission versus the ground truth transmission on the test patches. Clearly, the predicted transmission centers around the 45 degree line in BReLU result. However, the predicted transmission of ReLU is always higher than the true transmission, and there are some predicted transmissions over the limit value t max = 1. Due to the curvature of Sigmoid function, the predicted transmission is far away from the true transmission, closing to 0 and 1. The MSE on the test set of BReLU is 1.19e-2, and that of ReLU and Sigmoid are 1.28e-2 and 1.46e-2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Filter number and size</head><p>To investigate the best trade-off between performance and parameter size, we progressively modify the parameters of DehazeNet. Based on the default settings of DehazeNet, two experiments are conducted: (1) one is with a larger filter number, and (2) the other is with a different filter size. Similar to Sec. III-C2, these models are trained on the same dataset and <ref type="table" target="#tab_0">Table II</ref> shows the training/testing MSEs with the corresponding parameter settings.</p><p>In general, the performance would improve when increasing the network width. It is clear that superior performance could be achieved by increasing the number of filter. However, if a <ref type="figure">Fig. 8</ref>. The plots between predicted and truth transmission on different activation function in the non-linear regression F 4 fast dehazing speed is desired, a small network is preferred, which could still achieve better performance than other popular methods. In this paper, the lightened network is adopted in the following experiments.</p><p>In addition, we examine the network sensitivity to different filter sizes. The default network setting, whose specifics are shown in <ref type="table" target="#tab_0">Table I</ref>, is denoted as 5-M-7-6. We first analyze the effects of varying filter sizes in the second layer F 2 . <ref type="table" target="#tab_0">Table  II</ref> indicates that a reasonably larger filter size in F 2 could grasp richer structural information, which in turn leads to better results. The multi-scale feature mapping with the filter sizes of 3/5/7 is also adopted in F2 of DehazeNet, which achieves similar testing MSE to that of the single-scale case of 7 ? 7 filter. Moreover, we demonstrate in Section IV-D that the multiscale mapping is able to improve the scale robustness.</p><p>We further examine networks with different filter sizes in the third and fourth layer. Keeping the same receptive field of network, the filter sizes in F 3 and F 4 are adjusted simultaneously. It is showed that the larger filter size of nonlinear regression F 4 enhances the fitting effect, but may lead to over-fitting. The local extremum in F 3 could improve the robustness on testing dataset. Therefore, we find the best filter setting of F 3 and F 4 as 5-M-7-6 in DehazeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative results on synthetic patches</head><p>In recent years, there are three methods based on learning framework for haze removal. In <ref type="bibr" target="#b17">[18]</ref>, dehazing parameters are learned by a linear model, to estimate the scene depth under Color Attenuation Prior (CAP). A back propagation neural network (BPNN) <ref type="bibr" target="#b39">[40]</ref> is used to mine the internal link between color and depth from the training samples. In <ref type="bibr" target="#b16">[17]</ref>, Random Forests (RF) are used to investigate haze-relevant features for haze-free image restoration. All of the above methods and DehazeNet are trained with the same method as RF. According to the testing measure of RF, 2000 image patches are randomly sampled from haze-free images with 10 random transmission t ? (0, 1) to generate 20,000 hazy patches for testing. We run DehazeNet and CAP on the same testing dataset to measure the mean squared error (MSE) between the predicted transmission and true transmission. DCP <ref type="bibr" target="#b8">[9]</ref> is a classical dehazing method, which is used as a comparison baselines. <ref type="table" target="#tab_0">Table III</ref> shows the MSE between predicted transmissions and truth transmissions on the testing patches. DehazeNet achieves the best state-of-the-art score, which is 1.19e-2; the MSE between our method and the next state-of-art result (RF <ref type="bibr" target="#b16">[17]</ref>) in the literature is 0.07e-2. Because in <ref type="bibr" target="#b16">[17]</ref>, the feature values of patches are sorted to break the correlation between the haze-relevant features and the image content. However, the content information concerned by DehazeNet is useful for the transmission estimation of the sky region and white objects. Moreover, CAP <ref type="bibr" target="#b17">[18]</ref> achieves satisfactory results in follow-on experiments but poor performance in this experiment, due to some outlier values (greater than 1 or less than 0) in the linear regression model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Quantitative results on synthetic images</head><p>To verify the effectiveness on complete images, DehazeNet is tested on synthesized hazy images from stereo images with a known depth map d (x), and it is compared with DCP <ref type="bibr" target="#b8">[9]</ref>, FVR <ref type="bibr" target="#b37">[38]</ref>, BCCR <ref type="bibr" target="#b10">[11]</ref>, ATM <ref type="bibr" target="#b38">[39]</ref>, CAP 2 <ref type="bibr" target="#b17">[18]</ref>, and RF <ref type="bibr" target="#b16">[17]</ref>. There are 12 pairs of stereo images collected in Middlebury Stereo Datasets <ref type="bibr">(2001)</ref><ref type="bibr">(2002)</ref><ref type="bibr">(2003)</ref><ref type="bibr">(2004)</ref><ref type="bibr">(2005)</ref><ref type="bibr">(2006)</ref>  <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>. In <ref type="figure" target="#fig_8">Fig. 9</ref>, the hazy images are synthesized from the haze-free stereo images based on (1), and they are restored to haze-free images by DehazeNet.</p><p>To quantitatively assess these methods, we use a series of evaluation criteria in terms of the difference between each pair of haze-free image and dehazing result. Apart from the widely used mean square error (MSE) and the structural similarity (SSIM) <ref type="bibr" target="#b46">[47]</ref> indices, we used additional evaluation matrices, namely peak signal-to-noise ratio (PSNR) and weighted peak signal-to-noise ratio (WPSNR) <ref type="bibr" target="#b47">[48]</ref>. We define one-pass evaluation (OPE) as the conventional method, which we run with standard parameters and report the average measure for the performance assessment. In <ref type="table" target="#tab_0">Table IV</ref>, DehazeNet is compared with six state-of-the-art methods on all of the hazy images by OPE (hazy images are synthesized with the single scattering  . It is exciting that, although DehazeNet is optimized by the MSE loss function, it also achieves the best performance on the other types of evaluation matrices. The dehazing effectiveness is sensitive to the haze density, and the performance with a different scattering coefficient ? could become much worse or better. Therefore, we propose an evaluation to analyze dehazing robustness to scattering coefficient ? ? {0.75, 1.0, 1.25, 1.5}, which is called as coefficient robustness evaluation (CRE). As shown in <ref type="table" target="#tab_4">Table V</ref>, CAP <ref type="bibr" target="#b17">[18]</ref> achieve better performances on the mist (? = 0.75), but the dehazing performance reduces gradually when the amount of haze increases. The reason is that CAP estimates the medium transmission based on predicted scene depth and a assumed scattering coefficient (? = 1). In <ref type="bibr" target="#b16">[17]</ref>, 200 trees are used to build random forests for non-linear regression and shows greater coefficient robustness. However, the high-computation of random forests in every pixel constraints to its practicality. For DehazeNet, the medium transmission is estimated directly by a non-linear activation function (Maxout) in F 1 , resulting Due to the color offset of haze particles and light sources, the atmosphere airlight is not a proper pure-white. An airlight robustness evaluation (ARE) is proposed to analyze the dehazing methods for different atmosphere airlight ?. Although DehazeNet is trained from the samples generated by setting ? = 1, it also achieves the greater robustness on the other values of atmosphere airlight. In particular, DehazeNet performs better than the other methods when sunlight haze is [1.0, 1.0, 0.9]. Therefore, DehazeNet could also be applied to remove halation, which is a bright ring surrounding a source of light as shown in <ref type="figure" target="#fig_2">Fig. 10</ref>.</p><p>The view field transformation and image zoom occur often in real-world applications. The scale robustness evaluation (SRE) is used to analyze the influence from the scale variation. Compared with the same state-of-the-art methods in OPE, there are 4 scale coefficients s selected from 0.4 to 1.0 to generate different scale images for SER. In <ref type="table" target="#tab_4">Table V</ref>, DehazeNet shows excellent robustness to the scale variation due to the multi-scale mapping in F 2 . The single scale used in CAP <ref type="bibr" target="#b17">[18]</ref>, DCP <ref type="bibr" target="#b8">[9]</ref> and ATM <ref type="bibr" target="#b38">[39]</ref> results in a different prediction accuracy on a different scale. When an image shrinks, an excessively large-scale processing neighborhood will lose the image's details. Therefore, the multi-scale mapping in DehazeNet provides a variety of filters to merge multi-scale features, and it achieves the best scores under all of different scales.</p><p>In most situations, noise is random produced by the sensor or camera circuitry, which will bring in estimation error. We also discuss the influences of varying degrees of image noise to our method. As a basic noise model, additive white Gaussian (AWG) noise with standard deviation ? ? {10, 15, 20, 25} is used for noise robustness evaluation (NRE). Benefiting from the Maxout suppression in F 1 and the local extremum in F 3 , DehazeNet performs more robustly in NRE than the others do. RF <ref type="bibr" target="#b16">[17]</ref> has a good performance in most of the evaluations but fails in NRE, because the feature values of patches are sorted to break the correlation between the medium transmission and the image content, which will also magnify the effect of outlier. <ref type="figure" target="#fig_2">Fig. 11</ref> shows the dehazing results and depth maps restored by DehazeNet, and more results and comparisons can be found at http://caibolun.github.io/DehazeNet/. Because all of the dehazing algorithms can obtain truly good results on general outdoor images, it is difficult to rank them visually. To compare them, this paper focuses on 5 identified challenging images in related studies <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>. These images have large white or gray regions that are hard to handle, because most existing dehazing algorithms are sensitive to the white color. <ref type="figure" target="#fig_2">Fig. 12</ref> shows a qualitative comparison with six stateof-the-art dehazing algorithms on the challenging images. <ref type="figure" target="#fig_2">Fig.  12</ref> (a) depicts the hazy images to be dehazed, and <ref type="figure" target="#fig_2">Fig. 12</ref> (b-g) shows the results of ATM <ref type="bibr" target="#b38">[39]</ref>, BCCR <ref type="bibr" target="#b10">[11]</ref>, FVR <ref type="bibr" target="#b37">[38]</ref>, DCP <ref type="bibr" target="#b8">[9]</ref>, CAP <ref type="bibr" target="#b17">[18]</ref> and RF <ref type="bibr" target="#b16">[17]</ref>, respectively. The results of DehazeNet are given in <ref type="figure" target="#fig_2">Fig. 12 (h)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Qualitative results on real-world images</head><p>The sky region in a hazy image is a challenge of dehazing, because clouds and haze are similar natural phenomenons with the same atmospheric scattering model. As shown in the first three figures, most of the haze is removed in the (bd) results, and the details of the scenes and objects are well restored. However, the results significantly suffer from overenhancement in the sky region. Overall, the sky region of these images is much darker than it should be or is oversaturated and distorted. Haze generally exists only in the atmospheric surface layer, and thus the sky region almost does not require handling. Based on the learning framework, CAP and RF avoid color distortion in the sky, but non-sky regions are enhanced poorly because of the non-content regression model (for example, the rock-soil of the first image and the green flatlands in the third image). DehazeNet appears to be capable of finding the sky region to keep the color, and assures a good dehazing effect in other regions. The reason is that the patch attribute can be learned in the hidden layer of DehazeNet, and it contributes to the dehazing effects in the sky.</p><p>Because transmission estimation based on priors are a type of statistics, which might not work for certain images. The fourth and fifth figures are determined to be failure cases in <ref type="bibr" target="#b8">[9]</ref>. When the scene objects are inherently similar to the atmospheric light (such as the fair-skinned complexion in the fourth figure and the white marble in the fifth <ref type="figure">figure)</ref>, the estimated transmission based on priors (DCP, BCCR, FVR) is not reliable. Because the dark channel has bright values near such objects, and FVR and BCCR are based on DCP which has an inherent problem of overestimating the transmission. CAP and RF learned from a regression model is free from oversaturation, but underestimates the haze degree in the distance (see the brown hair in the fourth image and the red pillar in the fifth image). Compared with the six algorithms, our results avoid image oversaturation and retain the dehazing validity due to the non-linear regression of DehazeNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a novel deep learning approach for single image dehazing. Inspired by traditional haze-relevant features and dehazing methods, we show that medium transmission estimation can be reformulated into a trainable end-to-end system with special design, where the feature extraction layer and the non-linear regression layer are distinguished from classical CNNs. In the first layer F 1 , Maxout unit is proved similar to the priori methods, and it is more effective to learn haze-relevant features. In the last layer F 4 , a novel activation function called BReLU is instead of ReLU or Sigmoid to keep bilateral restraint and local linearity for image restoration. With this lightweight architecture, De-hazeNet achieves dramatically high efficiency and outstanding dehazing effects than the state-of-the-art methods.</p><p>Although we successfully applied a CNN for haze removal, there are still some extensibility researches to be carried out. That is, the atmospheric light ? cannot be regarded as a global constant, which will be learned together with medium transmission in a unified network. Moreover, we think <ref type="figure" target="#fig_2">Fig. 11</ref>. The haze-free images and depth maps restored by DehazeNet (a) Hazy image (b) ATM <ref type="bibr" target="#b38">[39]</ref> (c) BCCR <ref type="bibr" target="#b10">[11]</ref> (d) FVR <ref type="bibr" target="#b37">[38]</ref> (e) DCP <ref type="bibr" target="#b8">[9]</ref> (f) CAP <ref type="bibr" target="#b17">[18]</ref> (g) RF <ref type="bibr" target="#b16">[17]</ref> (h) DehazeNet atmospheric scattering model can also be learned in a deeper neural network, in which an end-to-end mapping between haze and haze-free images can be optimized directly without the medium transmission estimation. We leave this problem for future research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Image dehazing is an inherently ill-posed problem. Based on empirical observations, existing methods propose various assumptions or prior knowledge that are utilized to compute intermediate haze-relevant features. Final haze removal can be achieved based on these haze-relevant features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) The process of imaging in hazy weather. The transmission attenuation J (x) t (x) caused by the reduction in reflected energy, leads to low brightness intensity. The airlight ? (1 ? t (x)) formed by the scattering of the environmental illumination, enhances the brightness and reduces the saturation.(b) Atmospheric scattering model. The observed hazy image I (x) is generated by the real scene J (x), the medium transmission t (x) and the global atmospheric light ?.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 .</head><label>1</label><figDesc>Imaging in hazy weather and atmospheric scattering model 1) Dark Channel:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of DehazeNet. DehazeNet conceptually consists of four sequential operations (feature extraction, multi-scale mapping, local extremum and non-linear regression), which is constructed by 3 convolution layers, a max-pooling, a Maxout unit and a BReLU activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Rectified Linear Unit (ReLU) and Bilateral Rectified Linear Unit (BReLU)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Opposite filter (b) All-pass filter (c) Round filter (d) Maxout (e) The actual kernels learned from DehazeNet Filter weight and Maxout unit in the first layer operation F 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The training process with different low-dimensional mapping in F 1 with the same number of 3 layers are regarded as baseline models. The number of parameters of DehazeNet, SRCNN, and CNN-L is 8,240, 18,400, and 67,552 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>The training process with different activation function in F 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Synthetic images based on Middlebury Stereo Datasets and DehazeNet results coefficient ? = 1 and the pure-white atmospheric airlight ? = 1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Image enhancement for anti-halation by DehazeNet in excellent robustness to the scattering coefficient.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 12 .</head><label>12</label><figDesc>Qualitative comparison of different methods on real-world images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I .</head><label>I</label><figDesc>THE ARCHITECTURES OF THE DEHAZENET MODEL</figDesc><table><row><cell>Formulation</cell><cell>Type</cell><cell>Input Size</cell><cell>Num n</cell><cell>Filter f ? f</cell><cell>Pad</cell></row><row><cell>Feature</cell><cell>Conv</cell><cell>3 ? 16 ? 16</cell><cell>16</cell><cell>5 ? 5</cell><cell>0</cell></row><row><cell>Extraction</cell><cell>Maxout</cell><cell>16 ? 12 ? 12</cell><cell>4</cell><cell>-</cell><cell>0</cell></row><row><cell>Multi-scale Mapping</cell><cell>Conv</cell><cell>4 ? 12 ? 12</cell><cell>16 16 16</cell><cell>3 ? 3 5 ? 5 7 ? 7</cell><cell>1 2 3</cell></row><row><cell>Local Extremum</cell><cell>Maxpool</cell><cell>48 ? 12 ? 12</cell><cell>-</cell><cell>7 ? 7</cell><cell>0</cell></row><row><cell>Non-linear</cell><cell>Conv</cell><cell>48 ? 6 ? 6</cell><cell>1</cell><cell>6 ? 6</cell><cell>0</cell></row><row><cell>Regression</cell><cell>BReLU</cell><cell>1 ? 1</cell><cell>1</cell><cell>-</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II</head><label>II</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="5">THE RESULTS OF USING DIFFERENT FILTER NUMBER OR</cell></row><row><cell></cell><cell></cell><cell cols="3">SIZE IN DEHAZENET (?10 ?2 )</cell></row><row><cell>Filter</cell><cell></cell><cell>Architecture</cell><cell>Train MSE</cell><cell>Test MSE</cell><cell>#Param</cell></row><row><cell>Number (n1-n2)</cell><cell></cell><cell>4-(16?3) 8-(32 ? 3) 16-(64 ? 3)</cell><cell>1.090 0.972 0.902</cell><cell>1.190 1.138 1.112</cell><cell>8,240 27,104 96,704</cell></row><row><cell></cell><cell></cell><cell>5-3-7-6</cell><cell>1.184</cell><cell>1.219</cell><cell>4,656</cell></row><row><cell>F2 Size</cell><cell></cell><cell>5-5-7-6</cell><cell>1.133</cell><cell>1.225</cell><cell>7,728</cell></row><row><cell cols="2">(f1-f2-f3-f4)</cell><cell>5-7-7-6</cell><cell>1.021</cell><cell>1.184</cell><cell>12,336</cell></row><row><cell></cell><cell></cell><cell>5-M-7-6</cell><cell>1.090</cell><cell>1.190</cell><cell>8,240</cell></row><row><cell cols="2">F4 Size (f1-f2-f3-f4)</cell><cell>5-M-6-7 5-M-7-6 5-M-8-5</cell><cell>1.077 1.090 1.103</cell><cell>1.192 1.190 1.201</cell><cell>8,864 8,240 7,712</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III</head><label>III</label><figDesc></figDesc><table><row><cell>.</cell><cell cols="5">MSE BETWEEN PREDICTED TRANSMISSION AND GROUND</cell></row><row><cell></cell><cell cols="3">TRUTH ON SYNTHETIC PATCHES</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>DCP [9]</cell><cell>BPNN [40]</cell><cell>CAP [18]</cell><cell>RF [17]</cell><cell>DehazeNet</cell></row><row><cell>MSE(?10 ?2 )</cell><cell>3.18</cell><cell>4.37</cell><cell>3.32</cell><cell>1.26</cell><cell>1.19</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV .</head><label>IV</label><figDesc>THE AVERAGE RESULTS OF MSE, SSIM, PSNR AND WSNR ON THE SYNTHETIC IMAGES (? = 1 AND ? = 1)</figDesc><table><row><cell>Metric</cell><cell>ATM [39]</cell><cell>BCCR [11]</cell><cell>FVR [38]</cell><cell>DCP [9]</cell><cell>CAP 2 [18]</cell><cell>RF [17]</cell><cell>DehazeNet</cell></row><row><cell>MSE</cell><cell>0.0689</cell><cell>0.0243</cell><cell>0.0155</cell><cell>0.0172</cell><cell>0.0075 (0.0068)</cell><cell>0.0070</cell><cell>0.0062</cell></row><row><cell>SSIM</cell><cell>0.9890</cell><cell>0.9963</cell><cell>0.9973</cell><cell>0.9981</cell><cell>0.9991 (0.9990)</cell><cell>0.9989</cell><cell>0.9993</cell></row><row><cell>PSNR</cell><cell>60.8612</cell><cell>65.2794</cell><cell>66.5450</cell><cell>66.7392</cell><cell>70.0029 (70.6581)</cell><cell>70.0099</cell><cell>70.9767</cell></row><row><cell>WSNR</cell><cell>7.8492</cell><cell>12.6230</cell><cell>13.7236</cell><cell>13.8508</cell><cell>16.9873 (17.7839)</cell><cell>17.1180</cell><cell>18.0996</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V .</head><label>V</label><figDesc>THE MSE ON THE SYNTHETIC IMAGES BY DIFFERENT SCATTERING COEFFICIENT, IMAGE SCALE AND ATMOSPHERIC AIRLIGHT</figDesc><table><row><cell></cell><cell>Evaluation</cell><cell>ATM [39]</cell><cell>BCCR [11]</cell><cell>FVR [38]</cell><cell>DCP [9]</cell><cell>CAP 2 [18]</cell><cell>RF [17]</cell><cell>DehazeNet</cell></row><row><cell></cell><cell>0.75</cell><cell>0.0581</cell><cell>0.0269</cell><cell>0.0122</cell><cell>0.0199</cell><cell>0.0043 (0.0042)</cell><cell>0.0046</cell><cell>0.0063</cell></row><row><cell>CRE</cell><cell>1.00</cell><cell>0.0689</cell><cell>0.0243</cell><cell>0.0155</cell><cell>0.0172</cell><cell>0.0077 (0.0068)</cell><cell>0.0070</cell><cell>0.0062</cell></row><row><cell>(? =)</cell><cell>1.25</cell><cell>0.0703</cell><cell>0.0230</cell><cell>0.0219</cell><cell>0.0147</cell><cell>0.0141 (0.0121)</cell><cell>0.0109</cell><cell>0.0084</cell></row><row><cell></cell><cell>1.50</cell><cell>0.0683</cell><cell>0.0219</cell><cell>0.0305</cell><cell>0.0134</cell><cell>0.0231 (0.0201)</cell><cell>0.0152</cell><cell>0.0127</cell></row><row><cell cols="2">CRE Average</cell><cell>0.0653</cell><cell>0.0254</cell><cell>0.0187</cell><cell>0.0177</cell><cell>0.0105 (0.0095)</cell><cell>0.0094</cell><cell>0.0084</cell></row><row><cell></cell><cell>[1.0, 1.0, 1.0]</cell><cell>0.0689</cell><cell>0.0243</cell><cell>0.0155</cell><cell>0.0172</cell><cell>0.0075 (0.0068)</cell><cell>0.0070</cell><cell>0.0062</cell></row><row><cell>ARE</cell><cell>[0.9, 1.0, 1.0]</cell><cell>0.0660</cell><cell>0.0266</cell><cell>0.0170</cell><cell>0.0210</cell><cell>0.0073 (0.0069)</cell><cell>0.0071</cell><cell>0.0072</cell></row><row><cell>(? =)</cell><cell>[1.0, 0.9, 1.0]</cell><cell>0.0870</cell><cell>0.0270</cell><cell>0.0159</cell><cell>0.0200</cell><cell>0.0070 (0.0067)</cell><cell>0.0073</cell><cell>0.0074</cell></row><row><cell></cell><cell>[1.0, 1.0, 0.9]</cell><cell>0.0689</cell><cell>0.0239</cell><cell>0.0152</cell><cell>0.0186</cell><cell>0.0081 (0.0069)</cell><cell>0.0083</cell><cell>0.0062</cell></row><row><cell cols="2">ARE Average</cell><cell>0.0727</cell><cell>0.0255</cell><cell>0.0159</cell><cell>0.0192</cell><cell>0.0075 (0.0068)</cell><cell>0.0074</cell><cell>0.0067</cell></row><row><cell></cell><cell>0.40</cell><cell>0.0450</cell><cell>0.0238</cell><cell>0.0155</cell><cell>0.0102</cell><cell>0.0137 (0.0084)</cell><cell>0.0089</cell><cell>0.0066</cell></row><row><cell>SRE</cell><cell>0.60</cell><cell>0.0564</cell><cell>0.0223</cell><cell>0.0154</cell><cell>0.0137</cell><cell>0.0092 (0.0071)</cell><cell>0.0076</cell><cell>0.0060</cell></row><row><cell>(s =)</cell><cell>0.80</cell><cell>0.0619</cell><cell>0.0236</cell><cell>0.0155</cell><cell>0.0166</cell><cell>0.0086 (0.0066)</cell><cell>0.0074</cell><cell>0.0062</cell></row><row><cell></cell><cell>1.00</cell><cell>0.0689</cell><cell>0.0243</cell><cell>0.0155</cell><cell>0.0172</cell><cell>0.0077 (0.0068)</cell><cell>0.0070</cell><cell>0.0062</cell></row><row><cell cols="2">SRE Average</cell><cell>0.0581</cell><cell>0.0235</cell><cell>0.0155</cell><cell>0.0144</cell><cell>0.0098 (0.0072)</cell><cell>0.0077</cell><cell>0.0062</cell></row><row><cell></cell><cell>10</cell><cell>0.0541</cell><cell>0.0138</cell><cell>0.0150</cell><cell>0.0133</cell><cell>0.0065 (0.0070)</cell><cell>0.0086</cell><cell>0.0059</cell></row><row><cell>NRE (? =)</cell><cell>15 20 25</cell><cell>0.0439 --</cell><cell>0.0144 0.0181 0.0224</cell><cell>0.0148 0.0151 0.0150</cell><cell>0.0104 0.0093 0.0082</cell><cell>0.0072 (0.0074) 0.0083 (0.0085) 0.0100 (0.0092)</cell><cell>0.0112 0.0143 0.0155</cell><cell>0.0061 0.0058 0.0051</cell></row><row><cell></cell><cell>30</cell><cell>-</cell><cell>0.0192</cell><cell>0.0151</cell><cell>0.0085</cell><cell>0.0119 (0.0112)</cell><cell>0.0191</cell><cell>0.0049</cell></row><row><cell cols="2">NRE Average</cell><cell>-</cell><cell>0.0255</cell><cell>0.0150</cell><cell>0.0100</cell><cell>0.0088 (0.0087)</cell><cell>0.0137</cell><cell>0.0055</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The results outside the parenthesis are run with the code implemented by authors<ref type="bibr" target="#b17">[18]</ref>, and the results in the parenthesis are re-implemented by us.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contrast enhancement system using spatially adaptive histogram equalization with temporal filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Paik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Consumer Electronics</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="82" to="87" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adaptive image contrast enhancement using generalizations of histogram equalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Stark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="896" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image-dependent color saturation correction in a natural scene pictorial image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eschbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">W</forename><surname>Kolpatzik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995-12" />
			<biblScope unit="page">217</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 5,450</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Instant dehazing of images using polarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">325</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep photo: Model-based photograph enhancement and viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Neubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Deussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visibility in bad weather from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Single image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">72</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Single image haze removal using dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2341" to="2353" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian defogging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nishino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lombardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficient image dehazing with boundary constraint and contextual regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="617" to="624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An investigation of dehazing effects on image and video coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">B</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="662" to="673" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2201" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Physics-based fast single image fog removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Signal Processing</title>
		<imprint>
			<publisher>ICSP</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1048" to="1052" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Guided image filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1397" to="1409" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A closed-form solution to natural image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="242" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Investigating haze-relevant features in a learning framework for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2995" to="3002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A fast single image haze removal algorithm using color attenuation prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3522" to="3533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Robust face recognition via multimodal deep face representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Image super-resolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="633" to="640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06201</idno>
		<title level="m">Adjustable bounded rectifiers: Towards deep binary representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Optics of the atmosphere: scattering by molecules and particles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Mccartney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976" />
			<publisher>John Wiley and Sons, Inc</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vision in bad weather</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Contrast restoration of weather degraded images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="713" to="724" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A fast semiinverse approach to detect and remove the haze from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ancuti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bekaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ACCV 2010</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="501" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Intracellular measurements of spatial integration and the max operation in complex cells of the cat primary visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lampl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ferster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neurophysiology</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2704" to="2713" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fast visibility restoration from a single color or gray level image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>Tarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hautiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2201" to="2208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic recovery of the atmospheric light in hazy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sulami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Glatzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fattal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Back propagation neural network dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Biomimetics (ROBIO)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pcanet: A simple deep learning baseline for image classification?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5017" to="5032" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Mullert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks for signal processing IX</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="7" to="42" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">High-accuracy stereo depth maps using structured light</title>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">195</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning conditional random fields for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The effects of a visual fidelity criterion of the encoding of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Mannos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sakrison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="525" to="536" />
			<date type="published" when="1974" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
