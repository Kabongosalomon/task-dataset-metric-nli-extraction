<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2021 DIRECTED ACYCLIC GRAPH NEURAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
							<email>veronika.thost@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
							<email>chenjie@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT-IBM Watson AI Lab</orgName>
								<orgName type="institution" key="instit2">IBM Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2021 DIRECTED ACYCLIC GRAPH NEURAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph-structured data ubiquitously appears in science and engineering. Graph neural networks (GNNs) are designed to exploit the relational inductive bias exhibited in graphs; they have been shown to outperform other forms of neural networks in scenarios where structure information supplements node features. The most common GNN architecture aggregates information from neighborhoods based on message passing. Its generality has made it broadly applicable. In this paper, we focus on a special, yet widely used, type of graphs-DAGs-and inject a stronger inductive bias-partial ordering-into the neural network design. We propose the directed acyclic graph neural network, DAGNN, an architecture that processes information according to the flow defined by the partial order. DAGNN can be considered a framework that entails earlier works as special cases (e.g., models for trees and models updating node representations recurrently), but we identify several crucial components that prior architectures lack. We perform comprehensive experiments, including ablation studies, on representative DAG datasets (i.e., source code, neural architectures, and probabilistic graphical models) and demonstrate the superiority of DAGNN over simpler DAG architectures as well as general graph architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph-structured data is ubiquitous across various disciplines <ref type="bibr" target="#b6">(Gilmer et al., 2017;</ref><ref type="bibr" target="#b35">Zitnik et al., 2018;</ref><ref type="bibr" target="#b19">Sanchez-Gonzalez et al., 2020)</ref>. Graph neural networks (GNNs) use both the graph structure and node features to produce a vectorial representation, which can be used for classification, regression <ref type="bibr" target="#b7">(Hu et al., 2020)</ref>, and graph decoding . Most popular GNNs update node representations through iterative message passing between neighboring nodes, followed by pooling (either flat or hierarchical <ref type="bibr" target="#b12">(Lee et al., 2019;</ref>), to produce a graph representation <ref type="bibr" target="#b13">(Li et al., 2016;</ref><ref type="bibr" target="#b9">Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b6">Gilmer et al., 2017;</ref>. The relational inductive bias <ref type="bibr" target="#b20">(Santoro et al., 2017;</ref><ref type="bibr" target="#b30">Xu et al., 2020</ref>)-neighborhood aggregation-empowers GNNs to outperform graph-agnostic neural networks. To facilitate subsequent discussions, we formalize a message-passing neural network (MPNN) architecture, which computes representations h v for all nodes v in a graph G in every layer and a final graph representation h G , as <ref type="bibr" target="#b6">(Gilmer et al., 2017)</ref>:</p><formula xml:id="formula_0">h v = COMBINE h ?1 v , AGGREGATE {h ?1 u | u ? N (v)} , = 1, . . . , L,<label>(1)</label></formula><formula xml:id="formula_1">h G = READOUT {h L v , v ? V} ,<label>(2)</label></formula><p>where h 0 v is the input feature of v, N (v) denotes a neighborhood of node v (sometimes including v itself), V denotes the node set of G, L is the number of layers, and AGGREGATE , COMBINE , and READOUT are parameterized neural networks. For notational simplicity, we omit edge attributes; but they can be straightforwardly incorporated into the framework (1)-(2).</p><p>Directed acyclic graphs (DAGs) are a special type of graphs, yet broadly seen across domains. Examples include parsing results of source code <ref type="bibr" target="#b0">(Allamanis et al., 2018)</ref>, logical formulas <ref type="bibr" target="#b3">(Crouse et al., 2019)</ref>, and natural language sentences, as well as probabilistic graphical models , neural architectures , and automated planning problems <ref type="bibr" target="#b15">(Ma et al., 2020)</ref>.</p><p>A directed graph is a DAG if and only if the edges define a partial ordering over the nodes. The partial order is an additionally strong inductive bias one naturally desires to incorporate into the neural network. For example, a neural architecture seen as a DAG defines the acyclic dependency of computation, an important piece of information when comparing architectures and predicting their performance. Hence, this information should be incorporated into the architecture representation for higher predictive power.</p><p>In this work, we propose DAGNNs-directed acyclic graph neural networks-that produce a representation for a DAG driven by the partial order. In particular, the order allows for updating node representations based on those of all their predecessors sequentially, such that nodes without successors digest the information of the entire graph. Such a processing manner substantially differs from that of MPNNs where the information landed on a node is limited by a multi-hop local neighborhood and thus restricted by the depth L of the network.</p><p>Modulo details to be elaborated in sections that follow, the DAGNN framework reads</p><formula xml:id="formula_2">h v = F h ?1 v , G {h u | u ? P(v)}, h ?1 v , = 1, . . . , L,<label>(3)</label></formula><formula xml:id="formula_3">h G = R {h v , = 0, 1, . . . , L, v ? T } ,<label>(4)</label></formula><p>where P(v) denotes the set of direct predecessors of v, T denotes the set of nodes without (direct) successors, and G , F , and R are parameterized neural networks that play similar roles to AGGREGATE , COMBINE , and READOUT, respectively.</p><p>A notable difference between (3)- <ref type="formula" target="#formula_3">(4)</ref> and <ref type="formula" target="#formula_0">(1)</ref>- <ref type="formula" target="#formula_1">(2)</ref> is that the superscript ? 1 inside the underlined part of (1) is advanced to in the counterpart in (3). In other words, MPNN aggregates neighborhood information from the past layer, whereas DAGNN uses the information in the current layer. An advantage is that DAGNN always uses more recent information to update node representations.</p><p>Equations (3)-(4) outline several other subtle but important differences between DAGNN and MPNNs, such as the use of only direct predecessors for aggregation and the pooling on only nodes without successors. All these differences are unique to the special structure a DAG enjoys. Exploiting this structure properly should yield a more favorable vectorial representation of the graph.</p><p>In Section 2, we will elaborate the specifics of (3)-(4). The technical details include (i) attention for node aggregation, (ii) multiple layers for expressivity, and (iii) topological batching for efficient implementation, all of which yield an instantiation of the DAGNN framework that is state of the art.</p><p>For theoretical contributions, we study topological batching and justify that this technique yields maximal parallel concurrency in processing DAGs. Furthermore, we show that the mapping defined by DAGNN is invariant to node permutation and injective under mild assumptions. This result reassures that the graph representation extracted by DAGNN is discriminative.</p><p>Because DAGs appear in many different fields, neural architectures for DAGs (including, notably, D-VAE ) or special cases (e.g., trees) are scattered around the literature over the years. Generally, they are less explored compared to MPNNs; and some are rather applicationspecific. In Section 3, we unify several representative architectures as special cases of the framework (3)-(4). We compare the proposed architecture to them and point out the differences that lead to its superior performance.</p><p>In Section 4, we detail our comprehensive, empirical evaluation on datasets from three domains: (i) source code parsed to DAGs <ref type="bibr" target="#b7">(Hu et al., 2020)</ref>; (ii) neural architecture search , where each architecture is a DAG; and (iii) score-based Bayesian network learning . We show that DAGNN outperforms many representative DAG architectures and MPNNs.</p><p>Overall, this work contributes a specialized graph neural network, a theoretical study of its properties, an analysis of a topological batching technique for enhancing parallel concurrency, a framework interpretation that encompasses prior DAG architectures, and comprehensive evaluations. Supported code is available at https://github.com/vthost/DAGNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THE DAGNN MODEL</head><p>A DAG is a directed graph without cycles. Denote by G = (V, E) a DAG, where V and E ? V ? V are the node set and the edge set, respectively. A (strong) partial order over a set S is a binary <ref type="figure">Figure 1</ref>: Processing of node v = 3 (orange). For each layer , we collect representations h v for all nodes v in a matrix H , where each row represents one node. The initial feature matrix is X = H 0 . In the first layer, the representations of the direct predecessors P(v) = {0, 1, 2} (blue) have been computed; they are aggregated together with the past representation of v (orange) to produce a message. The GRU treats the message as the hidden state and the past representation of v as input and outputs an updated representation for v (green). This new representation will be used by v's direct successors {4} in the same layer and also as input to the next layer. Note that the figure illustrates the processing of only one node. In practice, a batch of nodes is processed; see Section 2.2.</p><p>relation ? that is transitive and asymmetric. Some authors use reflexivity versus irreflexivity to distinguish weak partial order over strong partial order. To unify concepts, we forbid self-loops (which otherwise are considered cycles) in the DAG and mean strong partial order throughout. A set S with partial order ? is called a poset and denoted by a tuple (S, ?).</p><p>A DAG (V, E) and a poset (S, ?) are closely related. For any DAG, one can define a unique partial order ? on the node set V, such that for all pairs of elements u, v ? V, u ? v if and only if there is a directed path from u to v. On the other hand, for any poset (S, ?), there exists (possibly more than) one DAG that uses S as the node set and that admits a directed path from u to v whenever u ? v.</p><p>In a DAG, all nodes without (direct) predecessors are called sources and we collect them in the set S. Similarly, all nodes without (direct) successors are called targets and we collect them in the set T . Additionally, we let X = {h 0 v , v ? V} be the set of input node features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">MODEL</head><p>The main idea of DAGNN is to process nodes according to the partial order defined by the DAG. Using the language of MPNN, at every node v, we "aggregate" information from its neighbors and "combine" this aggregated information (the "message") with v's information to update the representation of v. The main differences to MPNN are that (i) we use the current-layer, rather than the past-layer, information to compute the current-layer representation of v and that (ii) we aggregate from the direct-predecessor set P(v) only, rather than the entire (or randomly sampled) neighborhood N (v). They lead to a straightforward difference in the final "readout" also. In the following, we propose an instantiation of Equations (3)-(4). See <ref type="figure">Figure 1</ref> for an illustration of the architecture.</p><p>One layer. We use the attention mechanism to instantiate the aggregate operator G . For a node v at the -th layer, the output message m v computed by G is a weighted combination of h u for all nodes u ? P(v) at the same layer :</p><formula xml:id="formula_4">m v message := G {h u | u ? P(v)}, h ?1 v = u?P(v) ? vu h ?1 v query , h u key h u value .<label>(5)</label></formula><p>The weighting coefficients ? vu follow the query-key design in usual attention mechanisms, whereby the representation of v in the past layer, h ?1 v , serves as the query. Specifically, we define</p><formula xml:id="formula_5">? vu h ?1 v , h u = softmax u?P(v) w 1 h ?1 v + w 2 h u ,<label>(6)</label></formula><p>where w 1 and w 2 are model parameters. We use the additive form, as opposed to the usual dotproduct form, 1 since it involves fewer parameters. An additional advantage is that it is straightforward to incorporate edge attributes into the model, as will be discussed soon.</p><p>The combine operator F combines the message m v with the previous representation of v, h ?1 v , and produces an updated representation h v . We employ a recurrent architecture, which is usually used for processing data in sequential order but similarly suits processing in partial order:</p><formula xml:id="formula_6">h v = F h ?1 v , m v = GRU h ?1 v , input message m v state ,<label>(7)</label></formula><p>where h ?1 v , m v , and h v are treated as the input, past state, and updated state/output of a GRU, respectively. This design differs from most MPNNs that use simple summation or concatenation to combine the representations. It further differs from GG-NN <ref type="bibr" target="#b13">(Li et al., 2016)</ref> (which also employs a GRU), wherein the roles of the two arguments are switched. In GG-NN, the message is treated as the input and the node representation is treated as the state. In contrast, we start from node features and naturally use them as inputs. The message tracks the processed part of the graph and serves better the role of a hidden state, being recurrently updated.</p><p>By convention, we define G (?, ?) = 0 for the aggregator so that for nodes with an empty directpredecessor set, the message (or, equivalently, the initial state of the GRU) is zero.</p><p>Bidirectional processing. Just like in sequence models where a sequence may be processed by either the natural order or the reversed order, we optionally invert the directions of the edges in G to create a reverse DAG G. We will use the tilde notation for all terms related to the reverse DAG. For example, the representation of node v in G at the -th layer is denoted by h v .</p><p>Readout. After L layers of (bidirectional) processing, we use the computed node representations to produce the graph representation. We follow a common practice-concatenate the representations across layers, perform a max-pooling across nodes, and apply a fully-connected layer to produce the output. Different from the usual practice, however, we pull across only the target nodes and concatenate the pooling results from the two directions. Recall that the target nodes contain information of the entire graph following the partial order. Mathematically, the readout R produces</p><formula xml:id="formula_7">h G = FC Max-Pool v?T L =0 h v Max-Pool u?S L =0 h u .<label>(8)</label></formula><p>Note that the target set T of G is the same as the source set S of G. If the processing is unidirectional, the right pooling in (8) is dropped.</p><p>Edge attributes. The instantiation of the framework so far has not considered edge attributes. It is in fact simple to incorporate them. Let ? (u, v) be the type of an edge (u, v) and let y ? be a representation of edges of type ? . We insert this information during message calculation in the aggregator. Specifically, we replace the attention weights ? vu defined in (6) by</p><formula xml:id="formula_8">? vu h ?1 v , h u = softmax u?P(v) w 1 h ?1 v + w 2 h u + w 3 y ? (u,v) .<label>(9)</label></formula><p>In practice, we experiment with slightly fewer parameters by setting w 3 = w 1 and find that the model performs equally well. The edge representations y ? are trainable embeddings of the model. Alternatively, if input edge features are provided, y ? (u,v) can be replaced by a neural networktransformed embedding for the edge (u, v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TOPOLOGICAL BATCHING</head><p>A key difference to MPNN is that DAGNN processes nodes sequentially owing to the nature of the aggregator G , obeying the partial order. Thus, for computational efficiency, it is important to maximally exploit concurrency so as to better leverage parallel computing resources (e.g., GPUs). One</p><formula xml:id="formula_9">1 The usual dot-product form reads ? vu (h ?1 v , h u ) = softmax( W 1 h ?1 v , W 2 h u )</formula><p>. We find that in practice the dot-product form and the additive form perform rather similarly, but the former requires substantially more parameters. We are indebted to Hyoungjin Lim who pointed out that, however, in the additive form, the query term will be canceled out inside the softmax computation. observation is that nodes without dependency may be grouped together and processed concurrently, if their predecessors have all been processed. See <ref type="figure" target="#fig_0">Figure 2</ref> for an illustration.</p><p>To materialize this idea, we consider topological batching, which partitions the node set V into ordered batches {B i } i?0 so that (i) the B i 's are disjoint and their union is V; (ii) for every pair of nodes u, v ? B i for some i, there is not a directed path from u to v or from v to u; (iii) for every i &gt; 0, there exists one node in B i such that it is the tail of an edge whose head is in B i?1 . The concept was propsoed by <ref type="bibr" target="#b3">Crouse et al. (2019)</ref>; 2 in what follows, we derive several properties that legitimizes its use in our setting. First, topological batching produces the minimum number of sequential batches such that all nodes in each batch can be processed in parallel. Theorem 1. The number of batches from a partitioning that satisfies (i)-(iii) described in the preceding paragraph is equal to the number of nodes in the longest path of the DAG. As a consequence, this partitioning produces the minimum number of ordered batches such that for all u ? v, if u ? B i and v ? B j , then i &lt; j. Note that the partial order ? is defined at the beginning of Section 2.</p><p>The partitioning procedure may be as follows. All nodes without direct predecessors, S, form the initial batch. Iteratively, remove the batch just formed from the graph, as well as the edges emitting from these nodes. The nodes without direct predecessors in the remaining graph form the next batch. Remark 1. To satisfy Properties (i)-(iii), it is not necessary that B 0 = S; but the above procedure achieves so. Applying this procedure on the reverse DAG G, we obtain B 0 = T . Note that the last batch for G may not be the same as T ; and the last batch for G may not be the same as S either. Remark 2. Topological batching can be straightforwardly extended to multiple graphs for better parallel concurrency: one merges the B i for the same i across graphs into a single batch. This is equivalent to treating the multiple DAGs as a single (albeit disconnected) DAG and applying topological batching on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PROPERTIES</head><p>In the following, we summarize properties of the DAGNN model; they are consistent with the corresponding results for MPNNs. To formalize these results, we let M : V ? E ? X ? h G denote the mapping defined by Equations <ref type="formula" target="#formula_2">(3)-(4)</ref>. For notational consistency, we omit bidirectional processing, and thus ignore the tilde term in (8). The first results state that DAGNN produces the same graph representation invariant to node permutation. Theorem 2. The graph representation h G is invariant to node indexing if all G , F , and R are so.</p><p>Corollary 3. The functions G , F , and R defined in (5)-(8) are invariant to node indexing. Hence, the resulting graph representation h G is, too.</p><p>The next result states that the framework will not produce the same graph representation for different graphs (i.e., non-isomorphic graphs), under a common condition. Theorem 4. The mapping M is injective if G , F , and R, considered as multiset functions, are so.</p><p>The condition required by Theorem 4 is not restrictive. There exist (infinitely many) injective multiset functions G , F , and R, although the ones instantiated by <ref type="formula" target="#formula_4">(5)-(8)</ref> are not necessarily injective. The modification to injection can be done by using the -trick applied in GIN , but, similar to the referenced work, the that ensures injection is unknown. In practice, it is either set as zero or treated as a tunable hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">COMPARISON TO RELATED MODELS</head><p>In this section, we compare to the most closely related architectures for DAGs, including trees. Natural language processing is a major source of these architectures, since semantic parsing forms a rooted tree or a DAG. Recently, D-VAE  has been suggested as a generalpurpose autoencoder for DAGs. Its encoder architecture is the most similar one to ours, but we highlight notable differences that support the improvement DAGNN gains over the D-VAE encoder. All the models we compare with may be considered as restricted cases of the framework (3)-(4).</p><p>Rooted trees do usually not come with directed edges, because either direction (top-down or bottomup) is sensible. Hence, we use the terminology "parent" and "child" instead. Unified under our framework, recursive neural networks tailored to trees <ref type="bibr" target="#b24">(Socher et al., 2011;</ref><ref type="bibr" target="#b25">2012;</ref><ref type="bibr" target="#b26">2013;</ref><ref type="bibr" target="#b4">Ebrahimi &amp; Dou, 2015)</ref> are applied to a fixed number of children when the aggregator acts on a concatenation of the child representations. Moreover, they assume that internal nodes do not come with input representations and hence the combine operator misses the first argument.</p><p>Tree-LSTM <ref type="bibr" target="#b27">(Tai et al., 2015;</ref><ref type="bibr" target="#b34">Zhu et al., 2015;</ref><ref type="bibr" target="#b33">Zhang et al., 2016;</ref><ref type="bibr" target="#b8">Kiperwasser &amp; Goldberg, 2016)</ref> and DAG-RNN <ref type="bibr" target="#b22">(Shuai et al., 2016)</ref>, like DAGNN, employ a recurrent architecture as the combine operator, but the message (hidden state) therein is a naive sum or element-wise product of child representations. In a variant of Tree-LSTM, the naive sum is replaced by a sum of child representations multiplied by separate weight matrices. A limitation of this variant is that the number of children must be the same and the children must be ordered. Another limitation is that both architectures assume that there is a single terminal node (in which case a readout is not invoked).</p><p>The most similar architecture to DAGNN is the encoder of D-VAE. There are two notable differences. First, D-VAE uses the gated sum as aggregator but we use attention which leverages the information of not only the summands (h u ) but also that of the node under consideration (h ?1 v ). This additional source of information enables attention driven by external factors and improves over self attention. Second, similar to all the aforementioned models, D-VAE does not come with a layer notion. On the contrary, we use multiple layers, which are more natural and powerful in the light of findings about general GNNs. Our empirical results described in the following section confirm so.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we demonstrate the effectiveness of DAGNN on multiple datasets and tasks over a comprehensive list of baselines. We compare timing and show that the training cost of DAGNN is comparable with that of other DAG architectures. We also conduct ablation studies to verify the importance of its components, which prior DAG architectures lack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">DATASETS, TASKS, METRICS, AND BASELINES</head><p>The OGBG-CODE dataset <ref type="bibr" target="#b7">(Hu et al., 2020)</ref> contains 452,741 Python functions parsed into DAGs. We consider the TOK task, predicting the tokens that form the function name; it is included in the Open Graph Benchmark (OGB). Additionally, we introduce the LP task, predicting the length of the longest path of the DAG. The metric for TOK is the F1 score and that for LP is accuracy. Because of the vast size, we also create a 15% training subset, OGBG-CODE-15, for similar experiments.</p><p>For this dataset, we consider three basic baselines and several GNN models for comparison. For the TOK task, the Node2Token baseline predicts tokens from the attributes of the second graph node, while the TargetInGraph baseline predicts tokens that appear in both the ground truth and in the attributes of some graph node. These baselines exploit the fact that the tokens form node attributes and that the second node's attribute contains the function name if it is part of the vocabulary. For the LP task, the MajorityInValid baseline constantly predicts the majority length seen from the validation set. The considered GNN models include four from OGB: GCN <ref type="bibr" target="#b9">(Kipf &amp; Welling, 2017)</ref>, GIN , GCN-VN, GIN-VN (where -VN means adding a virtual node connecting all existing nodes); two using attention/gated-sum mechanisms: GAT , GG-NN   The NA dataset  contains 19,020 neural architectures generated by the ENAS software. The task is to predict the architecture performance on CIFAR-10 under the weight-sharing scheme. Since it is a regression task, the metrics are RMSE and Pearson's r. To gauge performance with , we similarly train (unsupervised) autoencoders and use sparse Gaussian process regression on the latent representation to predict the architecture performance. DAGNN serves as the encoder and we pair it with an adaptation of the D-VAE decoder (see Appendix D).</p><p>We The BN dataset  contains 200,000 Bayesian networks generated by using the R package bnlearn. The task is to predict the BIC score that measures how well a BN fits the Asia dataset <ref type="bibr" target="#b11">(Lauritzen &amp; Spiegelhalter, 1988)</ref>. We use the same metrics and baselines as for NA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">RESULTS AND DISCUSSION</head><p>Prediction performance, token prediction (TOK), <ref type="table" target="#tab_0">Table 1</ref>. The general trend is the same across the full dataset and the 15% subset. DAGNN performs the best. GAT achieves the second best result, surprisingly outperforming D-VAE (the third best). Hence, using attention as aggregator during message passing benefits this task. On the 15% subset, only DAGNN, GAT, and D-VAE match or surpass the TargetInGraph baseline. Note that not all ground-truth tokens are in the vocabulary and thus the best achievable F1 is 90.99. Even so, all methods are far from reaching this ceiling performance. Furthermore, although most of the MPNN models (middle section of the table) use as many as five layers for message passing, the generally good performance of DAGNN and D-VAE indicates that DAG architectures not restricted by the network depth benefit from the inductive bias.  Prediction performance, length of longest path (LP), <ref type="table" target="#tab_0">Table 1</ref>. This analytical task interestingly reveals that many of the findings for the TOK task do not directly carry over. DAGNN still performs the best, but the second place is achieved by D-VAE while GAT lags far behind. The unsatisfactory performance of GAT indicates that attention alone is insufficient for DAG representation learning. The hierarchical pooling methods also perform disappointingly, showing that ignoring nodes may modify important properties of the graph (in this case, the longest path). It is worth noting that DAGNN and D-VAE achieve nearly perfect accuracy. This result corroborates the theory of <ref type="bibr" target="#b30">Xu et al. (2020)</ref>, who state that when the inductive bias is aligned with the reasoning algorithm (in this case, path tracing), the model learns to reason more easily and achieves better sample efficiency.</p><p>Prediction performance, scoring the DAG, <ref type="table" target="#tab_1">Table 2</ref>. On NA and BN, DAGNN also outperforms D-VAE, which in turn outperforms the other four baselines (among them, DeepGMG works the best on NA and S-VAE works the best on BN, consistent with the findings of .) While D-VAE demonstrates the benefit of incorporating the DAG bias, DAGNN proves the superiority of its architectural components, as will be further verified in the subsequent ablation study.</p><p>Time cost, <ref type="figure" target="#fig_3">Figure 3</ref>. The added expressivity of DAGNN comes with a tradeoff: the sequential processing of the topological batches requires more time than does the concurrent processing of all graph nodes, as in MPNNs. <ref type="figure" target="#fig_3">Figure 3</ref>  Ablation study, <ref type="table" target="#tab_2">Table 3</ref>. While the D-VAE encoder performs competitively owing similarly to the incorporation of the DAG bias, what distinguishes our proposal are several architecture components that gain further performance improvement. In <ref type="table" target="#tab_2">Table 3</ref>, we summarize results under the following cases: replacing attention in the aggregator by gated sum; reducing the multiple layers to one; replacing the GRUs by fully connected layers; modifying the readout by pooling over all nodes; and removing the edge attributes. One observes that replacing attention generally leads to the highest degradation in performance, while modifying other components yields losses too. There are two exceptions. One occurs on LP-15, where gated-sum aggregation surprisingly outperforms attention by a tight margin, considering the standard deviation. The other occurs on the modification of the readout for the BN dataset. In this case, a Bayesian network factorizes the joint distribution of all variables (nodes) it includes. Even though the DAG structure characterizes the conditional independence of the variables, they play equal roles to the BIC score and thus it is possible that emphasis of the target nodes adversely affects the predictive performance. In this case, pooling over all nodes appears to correct the overemphasis.  Sensitivity analysis, <ref type="table" target="#tab_3">Table 4</ref> and <ref type="figure">Figure 4</ref>. It is well known that MPNNs often achieve best performance with a small number of layers, a curious behavior distinct from other neural networks. It is important to see if such a behavior extends to DAGNN. In <ref type="table" target="#tab_3">Table 4</ref>, we list the results for up to four layers. One observes that indeed the best performance occurs at either two or three layers. In other words, one layer is insufficient (as already demonstrated in the ablation study) and more than three layers offer no advantage. We further extend the experimentation on TOK-15 with additional layers and plot the results in <ref type="figure">Figure 4</ref>. The trend corroborates that the most significant improvement occurs when going beyond a single layer. It is also interesting to see that a single layer yields the highest variance subject to randomization.</p><p>Structure learning, <ref type="figure" target="#fig_6">Figure 5</ref>. For an application of DAGNN, we extend the use of the BN dataset to learn the Bayesian network for the Asia data. In particular, we take the Bayesian optimization approach and optimize the BIC score over the latent space of DAGs. We use the graphs in BN as pivots and encode every graph by using DAGNN. The optimization yields a DAG with BIC score ?11107.29 (see <ref type="figure" target="#fig_6">Figure 5</ref>). This DAG is almost the same as the ground truth (see <ref type="figure" target="#fig_0">Figure 2</ref> of <ref type="bibr" target="#b11">Lauritzen &amp; Spiegelhalter (1988)</ref>), except that it does not include the edge from "visit to Asia?" to "Tuberculosis?". It is interesting to note that the identified DAG has a higher BIC score than that of the ground truth, ?11109.74. Furthermore, the BIC score is also much higher than that found by using the D-VAE encoder, ?11125.75 . This encouraging result corroborates the superior encoding quality of DAGNN and the effective use of it in downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We have developed DAGNN, a GNN model for a special yet widely used class of graphs-DAGs. It incorporates the partial ordering entailed by DAGs as a strong inductive bias towards representation learning. With the blessing of this inductive bias, we demonstrate that DAGNNs outperform MPNNs on several representative datasets and tasks. Through ablation studies, we also show that the DAGNN model is well designed, with several components serving as crucial contributors to the performance gain over other models that also incorporate the DAG bias, notably, D-VAE. Furthermore, we theoretically study a batching technique that yields maximal parallel concurrency in processing DAGs and prove that DAGNN is permutation invariant and injective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS</head><p>Proof of Theorem 1. Let (v 1 , v 2 , . . . , v d ) be a longest path of the DAG. The number of batches must be at least d, because otherwise there exists a batch that contains at least two nodes on this path, violating Property (ii). On the other hand, given the partitioning, according to Property (iii), one may trace a directed path, one node from each batch, starting from the last one. The longest path must be at least that long. In other words, the number of batches must be at most the number of nodes on the longest path. Hence, these two numbers are equal. The consequence stated by the theorem straightforwardly follows.</p><p>Proof of Theorem 2. We first show that h v is invairant to the indexing of v by double induction on and v. The base case is = 1 and v ? B 0 . In this case,</p><formula xml:id="formula_10">m 1 v = G 1 (?, h 0 v ) = 0 is invairant to the indexing of v. Then, h 1 v = F 1 (h 0 v , m 1 v ) is, too.</formula><p>In the induction, if for all &lt; and all v , and for = and v ? B 0 ? ? ? ? ? B i?1 , h v is invairant to the indexing of v , then for = and v ? B i , Proof of Corollary 3. The function G is invariant to node indexing because it is a weighted sum of the elements in its first argument, {h u }, whereas the weights are parameterized by using the same parameter w 2 for these elements.</p><formula xml:id="formula_11">m v = G ({h u | u ? P(v)}, h ?1 v ) and h v = F (h v ,<label>m</label></formula><p>The function F is invariant to node indexing because its two arguments are clearly distinguished.</p><p>The function R is invariant to node indexing because the FC layer applies to the pooling result of h v for a fixed set of v.</p><p>Proof of Theorem 4. Suppose two graphs G and G have the same representation h G = h G . Then, from the function R, they must have the same target set T and same node representations h v for all nodes v ? T and all layers . In particular, for the last layer = L, from the functions F L and G L , each of these nodes, v, from the two graphs must have the same set of direct predecessors P(v), each element u of which have the same representation h L u across graphs. By backward induction, the two graphs must have the same node set V and edge set E. Moreover, for each node v ? V, the last-layer representation h L v must be the same. Furthermore, from the injection property of F , if a node v shares the same node representation h v across graphs, then its past-layer representation h ?1 v must also be the same across graphs. A backward reduction traces back to the initial representation h 0 v , which concludes that the two graphs must have the same set of input node features X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET DETAILS</head><p>OGBG-CODE. The OGBG-CODE dataset was recently included in the Open Graph Benchmark (OGB) <ref type="bibr">(Hu et al., 2020, Section 6.3)</ref>. It contains 452,741 Python method definitions extracted from thousands of popular Github repositories. The method definitions are represented as DAGs by augmenting the abstract syntax trees with edges connecting the sequence of source code tokens. Hence, there are two types of edges. The min/avg/max numbers of nodes in the graphs are 11/125/36123, respectively. We use the node features provided by the dataset, including node type, attributes, depth in the AST, and pre-order traversal index.</p><p>The task suggested by <ref type="bibr" target="#b7">Hu et al. (2020)</ref> is to predict the sub-tokens forming the method name, also known as "code summarization". The task is considered a proxy measure of how well a model captures the code semantics <ref type="bibr" target="#b0">(Allamanis et al., 2018)</ref>. We additionally consider the task of predicting the length of the longest path in the graph. We treat it as a 275-way classification because the maximum length is 275. The distribution of the lengths/classes is shown in Appendix E. To avoid triviality, for this task we remove the AST depth from the node feature set.</p><p>We adopt OGB's project split, whose training set consists of Github projects not seen in the validation and test sets. We also experiment with a subset of the data, OGBG-CODE-15, which contains only randomly chosen 15% of the OGBG-CODE training data. Validation and test sets remain the same.</p><p>In addition to OGBG-CODE, we further experiment with two DAG datasets, NA and BN, used by  for evaluating their model D-VAE. To compare with the results reported in the referenced work, we focus on the predictive performance of the latent representations of the DAGs obtained from autoencoders. We adopt the given 90/10 splits.</p><p>Neural architectures (NA). This dataset is created in the context of neural architecture search. It contains 19,020 neural architectures generated from the ENAS software <ref type="bibr" target="#b17">(Pham et al., 2018)</ref>. Each neural architecture has 6 layers (i.e., nodes) sampled from 6 different types of components, plus an input and output layer. The input node vectors are one-hot encodings of the component types. The weight-sharing accuracy <ref type="bibr" target="#b17">(Pham et al., 2018</ref>) (a proxy of the true accuracy) on CIFAR-10 (Krizhevsky, 2009) is taken as performance measure. Details about the generation process can be found in <ref type="bibr">Zhang et al. (2019, Appendix H)</ref>.</p><p>Bayesian networks (BN). This dataset contains 200,000 random 8-node Bayesian networks generated by using the R package bnlearn <ref type="bibr" target="#b21">(Scutari, 2010)</ref>. The Bayesian Information Criterion (BIC) score is used to measure how well the DAG structure fits the Asia dataset <ref type="bibr" target="#b11">(Lauritzen &amp; Spiegelhalter, 1988)</ref>. The input node vectors are one-hot encodings of the node indices according to topological sort. See <ref type="bibr">Zhang et al. (2019, Appendix I)</ref> for further details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C BASELINE DETAILS</head><p>Baselines for OGBG-CODE. We use three basic measures to set up baseline performance, two for token prediction and one for the longest path task. (1) Node2Token: This method uses the attribute of the second node of the graph as prediction. We observe that the second node either contains the function name, if the token occurs in the vocabulary (which is not always the case because some function names consist of multiple words), or contains "None". (2) TargetInGraph: This method pretends that it knows the ground-truth tokens but predicts only those occurring in the graph. One would expect that a learning model may be able to outperform this method if it learns the associations of tokens outside the current graph. (3) MajorityInValid: This method always predicts the majority length seen in the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D MODEL CONFIGURATIONS AND TRAINING D.1 EXPERIMENT PROTOCOL AND HYPERPARAMETER TUNING</head><p>Our evaluation protocols and procedures closely follow those of <ref type="bibr" target="#b7">Hu et al. (2020)</ref>; . For OGBG-CODE, we only changed the following. We used 5-fold cross validation due to the size of the dataset and the number of baselines for comparison. Since we compared with vast kinds of models in addition to the OGB baselines, we swept over a large range of learning rates and, for each model, picked the best from the set {1e-4, 5e-4, 1e-3, 15e-4, 2e-3, 5e-3, 1e-2, 15e-3} based on performance on OGBG-CODE-15. We stopped training when the validation metric did not improve further under a patience of 20 epochs, for all models but D-VAE and DAGNN. For the latter two, we used a patience of 10. Moreover, for these two models we used gradient clipping (at 0.25) due to the recurrent layers and a batch size of 80. Note that OGB uses 10-fold cross validation with a fixed learning rate of 1e-3, a fixed epoch number 30, and a batch size 128.</p><p>For NA and BN, we followed the exact training settings of <ref type="bibr">Zhang et al. (2019, Appendix K)</ref>. For DAGNN, we started the learning rate scheduler at 1e-3 (instead of 1e-4) and stopped at a maximum number of epochs, 100 for NA and 50 for BN (instead of 300 and 100, respectively). We also trained a sparse Gaussian process (SGP) <ref type="bibr" target="#b23">(Snelson &amp; Ghahramani, 2005)</ref> as the predictive model, as described in <ref type="bibr">Zhang et al. (2019, Appendix L)</ref>, to evaluate the performance of the latent representations. The prediction results were averaged over 10 folds.</p><p>For the Bayesian network learning experiment we similarly took over the settings of , running ten rounds of Bayesian optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 BASELINE MODELS</head><p>All models were implemented in PyTorch <ref type="bibr" target="#b16">(Paszke et al., 2019)</ref>. For OGBG-CODE, we used the GCN and GIN models provided by the benchmark. We implemented a GAT model as described in  and GG-NN in <ref type="bibr" target="#b13">Li et al. (2016)</ref>. We used the SAGPool implementation of <ref type="bibr" target="#b12">Lee et al. (2019)</ref> and ASAP from the Pytorch Geometric Benchmark Suite https://github.com/ rusty1s/pytorch_geometric/tree/master/benchmark. All these models were implemented using PyTorch Geometric <ref type="bibr" target="#b5">(Fey &amp; Lenssen, 2019)</ref>. We used the parameters suggested in OGB (e.g., 5 GNN layers, with embedding and hidden dimension 300), with the exception of ASAP where we used 3 instead of 5 layers due to memory constraints.</p><p>Since the D-VAE implementation does not support topological batching as we do, and also because of other miscellaneous restrictions (e.g., a single source node and target node), we reimplement D-VAE by using our DAGNN codebase. The reimplementation reproduces the results reported by . See Appendix F for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 DAGNN IMPLEMENTATION</head><p>For DAGNN, we used hidden dimension 300. As suggested by OGB, we used independent linear classifiers to predict sub-tokens at each position of the sub-token sequence. Similarly, we used a linear classifier to predict the length of the longest path.</p><p>For the NA and BN datasets, we took the baseline implementations as well as training and evaluation procedures from . In particular, we used the corresponding configuration of D-VAE for the BN dataset. For DAGNN, we used the same hidden dimension 501 and adapted the decoder of D-VAE (by replacing the use of D-VAE encoder in part of the decoding process with our encoder). Additionally, we used bidirectional processing for token prediction over OGBG-CODE and for the experiment over BN. Since it did not offer improvement in performance for the longest path length prediction and for the experiment over NA but consumed too much time, for these cases we used unidirectional processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DETAILS ON THE LONGEST PATH EXPERIMENT</head><p>We observe that for the MPNN baselines, the longest path results shown in <ref type="table" target="#tab_0">Table 1</ref> are much worse on the 15% subset than on the full dataset. We speculate whether the poorer performance is caused <ref type="figure">Figure 6</ref>: Distribution of the longest path lengths, for OGBG-CODE (left) and OGBG-CODE-15 (right). To improve readability, we ignored a tiny amount of graphs whose longest path length &gt; 30. There are 58 such graphs in OGBG-CODE and 21 in OGBG-CODE-15.</p><p>by purely the size of training data, or additionally by the discrepancy of data distributions. <ref type="figure">Figure 6</ref> shows that the data distributions are rather similar. Hence, we conclude that the degrading performance of MPNNs on a smaller training set is due to their low sample efficiency, in contrast to DAG architectures (D-VAE and DAGNN) that perform similarly on both the full set and the subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F REIMPLEMENTATION OF D-VAE</head><p>The original D-VAE implementation processes nodes sequentially and thus is time consuming. Therefore, we reimplement D-VAE by using our DAGNN codebase, in particular supporting topological batching. <ref type="table" target="#tab_4">Table 5</ref> shows that our reimplementation reproduces closely the results obtained by the original D-VAE implementation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G ADDITIONAL ABLATION RESULTS</head><p>As mentioend in the main text, bidirectional processing is optional; it does not necessarily improve over unidirectional. Indeed, <ref type="table" target="#tab_5">Table 6</ref> shows that bidirectional works better on TOK-15 and BN, but unidirectional works better on LP-15 and NA. However, either way, DAGNN outperforms all baselines reported in <ref type="table" target="#tab_0">Table 1</ref> and 2, with only one exception: on LP-15, D-VAE performs worse than unidirectional but better than bidirectional. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Topological batching. Left: for the original graph G; right: for the reverse graph G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc><ref type="bibr" target="#b13">Li et al., 2016)</ref>; two hierarchical pooling approaches using attention: SAGPool<ref type="bibr" target="#b12">(Lee et al., 2019)</ref>, ASAP; and the D-VAE encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>compare to D-VAE and all the autoencoders compared therein: S-VAE (Bowman et al., 2016), GraphRNN (You et al., 2018), GCN (Zhang et al., 2019), and DeepGMG (Li et al., 2018).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Average training time per epoch, on logarithmic scale. Standard deviation is negligible.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>shows that such a trade-off is innate to DAG architectures, including the D-VAE encoder. Moreover, the figure shows that, when used as a component of a larger architecture (autoencoder), the overhead of DAGNN may not be essential. For example, in this particular experiment, DeepGMG (paired with the S-VAE encoder) takes an order of magnitude more time than does DAGNN (paired with the D-VAE decoder). Most importantly, not reflected in the figure is that DAGNN learns better and faster at larger learning rates, leading to fewer learning epochs. For example, DAGNN reaches the best performance at epoch 45, while D-VAE at around 200.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Figure 4: Extending Table 4 with further layers on TOK-15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>The Bayesian network identified by using Bayesian optimization over the latent space encoded by DAGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>v ) are both invairant to the indexing of v. Thus, by induction, for all = and all v, h v is invairant to the indexing of v. Then, by an outer induction, for all and all v, h v is invairant to the indexing of v. Therefore, h G = R({h v , = 0, 1, . . . , L, v ? T }) is invairant to the indexing of the nodes in T and thus of the entire node set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Prediction performance on the full dataset OGBG-CODE and a 15% subset OGBG-CODE-15 for two tasks: TOK and LP. Best results are boldfaced and second best are underlined.</figDesc><table><row><cell></cell><cell>TOK</cell><cell>TOK-15</cell><cell>LP</cell><cell>LP-15</cell></row><row><cell>Model</cell><cell>F1 ?</cell><cell>F1 ?</cell><cell>Acc ?</cell><cell>Acc ?</cell></row><row><cell>Node2Token</cell><cell cols="2">13.04?0.00 13.04?0.00</cell><cell>-</cell><cell>-</cell></row><row><cell>TargetInGraph</cell><cell cols="2">27.32?0.00 27.08?0.00</cell><cell>-</cell><cell>-</cell></row><row><cell>MajorityInValid</cell><cell>-</cell><cell>-</cell><cell>22.66?0.00</cell><cell>22.66?0.00</cell></row><row><cell>GCN</cell><cell cols="2">31.63?0.18 24.39?0.40</cell><cell>95.55?0.62</cell><cell>90.66?2.00</cell></row><row><cell>GCN-VN</cell><cell cols="2">32.63?0.13 24.44?0.25</cell><cell>96.62?0.44</cell><cell>92.87?1.19</cell></row><row><cell>GIN</cell><cell cols="2">31.63?0.20 21.49?0.61</cell><cell>98.36?0.32</cell><cell>92.53?2.30</cell></row><row><cell>GIN-VN</cell><cell cols="2">32.04?0.18 21.10?0.61</cell><cell>98.60?0.23</cell><cell>93.27?2.53</cell></row><row><cell>GAT</cell><cell cols="2">33.59?0.32 27.37?0.16</cell><cell>93.71?0.24</cell><cell>83.15?1.34</cell></row><row><cell>GG-NN</cell><cell cols="2">28.04?0.27 23.15?0.49</cell><cell>96.48?0.27</cell><cell>89.16?2.31</cell></row><row><cell>SAGPool</cell><cell cols="2">31.88?0.39 24.45?0.77</cell><cell>72.68?14.29</cell><cell>60.66?11.42</cell></row><row><cell>ASAP</cell><cell cols="2">28.30?0.72 25.06?0.37</cell><cell>87.84?2.77</cell><cell>71.56?3.76</cell></row><row><cell>D-VAE</cell><cell cols="2">32.64?0.17 27.08?0.39</cell><cell>99.90?0.02</cell><cell>99.78?0.01</cell></row><row><cell>DAGNN</cell><cell cols="2">34.41?0.38 29.11?0.44</cell><cell>99.93?0.01</cell><cell>99.86?0.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Predictive performance of latent representations for datasets NA and BN.</figDesc><table><row><cell></cell><cell></cell><cell>NA</cell><cell></cell><cell>BN</cell></row><row><cell>Model</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell></row><row><cell>S-VAE</cell><cell>0.521?0.002</cell><cell>0.847?0.001</cell><cell>0.499?0.006</cell><cell>0.873?0.002</cell></row><row><cell cols="2">GraphRNN 0.579?0.002</cell><cell>0.807?0.001</cell><cell>0.779?0.007</cell><cell>0.634?0.002</cell></row><row><cell>GCN</cell><cell>0.482?0.003</cell><cell>0.871?0.001</cell><cell>0.599?0.006</cell><cell>0.809?0.002</cell></row><row><cell cols="2">DeepGMG 0.478?0.002</cell><cell>0.873?0.001</cell><cell>0.843?0.007</cell><cell>0.555?0.003</cell></row><row><cell>D-VAE</cell><cell>0.375?0.003</cell><cell>0.924?0.001</cell><cell>0.281?0.004</cell><cell>0.964?0.001</cell></row><row><cell>DAGNN</cell><cell>0.264?0.004</cell><cell>0.964?0.001</cell><cell>0.122?0.004</cell><cell>0.993?0.000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation results.</figDesc><table><row><cell></cell><cell>TOK-15</cell><cell>LP-15</cell><cell></cell><cell>NA</cell><cell></cell><cell>BN</cell></row><row><cell>Configuration</cell><cell>F1 ?</cell><cell>Acc ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell></row><row><cell>DAGNN</cell><cell cols="3">29.11?0.44 99.86?0.04 0.264?0.004</cell><cell>0.964?0.001</cell><cell>0.122?0.004</cell><cell>0.993?0.000</cell></row><row><cell cols="4">Gated-sum aggr. 24.98?0.45 99.88?0.02 0.451?0.002</cell><cell>0.887?0.001</cell><cell>0.486?0.005</cell><cell>0.878?0.001</cell></row><row><cell>Single layer</cell><cell cols="3">28.39?0.80 99.74?0.10 0.277?0.003</cell><cell>0.960?0.001</cell><cell>0.324?0.008</cell><cell>0.950?0.001</cell></row><row><cell>FC layer</cell><cell cols="3">26.08?0.80 99.85?0.02 0.280?0.004</cell><cell>0.959?0.001</cell><cell>0.362?0.002</cell><cell>0.934?0.001</cell></row><row><cell>Pool all nodes</cell><cell cols="3">28.40?0.08 99.78?0.05 0.302?0.002</cell><cell>0.952?0.001</cell><cell>0.098?0.003</cell><cell>0.996?0.001</cell></row><row><cell>W/o edge attr.</cell><cell cols="2">28.85?0.24 99.82?0.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>DAGNN results for different numbers of layers.</figDesc><table><row><cell></cell><cell>TOK-15</cell><cell>LP-15</cell><cell></cell><cell>NA</cell><cell></cell><cell>BN</cell></row><row><cell># Layers</cell><cell>F1 ?</cell><cell>Acc ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell></row><row><cell>1</cell><cell cols="3">28.39?0.80 99.74?0.10 0.277?0.003</cell><cell>0.960?0.001</cell><cell>0.324?0.008</cell><cell>0.950?0.001</cell></row><row><cell>2</cell><cell cols="3">29.11?0.44 99.86?0.04 0.264?0.004</cell><cell>0.964?0.001</cell><cell>0.122?0.004</cell><cell>0.993?0.000</cell></row><row><cell>3</cell><cell cols="3">28.96?0.27 99.81?0.06 0.260?0.004</cell><cell>0.965?0.001</cell><cell>0.129?0.011</cell><cell>0.993?0.001</cell></row><row><cell>4</cell><cell cols="3">28.91?0.43 99.78?0.04 0.265?0.004</cell><cell>0.963?0.001</cell><cell>0.129?0.014</cell><cell>0.993?0.002</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Predictive performance of latent DAG representations for NA and BN. Comparison of the original implementation and our reimplementation. VAE (orig) 0.375?0.003 0.924?0.001 0.281?0.004 0.964?0.001 D-VAE (ours) 0.375?0.004 0.925?0.001 0.219?0.003 0.977?0.000</figDesc><table><row><cell></cell><cell></cell><cell>NA</cell><cell></cell><cell>BN</cell></row><row><cell>Model</cell><cell>RMSE</cell><cell>Pearson's r</cell><cell>RMSE</cell><cell>Pearson's r</cell></row><row><cell>D-</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Bidirectional vs. unidirectional processing.</figDesc><table><row><cell></cell><cell>TOK-15</cell><cell>LP-15</cell><cell></cell><cell>NA</cell><cell></cell><cell>BN</cell></row><row><cell>Bidirectional?</cell><cell>F1 ?</cell><cell>Acc ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell><cell>RMSE ?</cell><cell>Pearson's r ?</cell></row><row><cell>No</cell><cell cols="3">28.44?0.19 99.85?0.02 0.264?0.004</cell><cell>0.964?0.001</cell><cell>0.146?0.035</cell><cell>0.992?0.001</cell></row><row><cell>Yes</cell><cell cols="3">29.11?0.44 99.50?0.22 0.324?0.003</cell><cell>0.945?0.001</cell><cell>0.122?0.004</cell><cell>0.993?0.000</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published as a conference paper at ICLR 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See also an earlier implementation in https://github.com/unbounce/pytorch-tree-lstm</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is supported in part by DOE Award DE-OE0000910. Most experiments were conducted on the Satori cluster (satori.mit.edu).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A survey of machine learning for big code and naturalness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Earl</forename><forename type="middle">T</forename><surname>Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Premkumar</forename><forename type="middle">T</forename><surname>Devanbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<idno>81:1-81:37</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Relational inductive biases, deep learning, and graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vin?cius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Flores Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Francis</forename><surname>G?l?ehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelsey</forename><forename type="middle">R</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nash</surname></persName>
		</author>
		<idno>abs/1806.01261</idno>
		<ptr target="http://arxiv.org/abs/1806.01261" />
		<editor>Pushmeet Kohli, Matthew Botvinick, Oriol Vinyals, Yujia Li, and Razvan Pascanu</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<pubPlace>Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>J?zefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Computational Natural Language Learning, CoNLL</title>
		<editor>Yoav Goldberg and Stefan Riezler</editor>
		<meeting>of Conference on Computational Natural Language Learning, CoNLL</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Kenneth Forbus, and Achille Fokoue. Improving graph neural network representations of logical formulae with subgraph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronika</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Chain based RNN for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javid</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT</title>
		<meeting>of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1244" to="1249" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<meeting>of ICLR Workshop on Representation Learning on Graphs and Manifolds</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning, ICML</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>of International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno>abs/2005.00687</idno>
		<ptr target="https://arxiv.org/abs/2005.00687" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Easy-first dependency parsing with hierarchical tree lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliyahu</forename><surname>Kiperwasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/798" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="445" to="461" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with graph convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations, ICLR</title>
		<meeting>of International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Local computations with probabilities on graphical structures and their application to expert systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Spiegelhalter</surname></persName>
		</author>
		<idno>00359246</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="224" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning, ICML</title>
		<meeting>of International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="3734" to="3743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations, ICLR</title>
		<editor>Yoshua Bengio and Yann LeCun</editor>
		<meeting>of International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>abs/1803.03324</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Online planner selection with graph neural networks and adaptive scheduling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Ferber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Thirty-Fourth Conference on Artificial Intelligence, AAAI</title>
		<meeting>of Thirty-Fourth Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Advances in Neural Information Processing Systems</title>
		<meeting>of Advances in Neural Information essing Systems<address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning, ICML</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>of International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ASAP: adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><forename type="middle">P</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of The Thirty-Fourth Conference on Artificial Intelligence, AAAI</title>
		<meeting>of The Thirty-Fourth Conference on Artificial Intelligence, AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to simulate complex physics with graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Godwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<idno>abs/2002.09405</idno>
		<ptr target="https://arxiv.org/abs/2002.09405" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">W</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning bayesian networks with the bnlearn r package</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Scutari</surname></persName>
		</author>
		<idno>1548-7660</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>Articles</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dag-recurrent neural networks for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<meeting>of Conference on Computer Vision and Pattern Recognition, CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3620" to="3629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sparse gaussian processes using pseudo-inputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Snelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing, NIPS</title>
		<meeting>Advances in Neural Information essing, NIPS</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Chiung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning, ICML</title>
		<meeting>of International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semantic compositionality through recursive matrix-vector spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>of Joint Conference on Empirical Methods in Natural Language essing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>EMNLP-CoNLL</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1201" to="1211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>of Conference on Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai Sheng</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL</title>
		<meeting>of Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language essing of the Asian Federation of Natural Language essing, ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1556" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations</title>
		<meeting>of International Conference on Learning Representations</meeting>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations, ICLR</title>
		<meeting>of International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What can neural networks reason about?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><forename type="middle">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Learning Representations, ICLR</title>
		<meeting>of International Conference on Learning Representations, ICLR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep auto-regressive models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning, ICML</title>
		<editor>Jennifer G. Dy and Andreas Krause</editor>
		<meeting>of International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="5694" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">D-VAE: A variational autoencoder for directed acyclic graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shali</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Annual Conference on Neural Information Processing Systems</title>
		<meeting>of Annual Conference on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1586" to="1598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Top-down tree long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT</title>
		<editor>Kevin Knight, Ani Nenkova, and Owen Rambow</editor>
		<meeting>of Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="310" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Long short-term memory over recursive structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Dan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parinaz</forename><surname>Sobhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on Machine Learning, ICML</title>
		<meeting>of International Conference on Machine Learning, ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1604" to="1612" />
		</imprint>
	</monogr>
	<note>Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling polypharmacy side effects with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="457" to="466" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The latter two are extensions of the first two by including a virtual node (i.e., an additional node that is connected to all nodes in the graph)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gin</forename><surname>Gcn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gin-Vn ;</forename><surname>Gcn-Vn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Additionally, we compare with multiple GNN models. Some of them are the GNN implementations offered by OGB</title>
		<imprint>
			<publisher>Kipf &amp; Welling</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Note that the implementations do not strictly follow the architectures described in the original papers. In particular, edge types are incorporated and inverse edges are added for bidirectional message passing</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">2016) for comparison. We also include two representative hierarchical pooling approaches, which use attention to determine node pooling: SAGPool (Lee et al., 2019) and ASAP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Veli?kovi?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Since our model features attention mechanisms, we include GAT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Lastly, we compare with the encoder of D-VAE (Zhang et al., 2019, Appendix E, F)</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">2016) applies a standard GRU-based RNN variational autoencoder to the topologically sorted node sequence, with node features augmented by the information of incoming edges, and decodes the graph by generating an adjacency matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Baselines For</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Bn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>2018) similarly uses a GNN-based encoder but employs its own decoder. which is similar to the one in D-VAE). Note that all these baselines are autoencoders and our objective is to compare the performance of the latent representations</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
