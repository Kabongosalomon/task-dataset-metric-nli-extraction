<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving the Gating Mechanism of Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Paine</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Hoffman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
						</author>
						<title level="a" type="main">Improving the Gating Mechanism of Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recurrent neural networks (RNNs) are an established machine learning tool for learning from sequential data. However, RNNs are prone to the vanishing gradient problem, which occurs when the gradients of the recurrent weights become vanishingly small as they get backpropagated through time <ref type="bibr" target="#b17">(Hochreiter, 1991;</ref><ref type="bibr" target="#b4">Bengio et al., 1994;</ref><ref type="bibr" target="#b19">Hochreiter et al., 2001)</ref>. A common approach to alleviate the vanishing gradient problem is to use gating mechanisms, leading to models such as the long short term memory <ref type="bibr">(Hochreiter &amp; Schmidhuber, 1997, LSTM)</ref> and gated recurrent units 1 Stanford University, USA 2 DeepMind, London, UK. Correspondence to: Albert Gu &lt;albertgu@stanford.edu&gt;, Caglar Gulcehre &lt;caglarg@google.com&gt;.</p><p>Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 108, 2020. Copyright 2020 by the author(s). <ref type="bibr">(Chung et al., 2014, GRUs)</ref>. These gated RNNs have been very successful in several different application areas such as in reinforcement learning <ref type="bibr" target="#b23">(Kapturowski et al., 2018;</ref><ref type="bibr" target="#b11">Espeholt et al., 2018)</ref> and natural language processing <ref type="bibr" target="#b25">Ko?isk? et al., 2018)</ref>.</p><p>At every time step, gated recurrent networks use a weighted combination of the history summarized by the previous state, and a function of the incoming inputs, to create the next state. The values of the gates, which are the coefficients of the weighted combination, control the length of temporal dependencies that can be addressed. This weighted update can be seen as an additive or residual connection on the recurrent state, which helps signals propagate through time without vanishing. However, the gates themselves are prone to a saturating property which can also hamper gradient-based learning. This can be problematic for RNNs, where carrying information for very long time delays requires gates to be very close to their saturated states.</p><p>We address two particular problems that arise with the standard gating mechanism of recurrent models. Firstly, learning when gates are in their saturation regime is difficult because gradients through the gates vanish as they saturate. We derive a modification to standard gating mechanisms that uses an auxiliary refine gate (Section 3.1) to modulate a main gate. This mechanism allows the gates to have a wider range of activations without gradients vanishing as quickly. Secondly, typical initialization of the gates is relatively concentrated. This restricts the range of timescales the model can address at initialization, as the timescale of a particular unit is dictated by its gates. We propose uniform gate initialization (Section 3.2) that addresses this problem by directly initializing the activations of these gates from a distribution that captures a wider spread of dependency lengths.</p><p>The main contribution of this paper is the refine gate mechanism. As the refine gate works better in tandem with uniform gate initialization, we call this combination the UR gating mechanism. We focus on comparing the UR gating mechanism against other approaches in our experiments. These changes can be applied to any gate (i.e. parameterized bounded function) and have minimal to no overhead in terms of speed, memory, code complexity, parameters, or hyper parameters. We apply them to the forget gate of recurrent models, and evaluate on several benchmark tasks that re-arXiv:1910.09890v2 [cs.NE] 18 Jun 2020 quire long-term memory including synthetic memory tasks, pixel-by-pixel image classification, language modeling, and reinforcement learning. Finally, we connect our methods to other proposed gating modifications, introduce a framework that allows each component to be replaced with similar ones, and perform extensive ablations of our method. Empirically, the UR gating mechanism robustly improves on the standard forget and input gates of gated recurrent models. When applied to the LSTM, these simple modifications solve synthetic memory tasks that are pathologically difficult for the standard LSTM, achieve state-of-the-art results on sequential MNIST and CIFAR-10, and show consistent improvements in language modeling on the WikiText-103 dataset <ref type="bibr" target="#b34">(Merity et al., 2016)</ref> and reinforcement learning tasks .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Gated Recurrent Neural Networks</head><p>Broadly speaking, RNNs are used to sweep over a sequence of input data x t to produce a sequence of recurrent states h t ? R d summarizing information seen so far. At a high level, an RNN is just a parametrized function in which each sequential application of the network computes a state update u : (x t ,h t?1 ) ? h t . Gating mechanisms were introduced to address the vanishing gradient problem <ref type="bibr" target="#b17">(Hochreiter, 1991;</ref><ref type="bibr" target="#b4">Bengio et al., 1994;</ref><ref type="bibr" target="#b19">Hochreiter et al., 2001)</ref>, and have proven to be crucial to the success of RNNs. This mechanism essentially smooths out the update using the following equation,</p><formula xml:id="formula_0">h t = f t (x t ,h t?1 )?h t?1 +i t (x t ,h t?1 )?u(x t ,h t?1 ), (1)</formula><p>where the forget gate f t and input gate i t are [0,1] d -valued functions that control how fast information is forgotten or allowed into the memory state. When the gates are tied, i.e. f t +i t = 1 as in GRUs, they behave as a low-pass filter, deciding the time-scale on which the unit will respond <ref type="bibr" target="#b42">(Tallec &amp; Ollivier, 2018)</ref>. For example, large forget gate activations close to f t = 1 are necessary for recurrent models to address long-term dependencies. <ref type="bibr">1</ref> We will introduce our improvements to the gating mechanism primarily in the context of the LSTM, which is the most popular recurrent model.</p><formula xml:id="formula_1">f t = ?(P f (x t , h t?1 )),</formula><p>(2) i t = ?(P i (x t , h t?1 )),</p><p>(3) u t = tanh(P u (x t , h t?1 )), (4) c t = f t ?c t?1 +i t ?u t ,</p><p>(5) o t = ?(P o (x t , h t?1 )), (6) h t = o t tanh(c t ).</p><p>A typical LSTM (equations (2)- <ref type="formula" target="#formula_2">(7)</ref>) is an RNN whose state is represented by a tuple (h t , c t ) consisting of a "hidden" <ref type="figure">Figure 1</ref>. Refine mechanism. The refine mechanism improves flow of gradients through a saturating gate f . As f saturates, its gradient vanishes, and its value is unlikely to change (see <ref type="figure">Figure 4</ref>). The refine gate r is used to produce a bounded additive term ? that may push f lower or higher as necessary. The resulting effective gate g can achieve values closer to 0 and 1 and can change even when f is stuck. We apply it to the forget gate of an LSTM. The g is then used in place of f in the state update (5).</p><p>state and "cell" state. The state update equation <ref type="formula">(1)</ref> is used to create the next cell state c t (5). Note that the gate and update activations are a function of the previous hidden state h t?1 instead of c t?1 . Here, P stands for a parameterized linear function of its inputs with bias b , e.g.</p><formula xml:id="formula_3">P f (x t ,h t?1 ) = W f x x t +W f h h t?1 +b f .<label>(8)</label></formula><p>and ?(?) refers to the standard sigmoid activation function which we will assume is used for defining [0,1]-valued activations in the rest of this paper. The gates of the LSTM were initially motivated as a binary mechanism, switching on or off, allowing information and gradients to pass through. However, in reality, this fails to happen due to a combination of two factors: initialization and saturation. This can be problematic, such as when very long dependencies are present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed Gating Mechanisms</head><p>We present two solutions that work in tandem to address the previously described issues. The first is the refine gate, which allows for better gradient flow by reparameterizing a saturating gate, for example, the forget gate. The second is uniform gate initialization, which ensures a diverse range of gate values are captured at the start of training, which allows a recurrent model to have a multi-scale representation of an input sequence at initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Refine Gate</head><p>Formally, the full mechanism of the refine gate as applied to gated recurrent models is defined in equations (9)-(11). Note that it is an isolated change where the forget gate f t is modified to get the effective forget gate in (10) before applying the the standard update (1). <ref type="figure">Figure 1</ref> illustrates the refine gate in an LSTM cell. <ref type="figure" target="#fig_0">Figure 3</ref> illustrates how the refine gate r t is defined and how it changes the forget gate f t to produce an effective gate g t . The refine gate allows the effective gate g to reach much higher and lower activations than the constituent gates f and r, bypassing the saturating gradient problem. For example, this allows the effective forget gate to reach g = 0.99 when the forget gate is only f = 0.9.</p><p>Finally, to simplify comparisons and ensure that we always use the same number of parameters as the standard gates, when using the refine gate we tie the input gate to the effective forget gate, i t = 1 ? g t . 2 However, we emphasize that these techniques can be applied to any gate (or more broadly, any bounded function) to improve initialization distribution and help optimization. For example, our methods can be combined in different ways in recurrent models, e.g. an independent input gate can be modified with its own refine gate.</p><formula xml:id="formula_4">r t = ?(P r (x t , h t?1 )),<label>(9)</label></formula><formula xml:id="formula_5">g t = r t ?(1?(1?f t ) 2 )+(1?r t )?f 2 t , (10) c t = g t c t?1 +(1?g t )u t .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Uniform Gate Initialization</head><p>Standard initialization schemes for the gates can prevent the learning of long-term temporal correlations <ref type="bibr" target="#b42">(Tallec &amp; Ollivier, 2018)</ref>. For example, supposing that a unit in the cell state has constant forget gate value f t , then the contribution of an input x t in k time steps will decay by (f t ) k . This gives the unit an effective decay period or characteristic timescale of O( 1 1?ft ). 3 Standard initialization of linear layers L sets the bias term to 0, which causes the forget gate values (2) to concentrate around 0.5. A common trick of setting the forget gate bias to b f = 1.0 <ref type="bibr" target="#b22">(Jozefowicz et al., 2015)</ref> does increase the value of the decay period to 1 1??(1.0) ? 3.7. However, this is still relatively small and may hinder the model from learning dependencies at varying timescales easily.</p><p>We instead propose to directly control the distribution of forget gates, and hence the corresponding distribution of decay periods. In particular, we propose to simply initialize the value of the forget gate activations f t according to a uniform distribution U(0,1) 4 ,</p><formula xml:id="formula_6">b f ? ? ?1 (U[ ,1? ]).<label>(12)</label></formula><p>An important difference between UGI and standard or other (e.g. <ref type="bibr" target="#b42">Tallec &amp; Ollivier, 2018)</ref> initializations is that negative forget biases are allowed. input. Additionally, it introduces no additional parameters; it even can have less hyperparameters than the standard gate initialization, which sometimes tunes the forget bias b f <ref type="bibr" target="#b22">(Jozefowicz et al., 2015)</ref>. Appendix B.2 and B.3 further discuss the theoretical effects of UGI on timescales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The URLSTM</head><p>The URLSTM requires two small modifications to the vanilla LSTM. First, we present the way the biases of forget gates are initialized in Equation (12) with UGI. Second, the modifications on the standard LSTM equations to compute the refine and effective forget gates are presented in Equations (9)-(11). However, we note that these methods can be used to modify any gate (or more generally, bounded function) in any model. In this context, the URLSTM is simply defined by applying UGI and a refine gate r on the original forget gate f to create an effective forget gate g (Equation <ref type="formula">(10)</ref>). This effective gate is then used in the cell state update (11). Empirically, these small modifications to an LSTM are enough to allow it to achieve nearly binary activations and solve difficult memory problems ( <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">A Formal Treatment of Refine Gates</head><p>Given a gate f = ?(P f (x)) ? [0, 1], the refine gate is an independent gate r = ?(P r (x)) that modulates f to produce a value g ? [0,1] which will be used in place of f downstream. It is motivated by considering how to modify the output of a gate f in a way that promotes gradient-based learning, derived below.</p><p>An additive adjustment A root cause of the saturation problem is that the gradient ?f of a gate can be written solely as a function of the activation value as f (1 ? f ), decays rapidly as f approaches to 0 or 1. Thus when the activation f is past a certain upper or lower threshold, learning effectively stops. This problem cannot be fully addressed only by modifying the input of the sigmoid, as in UGI and other techniques, as the gradient will still vanish by backpropagating through the activation function.</p><p>Therefore to better control activations near the saturating regime, instead of changing the input to the sigmoid in f = ?(P(x)), we consider modifying the output. Modifying the gate with a multiplicative interaction can have unstable learning dynamics since when the gates have very small values, the multiplicative factor may need to be very large to avoid the gradients of the gates shrinking to zero. As a result, we consider adjusting f with an input-dependent additive update ?(f,x) for some function ?, to create an effective gate g = f +?(f,x) that will be used in place of f downstream such as in the main state update (1). This sort of additive ("residual") connection is a common technique to increase gradient flow, and indeed was the motivation of the LSTM additive gated update (1) itself <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997)</ref>.</p><p>Choosing the adjustment function ? Although there might be many choices that seem plausible for choosing an appropriate additive update ?, we first identify the desirable properties of such a function and then discuss how our refine gate mechanism satisfies those properties.</p><p>The desired properties of ? emerge considering the applications of the gating mechanisms in recurrent models:</p><p>? Boundedness: After the additive updates, the activations still need to be bounded between 0 and 1. ? Symmetricity: The resulting gating framework should be symmetric around 0, as sigmoid does. ? Smoothness: The refining mechanism should be differentiable, since we will be using backpropagation and gradient based optimization methods.</p><p>Let us note that, f t may need to be either increased or decreased, regardless of what value it has. This is because the gradients through the gates can vanish either when the activations get closer to 0 or 1. Therefore, an additive update to f should create an effective gate activation g t in the range f t ?? for some ?. We assume that the allowed adjustment range, ? = ?(f t ), needs to be a function of f to keep the g between 0 and 1. Since 0 and 1 are symmetrical in the gating framework, our adjustment rate should also satisfy ?(f ) = ?(1?f ). <ref type="figure">Figure 2a</ref> illustrates the general appearance of ?(f ) based on aforementioned properties. According to the Boundedness property, the adjustment rate should be be upper-bounded by min(f,1?f ) to ensure that g ? f ??(f ) is bounded between 0 and 1. As a consequence of this property, its derivatives should also satisfy, ? (0) ? 1 and ? (1) ? ?1. Symetricity also implies ? (f ) = ?? (1?f ), and smoothness implies ? is continuous. The simplest such function satisfying all these properties is the linear ? (f ) = 1?2f , yielding to our choice of adjustment function,</p><formula xml:id="formula_7">?(f ) = f ?f 2 = f (1?f ).</formula><p>However, when f is bounded between 0 and 1, ?(f ) will be positive.</p><p>Recall that the goal is to produce an effective activation</p><formula xml:id="formula_8">g = f +?(f,x) such that g ? f ??(f ) (Figure 2b) given. Our final observation is that the simplest such function ? satis- fying this is ?(f,x) = ?(f )?(f,x) where ?(f,x) ? [?1,1]</formula><p>decides the sign of adjustment, and it can also change its magnitude as well. The standard method of defining [?1,1]-valued differentiable functions is achieved by using a tanh non-linearity, and this leads to ?(f,x) = ?(f )(2r?1) for another gate r = ?(P(x)). The full refine update equation can be given as in Equation <ref type="formula" target="#formula_9">(13)</ref>,</p><formula xml:id="formula_9">g = f +?(f )(2r?1) = f +f (1?f )(2r?1) = (1?r)?f 2 +r?(1?(1?f ) 2 )<label>(13)</label></formula><p>Equation <ref type="formula" target="#formula_9">(13)</ref> has the elegant interpretation that the gate r linearly interpolates between the lower band f ??(f ) = f 2 and the symmetric upper band f + ?(f ) = 1 ? (1 ? f ) 2 <ref type="figure">(Figure 2b</ref>). In other words, the original gate f is the coarse-grained determinant of the effective gate g, while the gate r "refines" it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Gating Mechanisms</head><p>We highlight a few recent works that also propose small gate changes to address problems of long-term or variable-length dependencies. Like ours, they can be applied to any gated update equation. <ref type="bibr" target="#b42">Tallec &amp; Ollivier (2018)</ref> suggest an initialization strategy to capture long-term dependencies on the order of T max , by sampling the gate biases from b f ? log U(1,T max ? 1). Although similar to UGI in definition, chrono initialization (CI) has critical differences in the timescales captured, for example, by using an explicit timescale parameter and having no negative biases. Due to its relation to UGI, we provide a more detailed comparison in Appendix B.3. As mentioned in Section 3.4, techniques such as these that only modify the input to a sigmoid gate do not adequately address the saturation problem.</p><p>The Ordered Neuron (ON) LSTM introduced by <ref type="bibr" target="#b40">(Shen et al., 2018)</ref> aims to induce an ordering over the units in the hidden states such that "higher-level" neurons retain information for longer and capture higher-level information.</p><p>We highlight this work due to its recent success in NLP, and also because its novelties can be factored into introducing two mechanisms which only affect the forget and input gates, namely (i) the cumax := cumsum ? softmax activation function which creates a monotonically increasing vector in [0,1], and (ii) a pair of "master gates" which are ordered by cumax and fine-tuned with another pair of gates.</p><p>We observe that these are related to our techniques in that one controls the distribution of a gate activation, and the other is an auxiliary gate with modulating behavior. Despite its important novelties, we find that the ON-LSTM has drawbacks, including speed and scaling issues of its gates. We provide the formal definition and detailed analysis of the ON-LSTM in Appendix B.4. For example, we comment on how UGI can also be motivated as a faster approximation of the cumax activation. We also flesh out a deeper relationship between the master and refine gates and show how they can be interchanged for each other.</p><p>We include a more thorough overview of other related works on RNNs in Appendix B.1. These methods are mostly orthogonal to the isolated gate changes considered here and are not analyzed. We note that an important drawback common to all other approaches is the introduction of substantial hyperparameters in the form of constants, training protocol, and significant architectural changes. For example, even for chrono initialization, one of the less intrusive proposals, we experimentally find it to be particularly sensitive to the hyperparameter T max (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Gate Ablations</head><p>Our insights about previous work with related gate components allow us to perform extensive ablations of our contributions. We observe two independent axes of variation, namely, activation function/initialization (cumax, constant bias sigmoid, CI, UGI) and auxiliary modulating gates (master, refine), where different components can be replaced with each other. Therefore we propose several other gate combinations to isolate the effects of different gating mechanisms. We summarize a few ablations here; precise details are given in Appendix B.5. O-: Ordered gates. A natural simplification of the main idea of ON-LSTM, while keeping the hierarchical <ref type="table">Table 1</ref>. Summary of gate ablations. Summary of gating mechanisms considered in this work as applied to the forget/input gates of recurrent models. Some of these ablations correspond to previous work. --standard LSTMs, C- <ref type="bibr" target="#b42">(Tallec &amp; Ollivier, 2018)</ref>, and OM <ref type="bibr" target="#b40">(Shen et al., 2018)</ref> Name Initialization/Activation Auxiliary Gate bias on the forget activations, is to simply drop the auxiliary master gates and define f t ,i t (2)-(3) using the cumax activation function. UM: UGI master gates. This variant of the ON-LSTM's gates ablates the cumax operation on the master gates, replacing it with a sigmoid activation and UGI which maintains the same initial distribution on the activation values. OR: Refine instead of master. A final variant in between the UR gates and the ON-LSTM's gates combines cumax with refine gates. In this formulation, as in UR gates, the refine gate modifies the forget gate and the input gate is tied to the effective forget gate. The forget gate is ordered using cumax. <ref type="table">Table 1</ref> summarizes the gating modifications we consider and their naming conventions. Note that we also denote the ON-LSTM method as OM for mnemonic ease. Finally, we remark that all methods here are controlled with the same number of parameters as the standard LSTM, aside from OM and UM which use an additional 1 2C -fraction parameters where C is the downsize factor on the master gates (Appendix B.4). C = 1 unless noted otherwise.</p><formula xml:id="formula_10">-- Standard initialization N/A C- Chrono initialization N/A O- cumax activation N/A U- Uniform initialization N/A -R Standard</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We first perform full ablations of the gating variants (Section 4.1) on two common benchmarks for testing memory models: synthetic memory tasks and pixel-by-pixel image classification tasks. We then evaluate our main method on important applications for recurrent models including language modeling and reinforcement learning, comparing against baselines from literature where appropriate.</p><p>The main claims we evaluate for each gating component are (i) the refine gate is more effective than alternatives (the master gate, or no auxiliary gate), and (ii) UGI is more effective than standard initialization for sigmoid gates. In particular, we expect the *R gate to be more effective than *M or *-for any primary gate *, and we expect U* to be better than -* and comparable to O* for any auxiliary gate *.</p><p>The standard LSTM (--) uses forget bias 1.0 (Section <ref type="figure">Figure 4</ref>. Performance on synthetic memory: Copy task using sequences of length 500. Several methods including standard gates fail to make any progress (overlapping flat curves at baseline). Note that methods that combine the refine gate with a range of gate values (OR, UR) perform best. But the refine gate on its own does not perform well. Adding task using sequences of length 2000. Most methods eventually make progress, but again methods that combine the refine gate with a range of gate values (OR, UR) perform best.</p><p>2.2). When chrono initialization is used and not explicitly tuned, we set T max to be proportional to the hidden size. This heuristic uses the intuition that if dependencies of length T exist, then so should dependencies of all lengths ? T . Moreover, the amount of information that can be remembered is proportional to the number of hidden units.</p><p>All of our benchmarks have prior work with recurrent baselines, from which we used the same models, protocol, and hyperparameters whenever possible, changing only the gating mechanism without doing any additional tuning for the refine gating mechanisms. Full protocols and details for all experiments are given in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Synthetic Memory Tasks</head><p>Our first set of experiments is on synthetic memory tasks <ref type="bibr" target="#b18">(Hochreiter &amp; Schmidhuber, 1997;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2016)</ref> that are known to be hard for standard LSTMs to solve. For these tasks, we used single layer models with 256 hidden units, trained using Adam with learning rate 10 ?3 .</p><p>Copy task. The input is a sequence of N +20 digits where the first 10 tokens (a 0 ,a 1 ,...,a 9 ) are randomly chosen from {1,...,8}, the middle N tokens are set to 0, and the last ten tokens are 9. The goal of the recurrent model is to output (a 0 ,...,a 9 ) in order on the last 10 time steps, whenever the <ref type="figure">Figure 5</ref>. Distribution of forget gate activations before and after training. For the Copy task. We show the distribution of activations ft for four methods: --cannot learn large enough ft and makes no progress on the task. C-initializes with extremal activations which barely change during training. U-makes progress by encouraging a range of forget gate values, but this distribution does not change significantly during training due to saturation. UR starts with the same distribution as U-but is able to learn extreme gate values, which allows it to access the distal inputs, as necessary for this task. Appendix E.1 shows a reverse task where UR is able to un-learn from a saturated regime. cue token 9 is presented. We trained our models using cross-entropy with baseline loss log(8) (Appendix D.1).</p><p>Adding task. The input consists of two sequences: 1. N numbers (a 0 ,...,a N ?1 ) sampled independently from U[0,1] 2. an index i 0 ? [0, N/2) and i 1 ? [N/2, N ), together encoded as a two-hot sequence. The target output is a i0 +a i1 and models are evaluated by the mean squared error with baseline loss 1/6. <ref type="figure">Figure 4</ref> shows the loss of various methods on the Copy and Adding tasks. The only gate combinations capable of solving Copy completely are OR, UR, O-, and C-. This confirms the mechanism of their gates: these are the only methods capable of producing high enough forget gate values either through the cumax non-linearity, the refine gate, or extremely high forget biases. U-is the only other method able to make progress, but converges slower as it suffers from gate saturation without the refine gate. --makes no progress. OM and UM also get stuck at the baseline loss, despite OM's cumax activation, which we hypothesize is due to the suboptimal magnitudes of the gates at initialization (Appendix B.4). On the Adding task, every method besides --is able to eventually solve it, with all refine gate variants fastest. <ref type="figure">Figure 5</ref> shows the distributions of forget gate activations of sigmoid-activation methods, before and after training on the Copy task. It shows that activations near 1.0 are important for a model's ability to make progress or solve this task, and that adding the refine gate makes this significantly easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Pixel-by-pixel Image Classification</head><p>These tasks involve feeding a recurrent model the pixels of an image in a scanline order before producing a classification label. We test on the sequential MNIST (sMNIST), permuted MNIST (pMNIST) <ref type="bibr" target="#b28">(Le et al., 2015)</ref>, and sequential CIFAR-10 (sCIFAR-10) tasks. Each LSTM method was ran with a learning rate sweep with 3 seeds each. We found that many methods were quite unstable, with multiple seeds diverging. <ref type="figure" target="#fig_1">Figure 6</ref> shows the accuracy curves of each method at their best stable learning rate. The basic LSTM is noticeably worse than all of the others. This suggests that any of the gate modifications, whether better initialization, cumax non-linearity, or master or refine gates, are better than standard gates especially when long-term dependencies are present. Additionally, the uniform gate initialization methods are generally better than the ordered and chrono initialization, and the refine gate performs better than the master gate. <ref type="table" target="#tab_2">Table 2</ref> compares the test accuracy of our main model against other models from the literature. In addition, we tried variants of GRUs and the addition of a generic regularization technique-we chose Zoneout <ref type="bibr" target="#b27">(Krueger et al., 2016)</ref> with default hyperparameters (z c = 0.5, z h = 0.05). This combination even outperformed non-recurrent models on sequential MNIST and CIFAR-10.</p><p>From Sections 5.1 and 5.2, we draw a few conclusions about the comparative performance of different gate modifications. First, the refine gate is consistently better than comparable master gates. C-solves the synthetic memory tasks but is worse than any other variant outside of those. We find ordered (cumax) gates to be effective, but speed issues prevent us from using them in more complicated tasks. UR gates are consistently among the best performing and most stable.  <ref type="bibr" target="#b6">(Chang et al., 2017)</ref> 99.0 94.6 -IndRNN <ref type="bibr" target="#b29">(Li et al., 2018a)</ref> 99.0 96.0 r-LSTM <ref type="bibr" target="#b43">(Trinh et al., 2018)</ref> 98  We consider word-level language modeling on the WikiText-103 dataset, where (i) the dependency lengths are much shorter than in the synthetic tasks, (ii) language has an implicit hierarchical structure and timescales of varying lengths. We evaluate our gate modifications against the exact hyperparameters of a SOTA LSTM-based baseline  without additional tuning (Appendix D). Additionally, we compare against ON-LSTM, which was designed for this domain <ref type="bibr" target="#b40">(Shen et al., 2018)</ref>, and chrono initialization, which addresses dependencies of a particular timescale as opposed to timescale-agnostic UGI methods. In addition to our default hyperparameter-free initialization, we tested models with the chrono hyperparameter T max manually set to 8 and 11, values previously used for language modeling to mimic fixed biases of about 1.0 and 2.0 respectively <ref type="bibr" target="#b42">(Tallec &amp; Ollivier, 2018)</ref>. <ref type="table" target="#tab_4">Table 3</ref> shows Validation and Test set perplexities for various models. We find that OM, U-, and UR improve over --with no additional tuning. However, although OM was designed to capture the hierarchical nature of language with the cumax activation, it does not perform better than U-and UR. Appendix D, <ref type="figure" target="#fig_3">Figure 11</ref> additionally shows validation perplexity curves, which indicate that UR overfits less than the other methods.</p><p>The chrono initialization using our aforementioned initial- <ref type="figure">Figure 7</ref>. Active match. <ref type="bibr" target="#b20">Hung et al. (2018)</ref>. The agent navigates a 3D world using observations from a first person camera. The task has three phases. In phase 1, the agent must search for a colored cue. In phase 2, the agent is exposed to apples which give distractor rewards. In phase 3, the agent must correctly recall the color of the cue and pick the sensor near the corresponding color to receive the task reward. An episode lasts between 450 and 600 steps, requiring long-term memory and credit assignment.</p><p>ization strategy makes biases far too large. While manually tweaking the T max hyperparameter helps, it is still far from any UGI-based methods. We attribute these observations to the nature of language having dependencies on multiple widely-varying timescales, and that UGI is enough to capture these without resorting to strictly enforced hierarchies such as in OM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Reinforcement Learning Memory Tasks</head><p>In most partially observable reinforcement learning (RL) tasks, the agent can observe only part of the environment at a time and thus requires a memory to summarize what it has seen previously. However, designing memory architectures for reinforcement learning problems has been a challenging task <ref type="bibr" target="#b36">(Oh et al., 2016;</ref><ref type="bibr" target="#b44">Wayne et al., 2018)</ref>. Many memory architectures for RL use an LSTM component to summarize what an agent has seen.</p><p>We investigated if changing the gates of these LSTMs can improve the performance of RL agents, especially on difficult tasks involving memory and long-term credit assignment. We chose the Passive match and Active match tasks from <ref type="bibr" target="#b20">Hung et al. (2018)</ref> using A3C agents . See <ref type="figure">Figure  7</ref> for a description of Active match. Passive match is similar, except the agent always starts facing the colored cue. As a result, Passive Match only tests long term memory, not longterm credit assignment. Only the final task reward is reported. <ref type="bibr" target="#b20">Hung et al. (2018)</ref> evaluated agents with different recurrent cores: basic LSTM, LSTM+Mem (an LSTM with memory), and RMA (which also uses an LSTM core), and found the standard LSTM was not able to solve these tasks. We modified the LSTM agent with our gate mechanisms. <ref type="figure" target="#fig_5">Figure 8</ref> shows the results of different methods on the Passive match and Active match tasks with distractors. These tasks are structurally similar to the synthetic tasks (Sec. 5.1) requiring retrieval of a memory over hundreds of steps to solve the <ref type="figure" target="#fig_5">Figure 8</ref>. Performance on reinforcement learning tasks that require memory. We evaluated the image matching tasks from <ref type="bibr" target="#b20">Hung et al. (2018)</ref>, which test memorization and credit assignment, using an A3C agent  with an LSTM policy core. We observe that general trends from the synthetic tasks (Section (5.1)) transfer to this reinforcement learning setting.</p><p>task, and we found that those trends largely transferred to the RL setting even with several additional confounders present such as agents learning via RL algorithms, being required to learn relevant features from pixels rather than being given the relevant tokens, and being required to explore in the Active Match case.</p><p>We found that the UR gates substantially improved the performance of the basic LSTM on both Passive Match and Active Match tasks with distractor rewards. The URLSTM was the was the only method able to get near optimal performance on both tasks, and achieved similar final performance to the LSTM+Mem and RMA agents reported in .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Additional Results and Experimental Conclusions</head><p>Appendix (E.1) shows an additional synthetic experiment investigating the effect of refine gates on saturation. Appendix (E.3) has results on a program execution task, which is interesting for having explicit long and variable-length dependencies and hierarchical structure. It additionally shows another very different gated recurrent model where the UR gates provide consistent improvement.</p><p>Finally, we would like to comment on the longevity of the LSTM, which for example was frequently found to outperform newer competitors when better tuned <ref type="bibr" target="#b32">(Melis et al., 2017;</ref><ref type="bibr" target="#b33">Merity, 2019)</ref>. Although many improvements have been suggested over the years, none have been proven to be as robust as the LSTM across an enormously diverse range of sequence modeling tasks. By experimentally starting from well-tuned LSTM baselines, we believe our simple isolated gate modifications to actually be robust improvements. In Appendix B.3 and B.4, we offer a few conclusions for the practitioner about the other gate components considered based on our experimental experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>In this work, we introduce and evaluate several modifications to the ubiquitous gating mechanism that appears in recurrent neural networks. We describe methods that improve on the standard gating method by alleviating problems with initialization and optimization. The mechanisms considered include changes on independent axes, namely initialization/activations and auxiliary gates, and we perform extensive ablations on our improvements with previously considered modifications. Our main gate model robustly improves on standard gates across many different tasks and recurrent cores, while requiring less tuning. Finally, we emphasize that these improvements are entirely independent of the large body of research on neural network architectures that use gates, and hope that these insights can be applied to improve machine learning models at large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pseudocode</head><p>We show how the gated update in a typical LSTM implementation can be easily replaced by UR-gates.</p><p>The following snippets show pseudocode for the the gated state updates for a vanilla LSTM model <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Further discussion on related methods</head><p>Section 4 briefly introduced chrono initialization <ref type="bibr" target="#b42">(Tallec &amp; Ollivier, 2018)</ref> and the ON-LSTM <ref type="bibr" target="#b40">(Shen et al., 2018)</ref>, closely related methods that modify the gating mechanism of LSTMs. We provide more detailed discussion on these in Sections B.3 and B.4 respectively. Section B.1 has a more thorough overview of related work on recurrent neural networks that address long-term dependencies or saturating gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Related Work</head><p>Several methods exist for addressing gate saturation or allowing more binary activations. <ref type="bibr" target="#b14">Gulcehre et al. (2016)</ref> proposed to use piece-wise linear functions with noise in order to allow the gates to operate in saturated regimes. <ref type="bibr" target="#b30">Li et al. (2018b)</ref> instead use the Gumbel trick <ref type="bibr" target="#b31">(Maddison et al., 2016;</ref><ref type="bibr" target="#b21">Jang et al., 2016)</ref>, a technique for learning discrete variables within a neural network, to train LSTM models with discrete gates. These stochastic approaches can suffer from issues such as gradient estimation bias, unstable training, and limited expressivity from discrete instead of continuous gates. Additionally they require more involved training protocols with an additional temperature hyperparameter that needs to be tuned explicitly.</p><p>Alternatively, gates can be removed entirely if strong constraints are imposed on other parts of the model. <ref type="bibr" target="#b29">(Li et al., 2018a)</ref> use diagonal weight matrices and require stacked RNN layers to combine information between hidden units. A long line of work has investigated the use of identity or orthogonal initializations and constraints on the recurrent weights to control multiplicative gradients unrolled through time <ref type="bibr" target="#b28">(Le et al., 2015;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2016;</ref><ref type="bibr" target="#b16">Henaff et al., 2016)</ref>. <ref type="bibr" target="#b5">(Chandar et al., 2019)</ref> proposed another RNN architecture using additive state updates and non-saturating activation functions instead of gates. However, although these gate-less techniques can be used to alleviate the vanishing gradient problem with RNNs, unbounded activation functions can cause less stable learning dynamics and exploding gradients.</p><p>As mentioned, a particular consequence of the inability of gates to approach extrema is that gated recurrent models struggle to capture very long dependencies. These problems have traditionally been addressed by introducing new components to the basic RNN setup. Some techniques include stacking layers in a hierarchy <ref type="bibr" target="#b9">(Chung et al., 2016)</ref>, adding skip connections and dilations <ref type="bibr" target="#b26">(Koutnik et al., 2014;</ref><ref type="bibr" target="#b6">Chang et al., 2017)</ref>, using an external memory <ref type="bibr" target="#b12">(Graves et al., 2014;</ref><ref type="bibr" target="#b45">Weston et al., 2014;</ref><ref type="bibr" target="#b44">Wayne et al., 2018;</ref><ref type="bibr" target="#b15">Gulcehre et al., 2017)</ref>, auxiliary semi-supervision <ref type="bibr" target="#b43">(Trinh et al., 2018)</ref>, and more. However, these approaches have not been widely adopted over the standard LSTM as they are often specialized for certain tasks, are not as robust, and introduce additional complexity. Recently the transformer model has been successful in many applications areas such as NLP <ref type="bibr" target="#b37">(Radford et al., 2019;</ref><ref type="bibr" target="#b10">Dai et al., 2019)</ref>. However, recurrent neural networks are still important and commonly used due their faster inference without the need to maintain the entire sequence in memory. We emphasize that the vast majority of proposed RNN changes are completely orthogonal to the simple gate improvements in this work, and we do not focus on them.</p><p>A few other recurrent cores that use the basic gated update (1) but use more sophisticated update functions u include the GRU, Reconstructive Memory Agent (RMA; <ref type="bibr" target="#b20">Hung et al., 2018)</ref>, and Relational Memory Core (RMC; <ref type="bibr" target="#b39">Santoro et al., 2018)</ref>, which we consider in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Effect of proposed methods on timescales</head><p>We briefly review the connection between our methods and the effective timescales that gated RNNs capture. Recall that Section 3.2 defines the characteristic timescale of a neuron with forget activation f t as 1/(1?f t ), which would be the number of timesteps it takes to decay that neuron by a constant.</p><p>The fundamental principle of gated RNNs is that the activations of the gates affects the timescales that the model can address; for example, forget gate activations near 1.0 are necessary to capture long-term dependencies.</p><p>Thus, although our methods were defined in terms of activations g t , it is illustrative to reason with their characteristic timescales 1/(1?g t ) instead, whence both UGI and refine gate also have clean interpretations.</p><p>First, UGI is equivalent to initializing the decay period from a particular heavy-tailed distribution, in contrast to standard initialization with a fixed decay period (1??(b f )) ?1 .</p><p>Proposition 1. UGI is equivalent to to sampling the decay period D = 1/(1 ? f t ) from a distribution with density proportional to P</p><formula xml:id="formula_11">(D = x) ? d dx (1 ? 1/x) = x ?2 , i.e. a Pareto(? = 2) distribution.</formula><p>On the other hand, for any forget gate activation f t with timescale D = 1/(1 ? f t ), the refine gate fine-tunes it between D = 1/(1?f 2 t ) = 1/(1?f t )(1+f t ) and 1/(1?f t ) 2 . Proposition 2. Given a forget gate activation with timescale D, the refine gate creates an effective forget gate with timescale in (D/2, D 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Chrono Initialization</head><p>The chrono initialization</p><formula xml:id="formula_12">b f ? log(U([1,T max ?1])) (14) b i = ?b f .<label>(15)</label></formula><p>was the first to explicitly attempt to initialize the activation of gates across a distributional range. It was motivated by matching the gate activations to the desired timescales.</p><p>They also elucidate the benefits of tying the input and forget gates, leading to the simple trick (15) for approximating tying the gates at initialization, which we borrow for UGI. (We remark that perfect tied initialization can be accomplished by fully tying the linear maps L f ,L i , but <ref type="formula" target="#formula_12">(15)</ref> is a good approximation.)</p><p>However, the main drawback of CI is that the initialization distribution is too heavily biased toward large terms. This leads to empirical consequences such as difficult tuning (due to most units starting in the saturation regime, requiring different learning rates) and high sensitivity to the hyperparameter T max that represents the maximum potential length of dependencies. For example, <ref type="bibr" target="#b42">Tallec &amp; Ollivier (2018)</ref> set this parameter according to a different protocol for every task, with values ranging from 8 to 2000. Our experiments used a hyperparameter-free method to initialize T max (Section 5), and we found that chrono initialization generally severely over-emphasizes long-term dependencies if T max is not carefully controlled.</p><p>A different workaround suggested by <ref type="bibr" target="#b42">Tallec &amp; Ollivier (2018)</ref> is to sample from P(T = k) ? 1 klog 2 (k+1) and setting b f = log(T ). Note that such an initialization would be almost equivalent to sampling the decay period from the distribution with density P(D = x) ? (x log 2 x) ?1 (since the decay period is (1 ? f ) ?1 = 1 + exp(b f )). This parameter-free initialization is thus similar in spirit to the uniform gate initialization (Proposition 1), but from a much heavier-tailed distribution that emphasizes very long-term dependencies.</p><p>These interpretations suggest that it is plausible to define a family of Pareto-like distributions from which to draw the initial decay periods from, with this distribution treated as a hyperparameter. However, with no additional prior information on the task, we believe the uniform gate initialization to be the best candidate, as it 1. is a simple distribution with easy implementation, 2. has characteristic timescale distributed as an intermediate balance between the heavy-tailed chrono initialization and sharply decaying standard initialization, and 3. is similar to the ON-LSTM's cumax activation, in particular matching the initialization distribution of the cumax activation. <ref type="table" target="#tab_5">Table 4</ref> summarizes the decay period distributions at initialization using different activations and initialization strategies.</p><p>In general, our experimental recommendation for CI is that it can be better than standard initialization or UGI when certain conditions are met (tasks with long dependencies and nearly fixed-length sequences as in Sections 5.1, 5.4) and/or when it can be explicitly tuned (both the hyperparameter T max , as well as the learning rate to compensate for almost all units starting in saturation). Otherwise, we recommend UGI or standard initialization. We found no scenarios where it outperformed UR-gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. ON-LSTM</head><p>In this section we elaborate on the connection between the mechanism of <ref type="bibr" target="#b40">(Shen et al., 2018)</ref> and our methods. We define the full ON-LSTM and show how its gating mechanisms can be improved. For example, there is a remarkable connection between its master gates and our refine gates -independently of the derivation of refine gates in Section 3.4, we show how a specific way of fixing the normalization of master gates becomes equivalent to a single refine gate.</p><p>First, we formally define the full ON-LSTM. The master gates are a cumax-activation gat?</p><formula xml:id="formula_13">f t = cumax(Lf (x t ,h t?1 )) (16) i t = 1?cumax(L?(x t ,h t?1 )).<label>(17)</label></formula><p>These combine with an independent pair of forget and input gates f t ,i t , meant to control fine-grained behavior, to create an effective forget/input gatef t ,? t which are used to update </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization method Timescale distribution</head><p>Constant</p><formula xml:id="formula_14">bias b f = b P(D = x) ? 1{x = 1+e b } Chrono initialization (known timescale T max ) P(D = x) ? 1{x ? [2,T max ]} Chrono initialization (unknown timescale) P(D = x) ? 1 xlog 2 x Uniform gate initialization P(D = x) ? 1 x 2 cumax activation P(D = x) ? 1 x 2</formula><p>the state (equation <ref type="formula">(1)</ref> or <ref type="formula">(5))</ref>.</p><formula xml:id="formula_15">? t =f t ?? t (18) f t = f t ?? t +(f t ?? t ) (19) i t = i t ?? t +(? t ?? t ).<label>(20)</label></formula><p>As mentioned in Section B.1, this model modifies the standard forget/input gates in two main ways, namely ordering the gates via the cumax activation, and supplying an auxiliary set of gates controlling fine-grained behavior. Both of these are important novelties and together allow recurrent models to better capture tree structures.</p><p>However, the UGI and refine gate can be viewed as improvements over each of these, respectively, demonstrated both theoretically (below) and empirically (Sections 5 and E.3), even on tasks involving hierarchical sequences.</p><p>Ordered gates Despite having the same parameter count and asymptotic efficiency as standard sigmoid gates, cumax gates seem noticeably slower and less stable in practice for large hidden sizes. Additionally, using auxiliary master gates creates additional parameters compared to the basic LSTM. <ref type="bibr" target="#b40">Shen et al. (2018)</ref> alleviated both of these problems by defining a downsize operation, whereby neurons are grouped in chunks of size C, each of which share the same master gate values. However, this also creates an additional hyperparameter.</p><p>The speed and stability issues can be fixed by just using the sigmoid non-linearity instead of cumax. To recover the most important properties of the cumax-activations at multiple timescales-the equivalent sigmoid gate can be initialized so as to match the distribution of cumax gates at initialization. This is just uniform gate initialization (equation <ref type="formula" target="#formula_6">(12)</ref>).</p><p>However, we believe that the cumax activation is still valuable in many situations if speed and instability are not issues. These include when the hidden size is small, when extremal gate activations are desired, or when ordering needs to be strictly enforced to induce explicit hierarchical structure. For example, Section (5.1) shows that they can solve hard memory tasks by themselves.</p><p>Master gates We observe that the magnitudes of master gates are suboptimally normalized. A nice interpretation of gated recurrent models shows that they are a discretization of a continuous differential equation. This leads to the leaky RNN model h t+1 = (1??)h t +?u t , where u t is the update to the model such as tanh(W x x t +W h h t +b). Learning ? as a function of the current time step leads to the simplest gated recurrent model 5 <ref type="bibr" target="#b42">Tallec &amp; Ollivier (2018)</ref> show that this exactly corresponds to the discretization of a differential equation that is invariant to time warpings and time rescalings. In the context of the LSTM, this interpretation requires the values of the forget and input gates to be tied so that f t + i t = 1. This weight-tying is often enforced, for example in the most popular LSTM variant, the GRU , or our UR-gates. In a large-scale LSTM architecture search, it was found that removing the input gate was not significantly detrimental <ref type="bibr" target="#b13">(Greff et al., 2016)</ref>.</p><formula xml:id="formula_16">f t = ?(L f (x t ,h t?1 )) u t = tanh(L u (x t ,h t?1 )) h t = f t h t?1 +(1?f t )u t .</formula><p>However, the ON-LSTM does not satisfy this conventional wisdom that the input and forget gates should sum to close to 1.</p><p>Proposition 3. At initialization, the expected value of the average effective forget gate activationf t is 5/6.</p><p>Let us consider the sum of the effective forget and input gates at initialization. Adding equations <ref type="formula" target="#formula_4">(19)</ref> and <ref type="formula" target="#formula_15">(20)</ref> yield?</p><formula xml:id="formula_17">f t +? t = (f t +i t )?? t +(f t +? t ?2? t ) =f t +? t +(f t +i t ?2)?? t .</formula><p>Note that the master gates (16), (17) sum 1 in expectation at initialization, as do the original forget and input gates. Looking at individual units in the ordered master gates, we have Ef (j) = j n ,E? (j) = 1? j n . Thus the above simplifies to</p><formula xml:id="formula_18">E[f t +? t ] = 1?E? t E[f (j) t +? (j) t ] = 1? j n (1? j n ) E E j?[n]f (j) t +? (j) t ? 1? 1 0 xdx+ 1 0 x 2 dx = 5 6 .</formula><p>The gate normalization can be fixed by re-scaling equations <ref type="formula" target="#formula_4">(19)</ref> and <ref type="formula" target="#formula_15">(20)</ref>. It turns out that tying the master gates and re-scaling is exactly equivalent to the mechanism of a refine gate. In this equivalence, the role of the master and forget gates of the ON-LSTM are played by our forget and refine gate respectively.</p><p>Proposition 4. Suppose the master gatesf t ,? t are tied and the equations (19)-(20) defining the effective gatesf t ,? t are rescaled such as to ensure E[f t +? t ] = 1 at initialization. The resulting gate mechanism is exactly equivalent to that of the refine gate.</p><p>Consider the following set of equations where the master gates are tied (f t +? t = 1, f t + i t = 1) and <ref type="formula" target="#formula_4">(19)</ref>- <ref type="formula" target="#formula_15">(20)</ref> are modified with an extra coefficient (rescaling in bold):</p><formula xml:id="formula_19">i t = 1?f t (21) ? t =f t ?? t (22) f t = 2?f t ?? t +(f t ?? t )<label>(23)</label></formula><formula xml:id="formula_20">i t = 2?i t ?? t +(? t ?? t )<label>(24)</label></formula><p>Now we havef t +? t =f t +? t +2(f t +i t ?1)?? t = 1+2(f t +i t ?1)?? t which has the correct scaling, i.e. E[f t +? t ] = 1 at initialization assuming that E[f t +i t ] = 1 at initialization.</p><p>But (23) can be rewritten as follows:</p><formula xml:id="formula_21">f = 2?f ??+(f ??) = 2?f ?f ?(1?f )+(f ?f ?(1?f )) = 2f ?f ?2f ?f 2 +f 2 = f ?2f ?f ?f 2 ?f ?f 2 +f 2 = f ?(1?(1?f )) 2 +(1?f )?f 2 .</formula><p>This is equivalent to the refine gate, where the master gate plays the role of the forget gate and the forget gate plays the role of the refine gate. It can be shown that in this case, the effective input gate? t (24) is also defined through a refine gate mechanism, where? t = 1?f t is refined by i t :</p><formula xml:id="formula_22">i = i?(1?(1??)) 2 +(1?i)?? 2 .</formula><p>Based on our experimental findings, in general we would recommend the refine gate in place of the master gate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Gate ablation details</head><p>For clarity, we formally define the gate ablations considered which mix and match different gate components.</p><p>We remark that other combinations are possible, for example combining CI with either auxiliary gate type, which would lead to CR-or CM-gates. Alternatively, the master or refine gates could be defined using different activation and initialization strategies. We chose not to consider these methods due to lack of interpretation and theoretical soundness.</p><p>O-This ablation uses the cumax activation to order the forget/input gates and has no auxiliary gates.</p><formula xml:id="formula_23">f t = cumax(L f (x t ,h t?1 )) (25) i t = 1?cumax(L i (x t ,h t?1 )).<label>(26)</label></formula><p>We note that one difficulty with this in practice is the reliance on the expensive cumax, and hypothesize that this is perhaps the ON-LSTM's original motivation for the second set of gates combined with downsizing.</p><p>UM-This variant of the ON-LSTM ablates the cumax operation on the master gates, replacing it with a sigmoid activation initialized with UGI. Equations (16), (17) are replaced with</p><formula xml:id="formula_24">u = U(0,1) (27) b f = ? ?1 (u) (28) f t = ?(Lf (x t ,h t?1 )+b f )<label>(29)</label></formula><formula xml:id="formula_25">i t = ?(L?(x t ,h t?1 )?b f )<label>(30)</label></formula><p>Equations <ref type="formula" target="#formula_3">(18)</ref>- <ref type="formula" target="#formula_15">(20)</ref> are then used to define effective gate? f t ,? t which are used in the gated update (1) or <ref type="formula">(5)</ref>.</p><p>OR-This ablation combines ordered main gates with an auxilliary refine gate.</p><formula xml:id="formula_26">f t = cumax(Lf (x t ,h t?1 )+b f )<label>(31)</label></formula><formula xml:id="formula_27">r t = ?(L r (x t , h t?1 )+b r )<label>(32)</label></formula><formula xml:id="formula_28">g t = r t ?(1?(1?f t ) 2 )+(1?r t )?f 2 t (33) i t = 1?g t<label>(34)</label></formula><p>g t ,i t are used as the effective forget and input gates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Analysis Details</head><p>The gradient analysis in <ref type="figure" target="#fig_0">Figure 3</ref> was constructed as follows. Let f, r, g be the forget, refine, and effective gates</p><formula xml:id="formula_29">g = 2rf +(1?2r)f 2 .</formula><p>Letting x,y be the pre-activations of the sigmoids on f and r, the gradient of g can be calculated as</p><formula xml:id="formula_30">? x g = 2rf (1?f )+(1?2r)(2f )(f (1?f )) = 2f (1?f )[r+(1?2r)f ] ? y g = 2f r(1?r)+(?2f 2 )r(1?r) = 2f r(1?r)(1?f ) ?g 2 = [2f (1?f )] 2 (r+f ?2f r) 2 +r 2 (1?r) 2 .</formula><p>Substituting the relation</p><formula xml:id="formula_31">r = g?f 2 2f (1?f ) ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>this reduces to the Equation 35</head><p>,</p><formula xml:id="formula_32">?g 2 = ((g?f 2 )(1?2f )+2f 2 (1?f )) 2 +(g?f 2 ) 2 1? g?f 2 2f (1?f ) 2 .</formula><p>( <ref type="formula">35)</ref> Given the constraint f 2 ? g ? 1?(1?f ) 2 , this function can be minimized and maximized in terms of g to produce the upper and lower bounds in <ref type="figure" target="#fig_0">Figure 3b</ref>. This was performed numerically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Experimental Details</head><p>To normalize the number of parameters used for models using master gates, i.e. the OM-and UM-gating mechanisms, we used a downsize factor on the main gates (see <ref type="bibr">Section B.4)</ref>. This was set to C = 16 for the synthetic and image classification tasks, and C = 32 for the language modeling and program execution tasks which used larger hidden sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Synthetic Tasks</head><p>All models consisted of single layer LSTMs with 256 hidden units, trained with the Adam optimizer <ref type="bibr" target="#b24">(Kingma &amp; Ba, 2014)</ref> with learning rate 1e-3. Gradients were clipped at 1.0.</p><p>The training data consisted of randomly generated sequences for every minibatch rather than iterating through a fixed dataset. Each method ran 3 seeds, with the same training data for every method.</p><p>Our version of the Copy task is a very minor variant of other versions reported in the literature, with the main difference being that the loss is considered only over the last 10 output tokens which need to be memorized. This normalizes the loss so that losses approaching 0 indicate true progress. In contrast, this task is usually defined with the model being required to output a dummy token at the first N +10 steps, meaning it can be hard to evaluate performance since low average losses simply indicate that the model learns to output the dummy token.</p><p>For <ref type="figure">Figure 4</ref>, the log loss curves show the median of 3 seeds, and the error bars indicate 60% confidence.</p><p>For <ref type="figure">Figure 5</ref>, each histogram represents the distribution of forget gate values of the hidden units (of which there are 256). The values are created by averaging units over time and samples, i.e., reducing a minibatch of forget gate activations of shape (batch size, sequence length, hidden size) over the first two diensions, to produce the average activation value for every unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Image Classification</head><p>All models used a single hidden layer recurrent network <ref type="bibr">(LSTM or GRU)</ref>. Inputs x to the model were given in batches as a sequence of shape (sequence length, num channels), (e.g. (1024,3) for CIFAR-10), by flattening the input image left-to-right, top-to-bottom. The outputs of the model of shape (sequence length, hidden size) were processed independently with a single ReLU hidden layer of size 256 before the final fully-connected layer outputting softmax logits. All training was performed with the Adam optimizer, batch size 50, and gradients clipped at 1.0. MNIST trained for 150 epochs, CIFAR-10 used 100 epochs over the training set.  <ref type="table" target="#tab_6">Table 5</ref> reports the highest validation score found. The GRU model swept over learning rates {2e?4,5e?4}; all methods were unstable at higher learning rates. <ref type="figure" target="#fig_1">Figure 6</ref> shows the median validation accuracy with quartiles (25/75% confidence intervals) over the seeds, for the best-performing stable learning rate (i.e. the one with highest average validation score on the final epoch). This was generally 5e?4 or 1e?3, with refine gate variants tending to allow higher learning rates. <ref type="table" target="#tab_2">Table 2</ref> The UR-LSTM and UR-GRU used 1024 hidden units for the sequential and permuted MNIST task, and 2048 hidden units for the sequential CIFAR task. The vanilla LSTM baseline used 512 hidden units for MNIST and 1024 for CIFAR. Larger hidden sizes were found to be unstable.</p><p>Zoneout parameters were fixed to reasonable default settings based on <ref type="bibr" target="#b27">Krueger et al. (2016)</ref>, which are z c = 0.5,z h = 0.05 for LSTM and z = 0.1 for GRU. When zoneout was used, standard Dropout <ref type="bibr" target="#b41">(Srivastava et al., 2014)</ref> with probability  . Performance on Reinforcement Learning Tasks that Require Memory. We evaluated the image matching tasks from <ref type="bibr" target="#b20">Hung et al. (2018)</ref>, which test memorization and credit assignment, using an A3C agent  with an LSTM policy core. We observe that general trends from the synthetic tasks (Section (5.1)) transfer to this reinforcement learning setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reinforcement Learning</head><p>The Active Match and Passive Match tasks were borrowed from <ref type="bibr" target="#b20">Hung et al. (2018)</ref> with the same settings. <ref type="figure" target="#fig_0">For Figures 9 and 13</ref>, the discount factor in the environment was set to ? = .96. For <ref type="figure" target="#fig_4">Figure 10</ref>, the discount factor was ? = .998. <ref type="figure" target="#fig_0">Figure 13</ref> corresponds to the full Active  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4. Program Evaluation</head><p>Protocol was taken from <ref type="bibr" target="#b39">Santoro et al. (2018)</ref>  For distributional initialization strategies, a trainable bias vector was sampled independently from the chosen distribution (i.e. equation <ref type="formula">(14)</ref> or <ref type="formula" target="#formula_6">(12)</ref>) and added/subtracted to the forget and input gate ((2)-(3)) before the non-linearity.</p><p>Additionally, each linear model such as W xf x t +W hf h t?1 had its own trainable bias vector, effectively doubling the learning rate on the pre-activation bias terms on the forget and input gates. This was an artifact of implementation and not intended to affect performance.</p><p>The refine gate update equation <ref type="formula">(10)</ref> can instead be implemented as</p><formula xml:id="formula_33">g t = r t ?(1?(1?f t ) 2 )+(1?r t )?f 2 t = 2r t ?f t +(1?2r t )?f 2 t</formula><p>Permuted image classification In an effort to standardize the permutation used in the Permuted MNIST benchmark, we use a particular deterministic permutation rather than a random one. After flattening the input image into a onedimensional sequence, we apply the bit reversal permutation. This permutation sends the index i to the index j such that j's binary representation is the reverse of i's binary representation. The intuition is that if two indices i,i are close, they must differ in their lower-order bits. Then the bitreversed indices will be far apart. Therefore the bit-reversal permutation destroys spatial and temporal locality, which is desirable for these sequence classification tasks meant to test long-range dependencies rather than local structure.</p><p>def bitreversal_po2(n): m = int(math.log(n) / math.log <ref type="formula">(2)</ref> Listing 3: Bit-reversal permutation for permuted MNIST.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Additional Experiments</head><p>E.1. Synthetic Forgetting <ref type="figure">Figure 5</ref> on the Copy task demonstrates that extremal gate activations are necessary to solve the task, and initializing the activations near 1.0 is helpful.</p><p>This raises the question: what happens if the initialization distribution does not match the task at hand; could the gates learn back to a more moderate regime? We point out that such a phenomenon could occur non-pathologically on more complex setups, such as a scenario where a model trains to remember on a Copy-like task and then needs to "unlearn" as part of a meta-learning or continual learning setup.</p><p>Here, we consider such a synthetic scenario and experimentally show that the addition of a refine gate helps models train much faster while in a saturated regime with extremal activations. We also point to the poor performance of C-outside of synthetic memory tasks when using our high hyperparameterfree initialization as more evidence that it is very difficult for standard gates to unlearn undesired saturated behavior.</p><p>For this experiment, we initialize the biases of the gates extremely high (effective forget activation ? ?(6). We then consider the Adding task (Section 5.1) of length 500, hidden size 64, learning rate 1e-4. The R-LSTM is able to solve the task, while the LSTM is stuck after 1e4 iterations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Reinforcement Learning</head><p>Figures 9 and 10 evaluated our gating methods with the LSTM and RMA models on the Passive Match and Active Match tasks, with and without distractors. We additionally ran the agents on an even harder version of the Active Match task with larger distractor rewards (the full Active Match from <ref type="bibr" target="#b20">Hung et al. (2018)</ref>). Learning curves are shown in <ref type="figure" target="#fig_0">Figure 13</ref>. Similarly to the other results, the UR-gated core is noticeably better than the others. For the DNC model, it is the only one that performs better than random chance.  <ref type="figure">Figure 12</ref>. Distribution of forget gate activations after extremal initialization, and training on the Adding task. The UR-LSTM is able to learn much faster in this saturated gate regime while the LSTM does not solve the task. The smallest forget unit for the UR-LSTM after training has characteristic timescale over an order of magnitude smaller than that of the LSTM.  <ref type="figure" target="#fig_0">Figure 13</ref>. The full Active Match task with large distractor rewards, using agents with LSTM or DNC recurrent cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. Program Execution</head><p>The Learning to Execute <ref type="bibr" target="#b46">(Zaremba &amp; Sutskever, 2014)</ref> dataset consists of algorithmic snippets from a programming language of pseudo-code. An input is a program from this language presented one character at a time, and the target output is a numeric sequence of characters representing the execution output of the program. There are three categories of tasks: Addition, Control, and Program, with distinctive types of input programs. We use the most difficult setting from <ref type="bibr" target="#b46">Zaremba &amp; Sutskever (2014)</ref>, which uses the parameters nesting=4, length=9, referring to the nesting depth of control structure and base length of numeric literals, respectively. Examples of input programs are shown in previous works <ref type="bibr" target="#b46">(Zaremba &amp; Sutskever, 2014;</ref><ref type="bibr" target="#b39">Santoro et al., 2018)</ref>.</p><p>We are interested in this task for several reasons. First, we are interested in comparing against the C-and OM-gate methods, because</p><p>? The maximum sequence length is fairly long (several hundred tokens), meaning our T max heuristic for C-gates is within the right order of magnitude of dependency lengths.</p><p>? The task has highly variable sequence lengths, wherein the standard training procedure randomly samples inputs of varying lengths (called the "Mix" curriculum in <ref type="bibr" target="#b46">Zaremba &amp; Sutskever (2014)</ref>). Additionally, the Control and Program tasks contain complex control flow and nested structure. They are thus a measure of a sequence model's ability to model dependencies of differing lengths, as well as hierarchical information. Thus we are interested in comparing the effects of UGI methods, as well as the full OM-gates which are designed for hierarchical structures <ref type="bibr" target="#b40">(Shen et al., 2018</ref>).</p><p>Finally, this task has prior work using a different type of recurrent core, the Relational Memory Core (RMC), that we also use as a baseline to evaluate our gates on different models . Both the LSTM and RMC were found to outperform other recurrent baselines such as the Differential Neural Computer (DNC) and EntNet.</p><p>Training curves are shown in <ref type="figure" target="#fig_9">Figure 14</ref>, which plots the median accuracy with confidence intervals. We point out a few observations. First, despite having a T max value on the right order of magnitude, the C-gated methods have very poor performance across the board, reaffirming the chrono initialization's high sensitivity to this hyperparameter.</p><p>Second, the U-LSTM and U-RMC are the best methods on the Addition task. Additionally, the UR-RMC vs. RMC on Addition is one of the very few tasks we have found where a generic substitution of the UR-gate does not improve on the basic gate. We have not investigated what property of this task caused these phenomena.</p><p>Aside from the U-LSTM on addition, the UR-LSTM outperforms all other LSTM cores. The UR-RMC is also the best core on both Control and Program, the tasks involving hierarchical inputs and longer dependencies. For the most part, the improved mechanisms of the UR-gates seem to transfer to this recurrent core as well. We highlight that this is not true of similar gating mechanisms. In particular, the OM-LSTM, which is supposed to model hierarchies, has good performance on Control and Program as expected (although not better than the UR-LSTM). However, the OM-gates' performance plummets when transferred to the RMC core.</p><p>Interestingly, the -LSTM cores are consistently better than the -RMC versions, contrary to previous findings on easier versions of this task using similar protocol and hyperparameters . We did not explore different hyperparameter regimes on this more difficult setting. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Refine gate activations and gradients.: (a) Contours of the effective gate gt as a function of the forget and refine gates ft, rt. High effective activations can be achieved with more modest ft,rt values. (b) The gradient ?gt as a function of effective gate activation gt. [Black, blue]: Lower and upper bounds on the ratio of the gradient with a refine gate vs. the gradient of a standard gate. For activation values near the extremes, the refine gate can significantly increase the gradients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 6 .</head><label>6</label><figDesc>Performance on pixel-by-pixel image classification. Performance is consistent with synthetic tasks. --performs the worst. Other gating variants improve performance. Note that methods that combine the refine gate with a range of gate values (OR, UR) perform best.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>f, r, u, o = Linear(x, prev_hidden) f_ = sigmoid(f + forget_bias) r_ = sigmoid(r -forget_bias) g = 2 * r_ * f_ + (1-2 * r_) * f_ ** 2 next_cell = g * prev_cell + (1-g) * tanh(u) next_hidden = sigmoid(o) * tanh(next_cell) Listing 2: UR-LSTM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 11</head><label>11</label><figDesc>shows smoothed validation perplexity curves showing the 95% confidence intervals over the last 1% of data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 10 .</head><label>10</label><figDesc>The addition of distractor rewards changes the task and relative performance of different gating mechanisms. For both LSTM and RMA recurrent cores, the UR-gates still perform best.Match task in<ref type="bibr" target="#b20">Hung et al. (2018)</ref>, whileFigure 10is their version with small distractor rewards where the apples in the distractor phase give 1 instead of 5 reward.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8</head><label>8</label><figDesc>used 5 seeds per method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Validation learning curves, illustrating training speed and generalization (i.e. overfitting) behavior.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 14 .</head><label>14</label><figDesc>Program Execution evaluation accuracies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The adjustment function. (a) An adjustment function ?(ft) satisfying natural properties is chosen to define a band within which the forget gate is refined. (b) The forget gate ft(x) is conventionally defined with the sigmoid function (black). The refine gate interpolates around the original gate ft to yield an effective gate gt within the upper and lower curves, gt ? ft ??(ft).</figDesc><table><row><cell>0.4 0.6 0.8 1.0 Band range c(f)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4 g 0.6 0.8 1.0</cell><cell>1 1 + e x 1 (1 + e x ) 2 1 1 (1 + e x ) 2</cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell></row><row><cell>0.0</cell><cell>0.0</cell><cell>0.2</cell><cell>0.4 Gate f 0.6</cell><cell>0.8</cell><cell>1.0</cell><cell>0.0</cell><cell>x 4 3 2 1 0 1 2 3 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell cols="2">Figure 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>The effect of UGI is that all</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>timescales are covered, from units with very high forget</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>activations remembering information (nearly) indefinitely,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>to those with low activations focusing solely on the incoming</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Comparison to prior methods for pixel-by-pixel image classification. Test acc. on pixel-by-pixel image classification benchmarks. Top: Recurrent baselines and variants. Middle: Nonrecurrent sequence models with global receptive field. r-LSTM has 2-layers with an auxiliary loss. Bottom: Our methods.</figDesc><table><row><cell>Method</cell><cell>sMNIST</cell><cell>pMNIST</cell><cell>sCIFAR-10</cell></row><row><cell>LSTM (ours)</cell><cell>98.9</cell><cell>95.11</cell><cell>63.01</cell></row><row><cell>Dilated GRU</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Language modelling results.</figDesc><table><row><cell>Perplexities on the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Distribution of the decay period D = (1?f ) ?1 using different initialization strategies.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>All models (LSTM and GRU) used hidden state size 512.</figDesc><table><row><cell>Learning rate swept in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Gate ablations on pixel-by-pixel image classification. Validation accuracies on pixel image classification. Asterisks denote divergent runs at the learning rate the best validation score was found at. * 65.60 67.78 67.63 71.85 * 67.73 * 70.41 67.29 * 71.05 sCIFAR (GRU) 71.30 * 64.61 69.81 * * 70.10 70.74 * 70.20 * 71.40 * * 69.17 * 71.040.5 was also applied to the output classification hidden layer.</figDesc><table><row><cell cols="2">Gating Method -</cell><cell>C-</cell><cell>O-</cell><cell>U-</cell><cell>R-</cell><cell>OM-</cell><cell>OR-</cell><cell>UM-</cell><cell>UR-</cell></row><row><cell>pMNIST</cell><cell cols="3">94.77  *  *  94.69 96.17</cell><cell cols="3">96.05 95.84  *  95.98</cell><cell>96.40</cell><cell>95.50</cell><cell>96.43</cell></row><row><cell cols="2">sCIFAR 63.24  D.3. Language Modeling</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Hyperparameters are taken from Rae et al. (2018)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">tuned for the vanilla LSTM, which consist of (chosen</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">parameter bolded out of sweep): {1, 2} LSTM layer,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">{0.0, 0.1, 0.2, 0.3} embedding dropout, {yes, no} layer</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">norm, and {shared, not shared} input/output embedding</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">parameters. Our only divergence is using a hidden size</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">of 3072 instead of 2048, which we found improved the</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">performance of the vanilla LSTM. Training was performed</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">with Adam at learning rate 1e-3, gradients clipped to 0.1,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">sequence length 128, and batch size 128 on TPU. The LSTM</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">state was reset between article boundaries.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>with minor changes to the hyperparameter search. All models were trained with the Adam optimizer, the Mix curriculum strategy from<ref type="bibr" target="#b46">Zaremba &amp; Sutskever (2014)</ref>, and batch size 128.RMC:The RMC models used a fixed memory slot size of 512 and swept over {2, 4} memories and {2, 4} attention heads for a total memory size of 1024 or 2048. They were trained for 2e5 iterations.LSTM: Instead of two-layer LSTMs with sweeps over skip connections and output concatenation, single-layer LSTMs of size 1024 or 2048 were used. Learning rate was swept in {5e-4, 1e-3}, and models were trained for 5e5 iterations. Note that training was still faster than the RMC models despite the greater number of iterations.D.5. Additional DetailsImplementation Details The inverse sigmoid function (12) can be unstable if the input is too close to {0,1}. Uniform gate initialization was instead implemented by sampling from the distribution U[1/d,1 ? 1/d] instead of U[0, 1], where d is the hidden size, to avoid any potential numerical edge cases. This choice is justified by the fact that with perfect uniform sampling, the expected smallest and largest samples would be 1/(d+1) and 1?1/(d+1).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this work, we use "gate" to alternatively refer to a [0, 1]valued function or the value ("activation") of that function.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In our experiments, we found that tying input/forget gates makes negligible difference on downstream performance, consistent with previous findings in the literature<ref type="bibr" target="#b13">(Greff et al., 2016;</ref><ref type="bibr" target="#b32">Melis et al., 2017)</ref>.3 This corresponds to the number of timesteps it takes to decay by 1/e.4  Since ? ?1 (0) = ?inf, we use the standard practice of thresholding with a small for stability.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">In the literature, this is called the JANET (van der Westhuizen &amp; Lasenby, 2018), which is also equivalent to the GRU without a reset gate<ref type="bibr" target="#b8">(Chung et al., 2014)</ref>, or a recurrent highway network with depth L = 1<ref type="bibr" target="#b47">(Zilly et al., 2017)</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Trellis networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06682</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards non-saturating recurrent units for modelling long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06704</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scalable distributed deep-rl with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2222" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Noisy activation functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3059" to="3068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08718</idno>
		<title level="m">Memory augmented neural networks with wormhole connections</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06662</idno>
		<title level="m">Recurrent orthogonal networks and long-memory tasks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universit?t M?nchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Optimizing agent behavior over long time scales by transporting value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carnevale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.06721</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Categorical reparameterization with Gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An empirical exploration of recurrent network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2342" to="2350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent experience replay in distributed reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kapturowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The NarrativeQA reading comprehension challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ko?isk?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="317" to="328" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Clockwork Rnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3511</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kram?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoneout</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01305</idno>
		<title level="m">Regularizing RNNs by randomly preserving hidden activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (IndRNN): Building a longer and deeper RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02988</idno>
		<title level="m">Towards binary-valued gates for robust LSTM training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the state of the art of evaluation in neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05589</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single headed attention rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.11423</idno>
	</analytic>
	<monogr>
		<title level="m">Stop thinking with your head</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pointer sentinel mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07843</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Control of memory, active perception, and action in Minecraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chockalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09128</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI Blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Fast parametric learning with activation memorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10049</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Relational recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7299" to="7310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09536</idno>
		<title level="m">Ordered neurons: Integrating tree structures into recurrent neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ollivier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.11188</idno>
		<title level="m">Can recurrent neural networks warp time</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning longer-term dependencies in RNNs with auxiliary losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Trinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Der Westhuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lasenby</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00144</idno>
		<idno>arXiv:1804.04849</idno>
	</analytic>
	<monogr>
		<title level="m">The unreasonable effectiveness of the forget gate</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Unsupervised predictive memory in a goal-directed agent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.10760</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.3916</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Memory networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.4615</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Learning to execute. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recurrent highway networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn?k</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="4189" to="4198" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
