<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Perceptual Mapping for Cross-Modal Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saquib Sarfraz</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
						</author>
						<title level="a" type="main">Deep Perceptual Mapping for Cross-Modal Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>International Journal of Computer Vision manuscript No. (will be inserted by the editor) the date of receipt and acceptance should be inserted later</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Cross modal face matching between the thermal and visible spectrum is a much desired capability for night-time surveillance and security applications. Due to a very large modality gap, thermal-to-visible face recognition is one of the most challenging face matching problem. In this paper, we present an approach to bridge this modality gap by a significant margin. Our approach captures the highly non-linear relationship between the two modalities by using a deep neural network. Our model attempts to learn a non-linear mapping from the visible to the thermal spectrum while preserving the identity information. We show substantive performance improvement on three difficult thermal-visible face datasets. The presented approach improves the state-of-the-art by more than 10% on the UND-X1 dataset and by more than 15-30% on the NVESD dataset in terms of Rank-1 identification. Our method bridges the drop in performance due to the modality gap by more than 40%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Face recognition, mainly, has been focused in the visible spectrum. This pertains to a large number of applications from biometrics, access control systems, social media tagging to person retrieval in multimedia. Among the main challenges in visible face recognition, the different lighting/illumination condition has proven to be one of the big factors for appearance change and performance degradation. Many prior studies such as <ref type="bibr" target="#b15">Li et al (2007)</ref>; <ref type="bibr" target="#b24">Socolinsky and Selinger (2002)</ref>; <ref type="bibr" target="#b18">Nicolo and Schmid (2012)</ref>; <ref type="bibr" target="#b12">Klare and Jain (2013)</ref> have stated better face recognition performance in the infra-red spectrum because it is invariant to ambient lighting. Relatively recently, few efforts have been devoted M.S. Sarfraz ? R. Stiefelhagen Institute of Anthropomatics &amp; Robotics, Karlsruhe Institute of Technology (KIT). Karlsruhe, Germany Tel.: +49-721-608-41694, Fax: +49-721-608-45939, E-mail: saquib.sarfraz@kit.edu in the cross-modal face recognition scenarios, where the objective is to identify a person captured in infra-red spectrum based on its stored high resolution visible face image. The motivation for this lies in the night-time or low light surveillance tasks where the image is captured discretely or covertly through active or passive infra-red sensors. In fact there does not exist a reliable solution for matching thermal faces, acquired covertly in such conditions, against the stored visible database such as police mugshots. This poses a significant research gap for such applications in aiding the law enforcement agencies. In the infra-red spectrum, thermal signatures emitted by skin tissues can be acquired through passive thermal sensors without using any active light source. This makes it an ideal candidate for covert night-time surveillance tasks.</p><p>As opposed to the visible spectrum wavelength (0.35?m to 0.74?m), the infra-red spectrum lies in four main ranges. Near infra-red 'NIR' (0.74?m-1?m), short-wave infra-red 'SWIR' (1-3?m), mid-wave infra-red 'MWIR' (3-5?m) and long-wave infra-red 'LWIR' <ref type="bibr">(8-14?m)</ref>. Since the NIR and SWIR bands are reflection dominant, they are more close to the visible spectrum. The MWIR and LWIR are the thermal spectrum and are emission dominant i.e. dependent on material emissivity and temperature. Skin tissue has high emissivity in both the MWIR and LWIR spectrum. Because of this natural difference between the reflective visible spectrum and sensed emissivity in the thermal spectrum, images taken in the two modalities are very different and have a large modality gap. This hinders reliable face matching across the two domains. It is, perhaps, for this reason that most of the earlier studies, aiming at cross-modal face recognition, rely only on visible-to-NIR face matching. While achieving very good results, NIR imaging use an active light source that makes it redundant for covert night-time surveillance. More recently, some attempts have been made in thermal-to-visible face recognition, indicating a significant performance gap due to the very challenging nature of the problem and the large modality gap.</p><p>In this paper, we seek to bridge this gap by trying to directly model the highly non-linear mapping between the two modalities. Our contribution is a useful model, based on a feed-forward deep neural network, and its effective design steps in order to map the perceptual differences between the two modalities while preserving the identity information. We show that this mapping can be learned from relatively little training data and that it works quite well in practice. Our model tries to learn a non-linear regression function between the visible and thermal data, where this data is comprised of densely pooled feature vectors from the images in the corresponding domain. <ref type="figure" target="#fig_0">Figure  1</ref> summarises our approach. The learned projection matrices capture the nonlinear relationship well and are able to bring the two closer to each other. Another contribution is to further the best published state-of-the-art performance on very challenging datasets i.e., the University of Notre Dame's 'UND-X1' LWIR-visible dataset by more than 10% and on Night Vision Electronics &amp; US Army Research Lab's 'NVESD' MWIR-visible and LWIR-visible dataset by 18% and 35% respectively. Our results show that this accounts for bridging the performance gap due to the modality difference by more than 40%. To the best of our knowledge, this is the first attempt in using deep neural networks to bridge the modality gap in cross-modal face recognition. <ref type="figure" target="#fig_0">Figure 1</ref> provides an overview of the approach. This is the extended version of our previously published conference paper <ref type="bibr" target="#b22">(Sarfraz and Stiefelhagen (2015)</ref>) including a more thorough evaluation and analysis of the method. We will start by discussing some of the related work in section 2. Section 3 will detail the presented approach. We will conclude in section 5 after presenting detailed experiments, results and implementation details in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>One of the very first comparative studies on visible and thermal face recognition was performed by <ref type="bibr" target="#b24">Socolinsky and Selinger (2002)</ref>. They concluded that "LWIR thermal imagery of human faces is not only a valid biometric, but almost surely a superior one to comparable visible imagery." A good survey on single model and cross modal face recognition methods can be found in <ref type="bibr" target="#b26">Zhou et al (2014)</ref>.</p><p>In the cross-modal (infra-red-visible) face recognition scenario, most of the earlier efforts focus only in the NIR to Visible matching. One of the first investigation by <ref type="bibr" target="#b25">Yi et al (2007)</ref> uses Linear discriminant analysis (LDA) and canonical correspondence analysis to perform linear regression between NIR and visible images. A number of approaches build on using local feature descriptors to represent the face. <ref type="bibr" target="#b16">Liao et al (2009)</ref> first used this approach on NIR to visible face recognition by processing face images with a difference of Gaussian (DoG) filter, and encoding them using multiblock local binary patterns. Gentle AdaBoost feature selection was used in conjunction with LDA to improve the recognition accuracy. <ref type="bibr" target="#b11">Klare and Jain (2010)</ref> followed this work on NIR to visible face recognition by also incorporating SIFT feature descriptors and an LDA scheme. <ref type="bibr" target="#b13">Lei and Li (2009)</ref> applied coupled spectral regression for NIR to visible recognition. Few methods have also focused on SWIR-to-visible face recognition <ref type="bibr" target="#b21">Ross and Hornak (2010)</ref>, <ref type="bibr" target="#b18">Nicolo and Schmid (2012)</ref>. NIR or SWIR to visible face matching produces relatively better results as both the modalities are very similar because of the small spectral gap. Because of their limited use in the night-time surveillance applications, a much needed research focus is required in the thermal to visible matching domain.</p><p>Only recently some interest in the thermal to visible face matching has emerged. In the thermal domain, most of these methods are evaluated in the MWIR to visible scenario. These methods employ similar techniques based on local features as in NIR to visible. A nice departure was proposed by <ref type="bibr" target="#b14">Li et al (2008)</ref> which is the only known method to perform recognition by trying to synthesize an equivalent visible image from the thermal counterpart. They provided some visual results and evaluated the method in a small hand collected data indicating an unsatisfactory performance. Another interesting local feature based approach has been put forward by <ref type="bibr" target="#b12">Klare and Jain (2013)</ref>, using local kernel prototypes. Their idea is to represent the two modalities by storing only similar vectors as prototypes during training using a similarity kernel space termed as prototype random subspace. They tested the method in different cross modal scenarios including MWIR to visible face recognition. <ref type="bibr" target="#b0">Bourlai et al (2012)</ref> proposed a MWIR-to-visible face recognition system based on simple preprocessing and feature matching. More recently <ref type="bibr" target="#b9">Hu et al (2014b)</ref> used a game theoretic based partitioning of the images to extract LBP type features and used SVM kernel matching for MWIR to visible face recognition achieving good results. A similar performance on the same dataset has been obtained in a very recent proposal <ref type="bibr" target="#b10">Hu et al (2015)</ref> using partial least square 'PLS' based mapping between the MWIR and visible domain. MWIR sensors acquire higher resolution images than the LWIR. They, however, also operate at lower emissivity response to sense than the LWIR. The most difficult acquisition scenario is the usage of LWIR thermal images. Because of the long wave range sensing in LWIR, the images produced are usually of considerably lower resolution making it more challenging to match. On the other hand, pertaining to their higher emissivity sensing range they are also well suited to operate in total darkness. Attempts have been made to address face matching in the LWIR to visible domain. <ref type="bibr" target="#b4">Choi et al (2012)</ref> presented a PLS based regression framework to model the large modality difference in the PLS latent space. The most recent and best state-of-the-art results on the same dataset have been achieved by <ref type="bibr" target="#b10">Hu et al (2015)</ref>. They used a discriminant PLS based approach by specifically building discriminant PLS gallery models for each subject by using thermal cross examples. They achieved a rank-1 identification of 50.4% using two gallery images per subject and testing against all the probe LWIR thermal images. <ref type="bibr" target="#b2">Chen and Ross (2015)</ref> put forward an interesting approach centred on learning common subspaces between visible and thermal facial images. The common subspaces are learnt via successive subspace learning process using factor analysis and common discriminant analysis on the image patches from both the domains. The projected vectors on these learnt subspaces are then directly matched. They provide convincing results on a private dataset. Finally, for facial feature detection in thermal images, <ref type="bibr" target="#b17">Mostafa et al (2013)</ref> presented a good method based on Haar features and Adaboost. They also performed face recognition experiments, but used both the visible and thermal modalities in the gallery and probe set.</p><p>Deep and large neural networks have exhibited impressive results for visible face recognition. Most notably, Facebook <ref type="bibr" target="#b24">(Taigman et al (2014)</ref>) and Google <ref type="bibr" target="#b23">(Schroff et al (2015)</ref>) showed excellent results using convolutional neural network to learn the face representation. Hu et al (2014a) employ a feed forward network for metric-learning using pairs of same and different visible images. In contrast, we use a network to learn the non-linear regression projections between visible and thermal data without trying to learn any distance metric or image representation. A similar objective is employed in the domain adaptation based approaches e.g., <ref type="bibr" target="#b6">Ganin and Lempitsky (2014)</ref>, where the idea is to use the knowledge of one domain to aid the recognition in another image domain. Our model, while trying to learn a non-linear regression mapping, is simple and different from these in terms of the objective function and the way the network is used. We use a fully-connected feed forward network with the objective of directly learning a perceptual mapping between the two different image modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Deep Perceptual Mapping (DPM)</head><p>The large perceptual gap between thermal and visible images is both because of the modality and resolution difference. The relationship between the two modalities is highly non-linear and difficult to model. Previous approaches try to use non-linear function mapping e.g., using the kernel trick or directly estimating the manifold where the two could lie closer together. Since such a manifold learning depends on defining the projections based on a given function, they are highly data dependent and require a good approximation of the underlying distribution of the data. Such a requirement is hard to meet in practice, especially in the case of thermal-visible face recognition. Here, deep neural networks can learn, to some extent, the non-linear mapping by adjusting the projection coefficients in an iterative manner over the training set. As the projection coefficients are learned and not defined, it may be able to capture the data specific projections needed to bring the two modalities closer together. Based on this foresight we attempt to learn such projections by using a multilayer fully-connected feed forward neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The DPM Model</head><p>The goal of training the deep network is to learn the projections that can be used to bring the two modalities together. Typically, this would mean regressing the representation from one modality towards the other. Simple feed forward neural network provides a good fitting architecture for such an objective. We construct a deep network comprising N + 1 layers with m (k) units in the k-th layer, where k = 1, 2, ? ? ? , N . For an input of x ? R d , each layer will output a non-linear projection by using the learned projection matrix W and the non-linear activation function g(?). The output of the k-th hidden</p><formula xml:id="formula_0">layer is h (k) = g(W (k) h (k?1) + b (k) ), where W (k) ? R m (k) ?m(k?1) is the pro- jection matrix to be learned in that layer, b (k) ? R m (k) is a bias vector and g : R m (k) ? R m (k) is the non-linear activation function. Note, h (0) is the input.</formula><p>Similarly, the output of the most top level hidden layer can be computed as:</p><formula xml:id="formula_1">H(x) = h (N ) = g(W (N ) h (N ?1) + b (N ) )<label>(1)</label></formula><p>where the mapping H :</p><formula xml:id="formula_2">R d ? R m (N )</formula><p>is a parametric non-linear perceptual mapping function learned by the parameters W and b over all the network layers. Since the objective of the model is to map one modality to the other, the final output layer of the network employs a linear mapping to the output in Equation 1 to obtain the final mapped output:</p><formula xml:id="formula_3">x = s(W (N +1) H(x))<label>(2)</label></formula><p>where W (N +1) ? R m (N ) ?d and s is the linear mapping. Equation 2 provides the final perceptually mapped outputx for an input x. To determine the parameters W and b for such a mapping, our objective function must seek to minimize the perceptual difference between the visible and thermal training examples in the least mean square sense. We, therefore, formulate the DPM learning as the following optimization problem.</p><p>arg min</p><formula xml:id="formula_4">W,b J = 1 M M i=1 (x i ? t i ) 2 + ? N N k=1 ( W (k) 2 F + b (k) 2 2 )<label>(3)</label></formula><p>The first term in the objective function corresponds to the simple squared loss between the network outputx given the visible domain input and the corresponding training example t from the thermal domain. The second term in the objective is the regularization term with ? as the regularization parameter. W F is the Frobenius norm of the projection matrix W. Given a training set X = {x 1 , x 2 , ? ? ? , x M } and T = {t 1 , t 2 , ? ? ? , t M } from visible and thermal domains respectively, the objective of training is to minimize the function in equation 3 with respect to the parameters W and b.</p><p>The DPM Training: There are some important design considerations for a meaningful training of the DPM network to provide an effective mapping from visual to thermal domain. First, the model is sensitive to the number of input and output units. A high dimensional input would require a very high amount of training data to be able to reasonably map between the two modalities. We propose to use the densely computed feature representations from overlapping small regions in the images. This proves very effective, as not only the model is able to capture the differing local region's perceptual differences well but also alleviate the need of large training images and nicely present the input in relatively small dimensions. The training set X and T, then, comprises of these vectors coming from the corresponding patches from the images of the same identity. Note, only using the corresponding images of the same identity ensures that the model will only learn the differences present due to the modality as the other appearance parameters e.g.identity, expression, lighting etc., would most likely be the same. The network is trained, over the training set, by minimizing the loss in Equation 3 w.r.tthe parameters. <ref type="figure" target="#fig_0">Figure 1</ref> encapsulates this process. By computing the gradient of the loss J in each iteration, the parameters are updated by standard back projection of error using stochastic gradient descent 'SGD' method of <ref type="bibr" target="#b7">Glorot and Bengio (2010)</ref>. For the non-linear activation function g(z) in the hidden layers, we have used the hyperbolic tangent 'tanh' as it worked better than other functions e.g.sigmoid and ReLU on our problem. tanh maps the input values between -1 to 1 using g(z) = e 2z ?1 e 2z +1 . Before commencing training, the parameters are initialized according to a uniform random initialization. As suggested in <ref type="bibr" target="#b7">Glorot and Bengio (2010)</ref>, the bias b is initialized as 0 and the W for each layer is initialized using the</p><formula xml:id="formula_5">uniform distribution W (k) ? U ? ? 6 ? m (k) +m (k?1) , ? 6 ? m (k) +m (k?1) , where m (k)</formula><p>is the dimensionality (number of units) of that layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Thermal-Visible Face Matching</head><p>After obtaining the mapping from visible to thermal domain, we can now pose the matching problem as that of comparing the thermal images with that of mapped visible data. Specifically, the mapped local descriptors from overlapping blocks of the visible gallery images are concatenated together to form a long vector. The resulting feature vector values are L 2 -normalized and then matched with the similarly constructed vector directly from the probe thermal image. Note that, only visible gallery images are mapped through the learned projections using Equations 1 and 2 whereas the densely computed vectors from thermal images are directly concatenated into the final representation vector.</p><p>The presented set-up is ideal for the surveillance scenario as the gallery images can be processed and stored offline while at test time no transformation and overhead is necessary. As we will show in the next section, without any computational overhead the probes can be matched in real-time. Note, however, the presented DPM is independent of the direction of mapping. We observed only slight performance variations using the opposite thermal to visible mapping.</p><p>The identity of the probe image is simply determined by computing the similarity with each of the stored gallery image vectors and assigning the identity for which the similarity is maximum. Since the vectors are L 2 -normalized, the cosine similarity can simply be computed by the dot product.</p><formula xml:id="formula_6">d(x i , t j ) =x i ? t j ?i = 1, 2, ? ? ? , G<label>(4)</label></formula><p>where t j is the j-th constructed probe thermal image vector and G is the total number of stored gallery vectors. Computationally, this is very efficient as at the test time, each incoming probe image can be compared with the whole gallery setX by a single matrix-vector multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>In this section we present the evaluation of the proposed approach on the difficult thermal (LWIR &amp; MWIR)-to-visible face recognition scenario. We report evaluations using typical identification and verification settings. The evaluation results, assessing the proposed DPM mapping, and the implementation details are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Database Description</head><p>Only few publicly available datasets include thermal and corresponding visible facial acquisitions. Among these, we use three very challenging datasets to evaluate the performance of the proposed cross-modal face matching method: UND Collection X1, Carl Database, and NVESD dataset. Each database was collected with different sensors containing image acquisitions under different experimental conditions e.g., different lighting, facial expressions, time-lapse, physical exercise and subject-to-camera range.</p><p>UND Collection X1: University of Notre Dame's UND collection X1 <ref type="bibr" target="#b3">(Chen et al (2005)</ref>) is a challenging LWIR and visible facial corpus. The dataset was collected using a Merlin uncooled LWIR sensor and a high resolution visible color camera. The resolution of the visible images is 1600 ? 1200 pixels and the acquired LWIR thermal images is 312 ? 239 pixels. This depicts the challenge not only due to the large modality gap but also due to a large resolution difference. The dataset includes images in three experimental settings: expression (neutral, smiling, laughing), lighting change (FERET lighting and mugshot) and time-lapse (subjects acquired in multiple sessions over time). Following the protocol used by the recently published competitive proposals on this dataset Hu et al <ref type="formula" target="#formula_1">(2015)</ref>; <ref type="bibr" target="#b4">Choi et al (2012)</ref>, we use all the available images in these three settings of the thermal probes to match against a single or multiple available high resolution visible gallery image/s. The dataset contains 4584 images of 82 subjects distributed evenly in visible and thermal domain. To compare our results, we use the exact same training and testing partitioning as used in the previous methods. The pool of subjects is divided into two equal subsets: visible and thermal images from 41 subjects are used for the gallery and probe images (partition A), while the remaining 41 subjects were used to train the DPM model. Note that the training and test sets are disjoint both in terms of images and subject's identities. All thermal images from partition-A are used as probes to compute recognition performance. The images are aligned based on provided eye and mouth locations and facial regions are cropped to 110 ? 150 pixels. <ref type="figure" target="#fig_1">Figure 2</ref> shows some sample visible and corresponding thermal images of one subject in the database.</p><p>Carl Database: The Carl dataset (Espinosa-Dur et al <ref type="formula" target="#formula_1">(2013)</ref>) is a relatively recently collected dataset containing images of 41 subjects acquired in visible, near-infrared and thermal (LWIR) spectrum. The dataset collects images under three illumination conditions (natural, artificial and infrared lighting) in 4 different sessions over time. The images primarily contain challenging appearance variations due to different illuminations, expressions, time-lapse and low image resolution. We use the thermal and visible corpus of the data which contains images across different lighting and acquisitions sessions totalling to 41x5x3x4x2=4920 images, for 41 subjects, 3 illumination conditions (5 image/subject/illumination), 4-sessions in both visible and thermal domain. The resolution of the visible images is 640 ? 480 and thermal (LWIR) images is 160 ? 120 pixels. The segmented face images (output of a face detector) are provided by the authors. The face images, resized to 100 ? 145 pixels, are not aligned w.r.t to eyes or nose position and therefore adds to the recognition challenge pertaining to misalignment errors. Since no earlier studies evaluate the full scale dataset in thermal to visible scenario, we provide the results to benchmark the full dataset in the following setting. We split the dataset equally with respect to the subjects. We use images of 20 subjects as the training set (1200 visible and 1200 thermal) and the rest as the probe. The training and test sets are, therefore, disjoint in terms of both subject's identities and images. Note, the probe set contains all the available thermal images of 21 subjects (from 3 illumination conditions in 4 sessions) totalling to 1260 images. The gallery set comprises visible images (1/subject and 2/subject from session 1, natural illumination) of all the 41 subjects. This protocol will ensure an effective evaluation on the challenging thermal image variations due to illumination, time lapse and expressions against a large gallery.</p><p>NVESD Dataset: The NVESD dataset <ref type="bibr" target="#b1">(Byrd (2013)</ref>) was acquired as a joint effort between the Night Vision Electronic Sensors Directorate of the U.S. Army Communications-Electronics Research and the U.S. Army Research Laboratory (ARL). We use the thermal subset of the data containing images in both MWIR and LWIR spectrum. The NVESD dataset primarily examined two conditions in both spectrum: physical exercise (fast walk) and subject-tocamera range (1 m, 2 m, and 4 m). The dataset contains images of a total of 50 subjects, where images of 25 subjects are acquired in both before exercise and after exercise condition across all three ranges. The image resolution for the thermal sensors (MWIR and LWIR) is considerably higher at 640 ? 480 pixels. The total number of thermal images in NVESD is 900 (450 MWIR, 450 LWIR) and the visible images are 450. The training set, correspond to subjects whose images are only acquired in before exercise condition, comprises 300 thermal images (150 MWIR, 150 LWIR) and 150 visible images. We train two separate models for MWIR-visible and LWIR-visible mapping using these training images.</p><p>Following the protocol in recently published state-of-the-art results on this dataset by <ref type="bibr" target="#b10">Hu et al (2015)</ref>, we use the 25 subject subset (before and after exercise across all three ranges) as the probe/test set to examine the performance. The total number of probe thermal images is 600 (300 MWIR &amp; 300 LWIR). The gallery set comprises the visible images of all the 50 subjects from before exercise condition at 1 m range. This protocol helps evaluate the performance in the presence of changed thermal signature (due to exercise) and subject-to-camera distance against the standard mugshot visible gallery image/s. The face images are normalised and aligned to 272 ? 322 pixels as in <ref type="bibr" target="#b10">Hu et al (2015)</ref> in order to have a fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Here we provide the parameters used and related details for implementation. To compare the results to previous methods, we use a similar preprocessing for the thermal and visible images. We use the median filtering to remove the dead pixels in thermal images, then zero-mean normalization and afterwards used Difference of Gaussian 'DoG' filtering to enhance edges and to further reduce the effect of lighting, <ref type="figure" target="#fig_2">Figure 3</ref> depicts aligned and preprocessed images of a subject. Dense features are then pooled from overlapping blocks in the image. The recognition performance varies between 2-5% based on the descriptor used and the block size/overlap. A large overlap will produce more redundant information causing the network to saturate early while the block size effects the amount of structural information the network can still preserve while trying to map only the perceptual/modality difference. Based on our extensive evaluations we provide here the parameters that work well in practice. We have used a block size of 20 ? 20 with a stride of 8 pixels at two-scales (Gaussian smoothed with ? = 0.6 and ? = 1) of the image. As many prior methods e.g. <ref type="bibr" target="#b12">Klare and Jain (2013)</ref>; <ref type="bibr" target="#b9">Hu et al (2014b</ref><ref type="bibr" target="#b10">Hu et al ( , 2015</ref>; <ref type="bibr" target="#b4">Choi et al (2012)</ref> overwhelmingly stated the better performance of both the SIFT and HOG features in the thermal-visible face recognition. We experimented with both SIFT and HOG feature descriptors. However, here we present results with SIFT features as HOG produces 2 ? 3% inferior results than SIFT.</p><p>For the DPM model training, the number of units in the input/output and the hidden layers is important and varies the result. We use PCA to decorrelate and reduce the features to 64 dimensions. Each descriptor is further embedded with the block center position (x, y), measured from the image center, to have the spatial position intact, this helps the model to better learn the local region specific transformations in the hidden layers. The number of units in the input/output layers are, therefore, 66.</p><p>For the DPM network, we obtained better results with a minimum of 3layers (N = 3 in Equations 1 and 2). Although, for comparison, we also report performance with using N = 2 i.e.one hidden layer. As our results show, the DPM mapping is highly effective even with this shallow DPM configuration. The minimum number of units in the hidden layers, to ensure good results, is set empirically by using different combinations. Here, we report results using 200 units in each of the two hidden layers for the deep configuration. While in the case of shallow (1 hidden layer configuration), 1000 hidden units in the single layer provide close results. It is worth noticing, that the the results vary slightly about 1-4% using different number of hidden units in the range 200 ? 1000 in a layer. Finally the DPM is trained by pooling over 1 million patch vectors from the training set for each of the three datasets separately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We provide results of our evaluations using the settings described before. We first establish two baselines that would directly enable us to compare and appreciate the power of the proposed model. Along with these baselines we also compare the results of the proposed model with the previously published state of the art methods. Baseline-1: As baseline-1 we use the same concatenated SIFT features without the DPM mapping. This would allow us to measure how much the learned mapping is contributing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline-2:</head><p>As baseline-2 we use a state-of-the-art manifold learning method to embed the same SIFT vectors in a latent space using Partial Least Square (PLS) analysis. Vectors from both the thermal and visible images are projected through the learned PLS model in the latent space and then matched. The idea of PLS regression is to maximize the covariance between the dependant variable Y and a weighted sum of the independent variable X by finding a weighing vector w. In our application, X and Y corresponds to the the visible and thermal vectors respectively. To obtain the weight matrix the PLS model is built by decomposing the (n ? m) input matrix X ? R m and the response matrix Y ? R m into</p><formula xml:id="formula_7">X = T P T + E Y = U Q T + F<label>(5)</label></formula><p>The (n ? p) matrices T and U are called scores that contain the latent vectors, the (m ? p) matrices P and Q are called loadings and (n ? m) matrices E and F are the residuals. By using a greedy algorithm, we can iteratively obtain a set of weight vectors stored in a matrix W . At the end of each iteration, the matrices X and Y are deflated by subtracting their rank-one approximations based on t and u until the desired number of latent vectors p is obtained. More details can be seen in <ref type="bibr" target="#b20">Rosipal and Kr?mer (2006)</ref>. After obtaining W , we can compute the latent regression vectors from X to Y or vice versa by Y = XB v and X = Y B t where the regression model B is B v = W (P T W ) ?1 and B t = W (Q T W ) ?1 for the corresponding visible and thermal projections.</p><p>To ensure a proper baseline we perform PLS directly on the m = 66 dimensional SIFT vectors space. The number of latent vectors is found by cross validation on the training set and is set to p = 20 in all of our experiments. Each of the 66 dimensional SIFT vector from visible and thermal image is projected to this p = 20 dimensional latent space using the procedure mentioned above. All the projected vectors from an image are then concatenated and matched directly, similar to the procedure we adopt in baseline-1 and with the proposed DPM mapping.</p><p>Note that the previously published state of the art methods e.g., <ref type="bibr" target="#b10">Hu et al (2015)</ref> also uses a similar PLS framework. While we also compare with their method, the difference in our PLS-baseline is apparent not only in the choice of feature vectors but also the way PLS model is learned. <ref type="bibr" target="#b10">Hu et al (2015)</ref> use a discriminant PLS framework by building specifically discriminant PLS models for each identity in the database.</p><p>With these baselines one can appreciate the benefit of using the proposed DPM mapping by directly comparing the results on the same features using 1) direct matching and 2) using a state of the art regression mapping in a latent space obtained by PLS by exploiting the same amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Identification Evaluation</head><p>Results on UND-X1: <ref type="table">Table 1</ref> presents the comparison results (Rank-1 identification score) with the baseline and the best published state-of-the-art re- sults on the UND-X1 dataset. We present results using three different gallery settings. The most difficult and restricted setting with only one neutral visible image per subject in the gallery, using two visible images per subject in the gallery (the first two images, neutral and with smiling expression) and using all the available visible images per subject as gallery. Note that all the available thermal images/subject are used as probes. The number of images for each subject in the thermal probe set varies from a minimum of 4 to a maximum of 40 depicting appearance variations due to expressions, lighting and time-lapse. As the results show, we improve the state-of-the-art best published results of <ref type="bibr" target="#b10">Hu et al (2015)</ref> by 9% in the 1-visible per subject gallery setting and more than 10% in 2 per subject and all-available per subject cases. We also report the results using shallow DPM configuration (with 1-layer). Our results show that even this shallow DPM configuration well surpasses the best published results on this dataset. <ref type="figure" target="#fig_3">Figure 4</ref> presents the cumulative match characteristics 'CMC' curves to measure the rank-wise identification performance of the presented method. It shows the effectiveness of the proposed DPM mapping in comparison with the baseline-1 (same features without the DPM mapping). Results on Carl Database: The Carl dataset is the most challenging dataset because of the present strong facial appearance variations due to different illumination changes , time lapse, expressions, alignment errors and very low resolution LWIR images (160?120 pixels). To benchmark the full scale dataset, we present the identification results in a similar varying visible images/subject gallery setting. As mentioned earlier, the probe set contains all the thermal (LWIR) images from 21 test subjects and matched against a gallery contain-  ing visible images of all the 41 subjects. <ref type="table">Table 2</ref> presents the obtained Rank-1 scores of both the baseline and the presented DPM mapping across different gallery image settings. Note while the Rank-1 scores on Carl dataset, despite a more difficult imaging scenario than UND-X1, are comparable in the 1 image per subject and 2 image per subject gallery setting. The relatively low Rank-1 score when using all available visible images per subject in the gallery is understandable considering the large number of gallery images (60 visible images/subject =2460 images form 41 subjects), where these visible images contain very difficult illumination variations (natural light, artificial light and infra-red illumination). We also tested the effect of overall gallery size i.e., only having visible images of the 21 test subjects in the gallery. Here, the Rank-1 scores using DPM improves as expected. We obtained an accuracy of 66.58% with the 1-image per subject, 72.92% with 2-images per subject and 82.50% with all available images per subject gallery setting. <ref type="figure" target="#fig_4">Figure 5</ref> presents CMC curves to measure the rank-wise identification performance of the presented method and the baseline-1 on Carl dataset. Results on NVESD Dataset: The NVESD dataset helps evaluate the method on both MWIR and LWIR thermal domains. Given the image acquisitions from relatively better thermal sensors (640 ? 480 pixel thermal images), the evaluations on NVESD resulted in much better performance. We evaluate and compare the Rank-1 identification results with that of the state-of-the-art results of <ref type="bibr" target="#b10">Hu et al (2015)</ref>, using the same test protocol.</p><p>We primarily evaluate the effect of exercise on the thermal signatures of the subjects across different subject-camera ranges <ref type="bibr">(1m, 2m, and 4m)</ref>. For this, the 25 subject subset both from MWIR and LWIR thermal sets is used as probe and matched against 2-visible images per subject in the gallery. The two visible-image per subject gallery images are the neutral mugshots, acquired at 1m range in before exercise condition. Following <ref type="bibr" target="#b10">Hu et al (2015)</ref>, we prepare 6 probe sets for each MWIR and LWIR thermal test sets (before exercise and after exercise condition ? three subject-to-camera ranges). Each probe set contains 50 images (2/subject) in each of the specific condition-range pair  <ref type="table">Table 3</ref> Rank-1 Identification accuracy (%) on NVESD: Using Before Exercise and After Exercise thermal probe images at each of the three distance ranges. (a) Using MWIR probe images, (b) using LWIR probe images. Gallery includes 2 visible image/subject acquired in before exercise condition at 1 m range of all the 50 subjects. Parenthesis (?) depict accuracies of the current best published results of <ref type="bibr" target="#b10">Hu et al (2015)</ref>, Our results are depicted in bold.</p><p>test. <ref type="table">Table 3</ref> presents the Rank-1 scores for MWIR-Visible and LWIR-Visible scenario of this experiment. As the results depict, DPM mapping improves the state-of-the-art by a considerable margin, especially at longer ranges. An intriguing result is the better accuracy at 1m and 2m range of after exercise condition than before exercise. A closer look at the misclassified image/s of before-exercise probe sets at these ranges reveals that the effect seems to be with facial expressions in these pairs causing the similarity scores falling slightly below the rank-1 threshold. Given that the training has not seen the images in after-exercise condition, this also implies that the DPM mapping is not effected by the small thermal change present in the images due to this. For MWIR-visible scenario we obtained on average a Rank-1 score of 88.6% versus an average of 70.6% of <ref type="bibr" target="#b10">Hu et al (2015)</ref>. We, therefore, improve the previous results by 18% on the NVESD's MWIR-visible dataset. Similarly for LWIR-visible, we obtained on average Rank-1 score of 88% versus an average of 53% of <ref type="bibr" target="#b10">Hu et al (2015)</ref>. Thus, on the NVESD's LWIR-visible dataset, we improve the result by 35%. These results show that the learned DPM mapping is relatively stable and effective against the thermal signature change due to exercise while performs significantly better on image acquisition at longer range. <ref type="table">Table 4</ref> and <ref type="table">Table 5</ref> provide the overall Rank-1 accuracy of the Baselines and the DPM in different visible-image/subject in the gallery settings on MWIR-visible and LWIR-visbile NVESD datasets. <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="figure" target="#fig_6">Figure 7</ref> provide the similar rank-wise performance of our method and compares it with the baseline-1 features in the 1-visible per subject and the all-available visible per subject gallery setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Verification Evaluation</head><p>We also evaluate the verification accuracy of such cross-modal scenario. We report here the results by using 2 visible images/subject in the gallery on UND-X1, Carl and NVESD datasets. Given the size of our thermal probe set, this amounts to having 1792 genuine and 71680 imposter attempts on UND-X1, 2520 genuine and 100800 imposter attempts on Carl database and 600 genuine and 29400 imposter attempts on both NVESD's LWIR and MWIR datasets.   Figures 8, 9, 10 and 11 present the ROC curves measuring the verification performance gain over the baseline-1 using the DPM mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Effect of modality gap</head><p>Finally, we present the experiment to measure the effect of the modality gap. Keeping everything fixed i.e., using the same baseline features and settings, we compute the Rank-1 identification score within the same modality. We use one-thermal image per subject (the neutral frontal image) to form the thermal gallery and test against all the remaining thermal images as probe. This is the same setting as we have used in the cross-modal thermal-visible case. <ref type="table">Table 6</ref> Fig. 8 Verification performance on UND X1: all thermal probes with 2 visible image/sub in gallery.   compares the results of Thermal-Thermal and Thermal-Visible identification on all three datasets to quantify the effect of modality gap.</p><p>On UND-X1, we obtain a Rank-1 score of 89.7% in the Thermal-Thermal identification scenario. While the rank-1 identification in the corresponding Thermal-Visible scenario (using the same baseline features) is 30.3%. This amounts to performance drop, purely due to modality change, of about 59%. This reflects the challenging nature of the problem and the existing research gap to tackle this. With DPM on the same features, the performance is improved by 25%. This amounts to bridging the existing modality gap of 59% by more than 40% on UND-X1. Similar observations can be made by interpreting the results on other datasets as given in <ref type="table">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Computational Time</head><p>Training the DPM on 12 cores 3.2-GHz CPU takes between 1 ? 1.5 hours on MATLAB. Preprocessing, features extraction and mapping using DPM only takes 45ms for one image. This is even less in the testing case since no mapping is required for thermal images. At test time identifying one probe only takes 35ms. Since we are using just the dot product between the extracted probe vector and the gallery set, this is therefore very fast and capable of running in real-time at ? 28 fps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion &amp; Conclusions</head><p>While providing quantitative evidence of the effectiveness of the proposed approach, we can also compare our results qualitatively with some other methods that evaluate the thermal-visible face recognition on similar private datasets. <ref type="bibr" target="#b0">(Bourlai et al 2012)</ref> reported results on a MWIR-visible face recognition task using a 39 subject dataset with four visible images per subject and three MWIR images per subject as probes with 1024 ? 1024 pixel resolution. They reported a rank-1 performance of 53.9%. Our result of 86% on the NVESD MWIR-visible dataset with just one visible image per subject in the gallery can be qualitatively compared to these results. Similarly, <ref type="bibr" target="#b2">Chen and Ross (2015)</ref> evaluated their method on a subset of the Carl dataset with 5 visible image per subject in the gallery and 5 thermal image per subject as probe. They, however, evaluate in a cross dataset scenario i.e., subspaces learned on another similar private dataset and tested on the Carl data subset. They reported a best rank-1 performance of 75.6%. Compared to this, we evaluate the full scale Carl dataset with 60 thermal images per subject in the probe set. Our results with just 2 visible image per subject and multiple visible images per subject in the gallery, as depicted in <ref type="table">Table 2</ref>, provide a qualitative motivation for the effectiveness of the proposed method. Finally, in a very recent proposal (Riggan et al <ref type="formula" target="#formula_1">(2016)</ref>) the authors evaluate their own implementation of our presented DPM method on polarimetric thermal to visible face recognition problem achieving one of the highest performance on this dataset. This further substantiates the applicability of the presented method on a general cross modal image matching scenario. Given the very high recognition ability of Convolutional Neural Network (CNN) features in the conventional visible face recognition, a possible future direction to further improve these results is to directly train a CNN model. On the cross-modal face matching problem, the CNN model can be trained in a similar metric learning fashion as presented by Google's FaceNet <ref type="bibr" target="#b23">(Schroff et al (2015)</ref>) or Oxford's VGG deep face model <ref type="bibr" target="#b19">(Parkhi et al (2015)</ref>). The current limitation for a meaningful training of such a model is the lack of enough thermal facial images. It is hoped that, provided, the vast and demanding application of cross-modal face matching and availability of low-priced accessible thermal sensors, more and more thermal facial data can be acquired. Note, a direct application of the current pre-trained CNN models on the thermal images do not yield meaningful results. These models are trained on three channel RGB colour images where as the thermal images are single channel 16-bit images. Our initial investigations to get a baseline using the pre-trained CNN models of <ref type="bibr" target="#b23">(Schroff et al (2015)</ref> and <ref type="bibr" target="#b19">Parkhi et al (2015)</ref>), by just replicating the thermal image in three channels, did not provide acceptable results. Here, however, a similar perceptual mapping strategy may be further investigated by training a CNN using only the image patches from both domains. Such a direction may provide interesting results even with relatively less training data.</p><p>Conclusively, thermal-visible face recognition is a very difficult problem due to the inherent large modality difference. Our presented method is very effective and has benefits for many related applied computer vision and domain adaptation problems from image matching, detection to recognition. Similarly it is also very attractive for remote sensing applications, where it is a common problem to register and match images from different modalities (e.g., coming from different satellites). This can be used as a very effective prior step to bridge the modality and/or large resolution difference before using the representation vectors in any common learning scheme. The presented DPM approach is very useful, easy to train, and real-time capable due to little computational overhead and it provides a practical solution for a large related application industry.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Deep Perceptual Mapping (DPM): densely computed features from the visible domain are mapped through the learned DPM network to the corresponding thermal domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Problem: Matching the thermal to the stored high resolution visible image. A wide modality gap exists between the visible and the thermal images of the same subject. Thermal images depict typical image variations, present, due to lighting, expressions and time-lapse.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Aligned and preprocessed images of the corresponding visible and thermal images of a subject.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4</head><label>4</label><figDesc>Rank-wise score on UND X1: comparison of baseline and DPM performance with 1-visible image/sub and all available visible images/sub in the gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5</head><label>5</label><figDesc>Rank-wise score on Carl Dataset: comparison of baseline and DPM performance with 1-visible image/sub and all available visible images/sub in the gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6</head><label>6</label><figDesc>Rank-wise score on NVESD MWIRvisible dataset: comparison of baseline and DPM performance with 1-visible image/sub and all available visible images/sub in the gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7</head><label>7</label><figDesc>Rank-wise score on NVESD LWIRvisible dataset: comparison of baseline and DPM performance with 1-visible image/sub and all available visible images/sub in the gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 9</head><label>9</label><figDesc>Verification performance on Carl Database: all thermal probes with 2 visible image/sub in gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 10</head><label>10</label><figDesc>Verification performance on NVESD MWIR dataset: all thermal probes with 2 visible image/sub in gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 11</head><label>11</label><figDesc>Verification performance on NVESD LWIR dataset: all thermal probes with 2 visible image/sub in gallery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Effect of Modality gap: Performance with 1 Gallery image/subjectTherm-Therm Therm-Vis Therm-Vis(DPM) Modal-gap bridged</figDesc><table><row><cell>UND-X1</cell><cell>89.4</cell><cell>30.3</cell><cell>55.3</cell><cell>? 42%</cell></row><row><cell>Carl Dataset</cell><cell>61.7</cell><cell>26.5</cell><cell>56.3</cell><cell>? 84%</cell></row><row><cell>NVESD MWIR</cell><cell>98.6</cell><cell>72</cell><cell>86</cell><cell>? 48%</cell></row><row><cell>NVESD LWIR</cell><cell>97</cell><cell>61</cell><cell>82.6</cell><cell>? 40%</cell></row><row><cell cols="5">Table 6 Performance drop due to Modality gap: Rank-1 identification using 1 im-</cell></row><row><cell cols="5">age/subject as gallery in Thermal-Thermal and Thermal-Visible matching using baseline</cell></row><row><cell>features.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A study on using mid-wave infrared images for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bourlai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hornak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Defense, Security, and Sensing, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Preview of the newly acquired nvesd-arl multimodal face database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Byrd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SPIE</title>
		<meeting>SPIE</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8734</biblScope>
			<biblScope unit="page" from="8734" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching thermal to visible face images using hidden factor analysis in a cascaded subspace learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ross</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2015.06.021</idno>
		<ptr target="http://www.sciencedirect.com/science/article/pii/S0167865515001932" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters DOI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ir and visible light face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Bowyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="332" to="358" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Thermal to visible face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Defense, Security, and Sensing, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page">711</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A new face database simultaneously acquired in visible, near-infrared and thermal spectrums</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Espinosa-Dur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Faundez-Zanuy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mekyska</surname></persName>
		</author>
		<idno type="DOI">10.1007/s12559-012-9163-2</idno>
		<ptr target="http://dx.doi.org/10.1007/s12559-012-9163-2" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="135" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>arXiv:14097495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1875" to="1882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Thermal-to-visible face recognition using multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPIE Defense+ Security, International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">909</biblScope>
			<biblScope unit="page">110</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Thermal-to-visible face recognition using partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Optical Society of America &apos;JOSA&apos; A</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="431" to="442" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition: Matching nir to visible light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2010 20th International Conference on</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1513" to="1516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition using kernel prototype similarities. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1410" to="1422" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Coupled spectral regression for matching heterogeneous faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1123" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hallucinating faces from thermal infrared images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="465" to="468" />
		</imprint>
	</monogr>
	<note>Image Processing</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Illumination invariant face recognition using nearinfrared images. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="627" to="639" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Heterogeneous face recognition from local structures of normalized appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Biometrics</title>
		<imprint>
			<biblScope unit="page" from="209" to="218" />
			<date type="published" when="2009" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Face recognition in low resolution thermal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mostafa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hammoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1689" to="1694" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long range cross-spectral face recognition: Matching swir against visible light images. Information Forensics and Security</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nicolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1717" to="1726" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimal feature learning and discriminative framework for polarimetric thermal to visible face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Nathaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shuowen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>WACV</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>British Machine Vision Conference Riggan BS</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Overview and recent advances in partial least squares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosipal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kr?mer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Subspace, latent structure and feature selection</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="34" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Cross-spectral face verification in the short wave infrared (swir) band</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tbnka</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bcl</forename><surname>Hornak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep perceptual mapping for thermal to visible face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sarfraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno>arXiv:150303832</idno>
		<title level="m">Facenet: A unified embedding for face recognition and clustering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A comparative analysis of face recognition performance with visible and thermal infrared imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Socolinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dtic Document Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
	<note>Deepface: Closing the gap to human-level performance in face verification</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face matching between near infrared and visible light images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Biometrics</title>
		<imprint>
			<biblScope unit="page" from="523" to="530" />
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Recent advances on singlemodal and multimodal face recognition: A survey. Human Machine Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Creighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hossny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nahavandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="701" to="716" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
