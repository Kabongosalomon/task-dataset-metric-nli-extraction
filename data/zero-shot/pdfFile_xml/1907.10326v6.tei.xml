<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Han</forename><surname>Lee</surname></persName>
							<email>jinhanlee@hanyang.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Computer Engineering</orgName>
								<orgName type="institution">Hanyang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myung-Kyu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Computer Engineering</orgName>
								<orgName type="institution">Hanyang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Wook</forename><surname>Ko</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Computer Engineering</orgName>
								<orgName type="institution">Hanyang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Suh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronics and Computer Engineering</orgName>
								<orgName type="institution">Hanyang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Big to Small: Multi-Scale Local Planar Guidance for Monocular Depth Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Estimating accurate depth from a single image is challenging because it is an ill-posed problem as infinitely many 3D scenes can be projected to the same 2D scene. However, recent works based on deep convolutional neural networks show great progress with plausible results. The convolutional neural networks are generally composed of two parts: an encoder for dense feature extraction and a decoder for predicting the desired depth. In the encoderdecoder schemes, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, and several techniques such as skip connections or multi-layer deconvolutional networks are adopted to recover the original resolution for effective dense prediction.</p><p>In this paper, for more effective guidance of densely encoded features to the desired depth prediction, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. We show that the proposed method outperforms the state-of-the-art works with significant margin evaluating on challenging benchmarks. We also provide results from an ablation study to validate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth estimation from 2D images has been studied in computer vision for a long time and is nowadays applied to robotics, autonomous driving cars, scene understanding, and 3D reconstruction. Those applications usually utilize, to perform depth estimation, multiple instances of the same scene such as stereo image pairs <ref type="bibr" target="#b39">[40]</ref>, multiple frames from moving camera <ref type="bibr" target="#b33">[34]</ref> or static captures under different lighting conditions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. As depth estimation from multiple observations achieves impressive progress, it naturally leads to depth estimation with a single image since it demands less cost and constraint.</p><p>However, estimating accurate depth from a single im- age is challenging, even for a human, because it is an illposed problem as infinitely many 3D scenes can project to the same 2D scene. To understand geometric configuration from a single image, humans consider use not only local cues, such as texture appearance in various lighting and occlusion conditions, perspective, or relative scales to the known objects, but also global context, such as entire shape or layout of the scene <ref type="bibr" target="#b18">[19]</ref>. After the first learning-based monocular depth estimation work from Saxena et al. <ref type="bibr" target="#b37">[38]</ref> was introduced, considerable improvements have been made along with rapid advances in deep learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. While most of the state-of-the-art works apply models based on deep convolutional neural networks (DCNNs) in supervised fashion, some works proposed semi- <ref type="bibr" target="#b23">[24]</ref> or selfsupervised learning methods which do not entirely rely on the ground truth depth data.</p><p>In the meantime, recent applications based on DCNNs are commonly composed in two parts: encoder for dense feature extraction and decoder for the desired prediction. As a dense feature extractor, very powerful deep networks such as VGG <ref type="bibr" target="#b42">[43]</ref>, ResNet <ref type="bibr" target="#b17">[18]</ref> or DenseNet <ref type="bibr" target="#b19">[20]</ref> are usu-ally adopted. In these networks, repeated strided convolution and spatial pooling layers lower the spatial resolution of transitional outputs, which can be a bottleneck to obtain desired dense predictions. Therefore, a number of techniques, for example, multi-scale networks <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b9">10]</ref>, skip connections <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b46">47]</ref> or multi-layer deconvolutional networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> are applied to consolidate feature maps from higher resolutions. Recently, atrous spatial pyramid pooling (ASPP) <ref type="bibr" target="#b4">[5]</ref> has been introduced for image semantic segmentation, which can capture large scale variations in observation by applying sparse convolutions with various dilation rates. Since the dilated convolution allows larger receptive field size, recent works in semantic segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b49">50]</ref> or depth estimation <ref type="bibr" target="#b11">[12]</ref> do not fully reduce the receptive field size by removing last few pooling layers and reconfigure the network with atrous convolutions to reuse pre-trained weights. Consequently, their methods have larger dense features (1/8 of input spatial resolution whereas 1/32 or 1/64 in the original base networks) and perform almost all of the decoding process on that resolution followed by a simple upsampling to recover the original spatial resolution.</p><p>To define explicit relation in recovering back to the full resolution, we propose a network architecture that utilizes novel local planar guidance layers located at multiple stages in the decoding phase. Specifically, based on an encodingdecoding scheme, at each decoding stage, which has spatial resolutions of 1/8, 1/4, and 1/2, we place a layer that effectively guides input feature maps to the desired depth with local planar assumption. Then, we combine the outputs to predict depth in full resolution. This differs from multiscale network <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> or image pyramid <ref type="bibr" target="#b15">[16]</ref> approaches in two aspects. First, the outputs from the proposed layers are not treated as separated global depth estimation in corresponding resolutions. Instead, we let the layers to learn 4-dimensional plane coefficients and use them together to reconstruct depth estimations in the full resolution for the final estimation. Second, as a consequence of the nonlinear combination, individual spatial cells in each resolution are distinctively trained while the training progresses. We can see example outputs from the proposed layers in <ref type="figure" target="#fig_0">Figures 1 and 3</ref>. Experiments on the challenging NYU Depth V2 dataset <ref type="bibr" target="#b41">[42]</ref> and KITTI dataset <ref type="bibr" target="#b14">[15]</ref> demonstrate that the proposed method achieves state-of-the-art results.</p><p>The rest of the paper is organized as follows. After a concise survey of related works in Section 2, we present in detail the proposed method in Section 3. Then, in Section 4, we provide results on two challenging benchmarks comparing with state-of-the-art works, and using various base networks as an encoder for the proposed network, we see how the performance varies along with each base network. In Section 4, we also provide an ablation study to validate the effectiveness of the proposed method. We conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Supervised Monocular Depth Estimation</head><p>In monocular depth estimation, supervised approaches take a single image and use depth data measured with range sensors such as RGB-D cameras or multi-channel laser scanners as ground truth for supervision in training. Saxena et al. <ref type="bibr" target="#b37">[38]</ref> propose a learning-based approach to get a functional mapping from visual cues to depth via Markov random field, and extend it to a patch-based model that first over-segments the input image and learns 3D orientation as well as the location of local planes that are well explained by each patch <ref type="bibr" target="#b38">[39]</ref>. Eigen et al. <ref type="bibr" target="#b9">[10]</ref> introduce a multi-scale convolutional architecture that learns coarse global depth predictions on one network and progressively refine them using another network. Unlike the previous works in single image depth estimation, their network can learn representations from raw pixels without handcrafted features such as contours, super-pixels, or low-level segmentation. Several works follow the success of this approach by incorporating strong scene priors for surface normal estimation <ref type="bibr" target="#b45">[46]</ref>, using conditional random fields to improve accuracy <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref> or changing the learning problem from regression to classification <ref type="bibr" target="#b2">[3]</ref>. A recent supervised approach from Fu et al. <ref type="bibr" target="#b11">[12]</ref> achieves the state-of-the-art result by also taking advantage of changing the regression problem to quantized ordinal regression. Xu et al. <ref type="bibr" target="#b48">[49]</ref> propose an architecture that exploits multi-scale estimations derived from inner layers by fusing them within a CRF framework. Gan et al. <ref type="bibr" target="#b12">[13]</ref> propose to explicitly model the relationships between different image locations with an affinity layer. Most recently, Yin et al. <ref type="bibr" target="#b50">[51]</ref> introduce a method using virtual normal directions that are determined by randomly chosen three points in the reconstructed 3D space as geometric constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semi-Supervised Monocular Depth Estimation</head><p>There are also attempts to train a depth estimation network in a semi-supervised or weakly supervised fashion. Chen et al. <ref type="bibr" target="#b5">[6]</ref> propose a new approach that uses information of relative depth and depth ranking loss function to learn depth predictions in unconstrained images. Recently, to overcome the difficulty in getting high-quality depth data, Kuznietsov et al. <ref type="bibr" target="#b23">[24]</ref> introduce a semi-supervised method to train the network using both sparse LiDAR depth data for direct supervision and image alignment loss as a secondary training objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Self-Supervised Monocular Depth Estimation</head><p>The self-supervised approach refers to a method that requires only rectified stereo image pairs to train the depth  <ref type="figure">Figure 2</ref>: Overview of the proposed network architecture. The network is composed of dense feature extractor (the base network), contextual information extractor (ASPP), local planar guidance layers and their dense connection for final depth estimation. Note that the outputs from the local planar guidance layers have the full spatial resolution H enabling shortcuts inside the decoding phase. We also use skip-connections from the base network to link with internal outputs in the decoding phase with corresponding spatial resolutions. estimation network. Garg et al. <ref type="bibr" target="#b13">[14]</ref> and Godard et al. <ref type="bibr" target="#b15">[16]</ref> propose self-supervised learning methods that smartly cast the problem from direct depth estimation to image reconstruction. Specifically, with a rectified stereo image pair, their networks try to synthesize one view from the other with estimated disparities and define the error between both as the reconstruction loss for the main training objective. In this way, because learning requires only well rectified, synchronized stereo pairs instead of the ground truth depth data well associated with the corresponding RGB images, it greatly reduces the effort to acquire datasets for new categories of scenes or environments. However, there is some accuracy gap when compared to the current best supervised approach <ref type="bibr" target="#b50">[51]</ref>. Garg et al. <ref type="bibr" target="#b13">[14]</ref> introduce an encoderdecoder architecture and to train the network using photometric reconstruction error. Xie et al. <ref type="bibr" target="#b46">[47]</ref> propose a network that also synthesizes one view from the other, and by using the reconstruction error, they produce a probability distribution of possible disparities for each pixel. Godard et al. <ref type="bibr" target="#b15">[16]</ref> finally propose a network architecture fully differentiable thus can perform end-to-end training. They also present a novel left-right consistency loss that improves training and predictions of the network. Most recently, Godard et al. <ref type="bibr" target="#b16">[17]</ref> propose a simple but effective architecture benefiting from associated design choices such as a robust reprojection loss, multi-scale sampling, and an automasking loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Video-Based Monocular Depth Estimation</head><p>There are also approaches using sequential data to perform the monocular depth estimation. Yin et al. <ref type="bibr" target="#b51">[52]</ref> propose an architecture consisting of two generative subnetworks that are jointly trained by adversarial learning for disparity map estimation organized in a cycle to provide mutual constraints. Mahjourian et al. <ref type="bibr" target="#b29">[30]</ref> presents an approach that explicitly considers the inferred 3D geometry of the whole scene, and enforce consistency of the estimated 3D point clouds and ego-motion across consecutive frames. Wang et al. <ref type="bibr" target="#b43">[44]</ref> adopt a differentiable pose predictor and train a monocular depth estimation network in an end-toend fashion while benefiting from the pose predictor. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe the proposed monocular depth estimation network with a novel local planar guidance layer located on multiple stages in the decoding phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>As it can be seen from <ref type="figure">Figure 2</ref>, we follow an encodingdecoding scheme that reduces feature map resolution to H/8 then recovers the original resolution H for dense prediction. After the backbone network that we use as a dense feature extractor which produces an H/8 feature map, we place a denser version <ref type="bibr" target="#b49">[50]</ref> of atrous spatial pyramid pooling layer <ref type="bibr" target="#b4">[5]</ref> as our contextual information extractor with various dilation rates r ? {3, 6, 12, 18, 24}. Then, at each stage in the decoding phase, where internal outputs are recovered to the full spatial resolution with a factor of 2, we employ the proposed local planar guidance (LPG) layer to locate geometric guidance to the desired depth estimation. We also place a 1?1 reduction layer to get the finest estimationc 1?1 ? R H?W ?1 after the last upconv layer. Finally, outputs from the proposed layers (i.e.,c k?k ) andc 1?1 are concatenated and fed into the final convolutional layer to get the depth estimationd.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-Scale Local Planar Guidance</head><p>Our key idea in this work is to define direct and explicit relations between internal features and the final output in an effective manner. Unlike the existing methods that recover back to the original resolution using simple nearest neighbor upsampling layers and skip connections from encoding stages, we place novel local planar guidance layers which  <ref type="figure">Figure 4</ref>: The local planar guidance layer. We use a stack of 1 ? 1 convolutions to get 4D coefficients estimations. (i.e., H/k?H/k?4). Then the channels are split to pass through two different activation mechanisms to ensure plane coefficients' constraint. Finally, they are fed into the planar guidance module to compute locally-defined relative depth estimations.</p><p>guide features to the full resolution with the local planar assumption and use them together to get the final depth estimationd. As can be seen from <ref type="figure">Figure 2</ref>, since the proposed layer recovers given an internal feature map to the full resolution H, it can be used as a skip connection inside the decoding phase allowing direct relations between internal features and the final prediction. Specifically, given a feature map having spatial resolution H/k, the proposed layers estimate for each spatial cell a 4D plane coefficients that fit a locally defined k ? k patch on the full resolution H, and they are concatenated together for the prediction through the final convolutional layers.</p><p>Please note that the proposed LPG layer is not designed to directly estimate global depth values on the corresponding scale because the training loss is only defined in terms of the final depth estimation (provided in Section 3.3). Together with outputs from the other LPG layers and re-duc1x1, each output is interpreted to the global depth by contributing as a part of the nonlinear combination through the final convolutional layers. Therefore, they can have distinct ranges, learned as a base or precise relative compensation from the base at a spatial location, as shown in <ref type="figure" target="#fig_0">Figures  1 and 3</ref>.</p><p>Here, we use the local planar assumption because, for a k ? k region, it enables an efficient reconstruction with only four parameters. If we adopt typical upconvs for the reconstruction, the layers should be learned to have k 2 values properly instead of four. Therefore, we can expect that our strategy can be more effective because conventional upsampling would not give details on enlarged resolutions, while the local linear assumption can provide effective guidance.</p><p>To guide features with the local planar assumption, we convert each estimated 4D plane coefficients to k ? k local depth cues using ray-plane intersection:</p><formula xml:id="formula_0">c i = n 4 n 1 u i + n 2 v i + n 3 ,<label>(1)</label></formula><p>where n = (n 1 , n 2 , n 3 , n 4 ) denotes the estimated plane coefficients, (u i , v i ) are k ? k patch-wise normalized coordinates of pixel i. <ref type="figure">Figure 4</ref> shows the detail of the proposed layer. Through a stack of 1 ? 1 convolutions which repeatedly reduce the number of channels by a factor of 2 until it reaches 3, we get a H/k ? H/k ? 3 feature map if we assume a square input. Then, we pass the feature map through two different ways to get local plane coefficient estimations: one way is a conversion to a unit normal vector (n 1 , n 2 , n 3 ), and the other is a sigmoid function defining the perpendicular distance n 4 between the plane and origin. After the sigmoid function we multiply the output with the maximum distance ? to get real depth values. Because a unit normal vector has only two degrees of freedom (i.e., polar and azimuthal angles ?, ? from predefined axes), we regard the first two channels of the given feature map as the angles and convert them to unit normal vectors using following equations. n 1 = sin(?) cos(?), n 2 = sin(?) sin(?), n 3 = cos(?).</p><p>(2) Finally, they are concatenated again and used for estimation ofc k?k using Equation 1.</p><p>We design the local depth cue as an additive depth defined in local regions (i.e., k ? k patches). Since features at the same spatial location in different stages are used together to predict the final depth, for efficient representation, we expect that global shapes would be learned at coarser scales while local details at finer scales. Also, they can interact with each other to compensate for erroneous estimations. We can represent the behavior of the last convolu-tional layer as follows.</p><formula xml:id="formula_1">d = f W 1c 1?1 + W 2c 2?2 + W 3c 4?4 + W 4c 8?8 , (3)</formula><p>where f is an activation function, W j , j ? {1, 2, 3, 4} denotes a corresponding linear transform representing the convolution. Please note that the proposed network learns on multiple scales, and by defining the training loss only in terms of the final estimation,d, we do not enforce parameters for each scale learns with the constant contribution. Therefore, in training, details for regions with sharp curvatures would be learned at finer scales while major structures at coarser scales. Also, there is the last chance inc 1?1 to recover broken assumptions in the upsampled estimations (c k?k , k ? {2, 4, 8}). From <ref type="figure" target="#fig_1">Figure 3</ref>, we can see small details behind the focused vehicle from blue-and blackboxed figures which demonstratec 2?2 andc 1?1 , respectively, while they are missing in the coarser scales,c 8?8 andc 4?4 . Also, there are thick black estimations inc 1?1 andc 2?2 on the boundary of the vehicle compensating the over-estimations inc 8?8 andc 4?4 . More examples are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Loss</head><p>In <ref type="bibr" target="#b10">[11]</ref>, Eigen et al introduce a scale-invariant error and inspired from it they use a following training loss:</p><formula xml:id="formula_2">D(g) = 1 T i g 2 i ? ? T 2 i g i 2 ,<label>(4)</label></formula><p>where g i = logd i ? log d i with the ground truth depth d i , ? = 0.5 and T denotes the number of pixels having valid ground truth values. By rewriting above equation,</p><formula xml:id="formula_3">D(g) = 1 T i g 2 i ? 1 T i g i 2 + (1 ? ?) 1 T i g i 2 ,</formula><p>we can see that it is a sum of the variance and a weighted squared mean of the error in log space. Therefore, setting a higher ? enforces more focusing on minimizing the variance of the error, and we use ? = 0.85 in this work. Also, we observe that properly scaling the range of the loss function improves convergence and the final training result. Finally, we define our training loss L as follows:</p><formula xml:id="formula_4">L = ? D(g),<label>(5)</label></formula><p>where ? is a constant we set to 10 for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To verify the effectiveness of our approach, we provide experimental results from challenging benchmarks with   various settings. After presenting the implementation details of our method, we provide experimental results on two challenging benchmarks covering both indoor and outdoor environments. We also provide scores on the online KITTI evaluation server comparing with published works. Then, we provide an ablation study to discuss a detailed analysis of the proposed core factors, and some qualitative results to demonstrate our approach comparing with competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement the proposed network using the open deep learning framework PyTorch <ref type="bibr" target="#b31">[32]</ref>. For training, we use Adam optimizer <ref type="bibr" target="#b21">[22]</ref> with ? 1 = 0.9, ? 2 = 0.999 and = 10 ?6 , learning is scheduled via polynomial decay from base learning rate 10 ?4 with power p = 0.9. The total number of epochs is set to 50 with batch size 16 on a desktop equipped with four NVIDIA 1080ti GPUs for all experiments in this work.</p><p>As the backbone network for dense feature extraction, we use ResNet-101 <ref type="bibr" target="#b17">[18]</ref>, ResNext-101 <ref type="bibr" target="#b47">[48]</ref> and DenseNet-161 <ref type="bibr" target="#b19">[20]</ref> with pretrained weights trained for image classification using ILSVRC dataset <ref type="bibr" target="#b35">[36]</ref>. Because weights at early convolutions are known to be well trained for primitive visual features, in the base networks, we fix the first two convolutional layers as well as batch normalization parameters in our training. Following <ref type="bibr" target="#b15">[16]</ref>, we use exponential linear units <ref type="bibr" target="#b6">[7]</ref> as an activation function, and upconv uses the nearest neighbor upsampling followed by a 3 ? 3 convolution layer <ref type="bibr" target="#b30">[31]</ref>.</p><p>To avoid over-fitting, we augment images before input to the network using random horizontal flipping as well as random contrast, brightness, and color adjustment in a range of [0.9, 1.1], with 50% of chance. We also use a random rotation of the input images in ranges of [?1, 1] and [?2.5, 2.5] degrees for KITTI and NYU datasets, respectively. We train our network on a random crop of size 352 ? 704 for KITTI and 416 ? 544 for NYU Depth V2 datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">NYU Depth V2 Dataset</head><p>The NYU Depth V2 dataset <ref type="bibr" target="#b41">[42]</ref>   <ref type="table">Table 4</ref>: Result from the ablation study using the NYU Depth V2 dataset. Baseline: a network composed of only the dense feature extractor and direct estimation from it followed by an upsampling with a factor of 8, A: ASPP module attached after the dense feature extractor, U: using upconv layers in <ref type="figure">Figure 2</ref> due to asynchronous capturing rates between RGB images and depth maps, we associate and sample them using timestamps by even-spacing in time, resulting in 24231 imagedepth pairs for the training set. Using raw depth images and camera projections provided by the dataset, we align the image-depth pairs for accurate pixel registrations. We use ? = 10 for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">KITTI Dataset</head><p>KITTI provides the dataset <ref type="bibr" target="#b14">[15]</ref> with 61 scenes from "city", "residential", "road" and "campus" categories. Because existing works commonly use a split proposed by Eigen et al. <ref type="bibr" target="#b10">[11]</ref> for the training and testing, we also follow it to compare with those works. Therefore, 697 images covering a total of 29 scenes are used for evaluation, and the remaining 32 scenes of 23,488 images are used for the training. We use ? = 80 for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Evaluation Result</head><p>For evaluation, we use following metrics used by previous works:</p><formula xml:id="formula_5">Threshold : % ofd i s.t. max(d i di , d? di ) = ? &lt; thr, Abs Rel : 1 |T | d ?T |d ? d|/d, Sq Rel : 1 |T | d ?T ||d ? d|| 2 /d, RMSE : 1 |T | d ?T ||d ? d|| 2 , log10 : 1 |T | d ?T | log 10d ? log 10 d|, RMSElog : 1 |T | d ?T || logd ? log d|| 2 ,</formula><p>where T denotes a collection of pixels that the ground truth values are available.</p><p>Using NYU Depth V2 dataset, the experimental results given in <ref type="table">Table 1</ref> show that Ours-DenseNet achieves the state-of-the-art result with a significant margin in both of the inlier measures (i.e., ? &lt; thr) and accuracy metrics (i.e., AbsRel, log10) except only RMSE. Our ResNet-based model also outperforms the method from Yin et al <ref type="bibr" target="#b50">[51]</ref> with a significant margin which has the same backbone network, ResNext-101 <ref type="bibr" target="#b47">[48]</ref>.</p><p>In the evaluation using the KITTI dataset provided in <ref type="table" target="#tab_4">Table 2</ref>, ours outperforms all existing works with a significant margin. Please note that our ResNet-based model already outperforms the methods from Fu et al <ref type="bibr" target="#b11">[12]</ref> and Yin et al <ref type="bibr" target="#b50">[51]</ref> which use ResNet-101 and ResNext-101 <ref type="bibr" target="#b47">[48]</ref> as their backbone network, respectively. Only the root mean squared errors (RMSE) from ours are behind that of Fu et al.'s in the capturing range 0-80m. However, in the capturing range 0-50m, ours-ResNet achieves more than 10% improvement in RMSE from the result of Fu et al. Also, the proposed method achieves notable improvements in the inlier metrics (i.e., ? &lt; thres), meaning more number of correctly estimated pixels as it can be seen from Figures 5 and 6 presenting qualitative comparison to our competitors.</p><p>We also evaluate the proposed method on the online KITTI benchmark server with a model trained using KITTI's official split. Apart from the training set, all other settings remain the same as in the experiment using KITTI's Eigen split. We trained Ours-DenseNet for 50 epochs with 28,654 image-ground truth pairs sampled from the official training and validation set. As shown in <ref type="table" target="#tab_5">Table 3</ref>, our method outperforms all the published works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Study</head><p>Here, we conduct evaluations with variants of our network to see the effectiveness of the core factors. From the baseline network, which only consists of the base network (i.e., ResNet-101) and a simple upsampling layer, we increment the network with the core modules to see how the added factor improves the performance. The result is given in <ref type="table">Table 4</ref>. As the core factors are added, the overall performance is improved, and the most significant improvement is made by adding the proposed local planar guidance layers. Please note that the LPG layers only require additional 0.1M trainable parameters used by 1x1 reduction layers.   network DenseNet-161, ours achieves state-of-the-art performance with the significant margin while it requires the less number of parameters than the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Experiments with Various Base Networks</head><p>Because the proposed network adopts existing models as an encoder for dense feature extraction, it is desirable to see how the performance varies with various base networks that are widely used for similar applications. By changing the encoder with various models while other settings remained, we experimented with the proposed method using both of the NYU Depth V2 and KITTI's Eigen split, and provide the result in Tables 5 and 6. Note that ResNet-101, ResNext-101 and DenseNet-161 are identical to Ours-ResNet, Ours-ResNext and Ours-DenseNet, respectively, in <ref type="table" target="#tab_4">Tables 1 and  2</ref>. Interestingly, for the NYU Depth V2 dataset, DenseNet-161 results in the best performance, while for KITTI's Eigen split, ResNext-101 achieves the state-of-the-art result. We consider this as an effect of the relatively lower variance of the data distribution in the indoor scenes of the NYU Depth V2 dataset, which can lead to a degeneration of performance with very deep models such as ResNext-101 in this experiment. Also, it is notable that our MobileNetV2based model results in performance drop about only 3% for inlier measures and less than 15% drop for accuracy measures while it contains less than half the number of parameters and shows about three times speedup when compared to our model based on the DenseNet-161.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Qualitative Result</head><p>Finally, we discuss qualitative results from ours and competing works. As we can see from <ref type="figure" target="#fig_3">Figures 5 and 6</ref>, ours show much more precise object boundaries. However, in results from experiments using KITTI, we can see artifacts in the sky or upper part of the scenes. We consider this as a consequence of the very sparse ground truth depth data. Because certain regions are lacking valid depth values across the dataset, the network cannot be appropriately trained for those regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have presented a supervised monocular depth estimation network and achieved state-of-the-art results. Benefiting from recent advances in deep learning, we design a network architecture that uses novel local planar guidance layers, giving an explicit relation from internal feature maps to the desired prediction for better training of the network. By deploying the proposed layers on multiple stages in the decoding phase, we have gained a significant improvement and shown several experimental results on challenging benchmarks to verify it. However, in experiments with the KITTI dataset, we observe frequent artifacts on the upper part of the scenes. We analyze this as an effect of the high sparsity of the ground truth across the dataset. As a consequence, we plan to investigate adopting into our framework a photometric reconstruction loss, which can provide far denser supervision to improve the performance further.   <ref type="figure">Figure 6</ref>: Qualitative results on the NYU Depth V2 test split. While the method from Yin et al <ref type="bibr" target="#b50">[51]</ref> show competitive results to ours, our method achieves more distinctive results especially on object boundaries.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example outputs from the proposed network. Top: from left to right, input image, predicted depth map, and the ground truth. Bottom: from left to right, outputs from the proposed local planar guidance layers having input feature resolutions of 1/2, 1/4, 1/8 to the input image, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Examples showing behavior of the proposed network. Top and middle rows showc k?k and their focused views. Bottom row shows the final depth estimation result with a focused view. The overestimated boundaries of the vehicle from lpg8x8 (yellow rectangle) and lpg4x4 (green rectangle) are compensated by the outputs from lpg2x2 (blue rectangle) and reduc1x1 (black rectangle) (i.e,c 2?2 andc 1?1 ), resulting the clear boundary in the final estimation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, L: the proposed local planar guidance layers. All variants are trained using ResNet-101 as the base network and Equation 4 with ? = 0.5 as the training loss. 'Ours-ResNet' and 'Ours-DenseNet' use the training loss given in Equation 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>(a) Input (b) Fu et al.<ref type="bibr" target="#b11">[12]</ref> (c) Yin et al.<ref type="bibr" target="#b50">[51]</ref> (d) Ours Qualitative results on the KITTI Eigen test split. The proposed method results clearer boundaries from the vehicle and traffic sign.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Method ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3 AbsRel RMSE log10 Saxena et al.</figDesc><table><row><cell>[39]</cell><cell>0.447</cell><cell>0.745</cell><cell>0.897</cell><cell>0.349</cell><cell>1.214</cell><cell>-</cell></row><row><cell>Wang et al. [45]</cell><cell>0.605</cell><cell>0.890</cell><cell>0.970</cell><cell>0.220</cell><cell>0.824</cell><cell>-</cell></row><row><cell>Liu et al. [29]</cell><cell>0.650</cell><cell>0.906</cell><cell>0.976</cell><cell>0.213</cell><cell cols="2">0.759 0.087</cell></row><row><cell>Eigen et al. [10]</cell><cell>0.769</cell><cell>0.950</cell><cell>0.988</cell><cell>0.158</cell><cell>0.641</cell><cell>-</cell></row><row><cell>Chakrabarti et al. [4]</cell><cell>0.806</cell><cell>0.958</cell><cell>0.987</cell><cell>0.149</cell><cell>0.620</cell><cell>-</cell></row><row><cell>Li et al. [28]</cell><cell>0.789</cell><cell>0.955</cell><cell>0.988</cell><cell>0.152</cell><cell cols="2">0.611 0.064</cell></row><row><cell>Laina et al. [25]</cell><cell>0.811</cell><cell>0.953</cell><cell>0.988</cell><cell>0.127</cell><cell cols="2">0.573 0.055</cell></row><row><cell>Xu et al. [49]</cell><cell>0.811</cell><cell>0.954</cell><cell>0.987</cell><cell>0.121</cell><cell cols="2">0.586 0.052</cell></row><row><cell>Lee at al. [26]</cell><cell>0.815</cell><cell>0.963</cell><cell>0.991</cell><cell>0.139</cell><cell>0.572</cell><cell>-</cell></row><row><cell>Fu et al. [12]</cell><cell>0.828</cell><cell>0.965</cell><cell>0.992</cell><cell>0.115</cell><cell cols="2">0.509 0.051</cell></row><row><cell>Qi et al. [33]</cell><cell>0.834</cell><cell>0.960</cell><cell>0.990</cell><cell>0.128</cell><cell cols="2">0.569 0.057</cell></row><row><cell>Yin et al. [51]</cell><cell>0.875</cell><cell>0.976</cell><cell>0.994</cell><cell>0.108</cell><cell cols="2">0.416 0.048</cell></row><row><cell>Ours-ResNet</cell><cell>0.871</cell><cell>0.977</cell><cell>0.995</cell><cell>0.113</cell><cell cols="2">0.407 0.049</cell></row><row><cell>Ours-ResNext</cell><cell>0.880</cell><cell>0.977</cell><cell>0.994</cell><cell>0.111</cell><cell cols="2">0.410 0.048</cell></row><row><cell>Ours-DenseNet</cell><cell>0.885</cell><cell>0.978</cell><cell>0.994</cell><cell>0.110</cell><cell cols="2">0.392 0.047</cell></row><row><cell cols="7">Table 1: Evaluation results on NYU Depth v2. Ours outper-</cell></row><row><cell cols="7">forms previous works with a significant margin in all mea-</cell></row><row><cell cols="3">sures except only from AbsRel.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Method caphigher is better lower is better ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Abs Rel Sq Rel RMSE RMSE log</figDesc><table><row><cell>Saxena et al. [39] 0-80m</cell><cell>0.601</cell><cell>0.820</cell><cell>0.926</cell><cell>0.280</cell><cell>3.012</cell><cell>8.734</cell><cell>0.361</cell></row><row><cell>Eigen et al. [11] 0-80m</cell><cell>0.702</cell><cell>0.898</cell><cell>0.967</cell><cell>0.203</cell><cell>1.548</cell><cell>6.307</cell><cell>0.282</cell></row><row><cell>Liu et al. [29] 0-80m</cell><cell>0.680</cell><cell>0.898</cell><cell>0.967</cell><cell>0.201</cell><cell>1.584</cell><cell>6.471</cell><cell>0.273</cell></row><row><cell>Godard et al. (CS+K) [16] 0-80m</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell></row><row><cell>Kuznietsov et al. [24] 0-80m</cell><cell>0.862</cell><cell>0.960</cell><cell>0.986</cell><cell>0.113</cell><cell>0.741</cell><cell>4.621</cell><cell>0.189</cell></row><row><cell>Godard et al. (CS+K) [16] 0-80m</cell><cell>0.861</cell><cell>0.949</cell><cell>0.976</cell><cell>0.114</cell><cell>0.898</cell><cell>4.935</cell><cell>0.206</cell></row><row><cell>Gan et al. [13] 0-80m</cell><cell>0.890</cell><cell>0.964</cell><cell>0.985</cell><cell>0.098</cell><cell>0.666</cell><cell>3.933</cell><cell>0.173</cell></row><row><cell>Fu et al. [12] 0-80m</cell><cell>0.932</cell><cell>0.984</cell><cell>0.994</cell><cell>0.072</cell><cell>0.307</cell><cell>2.727</cell><cell>0.120</cell></row><row><cell>Yin et al. [51] 0-80m</cell><cell>0.938</cell><cell>0.990</cell><cell>0.998</cell><cell>0.072</cell><cell>-</cell><cell>3.258</cell><cell>0.117</cell></row><row><cell>Ours-ResNet 0-80m</cell><cell>0.954</cell><cell>0.992</cell><cell>0.998</cell><cell>0.061</cell><cell>0.261</cell><cell>2.834</cell><cell>0.099</cell></row><row><cell>Ours-DenseNet 0-80m</cell><cell>0.955</cell><cell>0.993</cell><cell>0.998</cell><cell>0.060</cell><cell>0.249</cell><cell>2.798</cell><cell>0.096</cell></row><row><cell>Ours-ResNext 0-80m</cell><cell>0.956</cell><cell>0.993</cell><cell>0.998</cell><cell>0.059</cell><cell>0.245</cell><cell>2.756</cell><cell>0.096</cell></row><row><cell>Garg et al. [14] 0-50m</cell><cell>0.740</cell><cell>0.904</cell><cell>0.962</cell><cell>0.169</cell><cell>1.080</cell><cell>5.104</cell><cell>0.273</cell></row><row><cell>Godard et al. (CS+K) [16] 0-50m</cell><cell>0.873</cell><cell>0.954</cell><cell>0.979</cell><cell>0.108</cell><cell>0.657</cell><cell>3.729</cell><cell>0.194</cell></row><row><cell>Kuznietsov et al. [24] 0-50m</cell><cell>0.875</cell><cell>0.964</cell><cell>0.988</cell><cell>0.108</cell><cell>0.595</cell><cell>3.518</cell><cell>0.179</cell></row><row><cell>Gan et al. [13] 0-50m</cell><cell>0.898</cell><cell>0.967</cell><cell>0.986</cell><cell>0.094</cell><cell>0.552</cell><cell>3.133</cell><cell>0.165</cell></row><row><cell>Fu et al. [12] 0-50m</cell><cell>0.936</cell><cell>0.985</cell><cell>0.995</cell><cell>0.071</cell><cell>0.268</cell><cell>2.271</cell><cell>0.116</cell></row><row><cell>Ours-ResNet 0-50m</cell><cell>0.962</cell><cell>0.994</cell><cell>0.999</cell><cell>0.058</cell><cell>0.183</cell><cell>1.995</cell><cell>0.090</cell></row><row><cell>Ours-DenseNet 0-50m</cell><cell>0.964</cell><cell>0.995</cell><cell>0.999</cell><cell>0.057</cell><cell>0.175</cell><cell>1.949</cell><cell>0.088</cell></row><row><cell>Ours-ResNext 0-50m</cell><cell>0.964</cell><cell>0.994</cell><cell>0.999</cell><cell>0.056</cell><cell>0.169</cell><cell>1.925</cell><cell>0.087</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Performance on KITTI Eigen split. (CS+K) denotes a model pre-trained on Cityscapes dataset [8] and fine-tuned on KITTI.</figDesc><table><row><cell>Method</cell><cell cols="4">SILog sqErrorRel absErrorRel iRMSE</cell></row><row><cell cols="2">Yin et al. [51] 12.65</cell><cell>2.46</cell><cell>10.15</cell><cell>13.02</cell></row><row><cell cols="2">Diaz et al. [9] 12.39</cell><cell>2.49</cell><cell>10.10</cell><cell>13.48</cell></row><row><cell cols="2">Fu et al. [12] 11.77</cell><cell>2.23</cell><cell>8.78</cell><cell>12.98</cell></row><row><cell cols="2">Ours 11.67</cell><cell>2.21</cell><cell>9.04</cell><cell>12.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Result on the online KITTI evaluation server.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Abs Rel Sq Rel RMSE RMSE log log10</figDesc><table><row><cell cols="3">Variant ? &lt; 1Baseline # Params 63.0M 0.815</cell><cell>higher is better 0.958</cell><cell>0.990</cell><cell>0.142</cell><cell>0.084</cell><cell>lower is better 0.587</cell><cell>0.169</cell><cell>0.053</cell></row><row><cell>Baseline + A</cell><cell>67.4M</cell><cell>0.827</cell><cell>0.964</cell><cell>0.992</cell><cell>0.141</cell><cell>0.081</cell><cell>0.577</cell><cell>0.166</cell><cell>0.051</cell></row><row><cell>Baseline + A + U</cell><cell>68.4M</cell><cell>0.845</cell><cell>0.967</cell><cell>0.992</cell><cell>0.134</cell><cell>0.076</cell><cell>0.513</cell><cell>0.161</cell><cell>0.051</cell></row><row><cell>Baseline + A + U + L</cell><cell>68.5M</cell><cell>0.863</cell><cell>0.974</cell><cell>0.994</cell><cell>0.119</cell><cell>0.072</cell><cell>0.421</cell><cell>0.149</cell><cell>0.050</cell></row><row><cell>Ours-ResNet</cell><cell>68.5M</cell><cell>0.871</cell><cell>0.975</cell><cell>0.995</cell><cell>0.113</cell><cell>0.068</cell><cell>0.407</cell><cell>0.148</cell><cell>0.049</cell></row><row><cell>Ours-DenseNet</cell><cell>47.0M</cell><cell>0.885</cell><cell>0.978</cell><cell>0.994</cell><cell>0.110</cell><cell>0.066</cell><cell>0.392</cell><cell>0.142</cell><cell>0.047</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">contains 120K RGB</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">and depth pairs having a size of 480 ? 640 acquired as</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">video sequences using a Microsoft Kinect from 464 indoor</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">scenes. We follow the official train/test split as previous</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">works, using 249 scenes for training and 215 scenes (654</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">images) for testing. From the total 120K image-depth pairs,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The final improvement comes from using the training loss defined in Equation 5. Benefited from the robust base Variant # Params higher is better lower is better ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Abs Rel Sq Rel RMSE RMSE log log10</figDesc><table><row><cell>MobileNetV2 [37]</cell><cell>16.3M</cell><cell>0.860</cell><cell>0.974</cell><cell>0.993</cell><cell>0.121</cell><cell>0.080</cell><cell>0.431</cell><cell>0.156</cell><cell>0.052</cell></row><row><cell>ResNet-50 [18]</cell><cell>49.5M</cell><cell>0.865</cell><cell>0.975</cell><cell>0.993</cell><cell>0.119</cell><cell>0.075</cell><cell>0.419</cell><cell>0.152</cell><cell>0.051</cell></row><row><cell>ResNet-101 [18]</cell><cell>68.5M</cell><cell>0.871</cell><cell>0.977</cell><cell>0.995</cell><cell>0.113</cell><cell>0.068</cell><cell>0.407</cell><cell>0.148</cell><cell>0.049</cell></row><row><cell>ResNext-50 [48]</cell><cell>49.0M</cell><cell>0.867</cell><cell>0.977</cell><cell>0.995</cell><cell>0.116</cell><cell>0.070</cell><cell>0.414</cell><cell>0.150</cell><cell>0.050</cell></row><row><cell>ResNext-101 [48]</cell><cell>112.8M</cell><cell>0.880</cell><cell>0.977</cell><cell>0.994</cell><cell>0.111</cell><cell>0.069</cell><cell>0.399</cell><cell>0.145</cell><cell>0.048</cell></row><row><cell>DenseNet-121 [20]</cell><cell>21.2M</cell><cell>0.871</cell><cell>0.977</cell><cell>0.993</cell><cell>0.118</cell><cell>0.072</cell><cell>0.410</cell><cell>0.149</cell><cell>0.050</cell></row><row><cell>DenseNet-161 [20]</cell><cell>47.0M</cell><cell>0.885</cell><cell>0.978</cell><cell>0.994</cell><cell>0.110</cell><cell>0.066</cell><cell>0.392</cell><cell>0.142</cell><cell>0.047</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Experimental results using NYU Depth V2 with various base netowrks.</figDesc><table><row><cell>Variant</cell><cell># Params</cell><cell cols="8">higher is better ? &lt; 1.25 ? &lt; 1.25 2 ? &lt; 1.25 3 Abs Rel Sq Rel RMSE RMSE log log10 lower is better</cell></row><row><cell>ResNet-50 [18]</cell><cell>49.5M</cell><cell>0.954</cell><cell>0.992</cell><cell>0.998</cell><cell>0.061</cell><cell>0.250</cell><cell>2.803</cell><cell>0.098</cell><cell>0.027</cell></row><row><cell>ResNet-101 [18]</cell><cell>68.5M</cell><cell>0.954</cell><cell>0.993</cell><cell>0.998</cell><cell>0.061</cell><cell>0.261</cell><cell>2.834</cell><cell>0.099</cell><cell>0.027</cell></row><row><cell>DenseNet-121 [20]</cell><cell>21.2M</cell><cell>0.951</cell><cell>0.993</cell><cell>0.998</cell><cell>0.063</cell><cell>0.256</cell><cell>2.850</cell><cell>0.100</cell><cell>0.028</cell></row><row><cell>DenseNet-161 [20]</cell><cell>47.0M</cell><cell>0.955</cell><cell>0.993</cell><cell>0.998</cell><cell>0.060</cell><cell>0.249</cell><cell>2.798</cell><cell>0.096</cell><cell>0.027</cell></row><row><cell>ResNext-50 [48]</cell><cell>49.0M</cell><cell>0.954</cell><cell>0.993</cell><cell>0.998</cell><cell>0.061</cell><cell>0.245</cell><cell>2.774</cell><cell>0.098</cell><cell>0.027</cell></row><row><cell>ResNext-101 [48]</cell><cell>112.8M</cell><cell>0.956</cell><cell>0.993</cell><cell>0.998</cell><cell>0.059</cell><cell>0.241</cell><cell>2.756</cell><cell>0.096</cell><cell>0.026</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Experimental results using KITTI's Eigen split with various base netowrks. In this experiment, we set the capturing range to 0 ? 80m.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Heliometric stereo: Shape from sun position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abrams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hawley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Photometric stereo with general, unknown lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="239" to="257" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimating depth from monocular images as classification using deep fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2658" to="2666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Single-image depth perception in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="730" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monocular depth estimation with affinity, vertical pooling, and label enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="224" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Bg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Brostow. Digging into self-supervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perceiving in depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">P</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">basic mechanisms</title>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unified depth prediction and intrinsic image decomposition from a single image via joint convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="143" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end training of hybrid cnn-crf models for stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kn?belreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Reinbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shekhovtsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised deep learning for monocular depth map prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuznietsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>St?ckler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6647" to="6655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single-image depth estimation based on fourier domain analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-S</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="330" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1119" to="1127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A two-streamed network for estimating fine-scaled depth maps from single rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 IEEE International Conference on Computer Vision</title>
		<meeting>the 2017 IEEE International Conference on Computer Vision<address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5667" to="5675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deconvolution and checkerboard artifacts. Distill</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. dAlche Buc, E. Fox, and R. Garnett, editors</editor>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
			<date type="published" when="2019" />
			<publisher>Curran Associates, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense monocular depth estimation in complex dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4058" to="4066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">A taxonomy and evaluation of dense two-frame stereo correspondence algorithms. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="7" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02351</idno>
		<title level="m">Fully connected deep structured networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="746" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning depth from monocular videos using direct methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Buenaposada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2022" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="539" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Multiscale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5354" to="5362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Denseaspp for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3684" to="3692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Geonet: Unsupervised learning of dense depth, optical flow and camera pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1983" to="1992" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
