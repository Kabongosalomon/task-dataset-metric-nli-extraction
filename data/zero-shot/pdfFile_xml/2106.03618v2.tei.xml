<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-level Relation Extraction as Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
							<email>xiangchen@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
							<email>chuanqi.tcq@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
							<email>chenmosha.cms@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Hangzhou Innovation Center</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Document-level Relation Extraction as Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level relation extraction aims to extract relations among multiple entity pairs from a document. Previously proposed graph-based or transformer-based models utilize the entities independently, regardless of global information among relational triples. This paper approaches the problem by predicting an entity-level relation matrix to capture local and global information, parallel to the semantic segmentation task in computer vision. Herein, we propose a Document U-shaped Network for document-level relation extraction. Specifically, we leverage an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. Experimental results show that our approach can obtain state-of-the-art performance on three benchmark datasets DocRED, CDR, and GDA 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction (RE) is an important task in the field of information extraction, which has widespread applications <ref type="bibr">[Zhang et al., 2021b;</ref><ref type="bibr">Zhang et al., 2021a]</ref>. Previous works <ref type="bibr" target="#b21">[Zeng et al., 2015;</ref><ref type="bibr" target="#b3">Feng et al., 2018]</ref> focused on identifying relations within a single sentence, which failed to recognize relations between entities across sentences. However, many relations are expressed over multiple sentences in real-world applications. According to <ref type="bibr" target="#b18">[Yao et al., 2019]</ref>, above 40.7% of relations can only be identified at the document level. Therefore, it is crucial for models to be able to extract documentlevel relations.</p><p>Recent studies <ref type="bibr" target="#b18">[Yao et al., 2019;</ref><ref type="bibr" target="#b23">Zhou et al., 2021]</ref> have extended sentence-level RE to the document level. Compared with sentence-level RE that only contains one entity pair to classify in a sentence, document-level RE requires the model * Equal contribution and shared co-first authorship. ? Corresponding author. <ref type="bibr">1</ref> The code and datasets are available in https://github.com/zjunlp/ DocuNet.</p><p>[1] Elias Brown <ref type="bibr">(May 9, 1793</ref><ref type="bibr">-July 7, 1857</ref>   to classify the relations of multiple entity pairs at once. Besides, the subject and object entities involved in a relation may appear in different sentences. Therefore a relation cannot be identified based solely on a single sentence. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, it is easy to identify the intrasentence relations, such as (Maryland, country, U.S.), (Baltimore, located in, Maryland), and (Eldersburg, located in, Maryland), owing to the occurrence of entities in the same sentence. However, it is more challenging for a model to recognize inter-sentence relations, such as those between Eldersburg and U.S. and between Baltimore and U.S. because these mentions occur in different sentences and have long-distance dependencies.</p><p>To extract relations among these inter-sentence entity pairs, most current studies constructed document-level graph module based on heuristics, structured attention or dependency structures <ref type="bibr" target="#b6">[Peng et al., 2017;</ref><ref type="bibr" target="#b2">Christopoulou et al., 2019;</ref><ref type="bibr" target="#b5">Nan et al., 2020;</ref>, followed by reasoning with graph neural models. Meanwhile, considering the transformer architecture can implicitly model long-distance dependencies, some studies <ref type="bibr" target="#b11">[Wang et al., 2019;</ref><ref type="bibr" target="#b23">Zhou et al., 2021]</ref> directly applied pretrained language models rather than explicit graph reasoning. In general, current approaches obtain entity representation via information passing through nodes on document-level graphs or transformer-based structure learning. However, they mainly focus on token-level syntactic features or contextual information rather than global interactions between entity pairs, neglecting the interdependency among the multiple relations in one context.</p><p>Concretely, the interdependency among multiple triples is advantageous and can provide guidance for relation classification in the case of many entities. For example, if the intrasentence relation (Maryland, country, U.S.) has been identified, it is implausible for U.S. to be in any other person-social relationship, such as "is the father of...". Besides, according to the triples that Eldersburg is located in Maryland and Maryland belongs to U.S., we can infer that Eldersburg belongs to U.S.. As described above, each relation triple can provide information to other relation triples in the same text.</p><p>To capture the interdependency among the multiple triples, we reformulate the document-level RE task as an entity-level classification problem <ref type="bibr" target="#b4">[Jiang et al., 2019]</ref>, also known as table filling <ref type="bibr">[Miwa and Sasaki, 2014;</ref><ref type="bibr" target="#b4">Gupta et al., 2016]</ref>, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. It is analogous to semantic segmentation (a well-known computer vision task), whose goal is to label each pixel of the image with the corresponding represented class by convolution network. Inspired by the above, we propose a novel model called Document U-shaped Network (DocuNet), which formulates document-level RE as semantic segmentation. In this manner, given relevant features between entity pairs as an image, the model predicts the relation type for each entity pair as a pixel-level mask. Specifically, we introduce an encoder module to capture the context information of entities and a U-shaped segmentation module over the image-style feature map to capture global interdependency among triples. We further propose a balanced softmax method to handle the imbalance relation distribution. Our contributions can be summarized as follows:</p><p>? To the best of our knowledge, this is the first approach that regards document-level RE as a semantic segmentation task. ? We introduce the model DocuNet to capture both local context information and global interdependency among triples for document-level RE. ? Experimental results on three benchmark datasets show that our model DocuNet can achieve state-of-the-art performance compared with baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Previous relation extraction approaches mainly concentrate on identifying the relation between two entities within a sentence. Many approaches <ref type="bibr" target="#b21">[Zeng et al., 2015;</ref><ref type="bibr" target="#b3">Feng et al., 2018;</ref><ref type="bibr" target="#b22">Zhang et al., 2018;</ref><ref type="bibr" target="#b22">Zhang et al., 2019;</ref><ref type="bibr" target="#b22">Zhang et al., 2020a;</ref><ref type="bibr" target="#b15">Wu et al., 2021;</ref><ref type="bibr" target="#b22">Zheng et al., 2021]</ref> have been proposed to tackle the sentence-level RE task effectively. However, sentence-level RE faces an inevitable restriction in that many real-world relations can only be extracted by reading multiple sentences. For this reason, document-level RE appeals to many researchers <ref type="bibr" target="#b5">Nan et al., 2020;</ref><ref type="bibr" target="#b16">Xiao et al., 2020]</ref>. Various approaches for document-level RE mainly include graph-based models and transformer-based models. Graphbased approaches are now widely adopted in RE because of [2020] proposed a novel model (LSR) that enables relational reasoning across sentences by automatically inducing a latent document-level graph.  proposed the graph aggregation-and-inference network (GAIN) with double graphs for document-level RE.  proposed an encoder-classifier reconstructor model (Het-erGSAN), which manages to reconstruct the ground-truth path dependencies from the graph representation. Explicit graph reasoning can bridge the gap between entities that occur in different sentences, thus mitigating long-distance dependency and achieving promising performance.</p><p>In contrast, considering the transformer architecture can implicitly model long-distance dependencies, some researchers directly leverage pre-trained language models without generating document graphs.  proposed a two-step training paradigm on DocRED using BERT as pre-trained word embedding. They observed an imbalance in the distribution of relation and disentangled the relation identification and classification for better inference. Tang et al.</p><p>[2020] proposed a hierarchical inference network (HIN) to make full use of the abundant information from the entity, sentence, and document levels to perform hierarchical reasoning. <ref type="bibr" target="#b23">Zhou et al. [2021]</ref> proposed a novel transformer-based model (ATLOP) of adaptive thresholding and localized context pooling based on BERT. However, most previous studies focused on the local entity representation, regardless of the high-level global connections between triples, which overlooked the interdependency between multiple relations. On the one hand, our work is inspired by <ref type="bibr">[Jin et al., 2020]</ref>, which was the first to consider the issue of global interaction between relations, and there have been few studies on RE. On the other hand, as these studies <ref type="bibr">[Nguyen and Grishman, 2015;</ref><ref type="bibr" target="#b7">Shen and Huang, 2016]</ref> have done, convolutional neural networks have been long used in the relation extraction area, which enlightens us to pay attention to the role of CNN in extracting information of the image-style feature map. Hence, our work is also related to the study of , who formulated incomplete utterance rewriting as a semantic segmentation task and motivated us to study the RE problem from a computer vision perspective. In this study, we leveraged the U-Net <ref type="bibr">[Ronneberger et al., 2015]</ref>, which consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. To the best of our knowledge, this is the first approach to formulate RE as a semantic segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>We first introduce the problem definition. With a document d containing a set of entities {e i } n i=1 , the task is to extract the relations between entity pairs (e s , e o ). In one document, each entity e i may occur multiple times. To model relation extraction between e s and e o , we define a N ? N matrix Y , where entry Y s,o indicates the relation type between e s and e o . Then, we obtain the output of matrix Y , analogous to the task of semantic segmentation. Entities in Y are arranged according to their first appearance in the document. We obtain the feature map via the entity-to-entity relevance estimation and take the feature map as an image. Note that the output entity-level relation matrix Y is parallel to the pixel-level mask in semantic segmentation, which bridges relation extraction and semantic segmentation. Our approach can also be applied to sentencelevel relation extraction. Since the document has relatively more entities, thus, entity-level relation matrix can learn more global information to boost the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Encoder Module</head><p>Given the document d = [x t ] L t=1 , we insert special symbols "&lt; e &gt;" and "&lt; /e &gt;" at the start and end of mentions to mark the entity positions. We leverage the pre-trained language model as an encoder to obtain the embedding as follows:</p><formula xml:id="formula_0">H = [h 1 , h 2 , ..., h L ] = Encoder([x 1 , x 2 , ..., x L ]).</formula><p>(1) where h i is the embedding of the token x i . Note that some documents are longer than 512, we thus leverage a dynamic window to encode whole documents. We average the embeddings of overlapping tokens of different windows to obtain the final representations. Then, we utilize the embeddings of "&lt; e &gt;" to represent mention following <ref type="bibr" target="#b10">[Verga et al., 2018]</ref>. We leverage a smooth version of max pooling, namely, logsumexp pooling  each entity e i , to obtain the entity embedding e i :</p><formula xml:id="formula_1">e i = log Ne i j=1 exp (m j ) .<label>(2)</label></formula><p>This pooling accumulates signals from mentions in the document. Thus, we obtain the entity embedding e i .</p><p>We calculate the entity-level relation matrix based on entity-to-entity relevance. For each entity e i in the matrix, their relevance is captured by a D-dimensional feature vector F(e s , e o ). We introduce two strategies for computing F(e s , e o ), namely, similarity-based method and contextbased method. Similarity-based method is produced by concatenating operation result of element-wise similarity, cosine similarity and bi-linear similarity between e s and e o as:</p><p>F(e s , e o ) = e s e o ; cos(e s ,e o ); e s W 1 e o ,</p><p>(3) For the context-based strategy, we leverage entity-aware attention with affine transformation to obtain the feature vector as follows:</p><formula xml:id="formula_2">F(e s , e o ) = W 2 Ha (s,o) (4) a (s,o) = sof tmax( K i=1 A s i ? A o i )<label>(5)</label></formula><p>where a (s,o) is the attention weight for entity-aware attention and A s i refers to the tokens' importance to the i-th entity, H is the document embedding, W 1 , W 2 is the learnable weight matrix, K is the number of head in the transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">U-shaped Segmentation Module</head><p>Taking the entity-level relation matrix F ? R N ?N ?D as a Dchannel image, we formulate the document-level relation prediction as the pixel-level mask in F . where N is the largest number of entities, counted from all the dataset samples. Specifically N is the largest number of entities, counted from all the dataset samples. To this end, we utilize U-Net <ref type="bibr">[Ronneberger et al., 2015]</ref>, which is a famous semantic segmentation model in computer vision. As can be seen in <ref type="figure" target="#fig_2">Figure  3</ref>, the module is formed as a U-shaped segmentation structure, which contains two down-sampling blocks and two upsampling blocks with skip connections. On the one hand, each down-sampling block has two subsequent max pooling and separate convolution modules. Further, the number of channels is doubled in each down-sampling block. As it shows in the <ref type="figure" target="#fig_1">Figure 2</ref>, the segmentation area in the entitylevel relation matrix refers to the co-occurrence of relations between entity pairs. The U-shaped segmentation structure can promote the information exchange between entity pairs in the receptive field analogy to implicit reasoning. Specifically, CNN and down-sampling block can enlarge the receptive field of current entity pair embedding F(e s , e o ), thus, providing rich global information for representation learning. On the other hand, the model has two up-sampling blocks with a subsequent deconvolution neural network and two separate convolution modules. Different from down-sampling, the number of channels is halved in each up-sampling block, which can distribute the aggregated information to each pixel.</p><p>Finally, we incorporate an encoding module and a Ushaped segmentation module to capture both local and global information Y as follows: is the learnable weight matrix in order to reduce the dimension of F and D is much smaller than D.</p><formula xml:id="formula_3">Y = U (W 3 F)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification Module</head><p>Given the entity pair embedding e s and e o with the entitylevel relation matrix Y , we map them to hidden representations z with a feedforward neural network. Then, we obtain the probability of relation via a bilinear function. Formally, we have:</p><formula xml:id="formula_4">z s = tanh (W s e s + Y s,o ) ,<label>(7)</label></formula><formula xml:id="formula_5">z o = tanh (W o e o + Y s,o ) ,<label>(8)</label></formula><formula xml:id="formula_6">P (r|e s , e o ) = ? (z s W r z o + b r ) , (9) where Y s,o is the entity-pair representation of (s, o) in ma- trix Y , W r ? R d?d , b r ? R, W s ? R d?d , and W o ? R d?d , are learnable parameters.</formula><p>Since previous work <ref type="bibr" target="#b11">[Wang et al., 2019]</ref> observed that there is an imbalance relation distribution for RE (many entity pairs have relation of NA), we introduce a balanced softmax method for training, which is inspired by the circle loss  from computer vision. Specifically, we introduce an additional category 0, hoping that the scores of the target category are all greater than s 0 and the scores of the non-target categories are all less than s 0 . Formally, we have <ref type="bibr" target="#b8">[Su, 2020]</ref>:</p><formula xml:id="formula_7">L = log ? ? e s0 + i??neg e si ? ? + log ? ? e ?s0 + j??pos e ?sj ? ? .</formula><p>(10) For simplicity, we set the threshold as zero and have the following:</p><formula xml:id="formula_8">L = log ? ? 1 + i??neg e si ? ? +log ? ? 1 + j??pos e ?sj ? ? . (11)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We evaluated our DocuNet model on three document-level RE datasets. We listed the dataset statistics in <ref type="table">Table 1.</ref> ? DocRED <ref type="bibr" target="#b18">[Yao et al., 2019]</ref> is a large-scale documentlevel relation extraction dataset by crowdsourcing. Do-cRED contains 3,053/1,000/1,000 instances for training, validating and test, respectively.   59.14 61.22 59.00 61.24 HeterGSAN-BERT base <ref type="bibr" target="#b17">[Xu et al., 2021]</ref> 58  <ref type="bibr" target="#b19">[Ye et al., 2020]</ref> 55.32 57.51 54.54 56.96 ATLOP-BERT base <ref type="bibr" target="#b23">[Zhou et al., 2021]</ref> 59   <ref type="bibr" target="#b23">[Zhou et al., 2021]</ref> 69.4 83.9</p><p>DocuNet-SciBERT base 76.3?0.40 85.3?0.50 <ref type="table">Table 3</ref>: Results (%) on the biomedical datasets CDR and GDA.</p><p>? CDR [Li et al., 2016] is a relation extraction dataset in the biomedical domain, which is aimed to infer the interactions between chemical and disease concepts. ? GDA <ref type="bibr" target="#b14">[Wu et al., 2019]</ref> is a dataset in the biomedical domain, which consists of 23,353 training samples. Differently, the dataset is aimed to predict the interactions between disease concepts and genes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Settings</head><p>Our model was implemented based on Pytorch. We used cased BERT-base, or RoBERTa-large as the encoder on Do-cRED and SciBERT-base <ref type="bibr" target="#b0">[Beltagy et al., 2019]</ref> on CDR and GDA. We optimize our model with AdamW using learning rates 2e?5 with a linear warmup for the first 6% of steps. We set the matrix size N = 42. The context-based strategy is utilized by default. We tuned the hyperparameters on the development set. We trained on one NVIDIA V100 16GB GPU and evaluated our model with Ign F1, and F1 following <ref type="bibr" target="#b18">[Yao et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results on the DocRED Dataset</head><p>We compare DocuNet with graph-based models, including GEDA , <ref type="bibr">LSR [Nan et al., 2020]</ref>, GLRE <ref type="bibr" target="#b12">[Wang et al., 2020a]</ref> and GAIN , HeterGSAN <ref type="bibr" target="#b17">[Xu et al., 2021]</ref>; and transformer-based models, including BERT base <ref type="bibr" target="#b11">[Wang et al., 2019]</ref>, BERT-TS base <ref type="bibr" target="#b11">[Wang et al., 2019]</ref>, HIN-BERT base , CorefBERT base <ref type="bibr" target="#b19">[Ye et al., 2020]</ref>, and ATLOP base on the DocRED dataset. From the <ref type="table" target="#tab_5">Table 2</ref>, we observed that our approach DocuNet-BERT base obtains better results than ATLOP-BERT base . Moreover, we found that our Do-cuNet model obtain a new state-of-the-art result with with RoBERTa-large. As of the IJCAI deadline on 20th of January 2021, we held the first position on the CodaLab scoreboard 2 under the alias DocuNet without external data 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Results on the Biomedical Datasets</head><p>In the biomedical datasets, we compare DocuNet with lots of baselines including: BRAN <ref type="bibr" target="#b10">[Verga et al., 2018]</ref>, EoG <ref type="bibr" target="#b2">[Christopoulou et al., 2019]</ref>, <ref type="bibr">LSR [Nan et al., 2020]</ref>, <ref type="bibr">DHG [Zhang et al., 2020c]</ref>, GLRE <ref type="bibr" target="#b12">[Wang et al., 2020a]</ref> and AT-LOP <ref type="bibr" target="#b23">[Zhou et al., 2021]</ref>. Following ATLOP <ref type="bibr" target="#b23">[Zhou et al., 2021]</ref>, we utilize the <ref type="bibr">SciBERT [Beltagy et al., 2019]</ref> which <ref type="bibr">[1]</ref> is the fourth studio album by American rapper , released on by Aftermath Entertainment, Shady Records, and Interscope Records. <ref type="bibr">[2]</ref> includes the commercially successful singles " ", "Cleanin' Out My Closet", "Superman", and "Sing for the Moment".?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DocuNet BERT</head><p>Performer <ref type="formula" target="#formula_3">(6)</ref> Part of <ref type="formula" target="#formula_1">(12)</ref> Performer <ref type="formula" target="#formula_3">(6)</ref> Part of <ref type="formula" target="#formula_1">(12)</ref> Publication Date <ref type="formula">(31)</ref> Publication Date <ref type="formula">(31)</ref> Performer <ref type="formula" target="#formula_3">(6)</ref> Publication Date (31) 6 . . .   is pre-trained on the scientific publication corpora. From the <ref type="table">Table 3</ref>, we observe model DocuNet-SciBERT base improved the F1 score by 6.9% and 1.4% on CDR and GDA compared with ATLOP-SciBERT base ,</p><formula xml:id="formula_9">... . . . ...</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entity-level Relation Matrix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We conducted an ablation study experiment to validate the effectiveness of different components of our approach. Do-cuNet (Similarity-based) means directly using similarity functions strategy to calculate the correlation between two entities as the input matrix, rather than context-based strategy. w/o U-shaped Segmentation means that our segmentation module is replaced by a feed-forward neural network. w/o balanced softmax refers to the model only with binary crossentropy loss. From <ref type="table" target="#tab_7">Table 4</ref>, we observe that all models have a performance decay without each module, which indicates that both components are beneficial. Besides, we observed that the U-shaped segmentation module and balanced softmax module are most important to model performance and sensitive to F 1 , leading to a drop of 2.18% and 1.32% in dev F 1 score respectively when removed from DocuNet. That reveals that global interdependency among triples captured by our model is effective for document-level RE. Moreover, compared with context-based strategy, our approach based on similarity functions strategy drop by 0.84 F 1 , which illustrates the context-based strategy is advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>We follow GAIN  to select the same example and conduct a case study to further illustrate the effectiveness of our model DocuNet compared with the baseline. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we notice that both BERT base and DocuNet-BERT base can successfully extract the "part of" relation between "Without Me" and "The Eminem Show". However, only our model DocuNet-BERT base is able to deduce that the "performer" and "publication date" of "Without Me" are the same as those of "The Eminem Show", namely, "Eminem" and "May 26, 2002", respectively. Intuitively we can observe that relation extraction mentioned above among those entities requires logical inference across sentences. This interesting observation indicates that our U-shaped segmentation structure over the entity-level relation matrix may implicitly conduct relational reasoning among entities. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Analysis</head><p>To assess the effectiveness of DocuNet in modeling global information for multiple entities, we evaluated models respec-tively trained with or without U-shaped segmentation module on different groups of development set in DocRED, which are divided by the number of entities. From <ref type="figure" target="#fig_4">Figure 5</ref>, we observe that the model w/ U-shaped segmentation module consistently outperforms the model w/o U-shaped segmentation module. We notice that when the number of entities increases, the improvement becomes larger. This indicates that our U-shaped segmentation module can implicitly learn the interdependency among the multiple triples in one context, thus improving the document-level RE performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this study, we took the first step in formulating documentlevel RE as a semantic segmentation task and introducing the Document U-shaped Network. Experimental results showed that our model could achieve better performance by capturing local and global information than baselines. We also empirically observe that convolution over entity-entity relation matrix may implicitly conduct relational reasoning among entities. In the future, we plan to apply our approach to other span-level classification tasks, such as aspect-based sentiment analysis and nest named recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example document with entity pairs and relations from DocRED. Entity mentions and relations only involved in these relation instances are colored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the entity-level relation matrix applied in our formulation. Each cell belongs to one relation type. their effectiveness and strength in relational reasoning. Jia et al. [2019] proposed a model that combines representations learned over various text spans throughout the document and across the sub-relation hierarchy. Christopoulou et al. [2019] proposed an edge-oriented graph neural model (EoG) for document-level RE. Li et al. [2020] characterized the complex interaction between sentences and potential relation instances with a graph-enhanced dual attention network (GEDA). Zhang et al. [2020c] proposed a novel graphbased model with a Dual-tier Heterogeneous Graph (DHG), which contains a structure modeling layer followed by a relation reasoning layer. Zhou et al. [2020] proposes a global context-enhanced graph convolutional network (GCGCN), composed of entities as nodes and the contexts of entity pairs as edges between nodes. Wang et al. [2020a] proposed a novel model (GLRE) that encodes the document information in terms of global and local entity representations as well as context relation representations. Nan et al.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture of our Document U-shaped Network (DocuNet) (Best viewed in color).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Case study on our proposed DocuNet and baseline model. The specific number in the figure indicates the corresponding label id.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Dev results in terms of number of entities on DocRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results (%) on the development and test set of DocRED. We run experiments five times with different random seeds and report the mean and standard deviation on the development set. We report the official test score on the CodaLab scoreboard with the best checkpoint on the development set.</figDesc><table><row><cell>Model</cell><cell>CDR</cell><cell>GDA</cell></row><row><cell>BRAN [Verga et al., 2018]</cell><cell>62.1</cell><cell>-</cell></row><row><cell>EoG [Christopoulou et al.,</cell><cell>63.6</cell><cell>81.5</cell></row><row><cell>2019]</cell><cell></cell><cell></cell></row><row><cell>LSR [Nan et al., 2020]</cell><cell>64.8</cell><cell>82.2</cell></row><row><cell>DHG [Zhang et al., 2020c]</cell><cell>65.9</cell><cell>83.1</cell></row><row><cell>GLRE [Wang et al., 2020a]</cell><cell>68.5</cell><cell>-</cell></row><row><cell>SciBERT base [Beltagy et al.,</cell><cell>65.1</cell><cell>82.5</cell></row><row><cell>2019]</cell><cell></cell><cell></cell></row><row><cell>ATLOP-SciBERT base</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Ablation study of DocuNet on DocRED.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://competitions.codalab.org/competitions/20717#results 3 The SSAN ADAPT model leverages pre-training with external distance supervised data.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We want to express gratitude to the anonymous reviewers for their hard work and kind comments. We thank Ning Ding for helpful discussions and feedback on this paper. This work is funded by National Key R&amp;D Program of China (Funding No. 2018YFB1402800), NSFC91846204.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beltagy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adaprompt: Adaptive prompt-based finetuning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chen</surname></persName>
		</author>
		<idno>abs/2104.07650</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edgeoriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Christopoulou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP/IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5779" to="5786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robin Jia, Cliff Wong, and Hoifung Poon. Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03719</idno>
	</analytic>
	<monogr>
		<title level="m">Yongyi Yang, Xipeng Qiu, and Zheng Zhang. Relation of the relations: A new paradigm of the relation extraction problem</title>
		<editor>J. Li, Yueping Sun, Robin J. Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, A. P</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2537" to="2547" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>NAACL-HLT</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Makoto Miwa and Yutaka Sasaki. Modeling joint entity and relation extraction with table representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">C</forename><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Journal of Biological Databases and Curation</title>
		<editor>Phil Blunsom, Shay B</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
	<note>ACL. Nguyen and Grishman, 2015] Thien Huu Nguyen and Ralph Grishman. Relation extraction: Perspective from convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Nanyun Peng, Hoifung Poon, Chris Quirk, Kristina Toutanova, and Wen tau Yih. Crosssentence n-ary relation extraction with graph lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paramveer</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing</title>
		<editor>Olaf Ronneberger, Philipp Fischer, and Thomas Brox</editor>
		<meeting>the 1st Workshop on Vector Space Modeling for Natural Language Processing<address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015-06-05" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>LNCS</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention-based convolutional neural network for semantic relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang ; Yatian Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2016, 26th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, December 11-16</title>
		<editor>Nicoletta Calzolari, Yuji Matsumoto, and Rashmi Prasad</editor>
		<meeting><address><addrLine>Osaka, Japan</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2526" to="2536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extend &quot;softmax+cross entropy&quot; to multi-label classification problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Jianlin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<ptr target="https://kexue.fm/archives/7359" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Jiangxia Cao, Fang Fang, Shi Wang, and Pengfei Yin. Hin: Hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6398" to="6407" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Verga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Finetune bert for docred with two-step process. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3711" to="3721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Finding influential instances for distantly supervised relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2009.09841</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RE-COMB</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Curriculum-meta learning for order-robust continual relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Denoising relation extraction from document-level distant supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Docred: A large-scale document-level relation extraction dataset</title>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Coreferential reasoning learning for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastive triple extraction with generative transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Bridging text and knowledge with multi-prototype embedding for fewshot relational triple extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1753" to="1762" />
		</imprint>
	</monogr>
	<note>Distant supervision for relation extraction via piecewise convolutional neural networks</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Alicg: Fine-grained and evolvable conceptual graph construction for semantic search at alibaba</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<editor>KDD, 2021. [Zhang et al., 2021b] Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-Seng Chua, and Fei Wu</editor>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5259" to="5270" />
		</imprint>
	</monogr>
	<note>COLING</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
