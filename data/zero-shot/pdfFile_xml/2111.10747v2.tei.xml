<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaIL: A Unified Mask-Image-Language Trimodal Network for Referring Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Control Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Control Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbiao</forename><surname>Mei</surname></persName>
							<email>jianbiaomei@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Control Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Liu</surname></persName>
							<email>yongliu@iipc.zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">College of Control Science and Engineering</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MaIL: A Unified Mask-Image-Language Trimodal Network for Referring Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Referring image segmentation is a typical multi-modal task, which aims at generating a binary mask for referent described in given language expressions. Prior arts adopt a bimodal solution, taking images and languages as two modalities within an encoder-fusion-decoder pipeline. However, this pipeline is sub-optimal for the target task for two reasons. First, they only fuse high-level features produced by uni-modal encoders separately, which hinders sufficient cross-modal learning. Second, the uni-modal encoders are pre-trained independently, which brings inconsistency between pre-trained uni-modal tasks and the target multi-modal task. Besides, this pipeline often ignores or makes little use of intuitively beneficial instance-level features. To relieve these problems, we propose MaIL, which is a more concise encoder-decoder pipeline with a Mask-Image-Language trimodal encoder. Specifically, MaIL unifies uni-modal feature extractors and their fusion model into a deep modality interaction encoder, facilitating sufficient feature interaction across different modalities. Meanwhile, MaIL directly avoids the second limitation since no unimodal encoders are needed anymore. Moreover, for the first time, we propose to introduce instance masks as an additional modality, which explicitly intensifies instancelevel features and promotes finer segmentation results. The proposed MaIL set a new state-of-the-art on all frequentlyused referring image segmentation datasets, including Ref-COCO, RefCOCO+, and G-Ref, with significant gains, 3%-10% against previous best methods. Code will be released soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper focuses on referring image segmentation, which is a typical multi-modal task by generating segmentation on images with the guidance of language descrip-* Equal contribution ? Corresponding Author  tions. Since the real world contains not only vision but also multiple modalities, like audios and languages, this joint vision-text task could push a step further to better visionlanguage understanding, providing more user-friendly and convenient functions for practical applications like image manipulation and editing. The de facto paradigm to tackle the referring segmentation task is the encoder-fusion-decoder pipeline, which first separately extracts image features and text features from two independent uni-modal encoders respectively, then fuses these representations together in a joint embedding space, finally decodes the target mask ( <ref type="figure" target="#fig_1">Fig. 1(a)</ref>). Prior arts following this pipeline have achieved substantial success but still suffer from three limitations: (1) Insufficient modality interaction, especially on low-level features: the modality fusion module is always applied to high-level representations that are learned independently from each modality. A series of works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b36">37]</ref> have been proposed to focus on how to effectively fuse high-level features together after separate extraction, but these works may suffer from low-level detail information loss. Since referring segmentation is a pixel-level task, we believe that the lowlevel features like texture, colors and shapes in images and the features of the words in the language are important for precisely recognizing the target object, while only fusing the high-level features of deep uni-modality encoders always loses the low-level interaction. Therefore, fusing information from different modalities at both early and later stages is necessary to promote cross-modal representation for this task. (2) Unaligned pre-trained tasks: the uni-modal encoders are often pre-trained on uni-modal tasks different from the target task. In detail, the visual feature extractors are usually pre-trained with image classification task on ImageNet <ref type="bibr" target="#b4">[5]</ref> and the language encoders are usually randomly initialized or pre-trained on language classification and generation tasks <ref type="bibr" target="#b5">[6]</ref>. The pre-trained tasks guarantee network's ability on feature extraction in specific modality but inevitably bring inconsistency across different modalities for the cross-modal representations learning in referring image segmentation. (3) Incomplete utilization of instancelevel features: visual embeddings are always treated equally in terms of every location without highlighting in instances. Most of the previous methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref> in this area directly use the global image representations without considering the instance-level features. However, for referring segmentation, the instance-level features should be highlighted since the referent in expression is often prone to describe instances. Some works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref> focus on the instancelevel features by using regions or heatmaps, which are in a coarse and implicit manner.</p><p>To address the above limitations, this paper proposes MaIL, a unified trimodal Mask-Image-Language framework for joint image, language and mask learning, as illustrated in <ref type="figure" target="#fig_1">Fig. 1(b)</ref>. MaIL is mainly different from previous methods in two aspects. First, MaIL models the referring segmentation task as a deep modality interaction problem with a more concise and effective encoder-decoder pipeline. We employ a unified transformer structure as the encoder to fuse features from the input of each modality directly. This pipeline could directly handle the first two limitations since thorough cross-modal interaction is performed in the encoder and no uni-modal pre-training is needed anymore by discarding the deep single-modal encoders. We could adopt multi-modal pre-trained weights for our model, providing better aligned representations for this task compared to pre-training on unimodal tasks. The second different aspect is that our MaIL is a trimodal framework rather than previous bimodal structure by explicitly considering the instance-level object masks as a modality. Intuitively, explicitly introducing the pre-segmented mask information could help the images and languages to pay more attentions to the instance-level features through modality interaction and strengthen the image features for finer prediction. To investigate this, we propose to regard the masks as a kind of modality, then we could build deep interactions among masks, language and image features at different feature levels. Moreover, in the decoder, we adaptively process all the candidate mask features to select the most relevant and informative one and compensate the image features with it as a visual prior to obtain the final prediction.</p><p>Our contributions are three folds:</p><p>? We propose MaIL, which is a new unified Mask-Image-Language trimodal framework for referring image segmentation. The proposed deep modality interaction structure greatly simplifies the whole pipeline and sufficiently strengthens the interaction of multimodal features.</p><p>? For the first time, we propose to introduce object masks as an additional modality for this task. Together with the adaptive mask selection strategy, we make full use of the mask information and find that it indeed benefits a lot to this task. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Referring Image Segmentation</head><p>Referring image segmentation aims at generating binary mask for referent described in given language expressions. Hu et al. <ref type="bibr" target="#b11">[12]</ref> first proposes this task, and they construct an encoder-fusion-decoder pipeline to tackle this problem. They first encode language and image features from two separate encoders, then fuse them by simple concatenation and leverage a fully convolutional network for segmentation. Following this paradigm, proceeding works mainly focus on how to effectively fuse extracted language and image features. Recurrent network in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref> and pyramid feature map in <ref type="bibr" target="#b21">[22]</ref> are utilized to excavate more semantic context for fusion. Luo et al. <ref type="bibr" target="#b27">[28]</ref> proposes to learn referring segmentation and comprehension in a unified manner for better aligned representation. Inspired by the prevalence of attention mechanism in computer vision field, researchers resort to attention mechanism for an effective fusion of multi-modal representations. Ye et al. <ref type="bibr" target="#b36">[37]</ref> develops self-attention and cross-attention modules to capture longrange correlations between two modalities. Hu et al. <ref type="bibr" target="#b12">[13]</ref> constructs two kinds of attention: language-guide visual attention and visual-guided language attention for two modal-ities separately. Recent <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">16]</ref> further employ transformer architecture in fusion stage for its better ability on feature interaction. Additionally, <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36]</ref> regard the fusion stage as a reasoning process. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> utilize graph structure to highlight correct entity and suppress other irrelevant ones. The aforementioned methods all follow the encoderfusion-decoder pipeline and focus on how to better fuse the high-level representations of different modalities. Feng et al. <ref type="bibr" target="#b8">[9]</ref> manage to fuse language features into visual encoder for more thorough feature interaction, but their fusion only takes place in last several layers.</p><p>Unlike existing encoder-fusion-decoder methods, our MaIL follows a more compact encoder-decoder pipeline. With a unified transformer-based encoder, MaIL encourages sufficient feature interaction to begin at the early stage of network, thus better aligning the representations across modalities.</p><p>Top-down perspective is emphasized in another line of works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref> in referring image segmentation. They apply different methods to let network focus on instancelevel features and model their relationships with language features, instead of conducting feature interaction with the whole image's feature map equally. Yu et al. <ref type="bibr" target="#b37">[38]</ref> exploits modular relation scores into Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> framework for coarse region features, and Chen et al. <ref type="bibr" target="#b2">[3]</ref> introduces cycle-consistency to further improve the performance. In <ref type="bibr" target="#b17">[18]</ref> the top-down perspective is adopted implicitly by generating heat map as visual prior and concatenating it with image feature map. These methods utilize instance-level features in an implicit and coarse way. Recent ClawCraneNet <ref type="bibr" target="#b22">[23]</ref> in action segmentation field emphasizes top-down perspective as an object-level sensation that can capture high-level relations. However, their performance can be easily affected by the quality of the pretrained segmentation model since they lack appropriate way to refine the final prediction.</p><p>Different from them, we explicitly introduce object masks as an additional modality, which provides a strong prior and explicitly enhances the instance-level features for refined, precise results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Multi-modal learning</head><p>Multi-modal has been an active research area in recent years. A comprehensive survey of multi-modal learning is beyond the scope of this paper, so we only briefly review several related works here, especially on recent transformerbased image-language representation learning.</p><p>Existing methods for image-language learning could be classified into two categories. The first line takes a dualstream structure, which focuses on learning strong separate encoders for images and languages. Most of works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b33">34]</ref> in this type focus on learning strong representations with massive noisy web data. Methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35]</ref> in the second categories use a sing-stream framework. They mainly research on modeling the modality interaction after the uni-modal encoders. We are inspired by a work <ref type="bibr" target="#b18">[19]</ref> in this line, which designs its entire network as a transformer for efficiency. In this paper, we regard referring image segmentation as a deep multi-modal interaction task and build our network upon existing visual-language pretraining models instead of separate pre-trained image and language encoders. We further introduce the global instance masks as another modality to enhance the instance features, which dramatically improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The overall architecture of MaIL is shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. It first takes the image, the referring expression and the corresponding instance masks as input, then feeds them into the proposed trimodal transformer encoder for sufficient multimodality interaction, and finally generates the mask prediction by the decoder. In the following, we will first briefly revisit the previous encoder-fusion-decoder paradigm and present our compact encoder-decoder pipeline (Sec. 3.1). Then we give an in-depth illustration of our core trimodal transformer encoder (Sec. 3.2). In the end, we will illustrate the decoder with the proposed mask selection strategy in detail (Sec. 3.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Encoder-Decoder Pipeline</head><p>For referring image segmentation task, the input typically consists of an image I ? R H?W ?3 and its corresponding language expression L = {l t } t=1,...,T , where H and W denote the height and width of the input image. T indicates the length of the expression. Previous methods always use an encoder-fusion-decoder paradigm, which first adopts two uni-modal encoders (e.g. ResNet <ref type="bibr" target="#b10">[11]</ref> and GRU <ref type="bibr" target="#b3">[4]</ref>) to extract images features E I and languages features E L separately, and then designs a modality fusion module to fuse representations from different modalities to obtain the fused features F. In the end, F is fed into a decoder to generate the final segmentation prediction P. This paradigm can be formulated as three steps:</p><formula xml:id="formula_0">E I = Encoder I (I), E L = Encoder L (L); F = Fusion(E I , E L ); P = Decoder(F) (1)</formula><p>The above paradigm is faced with several limitations like the lack of interactions in low-level uni-modal features and the misalignment in the pre-trained uni-modal tasks and the target multi-modal task. To resolve these problems, we propose MaIL to combine the original uni-modal encoders and the modality fusion module into one unified  transformer encoder, resulting in a more succinct encoderdecoder paradigm. It encourages the cross-modal feature interaction to start at a very early stage of the network. This is more effective for the low-level feature fusion, especially for this pixel-level segmentation task. Formally, the encoder-decoder pipeline has two steps:</p><formula xml:id="formula_1">F = Encoder(I, L); P = Decoder(F)<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Trimodal Transformer Encoder</head><p>Instance-level features are essential for the referring segmentation task since the referent in expression is often prone to describe specific instances <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b37">38]</ref>. However, there are only a few methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38]</ref> in this area focusing on this point and their usages are usually coarse and implicit, such as using regions or heatmaps. This paper proposes to highlight the instance-level features by explicitly leveraging much more delicate binary masks M={m k } k=1,...,K (K instances in an image), which could be directly generated by an off-the-shelf instance segmentation model <ref type="bibr" target="#b9">[10]</ref>.</p><p>Specifically, MaIL introduces the generated masks as a modality and makes the encoder a trimodal one for joint learning with mask, image and language modalities. Then, the first step in Eq. 2 becomes:</p><formula xml:id="formula_2">F * = Encoder * (M, I, L)<label>(3)</label></formula><p>We adopt the vision transformer structure proposed in ViT <ref type="bibr" target="#b7">[8]</ref> as our encoder to handle the trimodal inputs. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, the input image I ? R H?W ?3 is first sliced into patches and flattened as? ? R N ?(3P 2 ) , where P is the patch size and N = HW/P 2 . Then, followed by the linear projection and added with the positional embedding, the input image is embedded into v I ? R N ?d , where d is the feature dimension of the transformer. A similar procedure is applied on the corresponding K masks. They are then embedded into v M ? R N ?d , where N = K k=1 H k W k /P 2 and {H k , W k } denotes the height and width of valid area in k-th mask. The referring language is embedded to v L ? R T ?d with a word embedding matrix and its position embedding matrix. After that, the embeddings of the three modalities are summed with their corresponding modality type embeddings respectively and then concatenated into a combined representation z 0 ? R (N +N +T )?d before sending to the transformer:</p><formula xml:id="formula_3">z 0 = [v M + t M ; v I + t I ; v L + t L ]<label>(4)</label></formula><p>where t M , t I and t L are the type embeddings of the three modalities respectively. The trimodal transformer encoder consists of D stacked blocks and each block contains a multi-head self-attention (MSA) layer and a multi-layer perceptron (MLP) layer, so the output of the d-th block is: Here LN denotes layer normalization <ref type="bibr" target="#b0">[1]</ref>. The contextualized vector sequence z d is iteratively updated through blocks, thus facilitating sufficient feature interaction across modalities due to the self-attention mechanism. The final output feature vectors of the trimodal encoder, z D , can be divided back to features of different modalities as</p><formula xml:id="formula_4">z d = MSA(LN(z d?1 )) + z d?1 , z d = MLP(LN(? d )) +? d .<label>(5)</label></formula><formula xml:id="formula_5">{z M , z I , z L }.</formula><p>Now the features of each modality are fully attended with the information from others via the trimodal transformer encoder. Finally, we reshape the image and mask features back to the original 2-D image size and obtain the encoder's output F * ={F I , F M }. Note that the dimensions are</p><formula xml:id="formula_6">F I ?R H ?W ?d and F M = F k M ? R H ?W ?d k=1,??? ,K ,</formula><p>where H =H/P and W =W/P due to the patch division.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Decoder</head><p>After sufficient feature interaction in the encoder, the output image and mask features can be regarded as two types of representations containing rich information across different modalities. The image features include the whole image's semantic content and the instance-level mask features focus on each instance's representation. In order to generate precise segmentation results, our decoder needs to combine features from both two aspects. Specifically, we propose an adaptive selection strategy to process various numbers of mask features and choose the most informative one while discarding others. A segmentation head is placed afterward to produce results based on the combination of the whole image features and selected instance-level features.</p><p>Adaptive Mask Selection Strategy. It is evident that using the information of all masks is redundant and utilizing all of them will bring extra distractors, as demonstrated in the experiment (Sec. 4.3). Therefore, we design an adaptive mask selection strategy to figure out the most informative mask features. As illustrated in <ref type="figure" target="#fig_3">Fig. 3</ref>, to make a fair comparison and choice in a common shared embedding space, our MaIL first crops features in the image feature maps (we use the word "aligned" for this type of feature for clarification) corresponding to the mask areas. In detail, for the k-th mask m k , we first obtain the aligned feature vector f k by average pooling on the features of the mask-cropped areas, and further use an MLP layer to derive a corresponding score s k for every mask. Mathematically, this process could be achieved by,</p><formula xml:id="formula_7">f k = AVG(F I m k ), s k = MLP(f k )<label>(6)</label></formula><p>Then, we could select the most informative mask feature F ? M ? R H ?W ?d according to the largest score for further processing, where ? is the index of the maximum score.</p><p>Specifically, we consider a mask feature as the most informative one if it is the most similar to the ground truth. To let the network learn how to distinguish the most relevant instance mask among others, we set a training objective to maximize the predicted score for the mask which has the largest overlap with the ground truth mask wile minimizing the other scores. In detail, we first compute the intersectionover-union (IoU) between the ground truth mask and each input candidate mask. Then, a contrastive loss is applied to the generated scores:</p><formula xml:id="formula_8">L select = ? log exp (s ? ) ? K k=1 exp (s k ) .<label>(7)</label></formula><p>where ? is the index of the mask which has the largest IoU with the ground truth. Next, the selected mask is used to supply the global image feature maps F I . More specifically, we first use two 1?1 convolution layers to reduce the channels by a factor of r to ease the computing cost on image feature map F I and F ? M respectively. Then we concatenate them with a 2-D spatial coordinate map O ? R H ?W ?2 and obtain the concatenated feature mapsF,</p><formula xml:id="formula_9">F = Concat(F I , F ? M , O)<label>(8)</label></formula><p>F will be sent to the next step, the segmentation head. Segmentation Head. To perform pixel-level segmentation in original space, we adopt a progressive up-sampling segmentation structure <ref type="bibr" target="#b39">[40]</ref>. It contains multiple stacked blocks, and the architecture of each block is: 3?3 conv + sync batch norm + ReLU + 2? bi-linear up-sampling, and one 1?1 conv attached at last block as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. We use log 2 P blocks to recover the input resolution. After that, the decoder generates the final mask prediction P ? R H?W ?1 , which has the identical shape to the original image. We use the Focal loss <ref type="bibr" target="#b23">[24]</ref> and DICE loss <ref type="bibr" target="#b30">[31]</ref> to supervise the generated mask, and sum up with ?L select (Eq. 7) as the final training objective of our proposed MaIL, where ? is used to balance the losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed method on three commonly used benchmark datasets for referring image segmenta- tion, including RefCOCO <ref type="bibr" target="#b38">[39]</ref>, RefCOCO+ <ref type="bibr" target="#b38">[39]</ref> and G-Ref <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>, with the mask intersection-over-union (IoU) and Precision@X as evaluation metrics. The details of the datasets and evaluation metrics are shown in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Unlike previous methods which use pre-trained deep unimodal encoders for separate feature extraction, here we first adopt a bert-base-uncased tokenizer to tokenize the text inputs and a publicly available pre-trained modality transformer <ref type="bibr" target="#b18">[19]</ref> for joint image, expression and masks feature extraction and modality interaction as mentioned in Sec. 3.2. The trimodal transformer adopts the original ViT <ref type="bibr" target="#b7">[8]</ref> structure, the patch size P = 32, the embedding dimension d=768, with D=12 blocks and each with 12 attention heads, all following the original setting. The decoder has 5 stacked blocks to resize the intermediate feature map size to 32?. We adopt two settings for image resolution: one is to resize all images to 416 ? 416 as in previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>, the other is to first resize the shorter edge to 384 and limit the longer edge to under 640 while preserving the aspect ratio as <ref type="bibr" target="#b18">[19]</ref>. r in the decoder is set to 3 and ? is set to 0.1 for loss balance. Our off-the-shelf segmentation model is a ResNet-50 Mask R-CNN <ref type="bibr" target="#b9">[10]</ref> pre-trained on instances in train splits of RefCOCO/RefCOCO+/G-Ref.</p><p>And the maximum length for expression is set to 15 for Re-fCOCO and RefCOCO+ and 20 for G-Ref.</p><p>We use AdamW <ref type="bibr" target="#b25">[26]</ref> optimizer with a base learning rate of 10 ?4 and weight decay of 10 ?2 . The training phase contains 10 epochs and the learning rate is warmed up for 10% of the total training steps and decayed linearly to zero for the rest of training. We train our network on 4 NVIDIA RTX3090 GPUs with a batch size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art</head><p>We compare the performance of our proposed MaIL with state-of-the-art methods on three widely-used datasets in Tab. 1. Following prior works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, we report the IoU score here for page limitation and present the full results in the appendix. There are two input resolution configurations implemented as mentioned above. We show the results of both the two versions in Tab. 1 and adopt the fixed aspect ratio in our final version.</p><p>It can be seen that our proposed method consistently outperforms existing methods across all three benchmark datasets by large margins. In particular, MaIL is higher than existing best results with obvious gains of about 3%-5% on the three sets of the RefCOCO. For the more difficult dataset RefCOCO+, MaIL achieves significant improvements on all the splits compared with the previous top results by almost 7%. Furthermore, for G-Ref with more extended and complex expressions, our method yields notable boosts of 8.05%, 6.22% and 9.88% on three splits compared to previous best results.  <ref type="table">Table 3</ref>. Ablation study of mask selection strategies.</p><p>Notably, MaIL achieves greater performance gains than previous methods as the complexity of the dataset increases. This success stems from two aspects: (1) the deep modality interaction of MaIL, where the long and complex sentences could sufficiently interact with the features of masks and images at different levels, then the most useful information for the target object could be easily found. (2) the introduction of the mask modality is beneficial for enhancing the informative instance-level features and build tight relationships between these instance features with the expressions to figure out the most possible candidate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>We conduct a series of ablation experiments to validate our method on the validation split of the RefCOCO+.</p><p>Is the encoder-decoder pipeline helpful? To demonstrate the benefit of our pipeline, we implement variant 1 and variant 6 in Tab. 2, where the former only uses the image and language modalities with a bimodal transformer encoder and the latter removes the multi-modal pre-trained weights from MaIL. Variant 1 obtains 59.20% of IoU score, which already surpasses all the methods in Tab. 1 with a large margin. Variant 6 drops about 8% compared to our MaIL (but still comparable to the state-of-the-art). The results verify our original motivations that our encoderdecoder pipeline could (1) empower the sufficient modality interaction and (2) take advantage of aligned multi-modal pre-training task.</p><p>Is the mask modality beneficial? We demonstrate this problem with different configurations of the modalities of the encoder and the decoder, respectively. For the encoder, we construct three variants: variant 1 (image + language), variant 2 (mask + language) and variant 5 (image + mask + language). Note that for variant 2, we use the maskcropped images to keep the original pixel information inside masks. The results between variant 1 and variant 5 demonstrate that introducing mask as an independent modality can bring 2.89% performance gain (IoU) since it enables the network pay more attention to the instance-level features. On the other hand, directly using mask modality like variant 2 leads to a large performance drop. We attribute it to the loss of the background features and the location information. We provide the visualization in <ref type="figure">Fig. 4</ref> for better understanding. For the input of decoder, we investigate the influence of different modalities when given three modalities inputs in encoder part. Specifically, we have three variants: variant 3 (image), variant 4 (mask) and variant 5 (image + mask). From which we have 3 findings: (1) Simply sending the masks into encoder without any further process in decoder (variant 3) brings little improvement against variant 1. (2) Only feeding mask representations to the decoder (variant 4) causes the performance degradation since image features also matter and provide rich information. (3) Explicitly combining features from mask and image modality (variant 5, i.e., our MaIL) with the proposed select strategy achieves the best results. Therefore, the final conclusion is that the mask modality is beneficial when the modality for both encoder and decoder is properly configured.</p><p>Mask Selection Strategies. Tab. 3 compares different approaches for choosing the mask features from the numerous K mask representations from the encoders. Four strategies are explored: Mean, Maximum, Weighted Sum and our proposed strategy. The first two are simple average pooling or max-pooling operations on the K mask features. Both attempts could improve the performance compared with using no mask features as variant 3 in Tab. 2. The Weighted Sum is the most related one to ours, where the difference is that it normalizes the learned scores then weighted sums all the mask features. This can bring minor improvement against "staring at camera" "vase cracked"</p><p>"orange closest to biggest orange" "remote at 9 o'clock"</p><formula xml:id="formula_10">(a) (b) (c) (d)</formula><p>"two apples together" "apple that is both green and red" the first two since it allows the network to learn where to focus and suppress influence from irrelevant areas. However, these strategies consider embeddings of all masks, thus the irrelevant areas' information can be misleading. Finally, our proposed adaptive selection strategy achieves the best results by directly using mask embedding with the highest score, which brings 2.57% improvement against variant 3. The visualization of the chosen masks in <ref type="figure">Fig. 5</ref> also proves the effectiveness of the proposed strategy.</p><formula xml:id="formula_11">(a) (b) (c) (d)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualization Results</head><p>In <ref type="figure">Fig. 4</ref>, we show the qualitative comparison of our method and when lack of image modality or mask modality, i.e. , variant 5 (MaIL), variant 1 and 2 in Tab. 2. Variant 1 mistakenly segments out referent and its surroundings, especially the distractors, because of the lack of instance-level mask modality. For instance, it fails to distinguish the target vase, fruits and remote with other adjacent objects belong-ing to the same categories. As for variant 2, which only has mask-cropped image areas as inputs and omits the background, its output can be obviously affected by the quality of the pre-segmented input masks and leading to completely wrong results like the examples in the visualization.</p><p>We also present the visualization of the mask selection results in <ref type="figure">Fig. 5</ref>. The mask with the highest score is highlighted using the red frame. It can be seen that our proposed adaptive mask selection strategy can select the most relevant mask. Besides, when looking at the comparison between the MaIL's final result and selected mask, it demonstrates the segmentation head with image feature map can amend flaws in the pre-generated mask. For instance, the selected mask in the first row of <ref type="figure">Fig. 5</ref> segments a spare piece of an area on the left of the target goat, while the final output mask successfully removes this part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present MaIL, a unified network for referring image segmentation. There are two fundamental differences from existing works: (i) Unify the uni-modal feature extractors and fusion models as one deep modality interaction transformer, simplifying the pipeline from encoder-fusion-decoder to encoder-decoder and enabling sufficient modality interaction in multi-level features; (ii) Explicitly introduce the instance masks as an additional modality, constructing a trimodal framework rather than previous bimodal networks and intensifying the beneficial instance-level features. Experiments across diverse benchmarks of this task show that proposed MaIL outperforms state-of-the-art methods with large margins (3%-10%), setting a new baseline for this task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>bread on the score side closest to us The piece of bread on the score side closest to us Decoder (a). Previous Encoder -Fusion -Decoder pipeline (b). Our proposed Encoder -Decoder framework</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Comparison between (a) traditional encoder-fusiondecoder pipeline and (b) our proposed encoder-decoder pipeline which considers referring segmentation as a deep modality interaction task and introduces mask modality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed MaIL. The trimodal encoder fuses feature from the input of each modality directly, jointly learning cross-modal representations among the mask, image and language modality. The decoder produces the segmentation results based on interacted representations with an adaptive mask selection strategy and a segmentation head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Proposed Adaptive Mask Selection Strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Examples of the segmentation results with different input modalities. Given referring expression and (a) image, the other three columns compare the results from (b) MaIL w/o mask modality, (c) MaIL w/o image modality and (d) MaIL. Best view in color. ?? ?? "a bowl of mix vegetables" "goat looking at us" Visualization of the adaptive mask selection results. (a) Input image and expression, (b) input masks, the one with largest score highlighted, (c) MaIL output, (d) ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of the pipeline and modality configuration on the validation set of RefCOCO+. Encoder Modality denotes input modalities of transformer encoder, while Decoder Modality denotes which modalities output of the encoder are fed into the decoder.</figDesc><table><row><cell>Variants</cell><cell>Encoder Modality</cell><cell cols="6">Decoder Modality Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9</cell><cell>IoU</cell></row><row><cell>1</cell><cell>Image + Language</cell><cell>Image</cell><cell>69.10</cell><cell>62.52</cell><cell>53.08</cell><cell>35.31</cell><cell>8.75</cell><cell>59.20</cell></row><row><cell>2</cell><cell>Mask + Language</cell><cell>Mask</cell><cell>55.62</cell><cell>51.52</cell><cell>42.45</cell><cell>24.48</cell><cell>04.27</cell><cell>46.19</cell></row><row><cell>3</cell><cell>Image + Mask + Language</cell><cell>Image</cell><cell>69.24</cell><cell>63.23</cell><cell>54.26</cell><cell>37.27</cell><cell>10.92</cell><cell>59.52</cell></row><row><cell>4</cell><cell>Image + Mask + Language</cell><cell>Mask</cell><cell>67.22</cell><cell>63.51</cell><cell>57.08</cell><cell>39.28</cell><cell>9.63</cell><cell>56.51</cell></row><row><cell>5</cell><cell>Image + Mask + Language</cell><cell>Image + Mask</cell><cell>72.35</cell><cell>67.79</cell><cell>60.54</cell><cell>45.82</cell><cell>15.29</cell><cell>62.23</cell></row><row><cell>6</cell><cell>Image + Mask + Language  ?</cell><cell>Image + Mask</cell><cell>61.89</cell><cell>58.35</cell><cell>52.47</cell><cell>40.52</cell><cell>13.66</cell><cell>53.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>? :</cell></row><row><cell cols="2">without multi-modal pre-train weight [19].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell cols="2">Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9 IoU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Mean</cell><cell cols="2">70.91 65.34 58.02 42.95 13.66 61.26</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Maximum</cell><cell cols="2">71.21 66.22 58.54 43.58 13.47 61.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Weighted Sum 71.07 66.13 59.71 44.34 14.25 61.36</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Adaptive Select 72.35 67.79 60.54 45.82 15.29 62.23</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">See-through-text grouping for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding-Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyng-Luh</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Referring expression object segmentation with caption-aware consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Wen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiantian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Yu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.04748</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Vision-language transformer and query generation for referring segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henghui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suchen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder fusion network with co-attention embedding for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="108" to="124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bi-directional relationship inferring network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Referring image segmentation via cross-modal progressive comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luoqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Linguistic structure guided context modeling for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaofei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sansi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faxi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Comprehensive multimodal interactions for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanishk</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gandhi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10412</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Locate then segment: A strong pipeline for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="9858" to="9867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Vilt: Visionand-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Deepak Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.07651</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Referring image segmentation via recurrent refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaican</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Chun</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Clawcranenet: Leveraging object-level relation for text-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10702</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cascade grouped attention network for referring expression segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-task collaborative network for joint referring expression comprehension and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oana</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Dynamic multimodal instance segmentation guided by natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Margffoy-Tuay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Botero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arbel?ez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Vlad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="792" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Referring image segmentation by generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikui</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1333" to="1344" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zirui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulia</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Bottom-up shift and reasoning for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cross-modal self-attention network for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrigank</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mattnet: Modular attention network for referring expression comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6881" to="6890" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
