<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Manuscript accepted to IEEE Access EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using Accelerated Neuroevolution with Weight Transfer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
							<email>wmcnally@uwaterloo.ca.</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Design Engineering</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo, Waterloo</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanav</forename><surname>Vats</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Design Engineering</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo, Waterloo</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Alexander</forename><surname>Wong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Design Engineering</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo, Waterloo</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mcphee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Systems Design Engineering</orgName>
								<orgName type="department" key="dep2">Artificial Intelligence Institute</orgName>
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo, Waterloo</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Mcnally</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Waterloo</orgName>
								<address>
									<postCode>N2L 3G1</postCode>
									<settlement>Waterloo</settlement>
									<region>ON</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Manuscript accepted to IEEE Access EvoPose2D: Pushing the Boundaries of 2D Human Pose Estimation using Accelerated Neuroevolution with Weight Transfer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/ACCESS.2021.3118207</idno>
					<note>Corresponding author:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search has proven to be highly effective in the design of efficient convolutional neural networks that are better suited for mobile deployment than hand-designed networks. Hypothesizing that neural architecture search holds great potential for human pose estimation, we explore the application of neuroevolution, a form of neural architecture search inspired by biological evolution, in the design of 2D human pose networks for the first time. Additionally, we propose a new weight transfer scheme that enables us to accelerate neuroevolution in a flexible manner. Our method produces network designs that are more efficient and more accurate than state-of-the-art hand-designed networks. In fact, the generated networks process images at higher resolutions using less computation than previous hand-designed networks at lower resolutions, allowing us to push the boundaries of 2D human pose estimation. Our base network designed via neuroevolution, which we refer to as EvoPose2D-S, achieves comparable accuracy to SimpleBaseline while being 50% faster and 12.7x smaller in terms of file size. Our largest network, EvoPose2D-L, achieves new state-of-the-art accuracy on the Microsoft COCO Keypoints benchmark, is 4.3x smaller than its nearest competitor, and has similar inference speed. The code is publicly available at https://github.com/wmcnally/evopose2d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INDEX TERMS</head><p>Artificial intelligence, computer vision, convolutional neural network, deep learning, human pose estimation, neural architecture search, neuroevolution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Two-dimensional human pose estimation is a visual recognition task dealing with the autonomous localization of anatomical human joints, or more broadly, "keypoints," in RGB images and video <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. It is widely considered a fundamental problem in computer vision due to its many downstream applications, including action recognition <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref> and human tracking <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b13">[14]</ref>. In particular, it is a precursor to 3D human pose estimation <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b16">[17]</ref>, which serves as a potential alternative to invasive marker-based motion capture.</p><p>In line with other streams of computer vision, the use of deep learning <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, and specifically deep convolutional neural networks <ref type="bibr" target="#b19">[20]</ref> (CNNs), has been prevalent in 2D human pose estimation <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref>- <ref type="bibr" target="#b23">[24]</ref>. The most accurate 2D human pose estimation methods use a two-stage, top-down pipeline, where an off-the-shelf person detector is first used to detect human instances in an image, and the 2D human pose network is run over the person detections to obtain keypoint predictions <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. This paper focuses on the latter stage of this commonly used top-down pipeline, but we emphasize that our method is applicable to the design of bottom-up human pose estimation networks <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b24">[25]</ref> as well.</p><p>Recently, there has been a growing interest in the use of machines to help design CNN architectures through a process VOLUME 4, 2016 1 arXiv:2011.08446v2 [cs.CV] 4 Oct 2021 known as neural architecture search (NAS) <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b28">[29]</ref>. NAS removes human bias from the design process and permits the automated exploration of diverse network architectures that often transcend human intuition and provide greater accuracy using less computation. Moreover, networks designed using NAS often have fewer parameters <ref type="bibr" target="#b29">[30]</ref>, which reduces the need for expensive main memory access on embedded hardware designed with small memory caches <ref type="bibr" target="#b30">[31]</ref>. Despite the widespread success of NAS in many areas of computer vision <ref type="bibr" target="#b31">[32]</ref>- <ref type="bibr" target="#b37">[38]</ref>, the design of 2D human pose networks has remained, for the most part, human-principled.</p><p>In this study, we explore the application of neuroevolution <ref type="bibr" target="#b38">[39]</ref>, a realization of NAS inspired by evolution in nature, to 2D human pose estimation for the first time. To run large-scale NAS experiments within a practical timeframe, we propose a new weight transfer scheme that is highly flexible and accelerates neuroevolution. We exploit this weight transfer scheme, along with large-batch training on high-bandwidth Tensor Processing Units (TPUs), to run fast neuroevolutions within a search space geared towards 2D human pose estimation. Our neuroevolution framework produces a 2D human pose network that has a relatively simple design, provides state-of-the-art accuracy when scaled, and uses fewer floating-point operations (FLOPs) and parameters than the best performing networks in the literature (see <ref type="figure">Fig.  1</ref>). The key contributions of this research are summarized as follows:</p><p>? We propose a new weight transfer scheme to accelerate neuroevolution and apply neuroevolution to 2D human pose estimation for the first time. In contrast to previous neuroevolution methods that exploit weight transfer, our method is not constrained by complete function preservation <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. Despite relaxing this constraint, our experiments indicate that the level of functional preservation afforded by our weight transfer scheme is sufficient to provide fitness convergence, thereby simplifying neuroevolution and making it more flexible. <ref type="bibr">?</ref> We present empirical evidence that large-batch training (i.e., batch size of 2048) can be used in conjunction with the Adam optimizer <ref type="bibr" target="#b41">[42]</ref> to accelerate the training of 2D human pose networks with no loss in accuracy. We reap the benefits of large-batch training in our neuroevolution experiments by maximizing training throughput on high-bandwidth TPUs. <ref type="bibr">?</ref> We design a search space conducive to 2D human pose estimation and leverage the above contributions to run a fast full-scale neuroevolution of 2D human pose networks (?1 day using eight v2-8 TPUs). As a result, we are able to produce a computationally efficient 2D human pose estimation model that achieves state-ofthe-art accuracy on the most widely used benchmark dataset. model. We review the three most relevant areas of the literature in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. LARGE-BATCH TRAINING OF DEEP NEURAL NETWORKS</head><p>It has been shown that training deep neural networks using large batch sizes with stochastic gradient descent causes a degradation in the quality of the model as measured by its ability to generalize to unseen data <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>. Recently, Goyal et al. <ref type="bibr" target="#b44">[45]</ref> implemented measures for mitigating the training difficulties caused by large batch sizes, including linear scaling of the learning rate, and an initial warmup period where the learning rate is gradually increased. Maximizing training efficiency using large-batch training is critical when the computational demand of training is very high, such as in neural architecture search. However, deep learning methods are often data-dependent and taskdependent, so it remains unclear whether the training measures imposed by Goyal et al. for image classification apply in the general case. It is also unclear whether the learning rate modifications are applicable to optimizers that use adaptive learning rates. Adam <ref type="bibr" target="#b41">[42]</ref> is an example of such an optimizer and is widely used in 2D human pose estimation. In this paper, we empirically investigate the use of large batch sizes in conjunction with the Adam optimizer in the training of 2D human pose networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 2D HUMAN POSE ESTIMATION USING DEEP LEARNING</head><p>The first use of deep learning for human pose estimation came in 2014, when Toshev and Svegedy <ref type="bibr" target="#b0">[1]</ref> regressed 2D keypoint coordinates directly from RGB images using a cascade of deep CNNs. Arguing that the direct regression of pose vectors from images was a highly non-linear and difficult to learn mapping, Tompson et al. <ref type="bibr" target="#b1">[2]</ref> introduced the notion of learning a heatmap representation. Mean squared error (MSE) was used to minimize the distance between the predicted and target heatmaps, where the targets were generated using Gaussians with small variance centered on the ground-truth keypoint coordinates.</p><p>Several of the methods that followed built upon iterative heatmap refinement in a multi-stage fashion including intermediate supervision <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b45">[46]</ref>. Remarking the inefficiencies associated with multi-stage stacking, Chen et al. <ref type="bibr" target="#b22">[23]</ref> proposed the Cascaded Pyramid Network (CPN), a holistic network constructed using a ResNet-50 <ref type="bibr" target="#b46">[47]</ref> feature pyramid <ref type="bibr" target="#b47">[48]</ref>. Xiao et al. <ref type="bibr" target="#b13">[14]</ref> presented yet another single-stage architecture called SimpleBaseline, which stacked transpose convolutions on top of ResNet. Sun et al. <ref type="bibr" target="#b23">[24]</ref> demonstrated with HRNet that maintaining high-resolution features throughout the entire network could provide greater accuracy. HRNet represents the state-of-the-art in 2D human pose estimation among peer-reviewed works at the time of writing.</p><p>An issue surrounding the 2D human pose estimation literature is that it is often difficult to make fair comparisons of model performance due to the heavy use of model-agnostic improvements. Examples include the use of different learning rate schedules <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b48">[49]</ref>, more data augmentation <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref>, loss functions that target more challenging keypoints <ref type="bibr" target="#b22">[23]</ref>, specialized post-processing steps <ref type="bibr" target="#b50">[51]</ref>, <ref type="bibr" target="#b51">[52]</ref>, or more accurate person detectors <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b51">[52]</ref>. These discrepancies in training algorithms can potentially account for the reported differences in accuracy. To directly compare our pose estimation architectures with the state-of-the-art, we re-implement Sim-pleBaseline <ref type="bibr" target="#b13">[14]</ref> and HRNet <ref type="bibr" target="#b23">[24]</ref> and train all networks under the same settings using the same hardware.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. NEUROEVOLUTION</head><p>Neuroevolution is a form of neural architecture search that harnesses evolutionary algorithms to search for optimal network architectures <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>. Network morphisms <ref type="bibr" target="#b40">[41]</ref> and function-preserving mutations <ref type="bibr" target="#b39">[40]</ref> are techniques that reduce the computational cost of neuroevolution. In essence, these methods iteratively mutate networks and perform weight transfer in such a way that the function of the network is completely preserved upon mutation, i.e., the output of the mutated network is identical to that of the parent network. Ergo, the mutated child networks need only be trained for a relatively small number of steps compared to when training from a randomly initialized state. As a result, these techniques are capable of reducing the search time to a matter of GPU days. However, function-preserving mutations can be challenging to implement and also restricting (e.g., complexity cannot be reduced <ref type="bibr" target="#b39">[40]</ref>). Our proposed weight transfer scheme serves as a more flexible alternative that addresses these issues, is effective in accelerating neuroevolution, and has a simpler implementation.</p><p>We briefly discuss the important distinctions between neuroevolution methods that leverage weight transfer, and alternative NAS approaches that leverage weight sharing, such as ENAS <ref type="bibr" target="#b54">[55]</ref> and DARTS <ref type="bibr" target="#b55">[56]</ref>. Weight sharing approaches are sometimes referred to as one-shot architecture search <ref type="bibr" target="#b56">[57]</ref>, because architectures are sampled from a single, over-parameterized supergraph encompassing the entire search space (one-shot model). The search is performed over a single training run of the supergraph, where subgraphs are selected, evaluated using the supergraph weights, and then ranked. The best performing subgraph is finally trained from scratch. One-shot methods are based around the hypothesis that the ranking of the candidate subgraphs correlates with their true ranking following final training. However, Yu et al. observe that this correlation is very weak, and ultimately find that ENAS and DARTS perform no better than a random search <ref type="bibr" target="#b57">[58]</ref>. Moreover, some one-shot methods require the entire supergraph to be kept in memory, which inherently limits the size of the search space. These issues are not a concern in neuroevolution because the candidate architectures are trained separately and thus do not share weights. In a recent benchmarking of NAS algorithms, neuroevolution methods were among the top performing algorithms and consistently outperformed random search <ref type="bibr" target="#b58">[59]</ref>. NAS algorithms have predominantly been developed and evaluated on small-scale image datasets <ref type="bibr" target="#b27">[28]</ref>. The use of NAS in more complex visual recognition tasks remains limited, in large part because the computational demands make it impractical. This is especially true for 2D human pose estimation, where training a single model can take several days <ref type="bibr" target="#b22">[23]</ref>. Nevertheless, the use of NAS in the design of 2D human pose networks has been attempted in a few cases <ref type="bibr" target="#b59">[60]</ref>- <ref type="bibr" target="#b61">[62]</ref>. Although some of the resulting networks provided superior computational efficiency as a result of having fewer parameters and operations, none managed to surpass the best performing hand-crafted networks in terms of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. ACCELERATING NEUROEVOLUTION USING WEIGHT TRANSFER</head><p>Suppose that a pretrained "parent" neural network is represented by the function P x | ? (P) , where x is the input to the network and ? (P) are its learned parameters. The foundation of the proposed neuroevolution framework lies in the process by which the unknown parameters ? (C) in a mutated child network C are inherited from ? (P) such that C x | ? (C) ? P x | ? (P) . That is, the output, or "function," of the mutated child network is similar to the parent, but not necessarily equal. To enable fast neural architecture search, the degree to which the parent's function is preserved must be sufficient to allow ? (C) to be trained to convergence in a small fraction of the number of steps normally required when training from a randomly initialized state.</p><p>To formalize the proposed weight transfer in the context of 2D convolution, we denote W (l) ? R kp1?kp2?ip?op as the weights used by layer l of the parent network, and V (l) ? R kc1?kc2?ic?oc as the weights of the corresponding layer in the mutated child network, where k is the kernel size, i is</p><formula xml:id="formula_0">VOLUME 4, 2016 W (3 x 3 x 32 x o p ) V 1 (5 x 5 x 16 x o c1 ) randomly initialized weights V 2 (3 x 3 x 64 x o c2 ) V W1 V W2 FIGURE 2.</formula><p>Two examples (W ? V1, W ? V2) of the weight transfer used in the proposed neuroevolution framework. The trained weights (shown in blue) in the parent convolutional filter W are transferred, either in part or in full (V W ), to the corresponding filter V in the mutated child network. The weight transfer extends to all output channels in the same manner as depicted here for input channels.</p><p>the number of input channels, and o is the number of output channels. For the sake of brevity, we consider the special case when k p1 = k p2 = k p , k c1 = k c2 = k c , and o p = o c , but the following definition can easily be extended to when</p><formula xml:id="formula_1">k p1 = k p2 , k c1 = k c2 , or o p = o c . The inherited weights V W are given by: V (l) W = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? W (l) p:p+kc, p:p+kc, :ic, : (i c &lt; i p ) ? (k c &lt; k p ) W (l) p:p+kc, p:p+kc, :, : (i c ? i p ) ? (k c &lt; k p ) W (l)</formula><p>:, :, :ic, :</p><formula xml:id="formula_2">(i c &lt; i p ) ? (k c ? k p ) W (l) (i c ? i p ) ? (k c ? k p )</formula><p>where p = 1 2 (k p ?k c ). V W is transferred to V and the remaining non-inherited weights in V are randomly initialized. An illustration of the weight transfer between two convolutional layers is shown in <ref type="figure">Fig. 2</ref>. In principle, the proposed weight transfer can be used with convolutions of any dimensionality (e.g., 1D, 2D, or 3D convolutions), and is permitted between convolutional operators with different kernel size, stride, dilation, input channels, and output channels. More generally, it can be applied to any operations with learnable parameters, including batch normalization and dense layers.</p><p>In essence, the proposed weight transfer method relaxes the function-preservation constraint imposed in <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>. In practice, we find that the proposed weight transfer preserves the majority of the function of deep CNNs following mutation. This enables us to perform network mutations in a simple and flexible manner while maintaining good parameter initialization in the mutated network. As a result, the mutated networks can be trained using fewer iterations, which accelerates the neuroevolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. FAST NEUROEVOLUTION OF 2D HUMAN POSE NETWORKS</head><p>This section includes the engineering details for our neuroevolution implementation that leverages the proposed weight transfer scheme to accelerate the evolution of a 2D human pose network. While we focus on the application of 2D human pose estimation, we note that our neuroevolution approach is generally applicable to all types of deep networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SEARCH SPACE</head><p>Neural architecture search helps moderate human involvement in the design of deep neural networks. However, neural architecture search is by no means fully automatic. To some extent, our role transitions from a network designer to a search designer. Decisions regarding the search space are particularly important because the search space encompasses all possible solutions to the optimization problem, and its size correlates with the amount of computation required to thoroughly explore the space. As such, it is common to exploit prior knowledge in order to reduce the size of the search space and ensure that the sampled architectures are tailored toward the task at hand <ref type="bibr" target="#b62">[63]</ref>.</p><p>Motivated by the simplicity and elegance of the Simple-Baseline architecture <ref type="bibr" target="#b13">[14]</ref>, we search for an optimal human pose estimation backbone using a search space inspired by <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b31">[32]</ref>. Specifically, the search space encompasses a single-branch hierarchical structure that includes seven modules stacked in series. Each module is constructed of chainlinked inverted residual blocks <ref type="bibr" target="#b30">[31]</ref> that use an expansion ratio of six and squeeze-excitation <ref type="bibr" target="#b63">[64]</ref>. For each module, we search for the optimal kernel size, number of inverted residual blocks, and output channels. Considering the newfound importance of spatial resolution in the deeper layers of 2D human pose networks <ref type="bibr" target="#b23">[24]</ref>, we additionally search for the optimal stride of the last three modules. Without going into too much detail, our search space can produce 10 14 unique backbones. To complete the network, an initial convolutional layer with 32 output channels precedes the seven modules, and three transpose convolutions with kernel size of 3x3, stride of 2, and 128 output channels are used to construct the network head. A diagram of the search space is provided in <ref type="figure" target="#fig_0">Fig. 3</ref>. Additional search space details are provided in Appendix A-A. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. FITNESS</head><p>To strike a balance between computational efficiency and accuracy, we perform a multi-objective optimization that minimizes a fitness function including the validation loss and the number of network parameters. Given a 2D pose network represented by the function N x | ? (N ) , the loss L i for a single RGB input image I ? R h?w?3 and corresponding target heatmap S ? R h ?w ?K is given by</p><formula xml:id="formula_3">L i (N , I) = 1 K K j=1 ?(v j &gt; 0) N I | ? (N ) j ? S j 2 2 (1)</formula><p>where K is the number of keypoints and v represents the keypoint visibility flags <ref type="bibr" target="#b0">1</ref> . The target heatmaps S are generated by centering 2D Gaussians with a standard deviation of h 64 pixels on the ground-truth keypoint coordinates and normalizing to a maximum intensity of 255. The overall validation loss is computed as:</p><formula xml:id="formula_4">L(N ) = 1 N N i=1 L i (N , I i )<label>(2)</label></formula><p>where N is the number of image samples in the validation dataset. Finally, the fitness of a network N is given by:</p><formula xml:id="formula_5">J (N ) = T n(? N ) ? L(N )<label>(3)</label></formula><p>where n(? N ) is the number of parameters in N , T is the target number of parameters, and ? controls the fitness tradeoff between the number of parameters and the validation loss. Minimizing the number of parameters instead of the number of floating-point operations (FLOPs) allows us to indirectly minimize FLOPs while not penalizing mutations that decrease the stride of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EVOLUTIONARY STRATEGY</head><p>The evolutionary strategy proceeds as follows. In generation "0", a common ancestor network is manually defined and trained from scratch for e 0 epochs. In generation 1, ? children are generated by mutating the ancestor network. The mutation details are provided in Appendix A-B. The weight transfer outlined in Section III is performed between the ancestor and each child (additional implementation details provided in Appendix A-C), after which the children's weights are trained for e epochs (e e 0 ). At the end of generation 1, the ? networks with the best fitness from the pool of (? + 1) networks (children + ancestor) become the parents in the next generation. In generation 2 and beyond, the mutation ? weight transfer ? training process is repeated and the top-? networks from the pool of (? + ?) networks (children + parents) become the parents in the next generation. The evolution continues until manual termination, typically after the fitness has converged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. LARGE-BATCH TRAINING</head><p>Even with the computational savings afforded by weight transfer, running a full-scale neuroevolution of 2D human pose networks at a standard input resolution of 256x192 would not be feasible within a practical time-frame using common GPU resources (e.g., 8-GPU server). To reduce the search time to within a practical range, we exploit large batch sizes when training 2D human pose networks on TPUs. In line with <ref type="bibr" target="#b44">[45]</ref>, we linearly scale the learning rate with the batch size and gradually ramp-up the learning rate during the first few epochs. In Section V-B, we empirically demonstrate that this training regimen can be used in conjunction with the Adam optimizer <ref type="bibr" target="#b41">[42]</ref> to train 2D human pose networks up to a batch size of 2048 with no loss in accuracy. To our best knowledge, the largest batch size previously used to train a 2D human pose network was 256, which required 8 GPUs <ref type="bibr" target="#b48">[49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. COMPOUND SCALING</head><p>It has been shown recently that scaling a network's resolution, width (channels), and depth (layers) together is more efficient than scaling one of these dimensions individually <ref type="bibr" target="#b31">[32]</ref>. Motivated by this finding, we scale the base network found through neuroevolution to different input resolutions using the following depth (c d ) and width (c w ) coefficients:</p><formula xml:id="formula_6">c d = ? ? c w = ? ? ? = log r ? log r s log ?<label>(4)</label></formula><p>where r s is the search resolution, r is desired resolution, and ?, ?, ? are scaling parameters. For convenience, we use the same scaling parameters as in <ref type="bibr" target="#b31">[32]</ref> (? = 1.2, ? = 1.1, ? = 1.15) but hypothesize that better results could be obtained if these parameters were tuned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. DATASETS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Microsoft COCO</head><p>The 2017 Microsoft COCO Keypoints dataset <ref type="bibr" target="#b64">[65]</ref> is the predominant dataset used to evaluate 2D human pose estimation models. It contains over 200k images and 250k person instances labeled with 17 keypoints. We fit our models to the training subset, which contains 57k images and 150k person instances. We evaluate our models on both the validation and test-dev sets, which contain 5k and 20k images, respectively. We report the standard average precision (AP) and average recall (AR) scores based on Object Keypoint Similarity (OKS) 1 : AP (mean AP at OKS = 0.50, 0.55, . . . , 0.90, 0.95), AP 50 (AP at OKS = 0.50), AP 75 , AP M (medium objects), AP L (large objects), and AR (mean AR at OKS = 0.50, 0.55, . . . , 0.90, 0.95).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) PoseTrack</head><p>PoseTrack <ref type="bibr" target="#b12">[13]</ref> is a large-scale benchmark for 2D human pose estimation and tracking in video. The dataset contains 1,356 video sequences, 46k annotated frames, and 276k person instances. The dataset was converted to COCO format and the COCO evaluation toolbox (pycocotools) was used to evaluate the accuracy in the multi-person human pose estimation task (i.e., using the same accuracy metrics as above). In experiments, we train our models on the 2018 training set (97k person instances), and evaluate on the 2018 validation set (45k person instances) using ground-truth bounding boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. LARGE-BATCH TRAINING OF 2D HUMAN POSE NETWORKS ON TPUS</head><p>To maximize training throughput on TPUs, we run experiments to investigate the training behaviour of 2D human pose networks using larger batch sizes than have been used previously. For these experiments, we re-implement the SimpleBaseline model of Xiao et al. <ref type="bibr" target="#b13">[14]</ref> and train it on the Microsoft COCO dataset. The SimpleBaseline network stacks three transpose convolutions with 256 channels and kernel size of 3x3 on top of a ResNet-50 backbone, which is pretrained on ImageNet <ref type="bibr" target="#b65">[66]</ref>. We run the experiments at an input resolution of 256 ? 192, which yields output heatmap predictions of size 64 ? 48. According to the TensorFlow profiler used, this model has 34.1M parameters and 5.21G FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Implementation Details</head><p>The following experimental setup was used to obtain the results for all models trained on COCO in this paper.   mizer <ref type="bibr" target="#b41">[42]</ref> with a cosine-decay learning rate schedule <ref type="bibr" target="#b66">[67]</ref> and L2 regularization with 1e?5 weight decay. The base learning rate l r was set to 2.5e?4 and was scaled to l r ? n 32 , where n is the global batch size. Additionally, a warmup period was implemented by gradually increasing the learning rate from l r to l r ? n 32 over the first five epochs. The validation loss was evaluated after every epoch using the ground-truth bounding boxes.</p><p>Testing. The common two-stage, top-down pipeline was used during testing <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. We use the same detections as <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref> and follow the standard testing protocol: the predicted heatmaps from the original and horizontally flipped images were averaged and the keypoint predictions were obtained after applying a quarter offset in the direction from the highest response to the second highest response. We do not use non-maximum suppression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Large Batch Training Results</head><p>The batch size was doubled from an initial batch size of 256 until the memory of the v3-8 TPU was exceeded. The maximum batch size attained was 2048. The loss curves for the corresponding training runs are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. While the final training loss increased marginally with batch size, the validation losses converged in the latter part of training, signifying that the networks provide similar accuracy. The AP values provided in <ref type="table" target="#tab_2">Table 1</ref> confirm that we are able to train up to a batch size of 2048 with no loss in accuracy.  <ref type="bibr" target="#b13">[14]</ref>. The bottom two rows highlight the importance of warmup and scaling the learning rate when using large batch sizes.</p><p>We hypothesize that the increase of 0.6 AP over the original SimpleBaseline implementation (AP of 70.4) was due to training for longer (200 epochs versus 140). Additionally, we demonstrate the importance of warmup and learning rate scaling. When training at the maximum batch size, removing warmup resulted in a loss of 1.3 AP, and removing learning rate scaling resulted in a loss of 0.7 AP. While preprocessing the data "online" on the TPU host CPU provides flexibility for training using different input resolutions and data augmentation, it ultimately causes a bottleneck in the input pipeline. This is evidenced by the training times in <ref type="table" target="#tab_2">Table 1</ref>, which decreased after increasing the batch size to 512, but leveled-off at around 5.3 hours using batch sizes of 512 or greater. We expect that the training time could be reduced substantially if preprocessing and augmentation were included in the TFRecord dataset, or if the TPU host CPU had greater processing capabilities. It is also noted that training these models for 140 epochs instead of 200, as in the original implementation <ref type="bibr" target="#b13">[14]</ref>, reduces the training time to 3.7 hours. Bypassing validation after every epoch speeds up training further. For comparison, training a model of similar size on eight NVIDIA TITAN Xp GPUs takes approximately 1.5 days <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. NEUROEVOLUTION</head><p>The neuroevolution described in Section IV was run under various settings on an 8-CPU, 40 GB memory virtual machine that called on eight v2-8 Cloud TPUs to train several generations of 2D human pose networks. The COCO training and validation sets were used for network training and fitness evaluation, respectively. The input resolution used was 256x192, and the target number of parameters T was set to 5M. Other settings, including ?, ?, and ?, are provided in the legend of <ref type="figure" target="#fig_3">Fig. 5 (top)</ref>. ImageNet pretraining was exploited by seeding the common ancestor network using the same inverted residual blocks as used in EfficientNet-B0 <ref type="bibr" target="#b31">[32]</ref>. The ancestor network was trained for 30 epochs, and we utilize the proposed weight transfer scheme to quickly fine-tune the mutated child networks over just 5 epochs. A batch size of 512 was used to provide near-optimal training efficiency (as per results in previous section) and prevent memory exhaustion mid-search. No learning rate warmup was used during neuroevolution, and the only data augmentation used was horizontal flipping. All other training details are the same as in Section V-B1. <ref type="figure" target="#fig_3">Fig. 5 (top)</ref> shows the convergence of fitness for three independent neuroevolutions E1, E2, and E3, which had runtimes of 1.5, 0.8 and 1.1 days, respectively. The gap between the fitness (solid line) and validation loss (dashed line) was larger in E2 and E3 compared to E1, indicating that smaller networks were favored more as a result of decreasing ?. After increasing the number of children from 32 in E2 to 64 in E3, it became apparent that using fewer children may provide faster convergence, but may also cause the fitness to converge to a local minimum. <ref type="figure" target="#fig_3">Fig. 5 (bottom)</ref> plots the validation loss against the number of parameters for all sampled networks. The prominent Pareto frontier near the bottom-left of the figure provides confidence that the search space was thoroughly explored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Neuroevolution Results</head><p>To explicitly demonstrate the benefit of our proposed weight transfer scheme, E3 was run without weight transfer following the same training schedule. As shown in <ref type="figure" target="#fig_3">Fig. 5  (top)</ref>, the fitness never decreased below that of the ancestor VOLUME 4, 2016  network. It stands that the child networks would need to be trained at least as long as the ancestor network (30 epochs in this case) to achieve the same level of convergence without using the proposed weight transfer scheme. As a result, the neuroevolution runtime would increase six-fold. The network with the lowest fitness from neuroevolution E3 was selected as the baseline network, which we refer to as EvoPose2D-S. Its architectural details are provided in <ref type="table" target="#tab_4">Table 2</ref>. Notably, the overall stride of EvoPose2D-S is less than what is typically seen in hand-designed 2D human pose networks. The lowest spatial resolution observed in the network is <ref type="bibr">1 16</ref> the input size, compared to <ref type="bibr">1 32</ref> in SimpleBaseline <ref type="bibr" target="#b13">[14]</ref> and HRNet <ref type="bibr" target="#b23">[24]</ref>. As a result, the output heatmap is twice as large.</p><p>The baseline network was scaled to various levels of computational expense. A lighter version (EvoPose2D-XS) was created by increasing the stride in Module 6, which halved the number of FLOPs. Using the compound scaling method described in Section IV, EvoPose2D-S was scaled to an input resolution of 384x288 (EvoPose2D-M), which is currently the highest resolution used in top-down 2D human pose estimation. We push the boundaries of 2D human pose estimation by scaling to an input resolution of 512x384 (EvoPose2D-L). Even at this high spatial resolution, EvoPose2D-L has roughly half the FLOPs compared to the largest version of HRNet. The scaling parameters for EvoPose2D-M/L are provided in Appendix A-D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Comparisons with the State-of-the-Art</head><p>Microsoft COCO. To directly compare EvoPose2D with the best methods in the literature, we re-implement SimpleBaseline ResNet-50 (SB-R50) and HRNet-W32 as per the implementation described in Section V-B1. In our implementation of HRNet, we use a strided transpose pointwise convolution in place of a pointwise convolution followed by nearestneighbour upsampling. This modification was required to make the model TPU-compatible, and did not change the number of parameters or FLOPs. The accuracy of our implementation is verified against the original in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>Comparing EvoPose2D-S with our SB-R50 implementation without ImageNet pretraining, we find that EvoPose2D-S provides comparable accuracy on the COCO validation set (see <ref type="table" target="#tab_5">Table 3</ref>) but is 50% faster and 12.7x smaller. We also compare EvoPose2D-S to a baseline that stacks the EvoPose2D network head on top of EfficientNet-B0 <ref type="bibr" target="#b31">[32]</ref>, and find that while EvoPose2D-S is 20% slower due to its decreased stride, its AP is 1.8 points higher and it is 2.2x smaller. Compared to our HRNet-W32 (256x192) implementation, we observe that EvoPose2D-M is more accurate by 1.5 AP while being 23% faster and 3.9x smaller.</p><p>Despite not using ImageNet pretraining, EvoPose2D-L achieves state-of-the-art AP on the COCO validation set 2 (with and without PoseFix <ref type="bibr" target="#b50">[51]</ref>) while being 4.3x smaller than HRNet-W48. Since EvoPose2D was designed using the COCO validation data, it is especially important to perform evaluation on the COCO test-dev set. We therefore show in <ref type="table" target="#tab_5">Table 3</ref> that EvoPose2D-L also achieves state-of-the-art accuracy on the test-dev dataset, again without ImageNet pretraining.</p><p>PoseTrack. For evaluation on PoseTrack, the networks were initialized with the weights pretrained on COCO and were fine-tuned on the Posetrack 2018 training set. All training details are consistent with Section V-B1, except the finetuning process was run for 20 epochs and early-stopping was used. As shown in <ref type="table" target="#tab_5">Table 3</ref>, the relative performance of EvoPose2D compared to the state-of-the-art is consistent with the COCO dataset: EvoPose2D-S and EvoPose2D-M provide higher accuracy than SB-R50 and HRNet-W32, respectively, despite having fewer parameters and FLOPS, and faster inference speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose a simple yet effective weight transfer scheme and use it, in conjunction with large-batch training, to accelerate a neuroevolution of efficient 2D human pose networks. To the best of our knowledge, this is the first application of neuroevolution to 2D human pose estimation. We additionally provide supporting experiments demonstrating that 2D human pose networks can be trained using a batch size of up to 2048 with no loss in accuracy. We exploit large-batch training and the proposed weight transfer to evolve a lightweight 2D human pose network design geared towards mobile deployment. When scaled to higher input resolution, the EvoPose2D network designed using neuroevolution proved to be more accurate than the best performing 2D human pose estimation models in the literature while having a lower computational cost. . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A SUPPLEMENTARY MATERIAL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. SEARCH SPACE DETAILS</head><p>A diagram of the hierarchical backbone search space is shown in <ref type="figure" target="#fig_0">Fig. 3</ref>. For each module, we search for the optimal number of blocks, kernel size, output channels, and stride (last three modules only). <ref type="table" target="#tab_7">Table 4</ref> shows the configuration of the common ancestor network used in our neuroevolution experiments. The kernel size options used were 3x3 and 5x5. The maximum number of blocks was set to four. The maximum number of output channels were set to the values in the common ancestor network (see rightmost column of <ref type="table" target="#tab_7">Table 4</ref>). <ref type="table" target="#tab_4">Table 2</ref> shows the architecture of EvoPose2D-S, the network with the best fitness in neuroevolution E3 (see Section V-C). The optimal number of output channels in the first five modules were at the upper bound, so it is possible that better results might be obtained if these limits were increased.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. MUTATION DETAILS</head><p>The sampled architectures were encoded into 7x4 integer arrays (# blocks, kernel size, output channels / 8, and stride, for each module), which we refer to as the genotype. The mutations used included increasing/decreasing the number of blocks by 1, changing the kernel size, increasing/decreasing the stride by 1, and increasing/decreasing the number of output channels by 8. During neuroevolution, the genotypes <ref type="bibr">VOLUME 4, 2016</ref> were cached to ensure that no genotype was sampled twice. The mutation function is provided in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Mutation</head><p>Input: parent genotype gp, ancestor genotype ga, genotype cache G Output: mutated child genotype gc gc ? gp while gc in G or gc = gp do gc ? gp i, j ? randint <ref type="formula">(7)</ref>, randint <ref type="formula" target="#formula_6">(4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. WEIGHT TRANSFER DETAILS</head><p>The child network architectures were decoded from the mutated child genotypes, and all weights in the child networks were randomly initialized. Then, the weight transfer scheme described in Section III was used to transfer the trained weights from the parents to the children. For batch normalization layers, the non-transferred weights were initialized with the means of the parent. When a new block was added as a result of a mutation, the weights from the parent's last block were transferred to the new block in the child.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. SCALING COEFFICIENTS</head><p>The scaling coefficients used for EvoPose2D-M and EvoPose2D-L are provided in <ref type="table" target="#tab_10">Table 5</ref>  He has authored over 560 refereed journal and conference papers and patents, in various fields, such as computational imaging, artificial intelligence, computer vision, graphics, image processing, and multimedia systems. His research interests focus on integrative biomedical imaging systems design, operational artificial intelligence, and scalable and explainable deep learning. He has received a number of awards, including two Outstanding Performance Awards, the Distinguished Performance Award, the Engineering Research Excellence Award, the Sandford PROFESSOR JOHN MCPHEE is the Canada Research Chair in System Dynamics at the University of Waterloo, Canada, which he joined in 1992. Prior to that, he held fellowships at Queen's University, Canada, and the Universit? de Li?ge, Belgium.</p><p>He pioneered the use of linear graph theory and symbolic computing to create real-time models and model-based controllers for multi-domain dynamic systems, with applications ranging from autonomous vehicles to rehabilitation robots and sports engineering. His research algorithms are a core component of the widely-used MapleSim modelling software, and his work appears in more than 160 journal publications.</p><p>Prof. McPhee is the past Chair of the International Association for Multibody System Dynamics, a co-founder of 2 international journals and 3 technical committees, a member of the Golf Digest Technical Panel, and an Associate Editor for 5 journals. He is a Fellow of the Canadian Academy of Engineering, the American and Canadian Societies of Mechanical Engineers, and the Engineering Institute of Canada. He has won 8 Best Paper Awards and, in 2014, he received the prestigious NSERC Synergy Award from the Governor-General of Canada.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 3 .</head><label>3</label><figDesc>Search space diagram.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>FIGURE 4 .</head><label>4</label><figDesc>Training (top) and validation (bottom) losses during training of SimpleBaseline (ResNet-50) [14] on a v3-8 Cloud TPU using various large batch sizes and learning rate schedules.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FIGURE 5 .</head><label>5</label><figDesc>Top: Tracking the network with the best fitness in three independent neuroevolutions. The dashed line represents the validation loss of the network with the lowest fitness. ?: fitness coefficient controlling trade-off between validation loss and number of parameters. ?: number of children. ?: number of parents. Bottom: Validation loss versus the number of network parameters for all sampled networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Fleming Teaching Excellence Award, the Early Researcher Award from the Ministry of Economic Development and Innovation, the Outstanding Paper Award at the CVPR Workshop on Adversarial Machine Learning in Real-World Computer Vision Systems and Online Challenges (2021), the Best Paper Award at the NIPS Workshop on Transparent and Interpretable Machine Learning (2017), the Best Paper Award at the NIPS Workshop on Efficient Methods for Deep Neural Networks (2016), the two Best Paper Awards by the Canadian Image Processing and Pattern Recognition Society in 2009 and 2014, respectively, the Distinguished Paper Award by the Society of Information Display (2015), the two Best Paper Awards for the Conference of Computer Vision and Imaging Systems in 2015 and 2017, respectively, the Synaptive Best Medical Imaging Paper Award (2016), the two Magna Cum Laude Awards and one Cum Laude Award from the Annual Meeting of the Imaging Network of Ontario, the AquaHacking Challenge First Prize (2017), the Best Student Paper at Ottawa Hockey Analytics Conference (2017), and the Alumni Gold Medal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 1 .</head><label>1</label><figDesc>Training times and final AP for large-batch training of SimpleBaseline on Cloud TPU. The original implementation reports an AP of 70.4</figDesc><table><row><cell>Batch size (n)</cell><cell>Warmup</cell><cell>Scale lr</cell><cell>Training Time (hrs)</cell><cell>AP</cell></row><row><cell>256</cell><cell>Y</cell><cell>Y</cell><cell>7.20</cell><cell>71.0</cell></row><row><cell>512</cell><cell>Y</cell><cell>Y</cell><cell>5.42</cell><cell>71.0</cell></row><row><cell>1024</cell><cell>Y</cell><cell>Y</cell><cell>5.25</cell><cell>71.2</cell></row><row><cell>2048</cell><cell>Y</cell><cell>Y</cell><cell>5.32</cell><cell>71.0</cell></row><row><cell>2048</cell><cell>N</cell><cell>Y</cell><cell>5.35</cell><cell>69.7</cell></row><row><cell>2048</cell><cell>Y</cell><cell>N</cell><cell>5.33</cell><cell>70.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 2 .</head><label>2</label><figDesc>The</figDesc><table><row><cell>architecture of our base 2D human pose network,</cell></row><row><cell>EvoPose2D-S, designed via neuroevolution. With h = 256, w = 192, and</cell></row><row><cell>K = 17, EvoPose2D-S contains 2.53M parameters and 1.07G FLOPs.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 3 .</head><label>3</label><figDesc>Comparisons on various datasets. The pycocotools package was used for evaluation. For the COCO datasets, the models in the bottom sections were implemented as per Section V-B1. For the PoseTrack dataset, all models were implemented as per Section V-C2. PT: ImageNet pretraining for evaluation on COCO, and COCO pretraining for evaluation on PoseTrack. *: including PoseFix post-processing [51]. ?: EvoPose2D network head stacked on top of EfficientNet-B0. ?: recalculated for consistency. Network file size provided for float32 models. Frames per second (FPS) averaged over 1k forward passes on NVIDIA TITAN Xp GPU using a batch size of 1. Best results shown in bold.</figDesc><table><row><cell>Method</cell><cell>PT</cell><cell>Input Size</cell><cell>Params (M)</cell><cell>FLOPs (G)</cell><cell>Size (MB)</cell><cell>FPS (GPU)</cell><cell>AP</cell><cell>AP 50</cell><cell>AP 75</cell><cell>AP M</cell><cell>AP L</cell><cell>AR</cell></row><row><cell cols="3">COCO 2017 Validation Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CPN [23]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>27.0</cell><cell>6.20</cell><cell>?</cell><cell>?</cell><cell>68.6</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>SB-R50 [14]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>34.0</cell><cell>5.21  ?</cell><cell>137  ?</cell><cell>67.7  ?</cell><cell>70.4</cell><cell>88.6</cell><cell>78.3</cell><cell>67.1</cell><cell>77.2</cell><cell>76.3</cell></row><row><cell>SB (R-101) [14]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>53.0</cell><cell>8.84  ?</cell><cell>214  ?</cell><cell>45.1  ?</cell><cell>71.4</cell><cell>89.3</cell><cell>79.3</cell><cell>68.1</cell><cell>78.1</cell><cell>77.1</cell></row><row><cell>SB (R-152) [14]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>68.6</cell><cell>12.5  ?</cell><cell>277  ?</cell><cell>34.4  ?</cell><cell>72.0</cell><cell>89.3</cell><cell>79.8</cell><cell>68.7</cell><cell>78.9</cell><cell>77.8</cell></row><row><cell>HRNet-W32 [24]</cell><cell>N</cell><cell>256 ? 192</cell><cell>28.5</cell><cell>7.65  ?</cell><cell>119  ?</cell><cell>29.0  ?</cell><cell>73.4</cell><cell>89.5</cell><cell>80.7</cell><cell>70.2</cell><cell>80.1</cell><cell>78.9</cell></row><row><cell>HRNet-W32 [24]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>28.5</cell><cell>7.65  ?</cell><cell>119  ?</cell><cell>29.0  ?</cell><cell>74.4</cell><cell>90.5</cell><cell>81.9</cell><cell>70.8</cell><cell>81.0</cell><cell>79.8</cell></row><row><cell>HRNet-W48 [24]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>63.6</cell><cell>15.7  ?</cell><cell>259  ?</cell><cell>21.7  ?</cell><cell>75.1</cell><cell>90.6</cell><cell>82.2</cell><cell>71.5</cell><cell>81.8</cell><cell>80.4</cell></row><row><cell>MSPN [49]</cell><cell>Y</cell><cell>256 ? 192</cell><cell>120</cell><cell>19.9</cell><cell>?</cell><cell>?</cell><cell>75.9</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>SB (R-152) [14]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>68.6</cell><cell>28.1  ?</cell><cell>277  ?</cell><cell>24.9  ?</cell><cell>74.3</cell><cell>89.6</cell><cell>81.1</cell><cell>70.5</cell><cell>79.7</cell><cell>79.7</cell></row><row><cell>HRNet-W32 [24]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>28.5</cell><cell>16.0  ?</cell><cell>119  ?</cell><cell>22.7  ?</cell><cell>75.8</cell><cell>90.6</cell><cell>82.7</cell><cell>71.9</cell><cell>82.8</cell><cell>81.0</cell></row><row><cell>HRNet-W48 [24]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>63.6</cell><cell>35.3  ?</cell><cell>259  ?</cell><cell>16.2  ?</cell><cell>76.3</cell><cell>90.8</cell><cell>82.9</cell><cell>72.3</cell><cell>83.4</cell><cell>81.2</cell></row><row><cell>HRNet-W48* [51]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>63.6</cell><cell>35.3  ?</cell><cell>259  ?</cell><cell>16.2  ?</cell><cell>77.3</cell><cell>90.9</cell><cell>83.5</cell><cell>73.5</cell><cell>84.4</cell><cell>82.0</cell></row><row><cell>SB-R50</cell><cell>N</cell><cell>256 ? 192</cell><cell>34.1</cell><cell>5.21</cell><cell>137</cell><cell>67.7</cell><cell>70.6</cell><cell>89.0</cell><cell>78.4</cell><cell>66.9</cell><cell>77.1</cell><cell>77.3</cell></row><row><cell>HRNet-W32</cell><cell>N</cell><cell>256 ? 192</cell><cell>28.6</cell><cell>7.65</cell><cell>119</cell><cell>29.0</cell><cell>73.6</cell><cell>89.9</cell><cell>80.5</cell><cell>70.1</cell><cell>80.0</cell><cell>80.0</cell></row><row><cell>EfficientNet-B0  ?</cell><cell>N</cell><cell>256 ? 192</cell><cell>5.82</cell><cell>0.60</cell><cell>23.9</cell><cell>123</cell><cell>68.4</cell><cell>88.4</cell><cell>76.5</cell><cell>65.0</cell><cell>74.6</cell><cell>75.2</cell></row><row><cell>EvoPose2D-XS</cell><cell>N</cell><cell>256 ? 192</cell><cell>2.53</cell><cell>0.47</cell><cell>10.8</cell><cell>136</cell><cell>68.0</cell><cell>87.9</cell><cell>76.1</cell><cell>64.5</cell><cell>74.3</cell><cell>75.0</cell></row><row><cell>EvoPose2D-S</cell><cell>N</cell><cell>256 ? 192</cell><cell>2.53</cell><cell>1.07</cell><cell>10.8</cell><cell>102</cell><cell>70.2</cell><cell>88.9</cell><cell>77.8</cell><cell>66.5</cell><cell>76.8</cell><cell>76.9</cell></row><row><cell>EvoPose2D-M</cell><cell>N</cell><cell>384 ? 288</cell><cell>7.34</cell><cell>5.59</cell><cell>30.7</cell><cell>35.8</cell><cell>75.1</cell><cell>90.2</cell><cell>81.9</cell><cell>71.5</cell><cell>81.7</cell><cell>81.0</cell></row><row><cell>EvoPose2D-L</cell><cell>N</cell><cell>512 ? 384</cell><cell>14.7</cell><cell>17.7</cell><cell>60.6</cell><cell>15.9</cell><cell>76.6</cell><cell>90.5</cell><cell>83.0</cell><cell>72.7</cell><cell>83.4</cell><cell>82.3</cell></row><row><cell>EvoPose2D-L*</cell><cell>N</cell><cell>512 ? 384</cell><cell>14.7</cell><cell>17.7</cell><cell>60.6</cell><cell>15.9</cell><cell>77.5</cell><cell>90.9</cell><cell>83.6</cell><cell>74.0</cell><cell>84.2</cell><cell>82.5</cell></row><row><cell cols="2">COCO 2017 Test Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CPN [23]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>72.1</cell><cell>91.4</cell><cell>80.0</cell><cell>68.7</cell><cell>77.2</cell><cell>78.5</cell></row><row><cell>SB (R-152) [14]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>68.6</cell><cell>28.1  ?</cell><cell>277  ?</cell><cell>24.9  ?</cell><cell>73.7</cell><cell>91.9</cell><cell>81.1</cell><cell>70.3</cell><cell>80.0</cell><cell>79.0</cell></row><row><cell>HRNet-W48 [24]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>63.6</cell><cell>35.3  ?</cell><cell>259  ?</cell><cell>16.2  ?</cell><cell>75.5</cell><cell>92.5</cell><cell>83.3</cell><cell>71.9</cell><cell>81.5</cell><cell>80.5</cell></row><row><cell>HRNet-W48* [51]</cell><cell>Y</cell><cell>384 ? 288</cell><cell>63.6</cell><cell>35.3  ?</cell><cell>259  ?</cell><cell>16.2  ?</cell><cell>76.7</cell><cell>92.6</cell><cell>84.1</cell><cell>73.1</cell><cell>82.6</cell><cell>81.5</cell></row><row><cell>EvoPose2D-L</cell><cell>N</cell><cell>512 ? 384</cell><cell>14.7</cell><cell>17.7</cell><cell>60.6</cell><cell>15.9</cell><cell>75.7</cell><cell>91.9</cell><cell>83.1</cell><cell>72.2</cell><cell>81.5</cell><cell>81.7</cell></row><row><cell>EvoPose2D-L*</cell><cell>N</cell><cell>512 ? 384</cell><cell>14.7</cell><cell>17.7</cell><cell>60.6</cell><cell>15.9</cell><cell>76.8</cell><cell>92.5</cell><cell>84.3</cell><cell>73.5</cell><cell>82.5</cell><cell>81.7</cell></row><row><cell cols="3">PoseTrack 2018 Validation Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SB-R50</cell><cell>Y</cell><cell>256 ? 192</cell><cell>34.1</cell><cell>5.21</cell><cell>137</cell><cell>67.7</cell><cell>54.3</cell><cell>84.8</cell><cell>63.5</cell><cell>30.5</cell><cell>57.7</cell><cell>59.9</cell></row><row><cell>EvoPose2D-S</cell><cell>Y</cell><cell>256 ? 192</cell><cell>2.53</cell><cell>1.07</cell><cell>10.8</cell><cell>102</cell><cell>55.1</cell><cell>84.9</cell><cell>64.7</cell><cell>32.4</cell><cell>58.5</cell><cell>60.6</cell></row><row><cell>HRNet-W32</cell><cell>Y</cell><cell>256 ? 192</cell><cell>28.6</cell><cell>7.65</cell><cell>119</cell><cell>29.0</cell><cell>58.7</cell><cell>87.0</cell><cell>70.2</cell><cell>36.5</cell><cell>62.0</cell><cell>64.3</cell></row><row><cell>EvoPose2D-M</cell><cell>Y</cell><cell>384 ? 288</cell><cell>7.34</cell><cell>5.59</cell><cell>30.7</cell><cell>35.8</cell><cell>60.4</cell><cell>87.9</cell><cell>71.8</cell><cell>36.1</cell><cell>64.0</cell><cell>64.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 4 .</head><label>4</label><figDesc>Module configuration for the common ancestor network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>. c d scales the number of blocks in each module, rounded to the nearest integer. c w scales the number of output channels used in each block, rounded to the nearest multiple of eight. See Eq. 4 for details on how these values were calculated.</figDesc><table><row><cell>Model</cell><cell>Input Size</cell><cell>?</cell><cell>c d</cell><cell>cw</cell></row><row><cell>EvoPose2D-M</cell><cell>384 ? 288</cell><cell>2.90</cell><cell>1.70</cell><cell>1.32</cell></row><row><cell>EvoPose2D-L</cell><cell>512 ? 384</cell><cell>4.96</cell><cell>2.47</cell><cell>1.60</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 5 .</head><label>5</label><figDesc>Scaling coefficients for EvoPose2D-M/L ALEXANDER WONG (M'06-SM'21) received the B.A.Sc. degree in computer engineering, the M.A.Sc. degree in electrical and computer engineering, and the Ph.D. degree in Systems Design Engineering from the University of Waterloo, Waterloo, ON, Canada, in 2005, 2007, and 2010, respectively. He is currently the Canada Research Chair of Artificial Intelligence and Medical Imaging, the Co-Director of the Vision and Image Processing Research Group, an Associate Professor with the Department of Systems Design Engineering, University of Waterloo, and a member of the College of the Royal Society of Canada.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More details available at https://cocodataset.org/#keypoints-eval.VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">   VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Higher AP has been reported using HRNet with model-agnostic improvements, including a better person detector and unbiased data processing<ref type="bibr" target="#b51">[52]</ref>. 8VOLUME 4, 2016   </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">   VOLUME 4, 2016   </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Applying pose estimation to predict amateur golf swing performance using edge processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Zohdy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Barker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="769" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with deeply learned multi-scale compositional models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="71" to="158" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action recognition in video sequences using deep bi-directional lstm with cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muhammad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Baik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1155" to="1166" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human action recognition based on integrating body pose, part shape, and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>El-Ghaish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shoukry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Onai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">55</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Star-net: Action recognition using spatio-temporal activation reprojection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcphee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CRV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action recognition using deep convolutional neural networks and compressed spatio-temporal pose encodings</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Vision and Imaging Systems</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">GolfDB: A video database for golf swing sequencing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mcnally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dulhanty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcphee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An adaptive viewpoint transformation network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="143" to="076" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Big data deep learning: challenges and perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE access</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="514" to="525" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional networks for images, speech, and time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The handbook of brain theory and neural networks</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">3361</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A survey on neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pedapati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01392</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Nas-unet: Neural architecture search for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="44" to="247" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mnasnet: Platform-aware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Searching for efficient multi-scale architectures for dense image prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Auto-deeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast neural architecture search of compact semantic segmentation models via auxiliary cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nekrasov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">V-nas: Neural architecture search for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Lighttrack: Finding lightweight neural networks for object tracking via one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A study of generalization and fitness landscapes for neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vanneschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="108" to="216" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep learning architecture search by neuro-cell-based evolution with function-preserving mutations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network morphism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T P</forename><surname>Tang</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Rethinking on multi-stage networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00148</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial semantic data augmentation for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Posefix: Model-agnostic general human pose refinement network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">The devil is in the details: Delving into unbiased data processing for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Designing neural networks through neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">O</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Nas-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Pose neural fabrics search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07068</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">AutoPose: Searching multi-scale branch aggregation for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.07018</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">CPNAS: Cascaded pyramid network via neural architecture search for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fujii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kobashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
