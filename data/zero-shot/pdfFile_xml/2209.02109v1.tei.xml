<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SR-GNN: Spatial Relation-aware Graph Neural Network for Fine-Grained Image Categorization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asish</forename><surname>Bera</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Zachary</forename><surname>Wharton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Yonghuai</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Nik</forename><surname>Bessis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ardhendu</forename><surname>Behera</surname></persName>
						</author>
						<title level="a" type="main">SR-GNN: Spatial Relation-aware Graph Neural Network for Fine-Grained Image Categorization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. 00, NO. 0, SEPTEMBER 2022 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Attention mechanism</term>
					<term>Convolutional Neural Networks</term>
					<term>Graph Neural Networks</term>
					<term>Human action</term>
					<term>Fine-grained visual recognition</term>
					<term>Relation-aware feature transformation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Over the past few years, a significant progress has been made in deep convolutional neural networks (CNNs)-based image recognition. This is mainly due to the strong ability of such networks in mining discriminative object pose and parts information from texture and shape. This is often inappropriate for fine-grained visual classification (FGVC) since it exhibits high intra-class and low inter-class variances due to occlusions, deformation, illuminations, etc. Thus, an expressive feature representation describing global structural information is a key to characterize an object/ scene. To this end, we propose a method that effectively captures subtle changes by aggregating contextaware features from most relevant image-regions and their importance in discriminating fine-grained categories avoiding the bounding-box and/or distinguishable part annotations. Our approach is inspired by the recent advancement in self-attention and graph neural networks (GNNs) approaches to include a simple yet effective relation-aware feature transformation and its refinement using a context-aware attention mechanism to boost the discriminability of the transformed feature in an end-to-end learning process. Our model is evaluated on eight benchmark datasets consisting of fine-grained objects and human-object interactions. It outperforms the state-of-the-art approaches by a significant margin in recognition accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE advent of deep convolutional neural networks (CNN) has significantly enhanced image recognition performance in the past decade. It is achieved mainly due to their abilities to provide a high-level description (e.g., global shape and appearance) of image content by capturing discriminative object-pose and -parts information from texture and shape. This high-level description is more apposite for the large-scale visual classification (LSVC) tasks consisting of distinctive categories (e.g., ImageNet and COCO datasets). However, their performance in solving fine-grained visual classification (FGVC) problems is not at the same level as in LSVC. This is mainly due to the subtle changes between hard-todistinguish object classes in FGVC, but most often visually measurable by humans. Common datasets in FGVC include different types of birds <ref type="bibr" target="#b0">[1]</ref>, flowers <ref type="bibr" target="#b1">[2]</ref>, dogs <ref type="bibr" target="#b2">[3]</ref>, aircraft <ref type="bibr" target="#b3">[4]</ref>, car models <ref type="bibr" target="#b4">[5]</ref>, etc. A typical observation in FGVC is that objects from different classes share visually similar structures (large inter-class similarities), and objects in the same class often exhibit significant variations due to different A. <ref type="bibr">Bera</ref>  structures, lighting, clutter and viewpoints (large intra-class variations). As a result, it is a challenging task to learn a unified and discriminative representation for each class. A key step to address this challenge is to extract discriminating features from vital object-parts and combine them for the representation of a consistent distinctive global structure of a given class. The current state-of-the-art (SotA) approaches are mainly craftily designed to extract such discriminative features and structures by exploring 1) part annotations from humans, and 2) automatically finding these discriminative parts from the whole image. We refer the interested readers to <ref type="bibr" target="#b5">[6]</ref> for a detailed survey. Most of the earlier works belong to the first category in which the locations of discriminative object-parts are given (e.g., bounding box or mask). Some methods learn part-based detectors, and some leverage semantic segmentation to localize distinct parts. The parts annotation is a cumbersome and expensive human labeling task that is often prone to human errors and requires expert knowledge. Moreover, partbased methods limit both scalability and practicality of realworld FGVC applications. Thus, many recent methods have used image-level labels to guide their models in identifying the key object parts to discriminate the sub-categories by exploring attention mechanisms in the image space or feature space <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b9">[10]</ref> to automatically mine discriminative features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:2209.02109v1 [cs.CV] 5 Sep 2022</head><p>Motivation: In this work, we propose a simple yet effective connection between the image space and feature space to discriminate subtle variances in FGVC. Our approach is motivated by the recent success of Graph Neural Networks (GNN) <ref type="bibr" target="#b10">[11]</ref> and attention mechanisms <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> in deep learning. Many SotA methods for FGVC use a pre-trained object/part detector (or proposal from mask R-CNN) in a weakly supervised manner, resulting in the absence of detailed description, which is indispensable to capture better object-part and part-part relationships to model the subtle changes. These parts can be affected by occlusions, noisy backgrounds, pose variations and ambiguous repetitive patterns. Thus, multiple partial descriptors for a part are potentially useful in disambiguating and discriminating subtle changes since the model learns meaningful complementary information to provide a rich representation of an object. Our method neither considers object-part proposal/bounding-box nor tries to localize them. Instead, it automatically learns a richer representation of an object by exploring the attention-driven visual-spatial relationships among a pool of geometrically-constrained regions. These regions are generated using the region proposal in the Regional Attention Network (RAN) <ref type="bibr" target="#b7">[8]</ref>, which uses cells and blocks in computing histograms of oriented gradients (HOG).</p><p>To describe a richer representation with the discrimination power for subtle variations, we design a spatial relation-aware GNN (SR-GNN) to model visual-spatial relations between regions. These relationships are captured using a novel relationaware feature transformation and its refinement via attentional context modeling, as conceptually shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. Firstly, a backbone CNN is used to extract high-level visual features. These are upsampled for feature pooling using geometricallyconstrained regions of various sizes and positions ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>). Secondly, it transforms these pooled features using relationaware feature transformation leveraging GNN that captures the visual-spatial relationships via propagating information between regions represented as the nodes of a connected graph ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) to enhance the discriminative power of features. To address the limitations of over-smoothing and a large number of learnable parameters in GNN <ref type="bibr" target="#b10">[11]</ref>, it adapts the topic sensitive PageRank <ref type="bibr" target="#b13">[14]</ref> using the approximate personalized propagation of neural predictions (APPNP) message-passing algorithm via power iteration, achieving linear computational complexity. Then it applies a novel gated attentional pooling to the learned graph nodes for final feature representation. Finally, it employs an attentional context modeling ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>) that explores self-attention <ref type="bibr" target="#b11">[12]</ref> and weighted attention <ref type="bibr" target="#b12">[13]</ref> in an innovative way to learn a weight vector, which is multiplied with the final relation-aware transformed feature extracted in the previous step as a refined feature before classification.</p><p>Performance-wise, CAP <ref type="bibr" target="#b6">[7]</ref> is currently top of the leaderboard for many FGVC datasets. It uses attention to accumulate features from integral regions in a context-aware fashion. Then it uses an LSTM to learn the spatial arrangements (context encoding) of these integral regions for subtle discrimination to tackle FGVC tasks. Finally, it aggregates information by grouping similar responses of the LSTM's hidden states to generate locally aggregated descriptors using NetVLAD in the classification step. Our proposed SR-GNN is significantly different from CAP in the following aspects: (a) introduction of a relation-aware spatial graph with the APPNP messagepassing algorithm to extract more expressive features by capturing visual-spatial relationships via propagating information between regions, (b) a graph-based gated attentional pooling to aggregate features from graph nodes, and (c) an attentional context modeling that consists of self-attention and weighted attention to compute a weight vector for the refinement of the final relation-aware features for classification. The only similarity in both approaches is the feature extraction using a CNN backbone. Although the context-aware attention in CAP and self-attention in SR-GNN (Section III-D) are inspired by the same self-attention mechanism in natural language processing <ref type="bibr" target="#b11">[12]</ref>, they are explored differently to solve the specific problem in hand. In CAP, it accumulates features from various regions and an LSTM is then applied for context encoding by considering sequential information. Whereas in SR-GNN, it is investigated in a novel way to compute a weight vector ( <ref type="figure" target="#fig_1">Fig.  2</ref>(c)) by exploring contextual information via adapting selfattention <ref type="bibr" target="#b11">[12]</ref> and weighted attention <ref type="bibr" target="#b12">[13]</ref>. Unlike in CAP, the weighted attention does not consider sequential information, but learns the weight vector from multiple regions by joint learning. To the best of our knowledge, we are the first to investigate the efficiency of the PageRank algorithm leveraging APPNP to advance the FGVC accuracy. These key concepts are also novel in comparison to other SotA methods, including more recent vision Transformers (ViT) <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>.</p><p>The main contributions of this paper are: 1) A novel relation-aware visual representation and its refinement via attentional spatial context for enriching region-level description to capture the subtle changes and eventually enhance the FGVC performance; 2) An easy-to-use end-to-end FGVC deep network that does not require object/parts bounding boxes annotation or proposal and thus has an advantage of easy implementation; 3) A proposal of a gated attentional pooling for the automatic aggregation of the relation-aware features; and 4) Ablation studies and visual analysis of the performance of SR-GNN.</p><p>The rest of this paper is organized as follows: Section II summarizes related works on FGVC. Section III describes the proposed framework. The experimental results are discussed in Section IV, and an in-depth ablation study is presented in Section V, followed by a conclusion in Section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Our work is closely related to weakly-supervised objectparts, attentional and GNN methods for FGVC, including human actions. We present a concise survey of these approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Object-Parts Based Methods</head><p>Informative object-parts are crucial and are explored <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref> for robust subtle discrimination. Distinct object-parts are selected at multiple scales from object proposals in <ref type="bibr" target="#b18">[19]</ref> to distinguish subtle variations. In <ref type="bibr" target="#b19">[20]</ref>, the objectness map is generated using deep features for part-level and object-level descriptions and their fusion for visual discrimination. Object detection and instance segmentation pipeline are iterated in Context Refinement: firstly, it computes an attention-focused context vector vr from region-pooled features using self-attention. Next, a context refinement weight-vector v is computed over the weighted summation of all vr. Finally, the context vector v is used to refine the transformed feature ft and then feeds it to the Softmax layer for classification. <ref type="bibr" target="#b20">[21]</ref> for complementary part localization, and then LSTM is used to encode contextual details. Similarly, local details are learned from distinct patches which are generated by shuffling the whole image into smaller patches <ref type="bibr" target="#b21">[22]</ref>- <ref type="bibr" target="#b23">[24]</ref>. The global image structure is randomly disrupted by a region confusion technique in <ref type="bibr" target="#b21">[22]</ref> to learn finer details and semantic correlation within sub-regions. Whereas random erasing in <ref type="bibr" target="#b22">[23]</ref> introduces additional noise by object-part occlusion to select informative patches. A region grouping sub-network in <ref type="bibr" target="#b23">[24]</ref> learns correlation weight coefficients between regions to select and refine discriminatory patches. Similarly, vision and language modalities are combined in <ref type="bibr" target="#b24">[25]</ref>. The vision localizes objects using saliency and co-segmentation, while the language applies cross-modal analysis to correlate natural language descriptions and discriminative object parts. A multi-scale and multi-granularity deep reinforcement learning in <ref type="bibr" target="#b25">[26]</ref> finds hierarchical discriminative regions in multiple granularities and automatically determines the number of such regions to boost the accuracy. Likewise, a hierarchical representation of image-regions enhances action classification accuracy in <ref type="bibr" target="#b26">[27]</ref>. Most of these approaches focus on locating the informative object-parts and then extract expressive feature descriptors. In sharp contrast, our method learns distinct features by mining visual-spatial correlations using contextual cues from a set of regions, relying on cells and blocks used in HOG <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Attention-based Approaches</head><p>The attention mechanism is proliferated to identify salient regions and/or subtle discriminatory features to attain superior performance <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. A trilinear attention sampling in <ref type="bibr" target="#b28">[29]</ref> learns features from hundreds of part proposals and then applies knowledge distillation to integrate them. Topdown and bottom-up attentions are combined in an attentional pyramid CNN <ref type="bibr" target="#b29">[30]</ref> to aggregate high-level semantic and lowlevel finer features. In <ref type="bibr" target="#b8">[9]</ref>, a feedback path is connected from a recognition agent to an attention agent to optimize region proposals. Regional attention network (RAN) <ref type="bibr" target="#b7">[8]</ref> presents a hybrid attention method that focuses on semantic informativeness from multiple regional contexts for fine-grained gesture/action recognition. Attend and guide network (AG-Net) <ref type="bibr" target="#b9">[10]</ref> applies to scale-invariant feature transform (SIFT) keypoints and Gaussian mixture model to propose regions that are guided by the attention mechanism for fine-grained visual categorization of objects and human actions. Modular attention in <ref type="bibr" target="#b27">[28]</ref> applies multiple attention modules to focus on regionbased predictions refined by attention gates. Attention on feature channels is explored in <ref type="bibr" target="#b30">[31]</ref> to focus on discriminatory regions. A sparse attentional framework in <ref type="bibr" target="#b31">[32]</ref> follows a selective sampling technique to estimate finer details. A counterfactual attention learning is proposed in <ref type="bibr" target="#b32">[33]</ref> to measure the visual attention quality that guides the learning process via counterfactual intervention to learn more useful attention for enhanced FGVC accuracy. Similarly, object extent learning and spatial context learning are integrated in look-into-object <ref type="bibr" target="#b34">[34]</ref> to understand the object structure by automatically modeling the context information among regions. In <ref type="bibr" target="#b36">[35]</ref>, attentive pairwise interaction network discovers contrastive cues from a pair of images, and discriminates them with pairwise attentional interaction in an end-to-end manner. More recently, a sequence of image patches with positional embedding and multi-head self-attention are integrated in vision Transformers <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> to enhance FGVC. Swin Transformer <ref type="bibr" target="#b15">[16]</ref> exploits a hierarchical shifting window-based self-attention with linear computational complexity. A part selection module is adapted to improve over pure ViT in <ref type="bibr" target="#b16">[17]</ref> by integrating raw attention weights of the Transformer into an attention map to guide the ViT. A complemental attention module and multi-feature fusion module are combined in <ref type="bibr" target="#b17">[18]</ref> using Swin Transformer. Inspired by these, we propose a simple yet effective attention mechanism to refine the GNN-driven features at multi-scale and their aggregation for further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Graph Neural Networks (GNN)</head><p>Following the CNN concept, GNN is proposed to explore problems consisting of non-Euclidean data. It is powerful for smoother messages passing between neighboring nodes to enhance performance <ref type="bibr" target="#b10">[11]</ref>. Recently, it has been explored in zero-shot recognition, multi-label image recognition, image captioning, visual question answering, and the others <ref type="bibr" target="#b37">[36]</ref>. However, its efficacy in FGVC is yet to be fully explored. In <ref type="bibr" target="#b38">[37]</ref>, GNN is used to learn latent attributes by modeling semantic correspondence between discriminative regions within the same sub-category. In <ref type="bibr" target="#b39">[38]</ref>, region correlation is explored to discover informative regions using the crisscross graph propagation sub-network and correlation feature via a unified framework. However, both methods are limited to a few regions per image (e.g., 4) which may be suboptimal for building and propagating information within subnetworks for effective context modeling to address FGVC. A graph-based relation discovery (GaRD) method <ref type="bibr" target="#b40">[39]</ref> learns the positional and semantic feature relationships and adopts a feature grouping strategy to tackle FGVC. We propose a GNNbased spatial relation-aware feature aggregation by considering multiple partial descriptors to propagate and capture finer complementary information between neighboring regions by approximating topic-sensitive PageRank <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED APPROACH</head><p>The proposed SR-GNN architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. It takes as input an input image, extracts a high-level convolutional feature map, applies region-based visual-spatial feature selection and refines the transformed features using an attentional spatial context modeling to advance FGVC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>To train an image classifier, a set of N images I = {I n |n = 1, 2, . . . , N } and their respective class labels are given. The classifier learns a mapping function F that predict? y n = F (I n ), which matches the true label y n . During training, it learns F by minimizing a loss L (y n ,? n ) between the true and the predicted labels. In this work, F is an end-to-end deep network in which we introduce a simple yet effective network modification to advance the SotA in FGVC. The mechanism focuses on two main components to capture the fine-grained changes in images: 1) relation-aware feature selection and transformation, and 2) an attentional context modeling to refine these transformed features. Therefore, the mapping function F consists of:</p><formula xml:id="formula_0">F = Softmax ? ? F 1 (I n ; ? t ) Feature Transform Attentional Context ?(F 2 (I n ; ? c )) ? ? ,<label>(1)</label></formula><p>where ? t and ? c are the learnable parameter sets for the feature transformation from the given image I n to a high-level descriptor and the attentional context refinement, respectively. ?(.) is an element-wise sigmoid function to regulate how much of the transformed feature should be considered in decision making.</p><formula xml:id="formula_1">(1) (2) (3)<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CNN Feature Map and Region Proposals</head><p>We use the lightweight Xception <ref type="bibr" target="#b41">[40]</ref> backbone for extracting CNN features like CAP <ref type="bibr" target="#b6">[7]</ref> and upsample it for region proposals as in <ref type="bibr" target="#b7">[8]</ref> by exploring cells and blocks in the HOG computation. The region proposal generates R possible regions of different aspect ratios and areas. For clarity, 4 regions are shown in <ref type="figure">Fig. 7</ref>. Each region is then represented with a feature vector f of dimension w(width)?h(height)?C(channels) via bilinear interpolation to implement differentiable image transformations (conceptualized in <ref type="figure" target="#fig_1">Fig. 2(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Relation-Aware Feature Transformation</head><p>We represent an image I n using R regions. The essential aim is to update each region's visual representation f by propagating information between regions to characterize their visual-spatial relationships, which capture the subtle variations between them. Thus, the first step to representing these relationships is to build a graph G = (R, E) with nodes R and the connections between them via edges E. As a result, GNN can then be used to learn and reason visual-spatial relationships by propagating messages from one region to its connected neighbors in the graph. The nodes are described by a set X = {f } consisting of a number R of input features f , and the respective output Y = {f } with transformed featuref per node. The graph G is described by the adjacency matrix A ? R R?R . The adjacency matrix? = A + I R denotes A with added self-loops and I R is the identity matrix ( <ref type="figure" target="#fig_1">Fig.  2(b)</ref>). A well-known message passing algorithm is the GNN <ref type="bibr" target="#b10">[11]</ref> in which a simple layer-wise propagation rule is used:</p><formula xml:id="formula_2">H (l+1) = ? ? H (l) W (l) , with H (0) = X and H (L) = Y, l = 0, 1, .</formula><p>. . , L ? 1 being the number of layers, W (l) is a weight matrix for the l-th layer and ?(.) is a non-linear activation function (e.g., ReLU).? =D ?1/2?D?1/2 is the symmetrically normalized adjacency matrix, andD is the diagonal node degree matrix of?. The GNN message passing algorithm is limited to a smaller neighborhood mainly due to 1) aggregation by averaging causes over-smoothing if too many layers are used and thus, loses its focus on the local neighborhood; and 2) a larger neighborhood significantly increases the depth and number of learnable parameters since the common aggregation schemes use learnable weight matrices in each layer. To address these, we adapt the approximate personalized propagation of neural predictions (APPNP) messagepassing algorithm <ref type="bibr" target="#b13">[14]</ref>. It achieves the linear computational complexity by approximating topic-sensitive PageRank via power iteration, relating to a random walk with restarts. Each power iteration step is calculated as:</p><formula xml:id="formula_3">H = MLP (GAP(X); ?) ,</formula><formula xml:id="formula_4">Y (0) = H, Y (k+1) = (1 ? ?)?Y (k) + ?H, Y (K) = Sigmoid (1 ? ?)?Y (K?1) + ?H ,<label>(2)</label></formula><p>where global average pooling (GAP) at each node reduces the dimension of f :</p><formula xml:id="formula_5">w ? h ? C ? 1 ? 1 ? C; ? ? (0, 1]</formula><p>is the teleport (or restart) probability influencing the size of the neighborhood for each node; and K is the number of power iteration steps (k ? [0, K ? 2]). MLP is a multi-layer perceptron with a parameter ? for predicting H that allows preserving the node's local neighborhood, and acts as both the starting vector and the teleport set. For example, every column of H defines a distribution over regions that acts as a teleport set. Note that MLP operates on each node's feature f independently, allowing for parallelization. Each node transforms a region into a feature vectorf . Now the aim is to aggregate all nodes' features (i.e., R ?f ) into a single image-level descriptor f t . We achieve this by adapting the gated attentional pooling <ref type="bibr" target="#b42">[41]</ref> that is computed as:</p><formula xml:id="formula_6">f t = R i=1 ?(f i W 1 + b 1 ) (f i W 2 + b 2 ),<label>(3)</label></formula><p>where weight matrices W 1 and W 2 , and biases b 1 and b 2 are learnable parameters.f i ? Y represents the i th node output feature of the graph G in <ref type="bibr" target="#b1">(2)</ref>. ?(.) is an element-wise sigmoid and acts as a soft attention mechanism that decides which regions are more relevant to the current graph-level task, and is the Hadamard product. The learnable feature transformation parameter in (1) is thus</p><formula xml:id="formula_7">? t = {?, W 1 , W 2 , b 1 , b 2 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Transformed Feature Refinement</head><p>It is inspired by the self-attention mechanism in natural language processing <ref type="bibr" target="#b11">[12]</ref>. The self-attention handles a long path-length contextual modeling by a lightweight gating mechanism in which the attention matrix is generated using a simple dot-product. In self-attention, query Q, key K and value V are learned from the same input, and are different from the traditional attention-based sequence-to-sequence models. Often, Q, K and V are learned by three independent transformation layers. The dot product of Q and K results in the attention weight matrix, which is multiplied with V to produce the desired transformed feature representation. We adapt this principle to compute attention within a given region r (selfloop) as well as between other regions r and r (r, r ? R and r = r). The aim is to generate an attention-focused context vector (i.e., value V) that enables our model to selectively focus on more relevant regions to generate holistic context information. Thus, in our self-attention although Q, K and V vectors are learned from the same input image but focus on different regions i.e., Q is learned from r whereas, K and V are learned from r . Let f r and f r be the highlevel convolutional features representing the regions r and r , respectively. The attention-focused context vector v r ? V for region r is computed as <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>):</p><formula xml:id="formula_8">v r = R r =1 a r,r f r , a r,r = Softmax(W a ? r,r + b a ) ? r,r = tanh( query Q W ? f r + key K W ? f r +b ? ),<label>(4)</label></formula><p>where W ? and W ? are weight matrices for computing Q and K from the respective regions r and r ; W a is their nonlinear combination; b a and b ? are the biases. The attention-driven context vector v r infers the strength of f r conditioned on itself and its neighborhood ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). The final context refinement weight vector v representing all the regions R is computed using an element-wise sigmoid activation function ?(.) over a weighted summation of all v r ? V using an attention</p><formula xml:id="formula_9">importance weight w r . v = ? R r=1v r w r , wherev r = GMP(v r ) and w r = Softmax(W ?vr + b ? ),<label>(5)</label></formula><p>where GMP is the global max-pooling, weight matrix W ? and bias b ? are learnable parameters. It is similar to the approach in <ref type="bibr" target="#b12">[13]</ref> for solving machine translation problems where the model searches for parts of a source sentence relevant to predicting a target word. However, our model does not consider the sequential information but learns to emphasize latent representations of multiple regions by joint learning. To improve the gradient flow, this refinement of weight vector v is used to enhance the relation-aware image-level feature f t in (3) via a skip connection i.e.,f t = f t + f t ? v before passing it to a Softmax layer for estimating the target class probability? n for the image I n . The learnable attentional context refinement parameter in <ref type="formula" target="#formula_0">(1)</ref> is thus</p><formula xml:id="formula_10">? c = {W ? , W ? , W a , b a , b ? , W ? , b ? }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL RESULTS AND DISCUSSIONS</head><p>We first present the datasets, experimental details followed by comparison to the SotA. Then, we analyze our model's complexity followed by qualitative analysis to get an insight into the decision-making process. Finally, we conduct ablation studies to evaluate its key components and parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>Our model avoids object/part bounding box labels for evaluation on eight benchmark datasets (fine-grained objects/ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>TensorFlow 2.0 is used for implementation. Like CAP <ref type="bibr" target="#b6">[7]</ref>, we use Xception <ref type="bibr" target="#b41">[40]</ref> as a backbone CNN. The output dimension 7?7?2048 is upsampled to 42?42?2048 for region pooling <ref type="figure" target="#fig_1">(Fig. 2(a)</ref>). Region proposal in <ref type="bibr" target="#b7">[8]</ref> is used with a HOG cell-size of 14?14 to generate 27 optimal region proposals (R), consisting of a minimum region of 2 cells to a maximum of the full image. The region-pooling size of w=h=7 is used. The feature transformation module ( <ref type="figure" target="#fig_1">Fig.  2(b)</ref>) consists of two GNN layers with an optimal output size of 1024. Each layer contains a single-layer MLP with the teleport probability ?=0.3. The number of channels is kept the same (C=2048) as Xception output. Source codes of SR-GNN will be available via the GitHub repository at https://github.com/ArdhenduBehera/SR-GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental Settings</head><p>Pre-trained ImageNet weights are used to initialize base CNN for faster convergence with the size of images being 256?256. We apply the data augmentation of random rotation (?15 degrees), random scaling (1?0.15), and then random cropping to select the image size of 224?224. The Stochastic Gradient Descent (SGD) is used to optimize the categorical cross-entropy loss function with an initial learning rate of 10 ?3 and multiplied by 0.1 after every 50 epochs. The model is trained for 150 epochs with a mini-batch size of 8 using NVIDIA Titan V GPU (12GB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Performance Comparisons with State-of-the-Art Methods</head><p>The accuracy (%) of SR-GNN over eight datasets and its comparisons to the previous best results (according to the best of our knowledge) are given in <ref type="table" target="#tab_1">Table I</ref>. The accuracy of SR-GNN over each dataset is compared with those of the top-10 SotA methods in the literature in <ref type="table" target="#tab_1">Table II</ref>. These SotA methods are based on attention mechanism [7]- <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b53">[51]</ref>, discriminative object-part localization <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b23">[24]</ref>, mutual reinforcement learning <ref type="bibr" target="#b8">[9]</ref>, GNN <ref type="bibr" target="#b38">[37]</ref>- <ref type="bibr" target="#b40">[39]</ref>, vision transformers <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref>, etc. SR-GNN clearly outperforms all previous methods over eight datasets and their accuracy gain over each dataset is given in parenthesis: Aircraft (0.5%), CUB-200 (0.1%), Cars (0.4%), Dogs (0.2%), Flowers (0.2%), NABirds (0.2%), Stanford-40 (1.0%), and PPMI-24 (0.3%). These margins of improvements are very significant since FGVC is a challenging task to discriminate various subcategories. This is evident from the top-10 SotA accuracies over each dataset <ref type="table" target="#tab_1">(Table II</ref>) that achieve the successive marginal gain between 0.1-0.3% over the past 2-3 years. For example, a cumulative gain of 1.2% is achieved by the top-10 SotA methods over the Cars dataset within the past 3 years with an average of 0.13% (9 successive differences). Our gain of 0.4% over CAP is thus significantly higher. Similarly, a cumulative gain of 1.9% (DCL to CAP) is achieved over the Aircraft dataset (average: 0.21%). Our SR-GNN gains 0.5% in comparison to CAP and is thus also significant. Moreover, some methods attain similar accuracy, e.g., GaRD <ref type="bibr" target="#b40">[39]</ref> and PMG <ref type="bibr" target="#b50">[48]</ref> on Cars: 95.1%. SR-GNN outperforms over eight datasets with a gain of between 0.1% to 1.0%. Moreover, our accuracy gain is 0.1% -1.2% over six FGVC datasets over CAP currently at the top of the leaderboard. Many SotA methods are weakly-supervised such as localization of objects/parts using pre-trained object/part detector and/or proposals using semantic segmentation (e.g., mask R-CNN or Grad-CAM). The process often includes at least two steps: firstly, detect the weakly-supervised regions and then apply the fine-grained recognition. Moreover, additional secondary datasets (e.g., COCO in <ref type="bibr" target="#b20">[21]</ref> for Dogs: 97.1%, and ImageNet in <ref type="bibr" target="#b30">[31]</ref> for Flowers: 97.7%) are used for further training to achieve SotA accuracy <ref type="bibr" target="#b20">[21]</ref>. In sharp contrast, our SR-GNN is a single-step process that is trained end-to-end using only the target datasets and is thus computationally efficient and easy to implement. We have explicitly compared the performance of our method with the SotA ones implemented with ResNet-50 backbone using image sizes of 224?224 and 448?448. The results are given in <ref type="table" target="#tab_1">Table III</ref>. It is evident that CAP performs the best among the existing methods on Aircraft (94.9%), CUB (90.9%), and NABirds (88.8%) with an image size of 224?224 and in this case, our method achieves a very competitive results (? 0.1%). Alternatively, AP-CNN (Cars: 95.4%) <ref type="bibr" target="#b52">[50]</ref>, MCL (Flowers: 96.8%) <ref type="bibr" target="#b30">[31]</ref>, and Cross-X (Dogs: 88.9%) <ref type="bibr" target="#b46">[45]</ref> use an image size of 448?448 with ResNet-50 instead. With such image size, our SR-GNN achieves 95.8% on Cars, and 98.0% on Flowers; and gains a margin of 8.2% over Cross-X on Dogs (SR-GNN: 97.1%). Though, we attain an accuracy of 97.1% over Dogs as CPM <ref type="bibr" target="#b20">[21]</ref>, the latter applies a complex training process using GoogleNet backbone. Clearly, our SR-GNN outperforms many SotA methods with an image size of 224?224 over all the datasets using Xception or ResNet-50 backbone.  <ref type="figure" target="#fig_1">. 2(A)</ref>), AND RELATION-AWARE FEATURE TRANSFORMATION BY GNN <ref type="figure" target="#fig_1">(FIG. 2(B)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>ResNet <ref type="formula">-</ref> More recently, vision Transformer such as ViT <ref type="bibr" target="#b14">[15]</ref> uses fixed-size patches, and Swin Transformer <ref type="bibr" target="#b15">[16]</ref> applies a shifted window scheme to construct a hierarchical representation of patches. In contrast, we use multi-scale regions leveraging GNN for subtle discrimination. ViT often requires largescale training datasets (e.g., JFT-300M, ImageNet-22K, etc.) and then fine-tuning on a target dataset to perform well for FGVC. Unlike CNNs, a Transformer is built with a relatively complex, larger, and heavier architecture. For example, ViT base model consists of 86M parameters and 55.4B GFLOPs <ref type="table" target="#tab_1">(Table IV)</ref>. Recently, CAMF <ref type="bibr" target="#b17">[18]</ref> has demonstrated that a Swin Transformer can achieve better performance than pure ViT with an image size of 448?448. Whereas our method (224?224) outperforms vision Transformers with both sizes of 448?448 and 224?224 with a clear margin on five FGVC datasets <ref type="table" target="#tab_1">(Table IV)</ref>. Our gain (in parenthesis) on each dataset is: Dogs (4.5%), Aircraft (2.1%), Cars (0.8%), CUB (0.7%), and NABirds (0.4%). Moreover, our method incurs significantly less computational overload than the Transformers. For example, SR-GNN (224?224) consists of 30.9M parameters and 9.8B GFLOPs. This is 57.1M parameters and 37.2B GFLOPs lesser than the Swin Transformer. Furthermore, SR-GNN expeditiously outperforms these SotA models with a notable margin with an end-to-end training and simple evaluation protocol avoiding additional secondary data and resource constraints, justifying its wider adaptability. Performance using other SotA base CNNs: SR-GNN uses the lightweight Xception <ref type="bibr" target="#b41">[40]</ref> as a backbone to extract CNN features for further processing. It can easily be integrated into other CNN backbones with a little computational overhead. In order to verify this, we have evaluated our SR-GNN using three different SotA CNN backbones: ResNet-50 <ref type="bibr" target="#b77">[75]</ref>, Inception-V3 <ref type="bibr" target="#b78">[76]</ref>, and NASNetMobile <ref type="bibr" target="#b79">[77]</ref>, with an image resolution of 224?224 over the six FGVC datasets. The results are provided in <ref type="table">Table V</ref>. Our method using these backbones is very similar to the one using Xception and consistently outperforms the SotA approaches in <ref type="table" target="#tab_1">Table II</ref> with the same backbones. However, SR-GNN's accuracy using Xception is slightly higher than the similar backbones such as Inception-V3 and ResNet-50, and is thus our optimal choice. The main reason could be the architectural design of Xception in which depth-wise separable convolutions are used within the Inception module. It is built with a linear stack of depth-wise separable convolutional layers with residual connections. The design leads to a better representation of high-level CNN features in comparison to the ResNet-50 and Inception-V3 architectures. NASNetMobile <ref type="bibr" target="#b79">[77]</ref> is a lightweight model that is designed for mobile and embedded vision systems. It involves significantly less computational cost. From the performance <ref type="table">(Table V)</ref>, the accuracy using this mobile architecture is as competitive as the standard CNNs. Generally, many approaches consider ResNet-50 and our method significantly outperforms those using the same ResNet-50 backbone, as evident from <ref type="table" target="#tab_1">Table II-III.</ref> A similar trend can be observed for Inception-V3. We have also evaluated our model by replacing the region proposals ( <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) and feature transformation using GNN ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) with the CAP [7] model. The results are given in <ref type="table">Table V</ref>. The performance is aligned with the original CAP i.e., the accuracy (Aircraft: 95.1%, CUB: 91.9%, Cars: 95.8%, Dogs: 96.6%, Flowers: 97.8% and NABirds: 90.7%) is superior to SotA methods including CAP, except NABirds on which CAP's accuracy is 91.0% <ref type="bibr" target="#b6">[7]</ref>. Nevertheless, the accuracy on NABirds (90.7%) is still superior to the other approaches in <ref type="table" target="#tab_1">Table II</ref>. Moreover, SR-GNN surpasses these results over Aircraft (0.3%), Cars (0.3%), Dogs (0.7%), Flowers (0.1%), and NABirds (0.5%) with a clear margin and achieves the same accuracy of 91.9% over CUB. This justifies the benefits of our novel GNN-driven relation-aware feature transformation ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) and attentional context refinement <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>) modules, and their significance in enhancing FGVC accuracy. Also, SR-GNN is lighter than CAP requiring 3.3M and 0.4B fewer <ref type="table" target="#tab_1">(Table VII)</ref> parameters and GFLOPs, respectively, implying its computational efficiency. Performance on Human-Object Interactions: To demonstrate our method under general data diversity, we tested our SR-GNN on the Stanford-40 actions <ref type="bibr" target="#b44">[43]</ref> and People Playing Musical Instruments (PPMI-24) <ref type="bibr" target="#b45">[44]</ref> datasets, representing fine-grained human-object interactions. Its accuracy is 98.8% on Stanford-40 and 98.9% on PPMI-24. It outperforms the best results attained by AG-Net (Stanford-40: 97.8%) <ref type="bibr" target="#b9">[10]</ref> and RAN (PPMI-24: 98.6%) <ref type="bibr" target="#b7">[8]</ref>. Our model also learns the importance (weight) of a region via a novel attentional context to refine the transformed features. Whereas, CAP uses LSTM to learn spatial arrangement between regions, and an LSTMdriven feature encoding to aggregate the information from its hidden states. AG-Net and RAN learn features from each region independently without feature interaction and use a Squeeze-and-Excitation block to extract features followed by an attention module.</p><p>A generalized conventional average and bilinear pooling, namely ?-pooling <ref type="bibr" target="#b74">[72]</ref>, achieves an accuracy of 86.0% over the Stanford-40 actions. The ?-pooling enhances the performance in implicit pose normalization (87.7%) <ref type="bibr" target="#b76">[74]</ref> over this dataset, and achieves SotA accuracy compared to other prior works. However, our method attains an impressive margin (11.1%) over this work. Even without feature refinement (W/o Refine), our model achieves the best result over this dataset.</p><p>Some prior methods have extracted traditional/hand-crafted feature descriptors (e.g., SIFT) and applied bag-of-feature encoding techniques <ref type="bibr" target="#b63">[61]</ref>, <ref type="bibr" target="#b69">[67]</ref> over which deep features attain better performance. A reinforcement learning method, DSFNet <ref type="bibr" target="#b75">[73]</ref> captures the global discriminative information and finegrained representations on PPMI-24. Hierarchical learning based on the spatial pyramid is presented in <ref type="bibr" target="#b26">[27]</ref>. Their pretrained networks achieve better performance than the other existing approaches on this dataset. However, our method gains a high margin of 16.6% over their approach. Comparison using mAP evaluation metric: Many works consider the mAP (mean average precision) as an evaluation metric on the above-mentioned two datasets. For a fair comparison, we have evaluated the performance of SR-GNN using mAP. Our approach achieves 96.6% mAP on Stanford-40 which is 0.4% improved over AG-Net (96.2%) <ref type="bibr" target="#b9">[10]</ref>. Similarly, we have attained higher mAP in comparison to the human mask loss (94.1%) <ref type="bibr" target="#b80">[78]</ref>, part-action network (91.2%) <ref type="bibr" target="#b81">[79]</ref>, and many recent works on Stanford-40. We have achieved improved mAP (95.3%) on PPMI-24 over existing works such as VLAD spatial pyramid (81.3%) <ref type="bibr" target="#b82">[80]</ref>, 10-model color fusion (65.9%) <ref type="bibr" target="#b72">[70]</ref>, and the others. While, the mAP of SR-GNN (95.3%) on PPMI-24 is 1.4% lower than RAN (96.7%), it attains 0.3% gain in accuracy.</p><p>The accuracy of our model is compared without the attentional context refinement module <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>) and is given in the 2nd-last row of <ref type="table" target="#tab_1">Table II</ref>. A notable observation is that even without context refinement ("W/o Refine"), SR-GNN outperforms many methods tested on Dogs (96.5%, 2nd-best), NABirds (89.9%, 3rd-best, same as ViT), Stanford-40 (97.9%, best), and PPMI-24 (97.9%, 3rd-best). Also, the accuracies on Aircraft (93.5%), CUB (90.2%), and Flowers (97.1%) are competitively retained within the accuracies of the top-10 SotA methods. Our attentional context refinement module enhances the overall accuracy on diverse datasets, while avoid-  <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) within our model, b) relation-aware transformed feature using GNN <ref type="figure" target="#fig_1">(Fig. 2(b)</ref>), c) attentional context refinement weight-vector v <ref type="figure" target="#fig_1">(Fig.  2(c)</ref>), and d) the final image-level feature mapft for classification ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). Each color represents a particular class. There are 50 classes chosen randomly from the Aircraft's test set. e) SR-GNN without the context refinement module, and f) Standalone Xception base CNN without our modules (re-trained on the Aircraft dataset). ing additional parts-level annotations, vision Transformers, secondary datasets and/or pre-trained subnetworks to enhance the accuracy.   <ref type="table" target="#tab_1">ALL TEST IMAGES  FROM AIRCRAFT DATASET. FOR THE TRAINING AND VALIDATION OF THE BACKBONE CNN (XCEPTION), WE USE THE STANDARD TRANSFER  LEARNING BY FINE-TUNING IT ON THE TARGET DATASET USING THE SAME DATA AUGMENTATION AND HYPER-PARAMETERS (SEC. IV). THE  CLUSTERS GENERATED BY SR-GNN ARE MORE COMPACT AND SEPARATED THAN THE BASELINE XCEPTION (LAST ROW). THE FINAL FEATURE  DESCRIPTION OF SR-GNN IS BETTER THAN THE INDIVIDUAL RELATION-AWARE FEATURE TRANSFORM AND CONTEXT REFINEMENT MODULES.</ref> Feature Extraction Point Aircraft Cars Flowers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SR-GNN's different extraction points</head><p>Base CNN <ref type="figure" target="#fig_1">(Fig. 2(a)</ref>) 4.00 ( <ref type="figure" target="#fig_3">Fig. 4(a)</ref>) 9.89 ( <ref type="figure">Fig. 8(a)</ref>) 1.52 ( <ref type="figure">Fig. 9(a)</ref>) Transformed feature ft <ref type="figure" target="#fig_1">(Fig. 2(b))</ref> 3.34 <ref type="figure" target="#fig_3">(Fig. 4(b)</ref>) 2.49 ( <ref type="figure">Fig. 8(b)</ref>) 1.35 ( <ref type="figure">Fig. 9(b)</ref>) Attentional Context Refinement Weight v <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>) 2.65 <ref type="figure" target="#fig_3">(Fig. 4(c)</ref>) 2.25 <ref type="figure">(Fig. 8(c)</ref>) 1.12 ( <ref type="figure">Fig. 9(c)</ref>) Final featureft = ft + ft ? v <ref type="figure" target="#fig_1">(Fig. 2(c))</ref> 2.07 ( <ref type="figure" target="#fig_3">Fig. 4(d)</ref>) 2.25 ( <ref type="figure">Fig. 8(d)</ref>) 1.02 ( <ref type="figure">Fig. 9(d)</ref>)</p><p>SR-GNN's without the context refinement (weight v) module</p><p>Final feature w/o refinementft = ft 3.42 ( <ref type="figure" target="#fig_3">Fig. 4(e)</ref>) 3.49 ( <ref type="figure">Fig. 8(e</ref>)) 1.19 ( <ref type="figure">Fig. 9</ref>(e))</p><p>Base CNN (Xception) trained on target dataset (Transfer Learning) Xception (baseline) 77.22 <ref type="figure" target="#fig_3">(Fig. 4(f)</ref>) 95.85 ( <ref type="figure">Fig. 8(f)</ref>) 38.67 ( <ref type="figure">Fig. 9(f)</ref>)</p><p>The key modules only add a little overhead to the base CNN in terms of trainable parameters and GFLOPs: 1) relationaware feature transformation ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>): 3.4M and 0.18B; and 2) attentional context modeling <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>): 6.4M and 0.51B. GFLOPs and model's trainable parameters are widely used by the community to compare the computational efficiency of various deep models <ref type="bibr" target="#b85">[83]</ref>. By considering this, our SR-GNN (param: 30.9M, GFLOPs: 9.8B) is computationally a lighter model than CAP (param: 34.2M, GFLOPs: 10.2B) using Xception that is more lightweight than the other SotA models in <ref type="table" target="#tab_1">Table VII</ref> The inference time is dependent on types of GPU, hardware and software environments used. For example, both our SR-GNN and CAP <ref type="bibr" target="#b6">[7]</ref> use the same Titan V GPU (12 GB) to run the model, but CAP is implemented using TensorFlow 1.x whereas, SR-GNN runs using TensorFlow 2.x. It is wellknown that the TensorFlow 2.x is significantly slower 1 than TensorFlow 1.x, resulting in increase of the inference time for SR-GNN (5.0ms) in comparison to CAP (4.2ms) even though the former is more lightweight (param: 30.9M, GFLOPs: 9.8B) than the latter (param: 34.2M, GFLOPs: 10.2B). Moreover, the per-image inference time of our SR-GNN is 0.8ms and 0.1ms higher than CAP and MRDMN-L <ref type="bibr" target="#b49">[47]</ref>, respectively. SR-GNN without refinement (4.9ms) shares the same inference time with MRDMN-L, but gains 7.5% higher accuracy on Dogs. A precise comparison with the existing top-10 SotA methods focusing on the inference time is given in the supplementary document, irrespective of the GPU and hardware configuration, deep learning tools (e.g., TensorFlow, PyTorch, MXNet, etc.) and related experimental constraints used in those works. Moreover, the Transformers are computationally more complex than SR-GNN as shown in <ref type="table" target="#tab_1">Table IV</ref>.</p><p>Some works <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b64">[62]</ref> improve the accuracy by exploring secondary data. Also, such methods involve multiple steps and are resource-intensive. For example, there are three steps in <ref type="bibr" target="#b20">[21]</ref>: 1) object detection and segmentation using Mask R-CNN and a conditional random field (CRF); 2) complementary 1 https://github.com/tensorflow/tensorflow/issues/33487 part mining using 512 regions; and 3) classification using context gating. Their model is trained using 4 GPUs (12GB each), per-image inference time is 27ms for Step 3 and extra 227ms in Step 2. Our model is trained on a single GPU (12GB) with per-image inference time of 5ms only. So, SR-GNN is faster and lighter than most of the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative Analysis and Visualization</head><p>To get insight into our model's decision-making process, we visualize the feature maps at key steps. Each step provides the discriminability of our model by visualizing the class separability and compactness. To achieve this, we use t-SNE <ref type="bibr" target="#b83">[81]</ref>, which is shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. Randomly selected 50 classes are chosen from the Aircraft test-set. The test images are processed to extract features from base CNN <ref type="figure" target="#fig_1">(Fig.  2(a)</ref>), relation-aware transformed feature ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>), context refinement weight vector <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>) and the final refined feature descriptor. The respective visualization (unique color per class) is presented in <ref type="figure" target="#fig_3">Fig. 4</ref>. It is evident that clusters representing both relation-aware features ( <ref type="figure" target="#fig_3">Fig. 4(b)</ref>) and context weight vector ( <ref type="figure" target="#fig_3">Fig. 4(c)</ref>) are further apart and more compact compared to the base CNN features ( <ref type="figure" target="#fig_3">Fig. 4(a)</ref>). Moreover, the clusters representing the final refined feature map ( <ref type="figure" target="#fig_3">Fig.  4(d)</ref>) is further enhanced, resulting in a clearer distinction between various clusters representing different classes. In addition, the importance of the context refinement task is visualized by avoiding it from the final feature vector ( <ref type="figure" target="#fig_3">Fig.  4(e)</ref>). Lastly, standalone Xception is fine-tuned by discarding our proposed modules, and its impact is shown in <ref type="figure" target="#fig_3">Fig. 4(f)</ref>. Overall, these qualitative visualizations evince the essence of key components and superior performance of SR-GNN.</p><p>We have further computed the Davies-Bouldin index <ref type="bibr" target="#b84">[82]</ref> to quantitatively evaluate the cluster similarities using the t-SNE outputs given in <ref type="table" target="#tab_1">Table VIII</ref>. This index signifies the similarity between clusters, where the similarity is the ratio of withincluster distances to between-cluster distances. As a result, a lower value implies better clustering. For the Aircraft testset, these values are 4.00 ( <ref type="figure" target="#fig_3">Fig. 4(a)</ref>), 3.34 ( <ref type="figure" target="#fig_3">Fig. 4(b)</ref>), 2.65 ( <ref type="figure" target="#fig_3">Fig. 4(c)</ref>), and 2.07 ( <ref type="figure" target="#fig_3">Fig. 4(d)</ref>). Whereas, the value increases without feature refinement 3.42 ( <ref type="figure" target="#fig_3">Fig. 4(e)</ref>), and it is very high  <ref type="figure" target="#fig_1">(FIG. 2(B)</ref>), <ref type="bibr">AND</ref>   <ref type="figure" target="#fig_1">. 2(A)</ref>).  <ref type="bibr">(77.22)</ref> using Xception backbone only ( <ref type="figure" target="#fig_3">Fig. 4(f)</ref>). The results on Cars and Flowers are given in the supplementary document.</p><p>In order to understand how the relation-aware transformation is exploited ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) in our SR-GNN, we also visualize the cosine similarity to measure the pairwise relationships between each pair of nodes (regions) in the graph as is shown in <ref type="figure">Fig. 5</ref>. It is not easy to visualize the graph structure associating object parts with categories. Thus, pairwise cosine similarities of nodes representing regions are explored to reflect how each object is overall represented and distinguished from each other. It is evident that the graph indeed captures the relational structure to discriminate subordinate categories. The main reason is that ? in (2) preserves locality to avoid oversmoothing by staying close to the root node and leveraging the information from a large neighborhood.</p><p>We have also looked inside our attentional context refinement module <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>) to visualize the class-specific visual relationships jointly learned during the training. These relationships are learned as an R?R joint attention map and are shown in <ref type="figure">Fig. 6(a)</ref> for the 'A340-200', 'ATR-72', 'DC-10', 'ERJ 135' aircraft sub-types where R=27. Each column represents a region conditioned on itself and other regions linking rows. Blue to red signifies the class-specific less to more attention towards that region. We further link the top-2 regions (cols) to their respective top-3 joint visual attentions (rows) by exploring the attention map. These are shown in <ref type="figure">Fig. 6</ref>(b) and 6(c) for the respective 'Boeing 737-600' and 'F-16A B' Aircraft sub-type. These regions are drawn in the original image to show their joint relationships. From both figures, it is evident that our model learns to focus on the key context information for discriminating subtle variations. More results for visualization are given in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ABLATION STUDY</head><p>The ablative study is conducted from several important aspects: suitability of using uniform-grid as an alternative to our adopted region proposals, exploring SotA graph-based pooling techniques <ref type="bibr" target="#b86">[84]</ref>, <ref type="bibr" target="#b87">[85]</ref>, efficacy of each key components of SR-GNN, impact of the number of regions (R) on recognition accuracy, influence of GNN layer's output dimensionality in performance, and the number of power-iterations in GNN layers.</p><p>1) Formation of Region Proposals and Key Modules: The regions with variable areas and aspect ratios that are akin to computing the HOG cells and blocks are preferred here. ViT uses uniform regions such as 16?16 or 14?14 for an image resolution of 224?224. Inspired by this, our method is tested with a uniform grid-structure as an alternative for generating region proposals. The results with 4?4 grid (region-size is 16?16 for up-sampled feature resolution of 64?64) on four datasets are given in <ref type="table" target="#tab_1">Table IX (row 1)</ref>, which is the best among other grid-sizes of 2?2 (regionsize: 32?32), 3?3 (region-size: 21?21), and 5?5 (regionsize: 13?13). Even though the accuracy with a regular grid of 4?4 is better than many existing approaches (reported in <ref type="table" target="#tab_1">Table II)</ref>, it is not a suitable choice for generating regions to learn finer details. This is evident from the accuracy gain of SR-GNN using regions of different aspect ratios and areas in comparison to the 4?4 uniform grid. These gains are: 3.2% over Cars, 1.1% over Aircraft, 1.0% over Dogs, and 0.9% over Flowers. These results show that regular regions are not pertinent enough to capture subtle variations for spatial relation modeling among the regions. Moreover, SR-GNN outperforms vision Transformers <ref type="table" target="#tab_1">(Table II)</ref> that use regular regions but fail to capture the overall object structure, and hence, do not achieve superior results as ours. Also, Mask-RCNN is used for region proposals in CPM <ref type="bibr" target="#b20">[21]</ref> which has attained 1.5% and 0.2% lower accuracy than ours over the respective CUB and Dogs datasets <ref type="table" target="#tab_1">(Table II)</ref>. Thus, all these results justify the benefits of our method in exploring multi-scale regions.</p><p>We have evaluated the efficacy of the existing SotA graphbased pooling methods to compare the performance with the chosen gated attentional pooling to pool features from the nodes of the relation-aware GNN. These are the global average pooling, global max pooling, global sum pooling <ref type="bibr" target="#b86">[84]</ref>, and sort pooling <ref type="bibr" target="#b87">[85]</ref>. The results are shown in <ref type="table" target="#tab_1">Table IX</ref>. It is evident that the gated attentional pooling performs better than these alternative methods. This is mainly because the gated attentional pooling uses element-wise sigmoid and acts as a soft attention mechanism that decides which nodes (regions) are more relevant to the current graph-level classification by selecting the most discriminative features from the regions and is thus more suitable to capture their subtle variances than the other pooling approaches. Next, the impact of varied key modules (shown in <ref type="figure" target="#fig_1">Fig. 2</ref>) are evaluated over 4 datasets <ref type="table" target="#tab_1">(Table  IX)</ref>  2) Number of region proposals: The impact of different numbers (R) of regions on the accuracy of our SR-GNN is given in <ref type="table">Table X</ref>. The regions are generated by varying the cell size (Section III-B) as suggested in <ref type="bibr" target="#b7">[8]</ref>. Four regions are shown in <ref type="figure">Fig. 7</ref>, and the rest are given in the supplementary document. Different regions are generated by controlling the HOG's cell size. The best accuracy is achieved for cell size of 14?14 i.e., R=27. Moreover, our model complexity and perimage inference time with increasing numbers of regions are presented in <ref type="table" target="#tab_1">Table VI</ref>. The number of trainable parameters does not depend on the number of regions, whereas the GFLOPs and the per-image inference time increase with the number of regions, as expected.</p><p>3) Impact of the neighborhood size of a given node on accuracy: The neighborhood size of a given node in our SR-GNN ( <ref type="figure" target="#fig_1">Fig. 2(b)</ref>) is controlled by ? in <ref type="bibr" target="#b1">(2)</ref>. In order to measure its impact on accuracy, we evaluate various values of ? ? [0.1, 0.8] on the Aircraft, Cars, Dogs and Flowers datasets. The results for GNN output dimensions of 512 and 1024 are shown in <ref type="table" target="#tab_1">Table XI</ref>. It is clear that the accuracy increases with the increasing value of ? and reaches a maximum of around 0.3, suggesting the optimal size of the local neighborhood of a given node. We observe a similar trend for the GNN layers with output dimensions of 512 and 1024, respectively. However, for the Dogs and Cars datasets, the accuracy is slightly higher for the latter. This is because different graphs characterize different neighborhood structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Number of power iteration steps in GNN:</head><p>We have assessed the performance with various power iteration steps K in (2) in the GNN layers. The iteration steps are varied from K=1 to K=10, and the results are given in <ref type="table" target="#tab_1">Table XII</ref>. For the Aircraft, the accuracy slightly decreases as K increases. This could be because our SR-GNN advances closer to the global PageRank solution after the first iteration. However, the accuracy variations are marginal for the Dogs and Flowers datasets, and we achieve the best performance with a single propagation step in GNN for all the datasets. This is desired in real-world applications for computational efficiency without loss of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have proposed a novel end-to-end deep network called SR-GNN to enhance the recognition accuracy of fine-grained objects and human-actions, avoiding any object-parts bounding-box annotation. The model introduces an innovative relation-aware visual feature transformation and its refinement via attentional spatial context modeling to enrich region-level description to capture subtle variations observed and required in FGVC. The model has also proposed a gated attentional pooling to automatically aggregate the relation-aware transformed features. Ultimately, our model's SotA quantitative and qualitative results on eight benchmark datasets and ablation study show the efficacy of SR-GNN.</p><p>In the near future, we will advance our SR-GNN focusing on following key aspects: 1) adapting it to a Graph Transformer Network (GTN) for generating new graph structures to learn a soft selection of connected regions and composite relations for generating useful multi-hop connections to further enhance the recognition accuracy, 2) evaluating SR-GNN on LSVC datasets consisting of distinctive categories (e.g., ImageNet and COCO), and 3) optimizing and extending it to recognize fine-grained actions and activities in videos.    <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) within our model, b) relation-aware transformed feature using GCN <ref type="figure" target="#fig_1">(Fig. 2(b)</ref>), c) attentional context refinement weight-vector v <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>), and d) the final image-level feature mapft for classification ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). Each color represents a particular class. There are 50 classes chosen randomly from the Car's test set. e) SR-GNN without the context refinement module, and f) Standalone Xception base CNN without our modules (re-trained on the Cars dataset). . SR-GNN's discriminability using t-SNE to visualize class separability and compactness using features from a) base CNN (Xception, <ref type="figure" target="#fig_1">Fig. 2(a)</ref>) within our model, b) relation-aware transformed feature using GNN <ref type="figure" target="#fig_1">(Fig. 2(b)</ref>), c) attentional context refinement weight-vector v <ref type="figure" target="#fig_1">(Fig. 2(c)</ref>), and d) the final imagelevel feature mapft for classification ( <ref type="figure" target="#fig_1">Fig. 2(c)</ref>). Each color represents a particular class. There are 50 classes chosen randomly from the Flower's test set. e) SR-GNN without the context refinement module, and f) Standalone Xception base CNN without our modules (re-trained on the Flowers dataset). </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Our SR-GNN consists of a GNN-based relation-aware feature transformation by propagating information between image regions and an attentional context modeling to refine these transformed features. They jointly tackle the challenge of describing and discriminating subtle variations in FGVC by exploring the visual-spatial relationships among regions and aggregating the context-aware features. For clarity, 4 different regions are shown here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of our SR-GNN. (a) Extract features using a set of regions from the upsampled CNN features of a given input image. (b) Relationaware Feature Transformation: it updates each region's visual-spatial relationships by propagating information between them using message propagation in GNN. Then, the transformed features from all regions are used by the gated attentional pooling to produce the final transformed features ft. (c) Attentional</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Various regions (4 shown for clarity) from the region proposal. These regions are used for bilinear pooling from the upsampled CNN features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(a) Base CNN within SR-GNN (b) Transformed Feature ft (c) Context Refinement (v) (d) Final featureft = ft + ft ? v (e) Final featureft = ft without the context refinement (weight v) module (f) Standalone Base CNN SR-GNN's discriminability of the Aircraft test-set using t-SNE [81] to visualize class separability and compactness using features from a) base CNN (Xception,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Visualization of the relation-aware transformation using cosine similarity to measure pairwise relationships (cool to warm ? weak to strong) between nodes in the graph. Top (Aircraft): 707-320, 737-400 and 737-200 (left to right). Bottom (Dogs): Japanese Spaniel, Shih Tzu and Toy Terrier.(a) Joint attentions map (b) 737-600: top-2 R (5 &amp; 12) and their top-3 joint attentions (c) F-16A B: top-2 R (15 &amp; 19) and their top-3 joint attentions Visualization of attentional context refinement (Fig. 2(c)) in our SR-GNN over the sub-types in the Aircraft dataset: a) joint attentional maps for 'A340-200', 'ATR-72', 'DC-10' and 'ERJ 135' aircraft sub-types. b) Top-2 regions (cols 5 &amp; 12) contributing towards sub-type 'Boeing 737-600' conditioned on the respective other top-3 regions (rows) in joint decision-making. The self-attention (self-loop) is also shown in the top-2 regions. c) Similarly, top-2 regions (cols 15 &amp; 19) contributing towards sub-type 'F-16A B' conditioned on the respective other top-3 regions (rows). Regions are shown in the respective original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>. Likewise, using ResNet-50 as a base CNN, SR-GNN (param: 33.4M, GFLOPs: 8.4B) is lighter than RAN (param: 49.0M, GFLOPs: 8.5B) and AG-Net (param: 54.8M, GFLOPs: 10.4B, and per-image inference time: 5.2 ms).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>. It includes only self-attention to refine local features i.e., without (W/o) GNN, without (W/o) self-attention and self-attention without (W/o) weighted-attention. These results justify the importance of each key component in our SR-GNN, without which accuracy degrades significantly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Zachary</head><label></label><figDesc>Wharton is currently an MRes student in the Department of Computer Science, Edge Hill University, UK. He obtained his Bachelor's degree in Computing from Edge Hill University in 2019. His interests include computer vision, deep learning, human-robot interaction (HRI) and pattern recognition. Yonghuai Liu is a professor and director of the Visual Computing Lab at Edge Hill University since 2018. He obtained his first PhD in 1997 from Northwestern Polytechnical University, P.R. China and second PhD in 2001 from The University of Hull, UK. He is an area/associate editor or editorial board member for a number of journals and conferences. He has published more than 180 papers in the top-ranked conferences and journals. His research interests lie in 3D computer vision, image processing, pattern recognition, machine learning, AI, and intelligent systems. He is a senior member of IEEE, Fellow of BCS, and Fellow of HEA of the UK. Nik Bessis received his BA from the T.E.I. Athens and his MA and PhD degrees from De Montfort University, UK. He is a full Professor (2010) and since 2015, the Head (Chair) of the Department of Computer Science at Edge Hill University, UK. He is a FHEA, FBCS and a senior member of IEEE. His research is on social graphs for network and big data analytics as well as developing data push and resource provisioning services in IoT, FI and inter-clouds. He is involved in a number of funded research and commercial projects in these areas. Prof Bessis has published over 300 works and won 4 best papers awards. Ardhendu Behera received the PhD degree in Computer Science from the University of Fribourg, Switzerland and MEng degree in System Science and Automation from the Indian Institute Science (IISc) Bangalore, India. He is currently a Reader in Computer Vision &amp; AI in the Department of Computer Science, Edge Hill University, UK. He has worked as a Research Fellow and Senior Research Fellow in Computer Vision Group at the University of Leeds. He is a Fellow of HEA and member of IEEE, British Machine Vision Association, Applied Vision Association, British Computing Society, affiliated member of IAPR and ECAI. His main interests include computer vision, deep learning, human-robot social interaction, activity analysis and recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .Fig. 8 .</head><label>78</label><figDesc>Bounding box displaying the optimal number (R = 27) of patches/regions in a given input image. These regions are used for bilinear pooling from the upsampled CNN features in Fig. 2(a) (Section III.B). The last region (#27) is the whole image.(a) Base CNN within SR-GNN (b) Transformed Feature ft (c) Context Refinement (v) (d) Final featureft = ft + ft ? v (e) Final featureft = ft without the context refinement (weight v) module (f) Standalone Base CNN SR-GNN's discriminability using t-SNE to visualize class separability and compactness using features from a) base CNN (Xception,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) Base CNN within SR-GNN (b) Transformed Feature ft (c) Context Refinement (v) (d) Final featureft = ft + ft ? v (e) Final featureft = ft without the context refinement (weight v) module (f) Standalone Base CNN Fig. 9</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 .Fig. 11 .</head><label>1011</label><figDesc>(a) AM General Hummer SUV 2000: top-2 regions (9 &amp; 22) and their top-3 joint attentions (Fig. 2c) (b) Ford GT Coupe 2006: top-2 regions (3 &amp; 17) and their top-3 joint attentions (Fig. 2c) Visualization within attentional context refinement (Fig. 2(c)): a) Top-2 regions (cols 9 &amp; 22) contributing towards sub-type 'AM General Hummer SUV 2000' conditioned on the respective other top-3 regions (rows) in joint decision-making. The self-attention (self-loop) is also shown in the top-2 regions. b) Similarly, top-2 regions (cols 3 &amp; 17) contributing towards sub-type 'Ford GT Coupe 2006' conditioned on the respective other top-3 regions (rows). Region proposals are shown in the respective original images. (a) Flower class 1: top-2 regions (1 &amp; 5) and their top-3 joint attentions (Fig. 2c) (b) Flower class 61: top-2 regions (5 &amp; 12) and their top-3 joint attentions(Fig. 2c) Visualization within attentional context refinement(Fig. 2(c)): a) Top-2 regions (cols 1 &amp; 5) contributing towards sub-type 'Flower class 1' conditioned on the respective other top-3 regions (rows) in joint decision-making. The self-attention (self-loop) is also shown in the top-2 regions. b) Similarly, top-2 regions (cols 5 &amp; 12) contributing towards sub-type 'Flower class 61' conditioned on the respective other top-3 regions (rows). Region proposals are shown in the respective original images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, A. Behera, Z. Wharton, Y. Liu, and N. Bessis are with the Department of Computer Science, Edge Hill University, UK.</figDesc><table /><note>* Equal contribution, ? Corresponding author, beheraa@edgehill.ac.uk Manuscript received Month 08, 2021; revised Month 02 and 04, 2022.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THE DATASETS USED IN OUR EXPERIMENTS. ACCURACY (%) OF THE BEST SOTA AND OUR SR-GNN.</figDesc><table><row><cell>Dataset</cell><cell>#Train / #Test</cell><cell>#Class</cell><cell>SotA</cell><cell>SR-GNN</cell></row><row><cell>Aircraft</cell><cell>6,667 / 3,333</cell><cell>100</cell><cell>94.9 [7]</cell><cell>95.4</cell></row><row><cell>CUB-200</cell><cell>5,994 / 5,794</cell><cell>200</cell><cell>91.8 [7]</cell><cell>91.9</cell></row><row><cell>Cars</cell><cell>8,144 / 8,041</cell><cell>196</cell><cell>95.7 [7]</cell><cell>96.1</cell></row><row><cell>Dogs</cell><cell>12,000 / 8,580</cell><cell>120</cell><cell>97.1 [21]</cell><cell>97.3</cell></row><row><cell>Flowers</cell><cell>2,040 / 6,149</cell><cell>102</cell><cell>97.7 [31]</cell><cell>97.9</cell></row><row><cell>NABirds</cell><cell>23,929 / 24,633</cell><cell>555</cell><cell>91.0 [7]</cell><cell>91.2</cell></row><row><cell>Stanford-40</cell><cell>4,000 / 5,532</cell><cell>40</cell><cell>97.8 [10]</cell><cell>98.8</cell></row><row><cell>PPMI-24</cell><cell>2,110 / 2,099</cell><cell>24</cell><cell>98.6 [8]</cell><cell>98.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II ACCURACY</head><label>II</label><figDesc>(%) COMPARISON WITH THE MOST RECENT TOP-10 SOTA METHODS. * INVOLVES TRANSFER/JOINT LEARNING STRATEGY FOR OBJECTS/PATCHES/REGIONS INVOLVING MORE THAN ONE DATASET (TARGET AND SECONDARY). ? APPLIES VISION TRANSFORMER. ? USES ADDITIONAL TEXT DESCRIPTION. THE LAST THREE ROWS SHOW THE ACCURACY OF BASE CNN (XCEPTION [40]), SR-GNN WITHOUT THE ATTENTIONAL REFINEMENT ("W/O REFINE") MODULE (FIG. 2(C)), AND FULL SR-GNN MODEL. THE FOLLOWING ABBREVIATIONS ARE USED TO DENOTE VARIOUS CNN BACKBONES: RN34/RN50/RN101/RN152 FOR RESNET-34/50/101/152; IN-V3 FOR INCEPTION-V3; BCNN FOR BILINEAR CNN; XCEP FOR XCEPTION, DN161/DN201 FOR DENSENET-161/201; VIT-B FOR VISION TRANSFORMER-B-16; SWIN FOR SWIN TRANSFORMER WITH SWIN-BASE-224; GN FOR GOOGLENET; WRN FOR WIDE RESIDUAL NETWORKS; SE FOR SQUEEZE-AND-EXCITATION NETWORKS. CODING IMPLIES ENCODING/CODEBOOK; PARAM AS PARAMETRIC, AND FUSION FOR MULTIPLE CNNS.</figDesc><table><row><cell cols="2">Aircraft</cell><cell cols="2">CUB-200</cell><cell cols="2">Cars</cell><cell></cell><cell>Dogs</cell><cell></cell></row><row><cell>Method</cell><cell>CNN</cell><cell>Acc Method</cell><cell>CNN</cell><cell>Acc Method</cell><cell>CNN</cell><cell>Acc Method</cell><cell>CNN</cell><cell>Acc</cell></row><row><cell>DCL [22]</cell><cell cols="2">RN50 93.0 CSC [37]</cell><cell cols="2">RN50 89.2 DCL [22]</cell><cell cols="2">RN50 94.5 Cross-X [45]</cell><cell>RN50</cell><cell>88.9</cell></row><row><cell>GCL [38]</cell><cell cols="2">RN50 93.2 DAN [46]</cell><cell>In-v3</cell><cell>89.4 S3Ns [32]</cell><cell cols="3">RN50 94.7 MRDMN [47] RN50</cell><cell>89.1</cell></row><row><cell cols="2">CAMF  ? [18] Swin</cell><cell>93.3 BARM [9]</cell><cell cols="2">DN161 89.5 TrnFG  ? [17]</cell><cell cols="2">ViT-B 94.8 APIN [35]</cell><cell>RN101</cell><cell>90.3</cell></row><row><cell>PMG [48]</cell><cell cols="2">RN50 93.6 GaRD [39]</cell><cell cols="2">RN50 89.6 CSC [37]</cell><cell cols="2">RN50 94.9 ViT  ? [15]</cell><cell>ViT-B</cell><cell>91.7</cell></row><row><cell>SCAP [49]</cell><cell cols="2">RN50 93.6 PMG [48]</cell><cell cols="2">RN50 89.9 GaRD [39]</cell><cell cols="2">RN50 95.1 DAN [46]</cell><cell>In-v3</cell><cell>92.2</cell></row><row><cell>CSC [37]</cell><cell cols="2">RN50 93.8 APIN [35]</cell><cell cols="2">DN161 90.0 PMG [48]</cell><cell cols="2">RN50 95.1 TrnFG  ? [17]</cell><cell>ViT-B</cell><cell>92.3</cell></row><row><cell>APIN [35]</cell><cell cols="2">DN161 93.9 CPM  *  [21]</cell><cell>GN</cell><cell>90.4 APIN [35]</cell><cell cols="2">DN161 95.3 CAMF  ? [18]</cell><cell>Swin</cell><cell>92.8</cell></row><row><cell>APCN [50]</cell><cell cols="3">RN50 94.1 CAMF  ? [18] Swin</cell><cell>91.2 CAMF  ? [18]</cell><cell>Swin</cell><cell>95.3 WARN [28]</cell><cell>WRN50</cell><cell>92.9</cell></row><row><cell>GaRD [39]</cell><cell cols="2">RN50 94.3 TrnFG  ? [17]</cell><cell cols="2">ViT-B 91.7 APCN [50]</cell><cell cols="2">RN50 95.4 CAP [7]</cell><cell>Xcep</cell><cell>96.1</cell></row><row><cell>CAP [7]</cell><cell cols="2">RN50 94.9 CAP [7]</cell><cell>Xcep</cell><cell>91.8 CAP [7]</cell><cell>Xcep</cell><cell>95.7 CPM  *  [21]</cell><cell>GN</cell><cell>97.1</cell></row><row><cell>Base CNN</cell><cell>Xcep</cell><cell>79.5 Base CNN</cell><cell>Xcep</cell><cell>75.6 Base CNN</cell><cell>Xcep</cell><cell>84.8 Base CNN</cell><cell>Xcep</cell><cell>82.7</cell></row><row><cell>W/o Refine</cell><cell></cell><cell>93.5 W/o Refine</cell><cell></cell><cell>90.2 W/o Refine</cell><cell></cell><cell>93.7 W/o Refine</cell><cell></cell><cell>96.5</cell></row><row><cell>SR-GNN</cell><cell></cell><cell>95.4 SR-GNN</cell><cell></cell><cell>91.9 SR-GNN</cell><cell></cell><cell>96.1 SR-GNN</cell><cell></cell><cell>97.3</cell></row><row><cell cols="2">Flowers</cell><cell cols="2">NABirds [42]</cell><cell cols="2">Stanford-40 [43]</cell><cell cols="2">PPMI-24 [44]</cell><cell></cell></row><row><cell>Method</cell><cell>CNN</cell><cell>Acc Method</cell><cell>CNN</cell><cell>Acc Method</cell><cell>CNN</cell><cell>Acc Method</cell><cell>CNN</cell><cell>Acc</cell></row><row><cell>MGE [51]</cell><cell cols="3">RN50 95.9 Cross-X [45] SE</cell><cell>86.4 CAM [52]</cell><cell>GN</cell><cell>72.6 LLC [53]</cell><cell>Coding</cell><cell>39.7</cell></row><row><cell>PBC [54]</cell><cell>GN</cell><cell>96.1 SPA [55]</cell><cell cols="2">Param 87.6 ProCRC [56]</cell><cell cols="2">VGG19 80.9 ScSPM [57]</cell><cell>Coding</cell><cell>41.5</cell></row><row><cell>IntAct [58]</cell><cell cols="2">VGG19 96.4 DSTL  *  [59]</cell><cell>In-v3</cell><cell>87.9 Introsp [60]</cell><cell cols="2">VGG16 81.7 CSDL [61]</cell><cell>Coding</cell><cell>48.8</cell></row><row><cell>SJFT  *  [62]</cell><cell cols="2">RN152 97.0 GaRD [39]</cell><cell cols="2">RN50 88.0 PKPCR [63]</cell><cell cols="2">VGG19 82.4 Exemplr [64]</cell><cell cols="2">Dictionary 49.3</cell></row><row><cell cols="2">OPAM  *  [65] VGG</cell><cell>97.1 APIN [35]</cell><cell cols="4">DN161 88.1 Concepts [66] VGG16 83.1 VLAD [67]</cell><cell>Coding</cell><cell>50.7</cell></row><row><cell cols="3">Cos.Ls  *  [68] RN50 97.2 CSPE [69]</cell><cell>In-v3</cell><cell>88.5 Color [70]</cell><cell cols="2">Fusion 84.2 Color [70]</cell><cell>Fusion</cell><cell>65.9</cell></row><row><cell>PMA  ? [71]</cell><cell cols="2">VGG16 97.4 MGE [51]</cell><cell cols="2">RN101 88.6 ?-pool [72]</cell><cell cols="2">VGGM 86.0 DSFNet [73]</cell><cell>RN34</cell><cell>72.3</cell></row><row><cell>DSTL  *  [59]</cell><cell>In-v3</cell><cell>97.6 ViT  ? [15]</cell><cell cols="2">ViT-B 89.9 Implicit [74]</cell><cell cols="2">RN50 87.7 Coding [27]</cell><cell>NASNet</cell><cell>82.3</cell></row><row><cell>MCL  *  [31]</cell><cell cols="2">BCNN 97.7 TrnFG  ? [17]</cell><cell cols="2">ViT-B 90.8 RAN [8]</cell><cell cols="2">RN50 97.4 AG-Net [10]</cell><cell>RN50</cell><cell>98.2</cell></row><row><cell>CAP [7]</cell><cell>Xcep</cell><cell>97.7 CAP [7]</cell><cell>Xcep</cell><cell>91.0 AG-Net [10]</cell><cell cols="2">RN50 97.8 RAN [8]</cell><cell>DN201</cell><cell>98.6</cell></row><row><cell>Base CNN</cell><cell>Xcep</cell><cell>91.9 Base CNN</cell><cell>Xcep</cell><cell>68.1 Base CNN</cell><cell>Xcep</cell><cell>80.0 Base CNN</cell><cell>Xcep</cell><cell>79.3</cell></row><row><cell>W/o Refine</cell><cell></cell><cell>97.1 W/o Refine</cell><cell></cell><cell>89.9 W/o Refine</cell><cell></cell><cell>97.9 W/o Refine</cell><cell></cell><cell>97.9</cell></row><row><cell>SR-GNN</cell><cell></cell><cell>97.9 SR-GNN</cell><cell></cell><cell>91.2 SR-GNN</cell><cell></cell><cell>98.8 SR-GNN</cell><cell></cell><cell>98.9</cell></row></table><note>human-actions, detailed in Table I): Aircraft [4], Caltech- UCSD Birds (CUB-200) [1], Stanford Cars [5], Stanford Dogs [3], Oxford Flowers [2], NABirds [42], Stanford-40 actions [43], and People Playing Musical Instruments (PPMI-24) [44]. The top-1 accuracy (%) is used for the evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III ACCURACY</head><label>III</label><figDesc>(%) OF OUR SR-GNN USING RESNET-50 BASE CNN WITH DIFFERENT SIZES OF IMAGES FOR FGVC.</figDesc><table><row><cell cols="3">224?224, CAP [7]</cell><cell></cell><cell>448?448 size</cell><cell></cell></row><row><cell>Airc.</cell><cell>CUB</cell><cell>NAB</cell><cell>Cars</cell><cell>Flowers</cell><cell>Dogs</cell></row><row><cell>SotA 94.9</cell><cell>90.9</cell><cell>88.8</cell><cell cols="3">95.4 [50] 96.8 [31] 88.9 [45]</cell></row><row><cell>Ours 94.8</cell><cell>91.0</cell><cell>88.8</cell><cell>95.8</cell><cell>98.0</cell><cell>97.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF OUR SR-GNN WITH VISION TRANSFORMERS. MODEL COMPLEXITY IS GIVEN IN PARAMETERS (MILLION) AND GFLOPS (BILLION), FOR INPUT-SIZE 384?384, AS PROVIDED IN [16]. TOP: INPUT-SIZE: 224?224, MID: 448?448, AND BOTTOM: 224?224. OF SR-GNN WITH OTHER SOTA BASE CNNS INSTEAD OF XCEPTION USING THE SAME TEST SETUP (224?224 IMG-SIZE, ? = 0.3 &amp; R = 27). CAP IS USED AS BACKBONE BY REPLACING THE CNN FEATURE MAP AND REGION PROPOSALS (FIG</figDesc><table><row><cell>Method</cell><cell cols="4">Transformer CUB Car Dog NAB Air Parm (GFlop)</cell></row><row><cell>Swin [16]</cell><cell>Swin-224</cell><cell>89.7 94.2 91.8 -</cell><cell>91.0</cell><cell>88 (15.4)</cell></row><row><cell>CAMF [18]</cell><cell>Swin-224</cell><cell>90.9 94.8 92.6 -</cell><cell>92.9</cell><cell>-</cell></row><row><cell>ViT [15]</cell><cell>ViT-B-16</cell><cell cols="2">90.3 93.7 91.7 89.9 -</cell><cell>86 (55.4)</cell></row><row><cell>TrnFG [17]</cell><cell>ViT-B-16</cell><cell cols="2">91.7 94.8 92.3 90.8 -</cell><cell>-</cell></row><row><cell>Swin [16]</cell><cell>Swin-224</cell><cell>90.7 94.8 92.5 -</cell><cell>93.0</cell><cell>88 (47.0)</cell></row><row><cell>CAMF [18]</cell><cell>Swin-224</cell><cell>91.2 95.3 92.8 -</cell><cell>93.3</cell><cell>-</cell></row><row><cell>SR-GNN</cell><cell>-</cell><cell cols="2">91.9 96.1 97.3 91.2 95.4</cell><cell>30.9 (9.8)</cell></row><row><cell></cell><cell></cell><cell>TABLE V</cell><cell></cell><cell></cell></row><row><cell>ACCURACY (%)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI</head><label>VI</label><figDesc>SR-GNN'S CAPACITY AND COMPUTATIONAL OVERHEAD FOR DIFFERENT REGIONS USING AN NVIDIA TITAN V GPU (12GB).</figDesc><table><row><cell>#No. of</cell><cell>#Trainable params</cell><cell>GFLOPs</cell><cell>Per-image inference time</cell><cell cols="4">Training time (batch size 8) in ?hours</cell></row><row><cell>Regions</cell><cell>in millions (?M)</cell><cell>in billions (?B)</cell><cell>in millisecond (?ms)</cell><cell>Aircraft</cell><cell>Cars</cell><cell>Dogs</cell><cell>Flowers</cell></row><row><cell>11</cell><cell>30.9</cell><cell>9.4</cell><cell>3.9</cell><cell>3.5</cell><cell>8.4</cell><cell>13.4</cell><cell>1.9</cell></row><row><cell>19</cell><cell>30.9</cell><cell>9.6</cell><cell>5.0</cell><cell>4.1</cell><cell>10.2</cell><cell>15.2</cell><cell>2.6</cell></row><row><cell>27</cell><cell>30.9</cell><cell>9.8</cell><cell>5.0</cell><cell>4.5</cell><cell>11.2</cell><cell>17.0</cell><cell>2.8</cell></row><row><cell>36</cell><cell>30.9</cell><cell>10.1</cell><cell>6.0</cell><cell>5.0</cell><cell>12.6</cell><cell>18.3</cell><cell>3.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII COMPUTATIONAL</head><label>VII</label><figDesc>COMPLEXITY COMPARISON OF THE PROPOSED SR-GNN WITH STATE-OF-THE-ARTS. Giga floating point operations), model size as a number of trainable parameters in millions (M) and per-image inference time in milliseconds (ms)<ref type="bibr" target="#b85">[83]</ref>. These values over four datasets is given inTable VI. Its comparison to the SotA methods is also provided in Table VII. For R=27, its complexity in terms of trainable parameters (base CNN: 20.9M, W/o Refine: 24.4M and full model: 30.9M), perimage inference time (base CNN: 2.7ms, W/o Refine: 4.9ms and SR-GNN: 5.0ms) and GFLOPs (base CNN: 9.2B, W/o Refine: 9.3B and full model: 9.8B) are given inTable VI.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>E. Model Complexity</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SR-GNN's capacity and computational complexity is as-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>sessed using GFLOPs (</cell></row><row><cell>Method</cell><cell cols="3">Param (M) GFLOPs (B) Inf. Time/img (ms)</cell></row><row><cell>AG-Net [10]</cell><cell>54.8</cell><cell>10.4</cell><cell>5.2</cell></row><row><cell>MRDMN-L [47]</cell><cell>51.2</cell><cell>14.0</cell><cell>4.9</cell></row><row><cell>TASN [29]</cell><cell>37.3</cell><cell>21.9</cell><cell>7.5</cell></row><row><cell>CAP [7]</cell><cell>34.2</cell><cell>10.2</cell><cell>4.2</cell></row><row><cell>Base CNN</cell><cell>20.9</cell><cell>9.2</cell><cell>2.7</cell></row><row><cell>W/o Refine</cell><cell>24.4</cell><cell>9.3</cell><cell>4.9</cell></row><row><cell>SR-GNN</cell><cell>30.9</cell><cell>9.8</cell><cell>5.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII DAVIES</head><label>VIII</label><figDesc>-BOULDIN<ref type="bibr" target="#b84">[82]</ref> INDEX (LOWER IS BETTER) TO QUANTIFY CLUSTER SIMILARITIES USING THE T-SNE<ref type="bibr" target="#b83">[81]</ref> OUTPUTS OVER</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE IX ACCURACY</head><label>IX</label><figDesc>(%) OF OUR SR-GNN WITH VARIED KEY MODULES. 1) UNIFORM PATCH-SIZE AS AN ALTERNATIVE TO GENERATE REGIONS 2) PERFORMANCE OF VARIOUS GRAPH POOLING TECHNIQUES IN OUR FEATURE TRANSFORM MODULE</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>3) EFFECTIVENESS OF MAJOR COMPONENTS OF SR-GNN.</figDesc><table><row><cell>Key Modules</cell><cell cols="4">Aircraft Cars Dogs Flowers</cell></row><row><cell>Uniform 4 ? 4 grid</cell><cell>94.3</cell><cell>92.9</cell><cell>96.3</cell><cell>97.0</cell></row><row><cell>Global average pooling</cell><cell>95.0</cell><cell>95.1</cell><cell>96.2</cell><cell>97.5</cell></row><row><cell>Global max pooling</cell><cell>94.9</cell><cell>95.5</cell><cell>96.4</cell><cell>97.8</cell></row><row><cell>Global sum pooling [84]</cell><cell>94.9</cell><cell>95.1</cell><cell>96.3</cell><cell>97.7</cell></row><row><cell>Sort pooling [85]</cell><cell>95.2</cell><cell>95.3</cell><cell>96.5</cell><cell>97.8</cell></row><row><cell>W/o GNN</cell><cell>93.5</cell><cell>94.5</cell><cell>96.0</cell><cell>94.9</cell></row><row><cell>W/o Refine</cell><cell>93.5</cell><cell>93.7</cell><cell>96.5</cell><cell>97.1</cell></row><row><cell>W/o self-attention</cell><cell>93.7</cell><cell>95.1</cell><cell>96.1</cell><cell>96.4</cell></row><row><cell>W/o weighted-attention</cell><cell>94.4</cell><cell>95.5</cell><cell>96.0</cell><cell>95.1</cell></row><row><cell>SR-GNN (full-model)</cell><cell>95.4</cell><cell>96.1</cell><cell>97.3</cell><cell>97.9</cell></row><row><cell></cell><cell>TABLE X</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">ACCURACY (%) OF SR-GNN WITH DIFFERENT NUMBERS OF REGION</cell></row><row><cell cols="2">PROPOSALS (FIG</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE XI ACCURACY</head><label>XI</label><figDesc>(%) OF SR-GNN WITH 512 AND 1024 OUTPUT DIMENSIONS AT DIFFERENT TELEPORT (OR RESTART) PROBABILITY ? ? [0.1, 0.8] IN (2). Aircraft 91.6 94.6 94.8 94.7 94.7 92.1 91.5 92.3 90.7 92.4 95.4 91.1 91.7 92.1 92.3 90.9 Cars 95.5 95.5 95.6 94.9 93.9 93.9 93.5 93.3 95.4 95.7 96.1 95.8 96.0 95.8 95.8 95.8 Dogs 96.7 96.7 96.9 96.5 96.4 96.5 96.5 96.1 96.8 97.0 97.3 97.1 96.7 96.7 96.7 96.6 Flowers 97.6 97.8 97.9 97.8 97.6 97.6 97.7 97.7 97.5 97.5 97.9 97.8 97.7 97.7 97.7 96.6 TABLE XII ACCURACY (%) WITH VARIOUS PROPAGATION STEPS IN GNN LAYERS.</figDesc><table><row><cell cols="2">Dataset</cell><cell></cell><cell></cell><cell cols="5">GNN output dimension = 512</cell><cell></cell><cell></cell><cell cols="4">GNN output dimension = 1024</cell></row><row><cell></cell><cell></cell><cell cols="2">0.1 0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.1 0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell></row><row><cell>Iteration</cell><cell>#1</cell><cell>#2</cell><cell>#3</cell><cell>#4</cell><cell>#5</cell><cell>#8</cell><cell>#10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Aircraft</cell><cell cols="7">95.4 94.7 94.9 94.7 94.8 94.8 94.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dogs</cell><cell cols="7">97.3 97.1 97.1 97.0 96.9 97.1 96.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Flowers</cell><cell cols="7">97.9 97.7 97.3 97.8 97.6 97.8 97.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE XIII PERFORMANCE</head><label>XIII</label><figDesc>COMPARISON BASED ON INFERENCE TIME WITH THE SOTA METHODS</figDesc><table><row><cell>Sl.</cell><cell>Method</cell><cell>Param (M)</cell><cell>GFLOPs (B)</cell><cell>Infer. time per</cell><cell>Accuracy</cell><cell>(%)</cell><cell cols="2">Our accuracy and (+gain) in</cell></row><row><cell>No</cell><cell></cell><cell></cell><cell></cell><cell>img. (ms)</cell><cell>(dataset)</cell><cell></cell><cell>%</cell><cell></cell></row><row><cell>1</cell><cell>CAP [7]</cell><cell>34.2</cell><cell>10.2</cell><cell>4.2</cell><cell>94.9 (Aircraft)</cell><cell></cell><cell>95.4 (+ 0.5)</cell><cell></cell></row><row><cell>2a</cell><cell>MRDMN-L [47]</cell><cell>51.2</cell><cell>14.0</cell><cell>4.9</cell><cell>89.0 (Dogs)</cell><cell></cell><cell cols="2">96.5 (+7.5), W/o refine 97.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(+8.3), SR-GNN</cell><cell></cell></row><row><cell>2b</cell><cell>SR-GNN (W/o Refine)</cell><cell>24.4</cell><cell>9.3</cell><cell>4.9</cell><cell>Paper Table II</cell><cell></cell><cell>Section IV-D</cell><cell></cell></row><row><cell>3</cell><cell>SR-GNN (Full Model)</cell><cell>30.9</cell><cell>9.8</cell><cell>5.0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4</cell><cell>AG-Net [10]</cell><cell>54.8</cell><cell>10.4</cell><cell>5.2</cell><cell>97.8 (Stanf.40)</cell><cell></cell><cell>98.8 (+1.0)</cell><cell></cell></row><row><cell>5</cell><cell>TASN [29]</cell><cell>37.3</cell><cell>21.9</cell><cell>7.5</cell><cell cols="2">87.9 (CUB-200)</cell><cell>91.9 (+4.0)</cell><cell></cell></row><row><cell>6</cell><cell>WARN [28]</cell><cell>-</cell><cell>-</cell><cell>11.3</cell><cell cols="2">85.6 (CUB-200)</cell><cell>91.9 (+6.3)</cell><cell></cell></row><row><cell>7</cell><cell>RG [86]</cell><cell>-</cell><cell>-</cell><cell>23.8</cell><cell cols="2">87.3 (CUB-200)</cell><cell>91.9 (+4.6)</cell><cell></cell></row><row><cell>8</cell><cell>SCAPNet [49]</cell><cell>-</cell><cell>-</cell><cell>24.4</cell><cell>93.6 (Aircraft)</cell><cell></cell><cell>95.4 (+1.8)</cell><cell></cell></row><row><cell>9</cell><cell>ME-ASN [87]</cell><cell>-</cell><cell>-</cell><cell>33.9</cell><cell cols="2">89.5 (CUB-200)</cell><cell>91.9 (+2.4)</cell><cell></cell></row><row><cell>10</cell><cell>NTS-Net [88]</cell><cell>-</cell><cell>-</cell><cell>35.0</cell><cell>93.9 (Cars)</cell><cell></cell><cell>96.1 (+2.2)</cell><cell></cell></row><row><cell>11</cell><cell>RA-CNN [89]</cell><cell>-</cell><cell>-</cell><cell>36.9</cell><cell>87.3 (Dogs)</cell><cell></cell><cell>97.3 (+10.0)</cell><cell></cell></row><row><cell>(1)</cell><cell>(2)</cell><cell>(3)</cell><cell>(4)</cell><cell>(5)</cell><cell>(6)</cell><cell>(7)</cell><cell>(8)</cell><cell>(9)</cell></row><row><cell>(10)</cell><cell>(11)</cell><cell>(12)</cell><cell>(13)</cell><cell>(14)</cell><cell>(15)</cell><cell>(16)</cell><cell>(17)</cell><cell>(18)</cell></row><row><cell>(19)</cell><cell>(20)</cell><cell>(21)</cell><cell>(22)</cell><cell>(23)</cell><cell>(24)</cell><cell>(25)</cell><cell>(26)</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT This research is supported by the UKIERI-DST grant CHARM (UKIERI-2018-19-10), and Research Investment Fund at Edge Hill University. The GPU used in this research was donated by the NVIDIA.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Document</head><p>The accuracy of our SR-GNN is higher than the state-of-theart on diverse datasets. To justify the benefits of our model, a precise comparison with the existing top-10 methods focused on the inference time is given in <ref type="table">Table XIII</ref>, irrespective of the GPU/hardware configuration, deep learning tools (e.g., TensorFlow, PyTorch, MXNet, etc.) and related experimental constraints used in those works. <ref type="table">Table XIII</ref> is incorporated in <ref type="table">Table VII</ref> with the model parameters and GFLOPs by comparing with the top-5 SotA approaches. Some approaches in <ref type="table">Table XIII</ref> do not provide these two metrics. We have also provided the accuracy comparison with those works to reflect a trade-off between the accuracy (%) and inference time in milliseconds (ms). For this purpose, we have specified the best performance of those referred works on a FGVC dataset and our performance and accuracy gain (in parenthesis) on the same dataset.</p><p>It shows that our SR-GNN (full-model) stands in the third position and outperforms other eight SotA approaches based on the inference time. From this study, it is evident that SR-GNN requires very competitive inference time with 0.8 ms more than CAP <ref type="bibr" target="#b6">[7]</ref>. It is noted that our SR-GNN without Refine module (4.9 ms) shares the second position with MRDMN-L [47] and achieves 7.5% accuracy gain on the Dogs dataset over this approach. On the contrary, SR-GNN computationally lighter and requires lesser parameters and GFLOPs than these two methods, mentioned in <ref type="table">Table VII</ref> of revised manuscript. Also, the accuracy gain of SR-GNN is the highest on various FGVC datasets compared to these works. In this context, it can be noted that SR-GNN offers an excellent balance to maintain the trade-off between the accuracy, model complexity, and inference time over a diverse category of recent approaches. Therefore, SR-GNN performs the best considering all the aspects of experimental analysis over the existing SotA methods. Particularly, it stands as the second (W/o Refine) and third (full-model) regarding the inference time in comparison with the top-10 SotA methods.</p><p>We have included additional visualizations related to our manuscript. (A) <ref type="figure">Fig. 7</ref> shows all the region proposals (R = 27) and it is related to <ref type="figure">Fig. 7</ref> in the main paper.</p><p>(B) t-SNE plots related to Table VIII (in the main paper) for Cars ( <ref type="figure">Fig. 8)</ref> and Flowers <ref type="figure">(Fig. 10)</ref> datasets.</p><p>(C) Joint attention maps are shown on Cars ( <ref type="figure">Fig. 10)</ref> and Flowers <ref type="figure">(Fig. 11)</ref> datasets, related to <ref type="figure">Fig. 6</ref> in the main manuscript.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conf. on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Novel dataset for fine-grained image categorization: Stanford dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jayadevaprakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR Workshop on Fine-Grained Visual Categorization</title>
		<meeting>CVPR Workshop on Fine-Grained Visual Categorization</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intl&apos; Conf. on Computer Vision Workshops</title>
		<meeting>of the IEEE Intl&apos; Conf. on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fine-grained image analysis with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context-aware attentional pooling (cap) for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hewage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 35th AAAI Conference on Artificial Intelligence</title>
		<meeting>35th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="929" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Regional attention network (ran) for head pose and finegrained gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghahremani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bessis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Affective Computing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bidirectional attention-recognition model for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attend and guide (ag-net): A keypoints-driven attention-based deep network for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wharton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bessis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3691" to="3704" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR)</title>
		<meeting>Int. Conf. Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predict then propagate: Graph neural networks meet personalized pagerank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conf. on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations (ICLR</title>
		<meeting>Int. Conf. Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comput. Vis</title>
		<meeting>Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transfg: A transformer architecture for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kortylewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07976</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Complemental attention multi-feature fusion network for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained categorization with part-based image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1713" to="1725" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Autobd: Automated bi-level description for scalable fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3034" to="3043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5157" to="5166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
		<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained image classification via correlation-guided discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th ACM International Conf. on Multimedia</title>
		<meeting>of the 27th ACM International Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1851" to="1860" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fine-grained image classification via combining vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5994" to="6002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Which and how many regions to gaze: Focus discriminative regions for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1235" to="1255" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Reassessing hierarchical representation for action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="61" to="386" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pay attention to the activations: a modular attention mechanism for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodr?guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Velazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonz?lez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="502" to="514" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Weakly supervised attention pyramid convolutional neural network for finegrained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Selective sparse sampling for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision</title>
		<meeting>of the IEEE International Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6599" to="6608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counterfactual attention learning for fine-grained visual categorization and re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<title level="m">IEEE/CVF International Conf. Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1025" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Look-into-object: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee/Cvf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Conf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Patt. Recognit</title>
		<imprint>
			<biblScope unit="page" from="11" to="774" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI Conf. on Artificial Intelligence</title>
		<meeting>AAAI Conf. on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="130" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Category-specific semantic coherency learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th ACM Intl&apos; Conf. on Multimedia</title>
		<meeting>of the 28th ACM Intl&apos; Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="174" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph-propagation based correlation learning for weakly supervised fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph-based high-order relation discovery for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15" to="079" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Patt. Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1251" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Represent. (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. Patt. Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Human action recognition by learning bases of action attributes and parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1331" to="1338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Grouplet: A structured image representation for recognizing human and object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Soc. Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Intl&apos; Conf. on Computer Vision</title>
		<meeting>of the IEEE Intl&apos; Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">See better before looking closer: Weakly supervised data augmentation network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.09891</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Multiresolution discriminative mixup network for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Your &quot;flamingo&quot; is my &quot;bird&quot;: Fine-grained, or not</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE/CVF Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE/CVF Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">485</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning scale-consistent attention part network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Ap-cnn: weakly supervised attention pyramid convolutional neural network for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2826" to="2836" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning a mixture of granularity-specific experts for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Computer Vision</title>
		<meeting>IEEE International Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8331" to="8340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Comp. Society Conf. on Comput. Vis. and Pattn. Recog</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3360" to="3367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Pbc: Polygon-based classifier for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="673" to="684" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Parametric classification of bingham distributions based on grassmann manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Antolovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5771" to="5784" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A probabilistic collaborative representation based approach for pattern classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2950" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Linear spatial pyramid matching using sparse coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1794" to="1801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Interactive: Interlayer activeness propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="270" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Large scale finegrained categorization and domain-specific transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recog</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4109" to="4118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visual concept recognition and localization via iterative introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comp. Vis</title>
		<meeting>Asian Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="264" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning category-specific dictionary and shared dictionary for fine-grained image categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">.-H</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="623" to="634" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1086" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Prior knowledge-based probabilistic collaborative representation for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1498" to="1508" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Recognising human-object interaction via exemplar based modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recog. (CVPR)</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recog. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3144" to="3151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Object-part attention model for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Action classification via concepts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th Int. Conf. Patt. Recognit</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1499" to="1505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Towards optimal vlad for human action recognition from still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="53" to="63" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep learning on small datasets without pretraining using cosine loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Barz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conf. on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1371" to="1380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Classification-specific parts for improving fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conf. on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">New colour fusion deep learning model for large-scale action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lavinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Verma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Vision and Robotics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="60" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Bi-modal progressive mask attention for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Generalized orderless pooling performs implicit salient matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE International Conf. on Computer Vision</title>
		<meeting>IEEE International Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4960" to="4969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Deep selective feature learning for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conf. on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">The whole is more than its parts? from explicit to implicit pose normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="749" to="763" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Loss guided activation for action recognition in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asian Conf. Comp. Vis</title>
		<meeting>Asian Conf. Comp. Vis</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="152" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Single image action recognition using semantic body part actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision</title>
		<meeting>of the IEEE International Conf. on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3391" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Action recognition from still images based on deep vlad spatial pyramids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing: Image Communication</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="118" to="129" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Accelerating t-sne using tree-based algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Jrnl. Mach. Learn. Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">A cluster separation measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Bouldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="224" to="227" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6105" to="6114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">in International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Interpretable and accurate fine-grained recognition via region grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8662" to="8672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Enhancing mixture-of-experts by leveraging attention for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. Comput. Vis. and Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Tech degree from the IIEST Shibpur, India. He is currently a Post-doctoral research associate at the Computer Science Department, Edge Hill University, UK. His current research interests include computer vision, deep learning, human activity recognition, and biometrics</title>
	</analytic>
	<monogr>
		<title level="m">Asish Bera received the PhD degree from the Jadavpur University, Kolkata, India, and the M</title>
		<imprint/>
	</monogr>
	<note>He is a member of IEEE</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
