<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<email>lichao15@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
							<email>zhongqiaoyong@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
							<email>xiedi@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
							<email>pushiliang@hikvision.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Hikvision Research Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Co-occurrence Feature Learning from Skeleton Data for Action Recognition and Detection with Hierarchical Aggregation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Skeleton-based human action recognition has recently drawn increasing attentions with the availability of large-scale skeleton datasets. The most crucial factors for this task lie in two aspects: the intra-frame representation for joint co-occurrences and the inter-frame representation for skeletons' temporal evolutions. In this paper we propose an end-to-end convolutional co-occurrence feature learning framework. The co-occurrence features are learned with a hierarchical methodology, in which different levels of contextual information are aggregated gradually. Firstly point-level information of each joint is encoded independently. Then they are assembled into semantic representation in both spatial and temporal domains. Specifically, we introduce a global spatial aggregation scheme, which is able to learn superior joint co-occurrence features over local aggregation. Besides, raw skeleton coordinates as well as their temporal difference are integrated with a two-stream paradigm. Experiments show that our approach consistently outperforms other state-of-the-arts on action recognition and detection benchmarks like NTU RGB+D, SBU Kinect Interaction and PKU-MMD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Analysis of human behavior such as action recognition and detection is one of the fundamental and challenging tasks in computer vision. It has a wide range of applications such as intelligent surveillance system, human-computer interaction, game-control and robotics. Articulated human pose, being also referred to as skeleton, provides a very good representation for describing human actions. On one hand, skeleton data are inherently robust against background noise and provide abstract information and high-level features of human action. On the other hand, compared with RGB data, skeleton data are extremely small in size, which makes it possible to design lightweight and hardware friendly models.</p><p>In this paper, we focus on the problem of skeleton-based human action recognition and detection <ref type="figure" target="#fig_0">(Figure 1</ref>). The interactions and combinations of skeleton joints play a key role to characterize an action. Many of the early works attempted to design and extract co-occurrence features from skeleton sequences, such as pairwise relative position of each joint , spatial orientation of pairwise joints <ref type="bibr" target="#b2">[Jin and Choi, 2012]</ref>, and the statistics-based features like Cov3DJ <ref type="bibr" target="#b0">[Hussein et al., 2013]</ref> and HOJ3D <ref type="bibr" target="#b6">[Xia et al., 2012]</ref>. On the other hand, the Recurrent Neural Networks (RNNs) with Long-Short Term Memory (LSTM) neurons are used to model the time series of skeleton prevalently <ref type="bibr" target="#b5">[Shahroudy et al., 2016;</ref>. Although the LSTM networks were designed to model long-term temporal dependency, it is difficult for them to learn high-level features from skeletons directly since the temporal modeling is done on the raw input space <ref type="bibr" target="#b5">[Sainath et al., 2015]</ref>. While fully connected layers are able to learn co-occurrence features for their ability of aggregating global information from all input neurons. In <ref type="bibr" target="#b9">[Zhu et al., 2016]</ref>, an end-to-end fully connected deep LSTM network was proposed to learn co-occurrence features from skeleton data. CNN models are equipped with excellent ability to extract high-level information, and they have been used to learn spatial-temporal features from skeletons <ref type="bibr" target="#b0">[Du et al., 2016;</ref><ref type="bibr" target="#b2">Ke et al., 2017]</ref>. These CNN-based methods represent a skeleton sequence as an image by encoding the temporal dynamics and the skeleton joints as rows and columns respectively, and then feed it into a CNN to recognize the underlying action just like image classification. However, in that case, only the neighboring joints within the convolutional kernel are considered to learn co-occurrence features. Although the receptive field covers all joints of a skeleton in later convolution layers, it is difficult to mine co-occurrences from all joints efficiently. Because of the weight sharing mechanism in spatial dimensions, CNN models can not learn free pa-rameters for each joint. This motivates us to design a model which is able to get global response from all joints to exploit the correlations between different joints.</p><p>We propose an end-to-end co-occurrence feature learning framework, which uses CNN to learn hierarchical cooccurrence features from skeleton sequences automatically. We find the output of a convolution layer is global response from all input channels. If each joint of a skeleton is treated as a channel, then the convolution layer can learn the cooccurrences from all joints easily. More specifically, we represent a skeleton sequence as a tensor of shape f rames ? joints ? 3 (the last dimension as channel). We first learn point-level features for each joint independently using convolution layers with kernel size n ? 1. And then we transpose the output of the convolution layers to make the dimension of joints as channel. After the transpose operation, the subsequent layers aggregate global features from all joints hierarchically. Furthermore, the two-stream framework <ref type="bibr">[Simonyan and Zisserman, 2014]</ref> is introduced to fuse the skeleton motion feature explicitly.</p><p>The main contributions of this work are summarized as follows:</p><p>? We propose to employ the CNN model for learning global co-occurrences from skeleton data, which is shown superior over local co-occurrences. ? We design a novel end-to-end hierarchical feature learning network, where features are aggregated gradually from point-level features to global co-occurrence features. ? We comprehensively exploit multi-person feature fusion strategies, which makes our network scale well to variable number of persons. ? The proposed framework outperforms all existing stateof-the-art methods on benchmarks for both action recognition and detection tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The LSTM networks were designed to model long-term temporal dependency problems. Thus they are a natural choice and have been well exploited for feature learning from skeleton sequences. However, more and more literatures adopted CNN to learn skeleton features and achieved impressive performance in recent years. For example, <ref type="bibr" target="#b0">[Du et al., 2016]</ref> proposed to cast the frame, joint and coordinate dimensions of a skeleton sequence into width, height and channel of an image respectively. Then they applied CNN for skeleton-based action recognition in the same way as CNN-based image classification. <ref type="bibr" target="#b2">[Ke et al., 2017]</ref> proposed an improved representation of skeleton sequences where the 3D coordinates are separated into three gray-scale images. In <ref type="bibr" target="#b4">[Li et al., 2017b]</ref>, a skeleton transformer module was introduced to learn a new representation of skeleton joints. And a two-stream convolutional network was proposed to incorporate skeleton motion features. Similar to the above works, we also adopt CNN to learn features from skeleton data. However, we attempt to model global co-occurrence patterns with CNN. And we explicitly formulate the problem into two levels of feature learning subproblems, i.e. independent point-level feature learning and cross-joint co-occurrence feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Co-occurrence Feature Learning with CNN</head><p>CNN is one of the most powerful and successful neural network models, which has been widely applied in image classification, object detection, video classification etc. Compared with sequential structures such as RNNs, it is capable of encoding spatial and temporal contextual information simultaneously. By investigating the convolution operation, we may decompose it into two steps ( <ref type="figure" target="#fig_1">Figure 2</ref>), i.e. local feature aggregation across the spatial domain (width and height) and global feature aggregation across channels. Then one can conclude a simple yet very practical way to regulate the aggregation extent on demand. Denote T as a d 1 ? d 2 ? d 3 3D tensor. We can assign different context by reorganizing (transposing) the tensor. Any information from dimension d i can be aggregated globally if is specified as channels while the other two encode local context. In all previous CNN-based methods <ref type="bibr" target="#b0">[Du et al., 2016;</ref><ref type="bibr" target="#b2">Ke et al., 2017;</ref><ref type="bibr" target="#b4">Li et al., 2017b]</ref>, joint coordinates are specified as channels. It causes a problem: the co-occurrence features are aggregated locally, which may not be able to capture long-range joint interactions involved in actions like wearing a shoe. To this end, we argue that aggregating co-occurrence features globally is of great importance and leads to better action recognition performance. It can be easily implemented by putting the joint dimension into channels of CNN's input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Explicit Skeleton Motion</head><p>Besides joint co-occurrences, temporal movements of joints are crucial cues to recognize the underlying action. Although the temporal evolution pattern can be learned implicitly with CNN, we argue that an explicit modeling is preferable. Thus we introduce a representation of skeleton motion and feed it explicitly into the network. For the skeleton of a person in frame t, we formulate it as</p><formula xml:id="formula_0">S t = {J t 1 , J t 2 , . . . , J t N }</formula><p>where N is the number of joint and J = (x, y, z) is a 3D joint coordinate. The skeleton motion is defined as the temporal difference of each joint between two consecutive frames:</p><formula xml:id="formula_1">M t = S t+1 ? S t = {J t+1 1 ? J t 1 , J t+1 2 ? J t 2 , ..., J t+1 N ? J t N }.</formula><p>Raw skeleton coordinates S and the skeleton motion M are fed into the network independently with a two-stream paradigm. To fuse information from the two sources, we concatenate their feature maps across channels in subsequent layers of the network (see <ref type="figure" target="#fig_2">Figure 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hierarchical Co-occurrence Network</head><p>In this section, we will elaborately describe the proposed Hierarchical Co-occurrence Network (HCN) framework, which is designed to learn the joint co-occurrences and the temporal evolutions jointly in an end-to-end manner. <ref type="figure" target="#fig_2">Figure 3</ref> shows the network architecture of the proposed framework. A skeleton sequence X can be represented with a T ? N ? D tensor, where T is the number of frames in the sequence, N is the number of joints in the skeleton and D is the coordinate dimension (e.g. 3 for 3D skeleton). The skeleton motion described above is of the same shape as X. They are fed into the network directly as two streams of inputs. The two network branches share the same architecture. However, their parameters are not shared and learned separately. Their feature maps are fused by concatenation along channels after conv4.</p><p>Given skeleton sequence and motion inputs, the features are learned hierarchically. In stage 1, point-level features are encoded with a 1 ? 1 (conv1) and n ? 1 (conv2) convolution layers. Since the kernel sizes along the joint dimension are kept 1, they are forced to learn point-level representation from 3D coordinates for each joint independently. After that, we transpose the feature maps with parameter (0, 2, 1) so that the joint dimension is moved to channels of the tensor. Then in stage 2, all subsequent convolution layers extract global cooccurrence features from all joints of a person as described in Section 3.1. Afterwards, the feature maps are flattened into a vector and go through two fully connected layers for final classification. Note that the skeleton's temporal evolutions are encoded all through the network with convolutions of kernel size equal to 3 along the frame dimension.</p><p>Our network contains about 0.8 million parameters, just two-thirds of the model in <ref type="bibr" target="#b4">[Li et al., 2017b]</ref>. While <ref type="bibr" target="#b2">[Ke et al., 2017]</ref> used an ImageNet-pretrained VGG19 model. The extremely small model size allows us to easily train the network from scratch without the needs of pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Scalability to Multiple Persons</head><p>In activities like hugging and shaking hands, multiple persons are involved. To make our framework scalable to multiperson scenarios, we perform a comprehensive evaluation on different feature fusion strategies.</p><p>Early fusion. All joints from multiple persons are stacked as input of the network. For variable number of persons, zero padding is applied if the number of persons is less than the pre-defined maximal number.</p><p>Late fusion. As illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, inputs of multiple persons go through the same subnetwork and their conv6 feature maps are merged with either concatenation along channels or element-wise maximum / mean operation.</p><p>Note that element-wise late fusion can generalize well to variable number of persons, while the others require a predefined maximal number. Besides, compared with single person, no extra parameter is introduced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Action Recognition and Detection</head><p>For the recognition task, a softmax function is used to normalize the output of the network. The probability that a sequence X belongs to the i th class is</p><formula xml:id="formula_2">P (C i |X) = e oi C j=1 e oj , i = 1, 2, . . . , C,<label>(1)</label></formula><p>where o = (o 1 , o 2 , . . . , o c ) T is the output of the network, C is the number of classes. We also extend the proposed network for temporal action detection. Previously the Faster R-CNN framework <ref type="bibr">[Ren et al., 2015]</ref> has been adapted for the task of temporal action detection <ref type="bibr" target="#b4">Li et al., 2017b]</ref>. Following these works, we briefly introduce our implementation and later show that with the proposed hierarchical co-occurrence features, detection performance also gets significantly improved. The detection framework is shown in <ref type="figure">Figure 5</ref>. Specifically, based on the backbone feature learning network in <ref type="figure" target="#fig_2">Figure 3</ref>, two subnetworks are appended after conv5, i.e. the temporal proposal subnetwork and the action classification subnetwork. The temporal proposal subnetwork predicts variablelength temporal segments that potentially contain an action. The corresponding feature maps of each proposal are extracted using a crop-and-resize operation. After that, the classification subnetwork predicts their action categories. In both subnetworks, window regression is performed to obtain more accurate localization. We use softmax loss for classification and smooth L 1 loss for window regression. The two tasks are optimized jointly. The objective function is given by:</p><formula xml:id="formula_3">L = 1 N cls i L cls (p i , p * i ) + ? 1 N reg i L reg (t i , t * i ). (2)</formula><p>N cls and N reg are the number of samples. ? is the weight applied on the regression loss (empirically set to 1 in our experiments). i is the index of an anchor or proposal in a batch. p is the predicted probability and p * is the groundtruth label. t = {t x , t w } is the predicted regression target and t * = {t * x , t * w } is the groundtruth target. Window regression is essentially an adaptation of bounding box regression <ref type="bibr" target="#b0">[Girshick et al., 2014]</ref>, where targets along one dimension rather than two are predicted. The target transformations are computed as follows:</p><formula xml:id="formula_4">t x = (x ? x a )/w a , t w = log(w/w a ),<label>(3)</label></formula><formula xml:id="formula_5">t * x = (x * ? x a )/w a , t * w = log(w * /w a ),<label>(4)</label></formula><p>where x and w denote the center and length of the temporal window. And x, x a and x * represent the predicted window, anchor window and groundtruth window respectively. The same rule applies for w.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate the proposed method on three common benchmark datasets, i.e. the NTU RGB+D <ref type="bibr" target="#b5">[Shahroudy et al., 2016]</ref> and SBU Kinect Interaction <ref type="bibr" target="#b8">[Yun et al., 2012]</ref> datasets for action recognition, and the PKU-MMD  dataset for temporal action detection. Besides, an ablation study is performed to show the importance of global cooccurrence feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NTU RGB+D</head><p>The NTU RGB+D dataset is so far the largest skeleton-based human action recognition dataset. It contains 56880 skeleton sequences, which are annotated as one of 60 action classes. There are two recommended evaluation protocols, i.e. Cross-Subject (CS) and Cross-View (CV). In the cross-subject setting, sequences of 20 subjects are used for training and sequences of the rest 20 subjects are used for validation. In the cross-view setting, samples are split by camera views. Samples from two camera views are used for training and the rest are used for testing. During training, we randomly crop a sub-sequence from the entire sequence. The cropping ratio is drawn from uniform distribution between [0.5, 1]. During inference, we center crop a sub-sequence with a ratio of 0.9. Since different actions last for various durations, the input sequences are normalized to a fixed length (32 in our experiments) with bilinear interpolation along the frame dimension. To alleviate the problem of overfitting, we append dropout after conv4, conv5, conv6 and fc7 with a dropout ratio of 0.5. A weight decay of 0.001 is applied on the weights of fc7. We train the model for 300k iterations in total, with a mini-batch size of 64. The Adam <ref type="bibr" target="#b2">[Kingma and Ba, 2015]</ref> optimizer is utilized. The learning rate is initialized to 0.001 and exponentially decayed every 1k steps with a rate of 0.99.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SBU Kinect Interaction</head><p>The SBU Kinect Interaction dataset <ref type="bibr" target="#b8">[Yun et al., 2012</ref>] is a Kinect captured human activity recognition dataset depicting two person interaction. It contains 282 skeleton sequences and 6822 frames of 8 classes. There are 15 joints for each skeleton. For evaluation we perform subject-independent 5fold cross validation as suggested in <ref type="bibr" target="#b8">[Yun et al., 2012]</ref>.</p><p>Considering the small size of the dataset, we simplify the network architecture in <ref type="figure" target="#fig_2">Figure 3</ref> accordingly. Specifically, the output channels of conv1, conv2, conv3, conv5, conv6 and fc7 are reduced to <ref type="bibr">32,</ref><ref type="bibr">16,</ref><ref type="bibr">16,</ref><ref type="bibr">32,</ref><ref type="bibr">64</ref> and 64 respectively. And the conv4 layer is removed. Besides, all the input sequences are normalized to a length of 16 frames rather than 32.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PKU-MMD</head><p>The PKU-MMD dataset  is currently the largest skeleton-based action detection dataset. It contains 1076 long untrimmed video sequences performed by 66 subjects in three camera views. 51 action categories are annotated, resulting almost 20,000 action instances and 5.4 million frames in total. Similar to NTU RGB+D, there are also two recommended evaluate protocols, i.e. cross-subject and cross-view. For detection-specific hyper-parameters, we basically follow the settings in <ref type="bibr">[Ren et al., 2015]</ref>. In particular, we use anchor scales of {50, 100, 200, 400} in the temporal proposal network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-person Feature Fusion</head><p>To evaluate different ways of multi-person feature fusion described in Section 3.4, we perform an ablation study on the NTU RGB+D dataset. As shown in <ref type="table">Table 1</ref>  <ref type="figure">Figure 5</ref>: The temporal action detection framework. The backbone network is described in <ref type="figure" target="#fig_2">Figure 3</ref>. Two subnetworks are designed for temporal proposal segmentation and action classification respectively. might be that features of different persons are better aligned and thus more compatible in high-level semantic space than in raw input space. Among the three late fusion implementations, the element-wise maximum operation achieves the best accuracy. This is due to the side effect of zero padding for single-person actions. That is, compared with multiperson samples, features of single-person samples get weakened with the padded zeros in both cases of concatenation and element-wise mean. While element-wise maximum does not suffer from this issue. In the following experiments the late fusion with element-wise maximum strategy is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to Other State-of-the-arts</head><p>A systematic evaluation of the proposed HCN framework is performed on the three datasets mentioned above. As shown in <ref type="table" target="#tab_3">Table 2</ref>, <ref type="table" target="#tab_4">Table 3</ref> and <ref type="table" target="#tab_6">Table 4</ref>, our approach consistently outperforms the current state-of-the-arts in terms of both action recognition accuracy and action detection mAP.</p><p>On the large-scale NTU RGB+D dataset <ref type="table" target="#tab_3">(Table 2)</ref>, our approach achieves the best action recognition accuracy. Compared with the state-of-the-art LSTM-based method <ref type="bibr" target="#b9">[Zhang et al., 2017]</ref>, the accuracy is improved by 7.3% in the crosssubject setting and 3.4% in the cross-view setting. Compared with the most recent two-stream CNN method <ref type="bibr" target="#b4">[Li et al., 2017b]</ref>, the accuracy is improved by 3.3% in the crosssubject setting and 1.8% in the cross-view setting. See <ref type="figure">Figure 7</ref> for visualization of our exemplary classifications.</p><p>On the small SBU Kinect Interaction dataset <ref type="table" target="#tab_4">(Table 3)</ref>, the proposed method also outperforms other methods by a large margin. Compared with the LSTM-based cooccurrence learning baseline <ref type="bibr" target="#b9">[Zhu et al., 2016]</ref>, the accuracy is improved by 8.2%, which proves the superiority of   69.2 77.7 STA-LSTM  73.4 81.2 Clips + CNN + MTLN <ref type="bibr" target="#b2">[Ke et al., 2017]</ref> 79.6 84.8 VA-LSTM <ref type="bibr" target="#b9">[Zhang et al., 2017]</ref> 79.2 87.7 Two-stream CNN <ref type="bibr" target="#b4">[Li et al., 2017b]</ref> 83.2 89.3 Proposed HCN 86.5 91.1 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%) Raw skeleton <ref type="bibr" target="#b1">[Ji et al., 2014]</ref> 79.4 Joint feature <ref type="bibr" target="#b1">[Ji et al., 2014]</ref> 86.9 ST-LSTM  88.6 Co-occurrence <ref type="bibr">RNN [Zhu et al., 2016]</ref> 90.4 STA-LSTM  91.5 ST-LSTM+Trust Gate  93.3 VA-LSTM <ref type="bibr" target="#b9">[Zhang et al., 2017]</ref> 97.6 Proposed HCN 98.6 our CNN-based global co-occurrence feature learning framework. Since the recognition task on SBU is much easier than on NTU RGB+D, the best accuracy reported has achieved 97.6% <ref type="bibr" target="#b9">[Zhang et al., 2017</ref>]. Yet we push the frontier further and achieve an accuracy of 98.6%. On the PKU-MMD action detection dataset <ref type="table" target="#tab_6">(Table 4)</ref>, we also achieve state-of-the-art performance. Compared with the Skeleton boxes method <ref type="bibr" target="#b3">[Li et al., 2017a]</ref>, in which an adapted VGGNet is employed for temporal action detection, our method improve the mAP by 38% in the cross-subject setting. Compared with the recent work in <ref type="bibr" target="#b4">[Li et al., 2017b]</ref>, where a similar detection framework is utilized, our method improves the mAP by 2.2% and 0.5% in the cross-subject and cross-view settings respectively. Note that our improvement over it is purely owing to better features learned with Methods mAP (%) CS CV STA-LSTM <ref type="bibr">[Song et al., 2017] 44.4 13.1</ref> JCRRNN  32.5 53.3 Skeleton boxes <ref type="bibr" target="#b3">[Li et al., 2017a]</ref>   the proposed framework. See <ref type="figure" target="#fig_5">Figure 8</ref> for visualization of our exemplary detections. From the results, we can conclude that our proposed hierarchical co-occurrence feature learning framework is superior over other state-of-the-art LSTM and CNN-based methods. It is scalable to datasets of different sizes. And the learned features generalize well across various tasks, from action classification to action detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Importance of Global Co-occurrence Feature Learning</head><p>To further understand the behavior of global co-occurrence feature learning, we perform an ablation study. Specifically, we deliberately prepend a Transpose layer with parameter (0, 2, 1) before conv1 so that the joint and coordinate dimensions are swapped. Then in subsequent layers after conv2, cooccurrence features are aggregated locally rather than globally, as in previous CNN-based methods <ref type="bibr" target="#b0">[Du et al., 2016;</ref><ref type="bibr" target="#b2">Ke et al., 2017;</ref><ref type="bibr" target="#b4">Li et al., 2017b]</ref>. The modified network is referred to as HCN-local. We train HCN-local models with the same hyper-parameters as HCN. The results are listed in Table 5. We can see that HCN consistently outperforms HCNlocal on all of the three datasets. In particular, performance gain of HCN over HCN-local is more significant in the case of cross-subject than cross-view. This observation indicates that with global co-occurrence feature, the variation across different persons can be well addressed.  For a detailed comparison, we further investigate the percategory change in accuracy. <ref type="figure" target="#fig_4">Figure 6</ref> shows the results, where the categories are sorted by accuracy gain. We can see that most actions get improved, in particular those involving long-range joint interaction. For example, over 10% absolute improvement is observed for wear a shoe, clapping, wipe face and take-off a shoe. For actions with no obvious joint interaction such as nausea and typing on a keyboard, global co-occurrence feature is not critical.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>What motivates us to explore the different levels of context aggregation is to figure out the importance of modeling interactions among joint points in action recognition. Given a certain type of action of one or more subjects, do the whole or partial interactions of joints contribute to the recognition on earth? The answer from our experiments is initially counter-intuitive but makes sense, which has been proven by many analogous works <ref type="bibr">[He et al., 2016;</ref><ref type="bibr" target="#b9">Zhong et al., 2017]</ref>. The so-called background context is an essential factor for boosting a task's performance and it is the same case in action recognition. For recognition of a specific action, e.g. make a phone call, the joints of uninterest, say ankle, play a similar role as background context and the contribution is encoded implicitly with CNNs. This is just the insights which our method benefits from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We present an end-to-end hierarchical co-occurrence feature learning framework for skeleton-based action recognition and detection. By exploiting the capability to global aggregation of CNN, we find that the joint co-occurrence features can be learned with CNN model simply and efficiently. In our method, we learn point-level features for each joint independently, Afterwards, we treat the feature of each joint as a channel of convolution layer to learn hierarchical cooccurrence features. And a two-stream framework is adopted to fuse the motion feature. Furthermore, we exploit the best way to deal with multi-person involved activities. Experiments on three benchmark datasets demonstrate that the pro-posed HCN model significantly improves the performance on both action recognition and detection tasks. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Workflow for skeleton-based human action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Decomposition of a 3 ? 3 convolution into two steps. (a) Independent 2D convolution in the spatial domain for each input channel, where features are aggregated locally from 3 ? 3 neighborhoods. (b) Element-wise summation across channels, where features are aggregated globally from all input channels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Overview of the proposed hierarchical co-occurrence network. Green blocks are convolution layers, where the last dimension denotes the number of output channels. A trailing "/2" means an appended MaxPooling layer with stride 2 after convolution. A Transpose layer permutes the dimensions of the input tensor according to the order parameter. ReLU activation function is appended after conv1, conv5, conv6 and fc7 to introduce non-linearity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Late fusion diagram for multi-person feature fusion. Maximum, mean and concatenation operations are evaluated in terms of performance and generalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Per-category change in accuracy of HCN over HCN-local on the NTU RGB+D dataset in the cross-subject setting. For clarity only categories with change greater than 1% are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Exemplary action detection results on the val set of the PKU-MMD dataset. Five samples are shown. For each sample, the horizontal axis represents frame indices. Groundtruth action segments are drawn in blue, while detected segments with confidence greater than 0.6 are displayed in red. The number attached to each segment is the category ID. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, all late fusion methods outperform the early fusion method. The reason</figDesc><table><row><cell>Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Backbone network</cell><cell></cell><cell>Proposal</cell><cell>Crop</cell><cell>Resize</cell><cell>Subnet</cell><cell>Start-end times</cell></row><row><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>Action scores</cell></row><row><cell>Skeleton sequence</cell><cell>Feature map</cell><cell cols="2">Temporal proposal subnetwork</cell><cell cols="3">Action classification and window regression subnetwork</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Action classification performance on the NTU RGB+D dataset. CS and CV mean the cross-subject and cross-view settings respectively.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Action classification performance on the SBU dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Action detection performance on the PKU-MMD dataset. mAP is measured at an IoU threshold of 0.5.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Comparison of HCN-local and HCN in terms of classification accuracy on the NTU RGB+D and SBU datasets and detection mAP on the PKU-MMD dataset.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Exemplary Results hugging other person take off a shoe falling throw GT: nausea or vomit Predict: sneeze/cough  <ref type="table">32  4  31  38  23  39  20  29  9  22  19 41  8  10  44  11  15  45  42  43   4  8  10  11  15  19  19  20  22  23  29  29  31  32  38  39  41  42  43  44  45   0  500  1000  1500  2000  2500  3000  3500  4000   50  3  2  48  37  25  33 34  6  7  13  17  36  49  30  51  46  40  1  5   1  2  3  5  6  7  13  17  25  30  33 34  36  37  40  47  48  49  50  51  51   0  200  400  600  800  1000  1200  1400   26  18  27  21  24  16  12  14   12  14  16  18  21  24  24  26  27   0  200  400  600  800  1000  1200  1400   24  12  21  27  26  18  14  16   12  14  16  18  21  21  24  26  27   0  500  1000  1500  2000  2500  3000  3500  4000   50  3  2  48  47  37  33  6  7  13 17  36 49  30  51  46  40  1  5</ref> </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3d joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xiangyu Zhang, Shaoqing Ren, and Jian Sun</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="639" to="683" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
	<note>IJCAI</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interactive body part contrast mining for human interaction recognition</title>
	</analytic>
	<monogr>
		<title level="m">ICMEW</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. A New Representation of Skeleton Sequences for 3D Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>Sou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho Jin Choi ;</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Ba</addrLine></address></meeting>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="203" to="220" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR</note>
	<note>Online human action detection using joint classificationregression recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton boxes: Solving skeleton based action detection with a single deep convolutional neural network</title>
	</analytic>
	<monogr>
		<title level="m">ICMEW</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="613" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">ICMEW</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: towards real-time object detection with region proposal networks</title>
	</analytic>
	<monogr>
		<title level="m">Tian-Tsong Ng, and Gang Wang. NTU RGB+D: A Large Scale Dataset for 3D Human Activity Analysis</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An end-to-end spatiotemporal attention model for human action recognition from skeleton data</title>
	</analytic>
	<monogr>
		<title level="m">CVPRW</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">R-C3D: Region Convolutional 3D Network for Temporal Activity Detection</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
	</analytic>
	<monogr>
		<title level="m">Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, and Nanning Zheng</title>
		<editor>CVPRW. IEEE</editor>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10749</idno>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3697" to="3703" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Cascade region proposal and global context for deep object detection</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
