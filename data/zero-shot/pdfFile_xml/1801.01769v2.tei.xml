<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D-DETNet: a Single Stage Video-Based Vehicle Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suichan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<postCode>230026</postCode>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">3D-DETNet: a Single Stage Video-Based Vehicle Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video vehicle detection</term>
					<term>3DCNN</term>
					<term>spatiotemporal feature</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-based vehicle detection has received considerable attention over the last ten years and there are many deep learning based detection methods which can be applied to it. However, these methods are devised for still images and applying them for video vehicle detection directly always obtains poor performance. In this work, we propose a new single-stage video-based vehicle detector integrated with 3DCovNet and focal loss, called 3D-DETNet. Draw support from 3D Convolution network and focal loss, our method has ability to capture motion information and is more suitable to detect vehicle in video than other single-stage methods devised for static images. The multiple video frames are initially fed to 3D-DETNet to generate multiple spatial feature maps, then sub-model 3DConvNet takes spatial feature maps as input to capture temporal information which is fed to final fully convolution model for predicting locations of vehicles in video frames. We evaluate our method on UA-DETAC vehicle detection dataset and our 3D-DETNet yields best performance and keeps a higher detection speed of 26 fps compared with other competing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Video-based vehicle detection for driver assistance and traffic surveillance has received considerable attention over the last ten years. Recently, deep learning based approaches which can be roughly divided into two streams including singlestage detectors 1, 2, 3 and two-stage detectors <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6</ref> , have achieved excellent results in still images. Compared to two-stage methods, single-stage methods always yield faster performance in speed which is more suitable for the real-time application. However, it is challenging to apply these methods for video object detection directly. The deteriorated object appearance invades which are seldom observed in still images, such as motion blur, video defocus, can damage recognition accuracy <ref type="bibr" target="#b6">7</ref> .</p><p>Nevertheless, the video has rich information about the same object instance. Such temporal information has been exploited in existing video classification and recognition methods <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> . These methods learn spatiotemporal features using 3D ConvNets. Du Tran et al. <ref type="bibr" target="#b8">9</ref> found that 3D ConvNets were more suitable for spatiotemporal feature learning compared to 2D ConvNets. On the other hand, recent work on single-stage detectors, such as YOLO 1, 2 and SSD 3 yield faster performance in speed, but with lower accuracy relative to state-of-the-art two-stage methods. In 10 , the central cause of why single-stage approaches have trailed the accuracy of two-stage detectors thus far was investigated, and a focal loss was proposed to address the problem. As motivated by the success of 3D ConvNets <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> and focal loss 10 , we propose a new single-stage video-based vehicle detector integrated with 3DCovNet and focal loss, called 3D-DETNet, to improve the per-frame vehicle detection performance and keep high detection speed at the same time. Our 3D-DETNet can be trained in an end to end fashion, and does not need training in multistage paradigm as <ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5</ref> follow. Our approach is evaluated on the recent DETRAC vehicle detection dataset <ref type="bibr" target="#b10">11</ref> . Rigorous evaluating study has verified that our method is effective and significantly improves upon single frame baselines. In addition, we compared it with other detectors and the results showed that our approach has better performance both in accuracy and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hand-engineered feature based detectors</head><p>Early works of vehicle detection used Haar-like features and Histograms of Oriented Gradient (HOG) <ref type="bibr" target="#b11">12</ref> to detect vehicles in images. In 13 , along with the histogram of oriented gradients (HOG), the authors proposed and implemented a new type of feature vector, i.e., HOG symmetry vectors for vehicle detection. The feature pyramids were also used to detect objects in <ref type="bibr" target="#b13">14</ref> . In 15 , a mixture of multi-scale deformable part models was used to represent highly variable objects. Nevertheless, the success of those approaches generally depends on the stability of data representation features, like moving objects' scale changes, translation, etc. <ref type="bibr" target="#b15">16</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Deep learning based object detectors</head><p>With the development of modern deep ConvNets, object detectors like R-CNN 4 and YOLO 2 showed dramatic improvements in accuracy. Based on a region proposal strategy, R-CNN 4 applies high capacity convolution neural networks (CNNs) to bottom-up region proposals to locate objects in image. YOLO 2 generates several candidate objects for anchor boxes in the image and locates the objects in a single shot. These methods based on convolution neural networks can automatically extract robust features without human assistance and achieve excellent results in still images. However, directly applying them for video object detection is challenging, due to motion blur, video defocus, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning temporal features with 3D ConvNets</head><p>Recently, 3DConvNets were proposed to learn spatial-temporal features for video classification and recognition methods. In 8 , a 3D CNN model was developed for human action recognition and 3D ConvNets were proposed to extract features both in spatial and temporal dimensions. Du Tran et al. <ref type="bibr" target="#b8">9</ref> found that 3D ConvNets were more suitable for spatiotemporal feature learning compared to 2D ConvNets. More recently, following the philosophy of ResNet 17 , Qiu, Z et al. <ref type="bibr" target="#b17">18</ref> proposed a new architecture, named Pseudo-3D Residual Net and achieved clear improvements on multiple benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MODEL ARCHITECTURE</head><p>We illustrate our 3D-DETNet in <ref type="figure">Figure 1</ref> and as follows: <ref type="figure">Figure 1</ref>.The architecture of our 3D-DETNet. Three sub-models are involved including DarkNet_Conv23, 3D Convolution Networks (3DConvNet) and 2D Convolution Networks (2DConvNet). The DarkNet_Conv23 is responsible for generating rich spatial feature representations from multiple frames. The 3DConvNet is mainly to capture the motion or temporal information encoded in multiple contiguous video frames. The 2DConvNet takes the feature maps from the previous 3D convolution Network (3DConvNet) as input and predicts final outputs. <ref type="figure">Figure 1</ref>, the multiple contiguous frames are initially fed to DarkNet-Conv23 2 to generate rich spatial features representations respectively. The DarkNet-Conv23 is pre-trained in ImageNet dataset <ref type="bibr" target="#b18">19</ref> and is capable of extracting rich spatial features. The output feature maps are sent to 3DConvNet model afterwards for further extraction of temporal features. 3DConvNet?The sub-model 3DConvNet is mainly to capture the motion or temporal information encoded in multiple contiguous video frames. However different from <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9</ref> , our 3DConvNet model takes the spatial feature maps from the previous DarkNet-23 model as input, instead of taking multiple video frames as input directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DarkNet-Conv23?As shown in</head><p>2DConvNet?The 2DConvNet which consists of several 2D convolution layers takes the feature maps from the previous 3D convolution Network (3DConvNet) as input. The model's final outputs include bounding box coordinates and class probability.</p><p>We describe below each individual model in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Darknet-Conv23 for spatial feature</head><p>Recently, VGG-16 <ref type="bibr" target="#b19">20</ref> was leveraged by most of detection method as the feature extractor base. VGG-16 is powerful but complex. In 2 , a new image classification network model called Darknet-19 was proposed, which is faster and requires fewer parameters than VGG-16. Darknet-19 has 19 convolution layers and 5 maxpooling layers, it was trained on IamgeNet dataset <ref type="bibr" target="#b18">19</ref> , and achieved 72.9% top-1 accuracy as well as 93.3% top-5 accuracy. We leverage DarkNet-19 as our feature extractor base by removing the last convolution layer, which called DarkNet_Conv23 (the final model has 23 layers), so our detector can benefit from extra big dataset, which is a common practice for transfer learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">3DConvNet for temporal feature</head><p>As mentioned earlier, video vehicle detection has a greater challenge than vehicle detection in static images. We want to extract motion information to overcome the challenge of video blur and so on. To this end, we propose to perform 3D convolutions in the convolution stages of CNNs to compute features in temporal dimensions and get the motion information. Similar to the case of 2D convolution, we can apply multiple 3D convolutions with distinct kernels to the same location in the previous layer to extract multiple types of features.</p><p>Formally, the value at position (x, y, z) on the jth feature map in the ith layer can be given by <ref type="bibr" target="#b0">(1)</ref> where , are the size of the 3D kernel along spatial dimension respectively, is the size of the 3D kernel along temporal dimension, is the (p, q, r)th value of the kernel connected to the mth feature map in the previous layer.</p><p>is bias term.</p><p>Notably, our 3DConvNet model does not take multiple video frames as input directly, but instead take the spatial feature maps from the previous DarkNet-23 model as input. To some extent, our 3DConvNet plays the role of feature fusion or aggregation.</p><p>Based on the 3D convolution described above, our 3DConvNet architecture can be devised. As shown in <ref type="figure">Figure 1</ref>, our 3DConvNet is composed of three 3D convolution layers. According to the findings in <ref type="bibr" target="#b8">9</ref> , small receptive fields of 3x3x3 convolution kernel yield best result. Hence, for our architecture, we set the kernel size to 3x3x3, 1x1x1, 3x3x3 with appropriate strides and padding size respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">2DConvolution prediction for detection</head><p>We use the convolution layer predicting both class probabilities and bounding box coordinates, instead of fully connected layer. Our 2DConvNet model consists of two 3x3 convolution layer with 1024 filters each followed by a 1 x 1 convolution layer, which has the number of outputs we need for vehicle detection. Follow YOLO 2 , we use anchor boxes to predict bounding boxes. We don't hand pick the bounding box dimensions, i.e. width and height, instead we run kmeans clustering on the DATRAC dataset to find good priors. Finally, we choose k=5 and get 5 anchor box priors.</p><p>Our model predicts 5 bounding boxes at each cell in the output feature map, and for each bounding box we predict 5 coordinates: x, y, w, h and confidence c. Like 2 , we don't regress the coordinates relative to original frames, instead we parameterize the coordinates by :</p><p>, , , ( , , , , )</p><p>x y w h c t t t t t <ref type="bibr" target="#b1">(2)</ref> where is offset of cell from the top left corner of the image, are width and height of anchor priors respectively, is a logistic activation function in range [0, 1]. is final prediction coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Focal loss and multi-part loss function</head><p>The state-of-the-art object detectors are based on a two-stage approach popularized by R-CNN 3 . One-stage detectors yield faster but with accuracy within 10-40% relative to state-of-the-art two-stage methods <ref type="bibr" target="#b9">10</ref> . The central cause of why one-stage approaches have trailed the accuracy of two-stage detectors thus far was investigated, and a focal loss was proposed to address the problem in <ref type="bibr" target="#b9">10</ref> . Formally, the focal loss can be defined as:</p><p>where which belongs to [0, 1] is the model's estimated probability for the class with positive label. is a tunable focusing parameter. See <ref type="figure" target="#fig_1">Figure 2</ref>, the focal loss is visualized for multiple values of . As motivated by focal loss, we leverage the focal loss for our classification loss. As mentioned earlier, our predictive output includes coordinates and categories information, so our loss function is a multi-part loss and can be formalized as following <ref type="bibr" target="#b3">(4)</ref> where and are the location regression loss and classification loss. is a balance factor. For regression loss, we use smooth L1 loss 5 defined as </p><formula xml:id="formula_2">x y w h c b b b b b ( ) (1 ) log( ) t t t t FL p p p g a = - - t p 0 g ? g g l = + ( , ) ( ) loc cls L L t s L s ( , ) loc L t s ( ) cls L s l<label>(5)</label></formula><p>For classification loss, we use focal loss as defined in Equation <ref type="formula" target="#formula_1">(3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Training</head><p>Follow YOLO 2 , we only want one bounding box predictor to be responsible for each object. So we assign one predictor as positive bounding box which has the highest IOU with the ground truth.</p><p>3D-DETNet is trained with stochastic gradient descent (SGD) for about 80 epochs on training sets. We set learning rate to 10e-3 for first 60 epochs and then decrease to 10e-4 for remaining epochs. For data augmentation, we use random crop and horizontal image flipping. We also randomly adjust the exposure and saturation of the image in HSV color space. We use the mini-batch size of 32, a weight decay of 0.0005 and momentum of 0.9.</p><p>Due to limitation of memory, we choose three frames as input of 3D-DETNet during training. Note that the neighbor frames are randomly sampled from a range [-10, 10] relative to the reference frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>We evaluate the experiments on the UA-DETRAC vehicle detection dataset <ref type="bibr" target="#b10">11</ref> . The dataset consists of 10 hours of videos captured in different scenarios including sunny, cloudy, rainy and night. There are more than 140 thousand frames in the UA-DETRAC dataset and 8250 vehicles that are manually annotated, leading to a total of 1.2M labeled bounding boxes of objects.</p><p>To evaluate the effectiveness of our detect framework, we conducted three experiments in this paper. The first experiment is evaluated on a small validation dataset to compare focal loss with cross-entropy loss and to find a best for focal loss. The second one evaluate the performance of our 3D-DETNet on validation dataset. The final one is evaluated in the DETRAC test dataset and compared with other competing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Focal loss vs. cross-entropy loss</head><p>In this section, we compare focal loss with cross-entropy loss which is utilized in many classification problems. On the other hand, focal loss has a tunable parameter , so we evaluate focal loss with multiple values of and find a best gamma for our task. The results are summarized in <ref type="table" target="#tab_0">Table 1</ref>. From the result, it can be seen that the focal loss yields better performance compared to standard cross-entropy loss which accords with the study in <ref type="bibr" target="#b9">10</ref> . For focal loss, we observe that 2 gets the best result for our task. For remaining experiments, we set 2 for focal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluate the effectiveness of 3D-DETNet</head><p>This experiment is mainly to evaluate the effectiveness of our 3D-DETNet. The 3DConvNet is proposed to capture the motion information encoded in multiple contiguous frames in our framework. If we remove the 3DConvNet in our</p><formula xml:id="formula_3">2 1 if 1 0.5 otherwise L x x smooth x ? &lt; ? = ? - ? ? g g g g g g = g =</formula><p>framework, the remaining network architecture is similar to YOLO 2 . To evaluate of effectiveness our methods, we compare 3D-DETNet with YOLO 2 on validation dataset. The <ref type="table" target="#tab_1">Table 2</ref> show the performance of our 3D-DETNet and YOLO on validation set in different scenarios. Quantitatively speaking, our 3D-DETNet performs better than YOLO. We note that our 3D-DETNet achieves overall mAP 82.86, as comparison, the YOLO has a lower overall mAP of 80.23. On the other hand, we note that our method outperforms YOLO by a larger margin under scenarios of rainy and night compared to sunny and cloudy scenarios, while rainy and night scenarios are more challenging than sunny and cloudy scenarios. These results indicate that our 3DConvNet has ability to capture strong temporal information and improve the effectiveness of per-frame vehicle detection.  In this experiment, we evaluate our model on the DETRAC official test dataset. We compare our method with three state-of-the-art vehicle detection approaches. The results are showed in table 3. Notably, our method performs the best on all subcategories and keeps a higher speed of 26 fps. We also report precision-recall curves in <ref type="figure" target="#fig_3">Figure 3</ref>. As shown in <ref type="figure" target="#fig_3">Figure 3</ref>, we note that our approach has better detection coverage as well as accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compare with other methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we propose a new one-stage vehicle detector integrated with 3DCovNet and focal loss, called 3D-DETNet. Draw support from 3D Convolution and focal loss, our method has ability to capture motion information and is able to improve the per-frame vehicle detection performance. Compared with other competing methods, our 3D-DETNet yields best performance and run at 26 fps on a moderate commercial GPU. In the future work, we plan to further investigate the ability of 3D Convolution network and improve the performance of vehicle detection in video.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Visualization of focal loss for multiple values of .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Precision-recall curve of different methods on test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell cols="2">The mean Average Precision (mAP) of cross-entropy and focal loss with different</cell><cell>on small validation dataset</cell></row><row><cell>Loss</cell><cell></cell><cell>mAP</cell></row><row><cell>Cross-entropy</cell><cell>0</cell><cell>81.52</cell></row><row><cell>Focal loss</cell><cell>1</cell><cell>82.16</cell></row><row><cell>Focal loss</cell><cell>2</cell><cell>83.85</cell></row><row><cell>Focal loss</cell><cell>3</cell><cell>83.64</cell></row><row><cell>Focal loss</cell><cell>4</cell><cell>83.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>The overall mAP in validation dataset.</figDesc><table><row><cell>Method</cell><cell>Overall</cell><cell>Sunny</cell><cell>Cloudy</cell><cell>Rainy</cell><cell>Night</cell></row><row><cell>YOLO</cell><cell>80.23</cell><cell>83.28</cell><cell>84.19</cell><cell>67.98</cell><cell>77.35</cell></row><row><cell>3D-DETNet</cell><cell>82.86</cell><cell>84.32</cell><cell>85.50</cell><cell>70.26</cell><cell>80.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Tabel 3. The mean Average Precision (mAP) and speed on DEATRAC test dataset generated by our method and other competing vehicle detection methods.Method Overall Easy Medium Hard Cloudy Night Rainy Sunny Speed(fps)</figDesc><table><row><cell>DPM 15</cell><cell>25.70 34.42</cell><cell>30.29</cell><cell>17.62 24.78 30.91 25.55 31.77</cell><cell>0.17</cell></row><row><cell>ACF 14</cell><cell>46.35 54.27</cell><cell>51.52</cell><cell>38.07 58.30 35.29 37.09 66.58</cell><cell>0.67</cell></row><row><cell cols="2">RCNN 4 48.95 59.31</cell><cell>54.06</cell><cell>39.47 59.73 39.32 39.06 67.52</cell><cell>0.10</cell></row><row><cell>Ours</cell><cell>53.30 66.66</cell><cell>59.26</cell><cell>43.22 63.30 52.90 44.27 71.26</cell><cell>26</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV 9905</title>
		<meeting>ECCV 9905</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv Neural Inf Process Syst</title>
		<imprint>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Flow-Guided Feature Aggregation for Video Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10025</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">3D convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04136</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision-based vehicle detection system with consideration of the detecting location</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Intell Transp Syst</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Pattern Anal. Machine Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1627" to="1645" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vehicle detection on a video traffic scene: Review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gazzah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mhalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E B</forename><surname>Amara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="448" to="454" />
		</imprint>
	</monogr>
	<note>Information and Digital Technologies (IDT</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
