<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Solving Inefficiency of Self-supervised Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
							<email>kezewang@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">DarkMatter AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<email>philip.torr@eng.ox.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Solving Inefficiency of Self-supervised Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised learning (especially contrastive learning) has attracted great interest due to its huge potential in learning discriminative representations in an unsupervised manner. Despite the acknowledged successes, existing contrastive learning methods suffer from very low learning efficiency, e.g., taking about ten times more training epochs than supervised learning for comparable recognition accuracy. In this paper, we reveal two contradictory phenomena in contrastive learning that we call under-clustering and over-clustering problems, which are major obstacles to learning efficiency. Under-clustering means that the model cannot efficiently learn to discover the dissimilarity between inter-class samples when the negative sample pairs for contrastive learning are insufficient to differentiate all the actual object classes. Over-clustering implies that the model cannot efficiently learn features from excessive negative sample pairs, forcing the model to over-cluster samples of the same actual classes into different clusters. To simultaneously overcome these two problems, we propose a novel self-supervised learning framework using a truncated triplet loss. Precisely, we employ a triplet loss tending to maximize the relative distance between the positive pair and negative pairs to address the under-clustering problem; and we construct the negative pair by selecting a negative sample deputy from all negative samples to avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model. We extensively evaluate our framework in several large-scale benchmarks (e.g., ImageNet, SYSU-30k, and COCO). The results demonstrate our model's superiority (e.g., the learning efficiency) over the latest stateof-the-art methods by a clear margin. See Codes 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, self-supervised learning (SSL) has shown remarkable results in representation learning. Among them, the results of contrastive learning are most promising in the <ref type="bibr">Figure 1</ref>. A comparison of learning efficiency among different SSL methods using ResNet-50. Here, the x-axis represents the training epochs of SSL, and the y-axis stands for the top-1 accuracy of ImageNet linear evaluation. All methods have lower learning efficiency than supervised learning, but our approach has a significantly higher learning efficiency than the existing SSL methods. (best view in color) computer vision tasks. Notable works include MoCo v1/v2 <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b8">8]</ref>, SimCLR <ref type="bibr" target="#b7">[7]</ref>, BYOL <ref type="bibr" target="#b22">[22]</ref>, and SimSiam <ref type="bibr" target="#b9">[9]</ref>. For example, on ImageNet <ref type="bibr" target="#b37">[36]</ref>, the top-1 accuracy of BYOL is 74.3%, which is close to that of supervised learning, i.e., 76.4% <ref type="bibr" target="#b51">[49,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b33">32]</ref> (see "goal line" in <ref type="figure">Figure 1</ref>). Despite the promising accuracies and high expectations, the learning efficiency of the state-of-the-art SSL methods is about ten times lower than the supervised learning methods. For instance, the supervised learning method typically takes about 100 epochs to train a ResNet50 on ImageNet. In comparison, SimCLR and BYOL have to cost 1,000 epochs, and MoCo v2 needs to cost 800 epochs (See <ref type="figure">Figure 1</ref>).</p><p>Attempting to address this issue, we rethink existing SSL methods' mechanism and attribute their inherited drawback to two opposing problems, i.e., under-clustering and over-clustering. Specifically, during batch training, contrastive learning randomly crops each image two times to obtain two views and study the similarity between these two views (called a positive sample pair 2 ). Meanwhile, some methods also study the dissimilarity between crossimage views (called negative sample pairs <ref type="bibr" target="#b2">3</ref> ). The optimization objective is to reduce the distance between posi-tive sample pairs and enlarge the distance between negative sample pairs. As suggested by metric learning <ref type="bibr" target="#b12">[12]</ref>, sufficient negative sample pairs are required to guarantee the learning efficiency. Otherwise, lacking negative sampleswhether due to the GPU memory constraints like SimCLR or (ii) algorithm design like BYOL and SimSiam <ref type="bibr" target="#b9">[9]</ref> -can make different object categories having overlaps. This is identified as the under-clustering problem. One evidence of under-clustering is shown in <ref type="table">Table 1</ref>  <ref type="bibr" target="#b3">4</ref> . As a result of under-clustering, SimCLR and BYOL have low learning efficiency because the model cannot efficiently discover the dissimilarity between inter-class samples. On the contrary, excessive negative samples can lead to an opposite problem, i.e., over-clustering, which implies the negative samples are false negative and the model over-clusters samples of the same actual categories into different clusters. In an extreme case, there would be 1.28M clusters for ImageNet! One evidence of over-clustering is in <ref type="table">Table 1</ref>  <ref type="bibr" target="#b4">5</ref> . Over-clustering also results in low learning efficiency since it encourages dissimilarity between intra-class samples in vain. As reported by <ref type="bibr" target="#b56">[54,</ref><ref type="bibr" target="#b2">3]</ref>, over-clustering can lead to unnecessary harmful representation learning. For example, <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b14">14]</ref> obtains an unsatisfied performance due to directly clarifying CIFAR-10 into 50K clusters. MoCo v1/v2 cannot further increase the accuracy, even leveraging the momentum to store plenty of negative samples. In summary, existing contrastive learning cannot avoid the under-clustering or over-clustering problems, so their learning efficiency is still low.</p><p>To tackle the above under-clustering and over-clustering problems, a few pioneering works have been proposed to analyze the negative samples' role in the contrastive loss <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b27">27]</ref>. As opposed to these methods that use overcomplicated contrastive losses, we propose an SSL framework using a quite simple truncated triplet loss. Specifically, a triplet loss can maximize the relative distance between the positive and negative pairs for each triplet unit. Having plenty of triplets, we can address the underclustering problem because wealthy triplets contain rich negative pairs that guarantee a considerable distance between negative sample pairs. Triplet loss largely addresses the under-clustering issue but raises the over-clustering problem. Hence, we propose a novel truncated triplet loss to avoid over-clustering samples from the same category into different clusters. The truncated triplets are ensured with confidence guaranteed by the Bernoulli Distribution model. This significantly improves SSL's learning efficiency and <ref type="bibr" target="#b3">4</ref> We calculate the class center for each category and compute the distances for every two class centers. These center-to-center distances are averaged to form a class divergence. We keep the variance equal so we can just compare class divergence. The small class divergence in <ref type="table">Table 1</ref> indicates BYOL does suffer under-clustering. <ref type="bibr" target="#b4">5</ref> We use Pr(?|A) (defined in Section 5) to denote the possibility of containing a false negative samples in a batch. The high probabilities in <ref type="table">Table 1</ref>  Over-clustering (Pr(?|A)) MoCo v2 Ours 1.0 0.0110 <ref type="table">Table 1</ref>. Qualitative analysis of over-/ under-clustering. We use Pr(?|A) (the larger, the higher over-clustering risk) and class divergence (the smaller, the higher under-clustering risk) to measure the over-/ under-clustering level, respectively. leads to state-of-the-art performance (See <ref type="figure">Figure 1)</ref>.</p><p>In summary, our contribution is three-fold.</p><p>? We analyze the existing best-performing contrastive learning methods and attribute their low learning inefficiency to the under-clustering and over-clustering, which result in unnecessary harmful representation learning just to memorize the data.</p><p>? To address the under-clustering and over-clustering problem, we propose a novel SSL framework using a truncated triplet loss. Precisely, we employ a triplet loss containing rich negative samples to address the under-clustering problem, and our triplet loss uses truncated/trimmed triplets to avoid over-clustering, guaranteed by the Bernoulli Distribution model.</p><p>? Our method significantly improves SSL's learning efficiency and thus leads to state-of-the-art performance in several large-scale benchmarks (e.g., ImageNet <ref type="bibr" target="#b37">[36]</ref>, SYSU-30k <ref type="bibr" target="#b48">[46]</ref>, and COCO 2017 <ref type="bibr" target="#b29">[29]</ref>) and varieties of downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Vanilla SSL. The recent renaissance of SSL originated from straightforward pretext tasks. Typical pretext tasks included image denoising <ref type="bibr" target="#b42">[41]</ref>, image inpainting <ref type="bibr" target="#b34">[33]</ref>, patch ordering <ref type="bibr" target="#b13">[13]</ref>, solving jigsaw puzzles <ref type="bibr" target="#b32">[31]</ref>, color jittering <ref type="bibr" target="#b57">[55]</ref>, and rotation prediction <ref type="bibr" target="#b19">[19]</ref>. Although these methods contributed to the renaissance of SSL, their learned representations did not generalize well.</p><p>Contrastive learning. Currently, the most effective SSL method in computer vision is contrastive learning <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b6">6]</ref>, in which the intra-class distances are encouraged to be small, and the inter-class distances are forced to be large <ref type="bibr" target="#b6">6</ref> . Plenty of positive and negative samples are needed to discover the similarity and dissimilarity, which requires large GPU memories <ref type="bibr" target="#b7">[7]</ref>. To address this problem, Sim-CLR <ref type="bibr" target="#b7">[7]</ref> employs multi-machine distributed computing to enlarge the batch. Nevertheless, due to GPU memory limitation, further increasing positive/negative samples is prohibitive in practice, which forms a barrier to improving SSL. We identify this as an under-clustering problem.</p><p>To avoid under-clustering, more elegantly, Mean Teacher <ref type="bibr" target="#b39">[38]</ref> is applied to produce sufficient negative <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b8">8]</ref> and  positive samples <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b6">6]</ref>. Moreover, Exemplar-CNN <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b14">14]</ref> directly clarifies each image in a dataset into a cluster, i.e., it categorizes CIFAR-10 into 50K clusters. However, it obtains an unsatisfactory performance. We identify this as an over-clustering problem. Specifically, since each image can be regarded as a cluster, excessive negative sample pairs can over-cluster samples from the same category into different clusters. This over-clustering can lead to bad representation learning since the network just memorizes the data instead of learning from the data <ref type="bibr" target="#b56">[54,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To reduce over-clustering, recent works rethink the necessity of negative samples and propose to remove negative samples at all. Notable works include BYOL <ref type="bibr" target="#b22">[22]</ref> and SimSiam <ref type="bibr" target="#b9">[9]</ref>. However, once the negative samples are removed, under-clustering may probably reoccur because the model cannot efficiently discover the dissimilarity between inter-class samples (see <ref type="table">Table 1</ref> for the evidence). Besides, a few other pioneering works are also proposed to analyze the negative samples' role in the contrastive loss. <ref type="bibr" target="#b3">[4]</ref> used empirical evidence to show that not all negatives are equally important for contrastive learning. <ref type="bibr" target="#b27">[27]</ref> used a complicated way to cancel false negatives. <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b36">35]</ref> observed that using extremely close samples is bad for contrastive learning and leverage distribution knowledge to solve the problem. As opposed to these methods that use over-complicated contrastive losses, we use a quite simple triplet loss.</p><p>Triplet loss. Triplet loss was proposed by Ding et al. <ref type="bibr" target="#b12">[12]</ref> and Schroff et al. <ref type="bibr" target="#b38">[37]</ref> independently for person reidentification and face recognition, respectively. It tends to maximize the relative distance between the positive pair and the negative pair for each triplet unit. Several improvements over triplet loss are conducted to discover the valuable triplets <ref type="bibr" target="#b26">[26]</ref>, to perform cross-batch triplet loss <ref type="bibr" target="#b50">[48]</ref>, and to apply to weakly supervised scenario <ref type="bibr" target="#b48">[46]</ref>. However, these classical triplet losses can also result in overclustering. In contrast, we propose a truncated triplet loss to address the over-clustering problem guaranteed by the Bernoulli Distribution model.</p><p>Recognizing the hardest negatives can lead to bad local minima in practice, <ref type="bibr" target="#b38">[37]</ref> proposes a semi-hard negative sampling strategy, sharing the merit of our method in avoiding over-trusting the hardest negative sample and benefiting representation learning. The differences between our truncated triplet loss and semi-hard sampling strategy are twofold. First, if we read the widely-used code in TensorFlow and Pytorch, we can find that a semi-hard triplet loss is a margin-based loss rather than a ranking loss. Second, our method trims negative samples while <ref type="bibr" target="#b38">[37]</ref> performs vanilla sampling. Our approach differs from <ref type="bibr" target="#b38">[37]</ref>'s vanilla sampling and mixture sampling and is thus a new one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Self-supervised Representation Learning</head><p>We first present the under-clustering and over-clustering problems of contrastive learning in Section 3.1. Then, we present our method in Section 3.2. The analysis of the effectiveness of our approach is presented in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Under-clustering and Over-clustering</head><p>Contrastive learning is proposed by <ref type="bibr" target="#b23">[23]</ref> and is widely used in SSL, achieving best-performing results on Ima-geNet. The most widely-adopted loss for contrastive learning is InfoNCE <ref type="bibr" target="#b41">[40]</ref>. Let x be a query image which has 1 positive sample x + and m negative samples {x ? j } j=1,??? ,m . InfoNCE calculates their inner products and normalize the products using softmax and have:</p><formula xml:id="formula_0">{ x T x + , x T x ? 1 , x T x ? 2 , ? ? ? , x T x ? m} =sof tmax({x T x + , x T x ? 1 , x T x ? 2 , ? ? ? , x T x ? m }).</formula><p>Then, the goal of contrastive learning is to minimize:</p><formula xml:id="formula_1">? 1 log x T x + ? 0 log x T x ? 1 ? 0 log x T x ? 2 ? ? ? ? 0 log x T x ? m.</formula><p>(see footnote <ref type="bibr" target="#b7">7 )</ref>, which can be interpreted as forcing x T x + to be close to 1 and forcing</p><formula xml:id="formula_2">x T x ? 1 , x T x ? 2 , ..., x T x ?</formula><p>m to be close to 0. This indicates that plenty of negative sample pairs are needed to guarantee the learning efficiency because the <ref type="bibr" target="#b7">7</ref> Generally, it's written as: ? log</p><formula xml:id="formula_3">exp(x T x + )/? exp(x T x + )/? + m j=1 exp(x T x ? j )/? , where ? is a temperature.</formula><p>model needs sufficient negative samples to discover the dissimilarity between inter-class samples. Especially, plenty of positive samples and negative samples are needed to enrich the similarity and dissimilarity in each batch of data. Under-clustering. Insufficient positive and negative examples can lead to under-clustering. Under-clustering is a critical problem in which different categories have a valid (but unwelcome) overlap. For example, in <ref type="figure" target="#fig_1">Figure 2</ref>  provided, the two dogs that belong to the same category are now assigned to two clusters. Similar phenomena also appear in cats and cows. This non-ideal over-clustering would prevent the model from learning discriminative representations summarizing essential features of a category since the network just memorizes the data instead of learning from the data <ref type="bibr" target="#b56">[54,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Ideally, we would like to use just the right amount of negative sample pairs to ensure that the images from the same category are close to each other and that the images from different classes are far away. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (b), all the dogs, cats, and cows are clustered correctly. Note that this is achieved in an unsupervised manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Truncated Triplet Loss</head><p>Triplet loss. Inspired by relative distance comparison <ref type="bibr" target="#b59">[57]</ref>, triplet loss was proposed by <ref type="bibr" target="#b12">[12]</ref> and <ref type="bibr" target="#b38">[37]</ref> independently for person re-identification and face recognition, respectively. In a triplet-loss method, a set of triplets, i.e.,</p><formula xml:id="formula_4">{(x i , x + i , x ? i )} i=1</formula><p>,??? ,m , are first generated. In general, a query image will have far more negative samples than positive samples (see <ref type="figure" target="#fig_3">Figure 3</ref> (a) for detail). For presentation simplicity, we use only one query image and one positive sample for illustration, i.e., we have a triplet set</p><formula xml:id="formula_5">{(x, x + , x ? i )} i=1,??? ,m .</formula><p>The oldest triplet loss is defined as:</p><formula xml:id="formula_6">Loss = m i=1 max d(x, x + ) ? d(x, x ? i ), C ,</formula><p>where d is a distance metric (e.g., cosine distance or Euclidean distance). Here, C is a margin deciding whether or not to drop a triplet. This is critical in machine learning algorithms since we usually drop the simple data and focus on the hard data near the decision boundary, as support vector machine <ref type="bibr" target="#b11">[11]</ref> suggests. To improve the learning efficiency of triplet loss, in practice, we usually use the hardest triplet to represent the overall triplets, i.e., only the triplet containing the negative sample of the highest similarity score overall negative samples are used (please refer to <ref type="figure" target="#fig_3">Figure 3</ref> (b) for detail). Finally, a triplet is formally defined as:</p><formula xml:id="formula_7">Loss = max d(x, x + ) ? d(x, x ? hardest ), C . (1) Since x ? hardest is the hardest negative sample, we have d(x, x ? hardest ) ? d(x, x ? i ) for all i.</formula><p>This indicates that when the hardest triplet loss meets the condition</p><formula xml:id="formula_8">d(x, x + ) ? d(x, x ? hardest )</formula><p>, all others triplets meet the condition. Therefore, the hardest triplet loss guarantees a considerable distance between negative sample pairs. Using triplet loss, we can address the under-clustering problem.</p><p>Although triplet loss largely reduces under-clustering, it increases over-clustering risk. Specifically, since contrastive learning can be considered a classification problem that identifies each image as a class, using the hardest triplet loss can lead to over-clustering. For example, in <ref type="figure" target="#fig_3">Figure 3</ref> (a), the two dogs belong to the same object category. Unsurprisingly, their feature similarity is high. But in SSL, the actual category labels are absent; thus, these two dogs can be reluctantly considered negative sample pairs <ref type="figure" target="#fig_3">(Figure 3 (b)</ref> and (c), top). This indicates they are the hardest negative sample pairs. Using the hardest triplet loss, the distance between these two dogs is enlarged. This results in an over-clustering problem in that the two dogs from the same category are over-clustered into two different clusters.</p><p>Truncated triplet loss. To avoid over-clustering, we construct the negative pair by truncating/trimming the hardest negative samples. We select a negative sample deputy to form a truncated triplet, i.e., we have:</p><formula xml:id="formula_9">Loss = max ?d(x, x + ) ? d(x, x ? deputy ), C . (2)</formula><p>Specifically, d(x, x ? deputy ) is obtained using the following steps. First, we compute the distance {d(x, x ? i )}, ?i. Then, we sort {d(x, x ? i )} by ascending. Finally, we obtain d(x, x ? deputy ) in two ways: ? rank-k triplet loss: the k-th element are selected from performance. We use the widely-used cosine distance for d, i.e., d(x, y) = ? xy x 2 y 2 , where ? 2 represents the L2 norm. The negative sign ("-") is used here because we usually consider that a higher similarity indicates a smaller distance. We set ? to be 2. Note that, according to our experiments, the smoothed-rank-k triplet loss achieves better performance than the rank-k triplet loss.</p><formula xml:id="formula_10">{d(x, x ? i )}, yielding: d(x, x ? deputy ) = d(x, x ? rank?k ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Analysis with Bernoulli Distribution Model</head><p>In contrastive learning, views from different images are considered negative sample pairs even they are from the same actual category (e.g., the two dogs in <ref type="figure" target="#fig_3">Figure 3 (a)</ref>). Unsurprisingly, these kinds of negative sample pairs have a high feature similarity. With a high probability, they will be in the hardest triplets (see <ref type="figure" target="#fig_3">Figure 3 (b)</ref>). Using the hardest triplet loss, the distance between these false-negative pairs is enlarged. This results in an over-clustering problem that the false-negative pairs from the same category are overclustered into two different clusters (see <ref type="figure" target="#fig_3">Figure 3</ref> (c) top).</p><p>But in our truncated triplet loss, we sort {d(x, x ? i )} by ascending and select the k-th element from {d(x, x ? i )} to form d(x, x ? deputy ). An over-clustering risk exists if this rank-k negative sample and the query image belong to the same actual category. We need to estimate the probability that these two images belong to the same category. We first have a reasonable assumption: with a high probability, the image pairs from the same category have higher feature similarities than other pairs, and the distances between these pairs are smaller than other pairs. Because we have sorted {d(x, x ? i )} by ascending, the event that the rank-k negative sample and the query belong to the same category indicates an event that at least k negative samples and the query belong to the same category. The probability of this event can be computed by using a Bernoulli Distribution model, i.e.,</p><formula xml:id="formula_11">Pr = m j=k C j m p j (1 ? p) m?j .</formula><p>Here, C is used to denote the combinations, and p is used to denote the probability that a negative sample and the query belong to the same class. For example, on ImageNet, we have p = 1 1000 . In our experiment, we let m be 104 and k be m 2 . Putting m, k into the above equation, we have Pr = 6.53e ?121 , which is almost zero. Even we let m be 104 and k be 5, we have Pr = 3.03e ?94 . This indicates it is a rare event that the rank-k negative sample and the query belong to the same category. Thus, our truncated triplet loss can avoid overclustering, guaranteed by the Bernoulli Distribution model, e.g., the two dogs at the bottom of <ref type="figure" target="#fig_3">Figure 3</ref> (c) can be identified correctly. Experimental results in Section 5 also verify the effectiveness of our method 8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Main results</head><p>Our SSL training protocols are as follows. Data augmentation protocol. Our augmentations are straightforward, including randomly cropping, randomly resizing, randomly flipping horizontally, arbitrary gray scaling, stochastic color jittering, Gaussian blurring, and solarization. Please see our codes.</p><p>Other protocols. In the unsupervised learning stage, the batch size is 104 images per GPU, and we use eight GPUs. The gradient update interval is five steps. The maximum epoch is 200. The learning rate starts from 4.8 and gradually decreases with cosine annealing. The weight decay factor is 1e ?6 . The optimizer is LARS <ref type="bibr" target="#b20">[20]</ref> with a momentum 0.9. The backbone is ResNet-50, which is the same as the previous methods. The models are trained by using the 1.28M training images of ImageNet but without their annotations. The protocols are in line with <ref type="bibr" target="#b58">[56]</ref>: we use the  <ref type="bibr" target="#b19">[19]</ref> 47.0 200 DeepCluster <ref type="bibr" target="#b4">[5]</ref> 46.9 200 NPID <ref type="bibr" target="#b52">[50]</ref> 56.6 200 ODC <ref type="bibr" target="#b55">[53]</ref> 53.4 200 SimCLR <ref type="bibr" target="#b7">[7]</ref> 60.6 200 SimCLR <ref type="bibr" target="#b7">[7]</ref> 69.3 1000 MoCo <ref type="bibr" target="#b24">[24]</ref> 61.9 200 MoCo v2 <ref type="bibr" target="#b8">[8]</ref> 67.0 200 MoCo v2 <ref type="bibr" target="#b8">[8]</ref> 71.1 800 SwAV <ref type="bibr" target="#b6">[6]</ref> (single-crop) 69.1 200 SwAV <ref type="bibr" target="#b6">[6]</ref> (multi-crop) 72.7 200 BYOL <ref type="bibr" target="#b22">[22]</ref> 71.5 200 BYOL <ref type="bibr" target="#b22">[22]</ref> 72.5 300 BYOL <ref type="bibr" target="#b22">[22]</ref> 74.3 1000 SimSiam <ref type="bibr" target="#b9">[9]</ref> 68.1 100 SimSiam <ref type="bibr" target="#b9">[9]</ref> 70.0 200 SimSiam <ref type="bibr" target="#b9">[9]</ref> 70.8 400 SimSiam <ref type="bibr" target="#b9">[9]</ref> 71. same momentum network, the same multilayer-perceptron head &amp; neck as BYOL. Also, following BYOL, our loss is symmetric w.r.t the positive pairs. Using protocols similar to <ref type="bibr" target="#b58">[56]</ref> makes it feasible to present comparisons on multiple datasets/tasks without the extra hyper-parameter search. We evaluate our method by comparing it with state-ofthe-art methods in four tasks, involving linear evaluation on ImageNet, person re-identification on SYSU-30k, and object detection on both COCO 2017 and "VOC07+12".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Linear evaluation on ImageNet</head><p>Linear evaluation is the most widely adopted evaluation protocol for validating the representation ability of different SSL methods. Standardly, the backbones of ResNet-50 are trained by using the above SSL training protocols and are frozen. A linear classifier is then added to the top of the frozen representation and is trained for each method. All methods are trained using the 1.28M training images of ImageNet and are evaluated using the 50K validation images of ImageNet. For the linear classification stage, the batch size is 256. The maximum epoch is 100. There is no weight decay in linear classification training. The optimizer is SGD. Single-scale center-crop top-1 accuracy is used.</p><p>Currently, the widely-used evaluation standard of SSL merely values the accuracy but regardless of the training epochs. Following this standard, we first compare our method with the competitors without considering the training epochs. <ref type="table" target="#tab_1">Table 2</ref> shows that our method achieves a promising result on ImageNet, i.e., 75.9%, outperforming the latest state-of-the-art methods by a clear margin.</p><p>Regarding learning efficiency, existing SSL methods' efficiency is lower than supervised learning. As shown in Table 2, SSL models are trained for about 1,000 epochs, while the supervised counterpart is merely trained for 100 epochs. SimCLR <ref type="bibr" target="#b7">[7]</ref> explains that training for a longer time does not bring gain for the supervised learning model (i.e., it reported a result of 76.4% vs. 76.3% for 1000 epochs' vs. 100 epochs' supervised training). But our observation is the opposite, i.e., our reproduction of a supervised model trained for 270 epochs can achieve 78.4% top-1 accuracy, which is significantly higher than all of the SSL models. Put together, a complete comparison regarding both accuracies and training epochs is presented in <ref type="figure">Figure 1</ref> and <ref type="table" target="#tab_1">Table 2</ref>, where we have three observations. First, previous SSL methods still have a long way to go. They have significantly lower learning efficiency than supervised learning. Second, as shown in <ref type="figure">Figure 1</ref>, our approach lies in the topleft corner of the figure, indicating our method achieves the best performance among the compared SSL methods. For example, SwAV <ref type="bibr" target="#b6">[6]</ref> achieves 72.7% (200 epochs), which is lower than our method (73.6%, 180 epochs). Note that SwAV uses an additional multi-crop augmentation that we don't use, which has a 3.6% gain 9 (without multi-crop augmentation, SwAV only achieves 69.1%). For a fair comparison, we add multi-crop augmentation to our method, leading to a further state-of-the-art result of 74.1% (200 epochs). This comparison verifies the effectiveness and efficiency of our approach. Third, the smoothed truncated triplet loss achieves better performance than the truncated triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transferring to downstream tasks</head><p>Transferring to COCO 2017 object detection. One of SSL's goals is to learn transferrable features. We test our learned representation's generalization ability by transferring to COCO 2017 object detection <ref type="bibr" target="#b29">[29]</ref>, which is (one of) the largest benchmarks for general object detection, con-taining about 119K training images. Specifically, the backbones of ResNet-50 are trained by using the above SSL training protocols, and the trained network weights serve as the initialization of Mask-RCNN <ref type="bibr" target="#b25">[25]</ref> with C4. We finetune all layers on the train2017 set. The training schedule is the default 2? schedule in <ref type="bibr" target="#b21">[21]</ref>. As suggested by <ref type="bibr" target="#b24">[24]</ref>, the distribution of features obtained by unsupervised pretraining and supervised pre-training is not the same. Hence, we set a higher learning rate than supervised counterparts for finetuning. We also finetune BN instead of freezing it following <ref type="bibr" target="#b24">[24]</ref>. The accuracy is tested on the val2017 set. We report the standard metric for the detection and instance segmentation: AP Box and AP Mask . <ref type="table">Table 3</ref> shows that using our approach for pretraining surpasses other SSL ImageNet pretraining on COCO 2017 detection. Our method indeed surpasses prior arts (including MoCo / MoCo v2) on object detection on COCO 2017. Moreover, our SSL pretraining even outperforms the supervised ImageNet pretraining, implying that SSL can obtain more universal representations. This is in line with previous works that also show that SSL pretraining can outperform supervised pretraining on object detection <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b30">30,</ref><ref type="bibr" target="#b6">6]</ref>.</p><p>Transferring to VOC07+12 object detection. In addition to COCO 2017, we also evaluate our method's transferability in PASCAL VOC object detection. Following <ref type="bibr" target="#b24">[24]</ref> and <ref type="bibr" target="#b58">[56]</ref>, the backbones of ResNet-50 are trained by using the above SSL training protocols, and the trained network weights serve as the initialization of Faster R-CNN <ref type="bibr" target="#b35">[34]</ref> with C4. Then, we fine-tune all layers on the train-val07+12 set of PASCAL. The image scale is <ref type="bibr">[480,</ref><ref type="bibr">880]</ref> pixels during training and 800 in the testing. We report the default VOC metric of AP 50 and the COCO-style AP and AP 75 . The evaluation is on the VOC test2017 set.</p><p>We show the results of different methods in <ref type="table">Table 4</ref>. As shown, our approach achieves state-of-the-art performance. Note that only MoCo v2 and our approach can catch up with the supervised pretraining counterpart that performs pretraining for 100 epochs. This verifies the effectiveness of our method and implies that our SSL can obtain universal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Person re-identification on SYSU-30k</head><p>In a general sense, all the above tasks (image classification, object detection, and segmentation) belong to visual categorization since detection and segmentation can be considered categorizing regions and pixels for a given image. The effectiveness of our approach beyond visual classification remains uninvestigated. In the following, we investigate a different task, i.e., person re-identification (re-ID), which is fundamental in video surveillance <ref type="bibr" target="#b17">[17]</ref>. Re-ID refers to the problem of re-identifying individuals across cameras <ref type="bibr" target="#b46">[45]</ref>. Mathematically, re-ID is a matching problem rather than a classification problem because it requires cal-  <ref type="bibr" target="#b12">[12]</ref> 10.3 Local CNN <ref type="bibr" target="#b54">[52]</ref> 23.0 MGN <ref type="bibr" target="#b49">[47]</ref> 23.6</p><p>Weakly supervised W-Local CNN <ref type="bibr" target="#b48">[46]</ref> 28.8 W-MGN <ref type="bibr" target="#b48">[46]</ref> 29.5</p><p>Self-supervised SimCLR <ref type="bibr" target="#b7">[7]</ref> 10.9 MoCo v2 <ref type="bibr" target="#b8">[8]</ref> 11.6 BYOL <ref type="bibr" target="#b22">[22]</ref> 12.7 truncated triplet <ref type="bibr">14.8</ref> culating distance metrics between two given images. As is proved by <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b44">43,</ref><ref type="bibr" target="#b48">46,</ref><ref type="bibr" target="#b60">58,</ref><ref type="bibr" target="#b43">42]</ref>, unsupervised representation learning is critical to visual matching; therefore, validating the effectiveness of our approach in re-ID is nontrivial. Dataset and protocol. We conduct experiments on the SYSU-30k dataset <ref type="bibr" target="#b48">[46]</ref>, which is the largest database for re-ID. This database contains 29,606,918 images of 30,508 pedestrians, which is about 30 times larger than ImageNet in terms of category number. Please note that the exact label of each image is unknown in this dataset. Both the lack of precise annotation and the massive number of images make this dataset set very suitable for unsupervised learning, especially SSL. Since we are the first to perform SSL on this database, no previous work provides an evaluation protocol for this dataset. We propose a new evaluation protocol for it: the training set of SYSU-30k is employed to perform SSL. Once the model is learned, we directly use it to extract features for visual matching on the test set of SYSU-30k without any finetuning. This is even more challenging than linear evaluation on ImageNet because linear evaluation learns an extra classifier for recognition, but no extra classifier is learned here. Regarding the superiority of using SYSU-30k mentioned above, we believe SYSU-30k is a perfect database to evaluate SSL effectiveness and recommend it to future SSL researchers.</p><p>Result analysis. We compare our method with SimCLR, MoCo-v2, BYOL, and current state-of-the-art results (not SSL methods). We use ResNet-50 as the backbone. The results in <ref type="table" target="#tab_4">Table 5</ref> show that our models achieve new stateof-the-art performance, i.e., a rank-1 accuracy of 14.8%. Note that this number is low, i.e., even lower than existing transfer learning and weakly supervised learning methods. This is attributed to the challenge of the SYSU-30k test set, which contains about 480,000 testing images. Moreover, there are 478,730 mismatching images as the wrong answer in the gallery. Thus, evaluation using the SYSU-30k test set is like searching for a needle in a haystack. We encourage future SSL researchers to use this dataset to evaluate the effectiveness of SSL. We can also observe that our approach surpasses other SSL methods by a clear margin (14.8 vs. 12.7 for ours vs. BYOL). This verifies the effectiveness of our approach on visual matching tasks like re-ID. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Ablation studies</head><p>In the rest of our paper, to reduce the training time and fast access to the results, we perform ablation studies using 20 training epochs. Please note that, due to our method's high learning efficiency, training for 20 epochs is sufficient for ablation studies. The linear evaluation training is also reduced to one epoch <ref type="bibr" target="#b10">10</ref> . Actually, previous works also use few training epochs for ablation studies, e.g., <ref type="bibr" target="#b7">[7]</ref> and <ref type="bibr" target="#b40">[39]</ref>. All the training protocols are the same as Section 4, except that we take the 20th epoch's checkpoint for one epoch's evaluation. This section only reports the results of top-1 accuracy on ImageNet under the linear evaluation protocol since it is the most widely adopted metric for validating the effectiveness of SSL methods.</p><p>Effect of avoiding over-clustering. As we discussed in Section 3.2, thanks to the truncated triplet loss, we can avoid over-clustering. For example, if k = 5, the probability of over-clustering is 3.03e ?94 . If k = m 2 = 52, the probability of over-clustering is 6.53e ?121 . However, whether this analysis is correct remains unclear. In the following, we provide empirical analysis. During batch training, all the batch samplings are considered the total event A. If a batch contains at least two images belonging to the same actual category, we call it an event B. If (at least) these two images are mis-considered a false-negative pair, we call this an event ?. We report the frequency Pr(?|A) and Pr(?|B) in <ref type="table" target="#tab_5">Table 6</ref> for different training epochs and different ks.</p><p>We have three observations from <ref type="table" target="#tab_5">Table 6</ref>. First, the rank-52 and rank-5 negative samples rarely belong to the same category as the query image, i.e., Pr(?|A) and Pr(?|B) are low. Second, with the training epochs increasing, the probability that the rank-k negative sample is a false negative decreases. This is attributed to the more and more discriminative features that have been learned as the training goes. Third, with k increasing, the probability that the rank-k negative sample is false negative decreases. Especially when k = 1, our truncated triplet loss reduces to the hardest triplet loss. As shown, the hardest triplet loss indeed has a risk of over-clustering because the probability Pr(?|B) is high. With k increasing, the probability Pr(?|B) decreases. This indicates that our truncated triplet loss can avoid the over-clustering problem, guaranteed by the Bernoulli Distribution model.</p><p>Please note that if a batch contains even one false negative sample pair, we consider the whole batch has the over-clustering risk. Therefore, the probability in <ref type="table" target="#tab_5">Table 6</ref> (e.g., 0.0110 or 0.2105) is higher than that in the analysis (e.g.,3.03e ?94 or 6.53e ?121 ).</p><p>Impact of margin. As we discussed in Section 3.2, there is a margin C deciding whether or not to drop a triplet. This is critical in machine learning algorithms since we usually drop the simple data and focus on the complex data near the decision boundary, as support vector machine <ref type="bibr" target="#b11">[11]</ref> suggests. To empirically verify this hypothesis, we train our method using different margins. The results are shown in <ref type="table">Table 7</ref>. As shown, different margins lead to a performance fluctuation. When C = ?100 or C = ?1.2, the performance is best. Hence, we use C = ?100 for default in all of the experiments if no otherwise specified.</p><p>Impact of rank-k. As we analyze in Section 3.3, we can use different ks for our truncated triplet loss. When k = 1, our truncated triplet loss reduces to the traditional hardest triplet loss. When k increases, the risks of overclustering reduces exponentially. To know the impact of using different ks, we train our method using different ks. As shown in <ref type="table">Table 8</ref>, different ks lead to a performance fluctuation. When k = 5 and k = 52, the performances are satisfied. Hence, we use k = 5 or k = 52.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Although SSL has shown promising results on Ima-geNet, its learning efficiency is still low. We attribute the inherited drawback of contrastive learning to under-clustering and over-clustering. To overcome these two problems, we propose a novel SSL framework using a truncated triplet loss. We employ triplet loss containing rich negative sample information to address the under-clustering problem. We trim negative samples to prevent the over-clustering problem, guaranteed by the Bernoulli Distribution model. Our method significantly improves the learning efficiency of SSL, leading to a state-of-the-art performance in several large-scale benchmarks and varieties of downstream tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of under-clustering and over-clustering. Each sample pair connected by a yellow line represents a negative pair.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a), a cluster may contains {dog, horse} or {cat, cow}, i.e., dogs and horse are mixed up. Without the annotation, we cannot identify the actual label of each data point. In other words, the dogs and horses have an overlap. An underclustering problem occurs when insufficient positive and negative samples are present. Over-clustering. As opposed to under-clustering caused by lacking negative samples, over-clustering is caused by overwhelming negative samples. Although contrastive learning implicitly regards each image as a class, we do not expect over-clustering. Excessive negative sample pairs can result in over-clustering that forces samples from the same category into different clusters. As is shown in Figure 2 (c), if excessive negative examples are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>? smoothed-rank-k triplet loss: selecting the top-2, top-3, ..., top-(2k+1) elements from {d(x,x ? i )} and yielding: d(x, x ? deputy ) = 1 2k 2k+1 j=2 d(x, x ? rank?j ).Note that when k = 1, the rank-k triplet loss reduces to the hardest triplet loss. We can show in Section 3.3 that replacing the hardest triplet with the rank-k triplet can indeed reduce the risk of over-clustering, guaranteed by the Bernoulli Distribution model. By default, we use k = m 2 (i.e., the triplet deputy) for most of the experiments, although using other values (e.g., k = 5) also yields good Illustration of our truncated triplet loss. Each sample pair connected by a red line represents a negative sample pair. Note that although labeled by a green word "Neg", in fact, the dog is a positive sample to the query image because they belong to the same category, i.e., "a dog". Traditional triplet loss using the hardest triplet (seeFigure (b)) results in over-clustering, in which the distance between these two dogs will be enlarged. The over-clustering result is shown at (c) top, and the ideal learning results is shown at (c) bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>verify MoCo v2 indeed suffers over-clustering.</figDesc><table><row><cell>Under-clustering (Divergence)</cell><cell>BYOL 5.3711</cell><cell>Ours 7.6803</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Top-1 accuracy and training epochs of state-of-the-art methods on ImageNet using linear classification for evaluation.</figDesc><table><row><cell>Method</cell><cell>top-1 acc.</cell><cell>train epochs</cell></row><row><cell>Random</cell><cell>4.4</cell><cell>0</cell></row><row><cell>Relative-Loc [13]</cell><cell>38.8</cell><cell>200</cell></row><row><cell>Rotation-Pred</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Object detection results on COCO 2017 for Mask-RCNN. Object detection results on VOC07+12 for Faster-RCNN.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="2">AP Box</cell><cell>AP M ask</cell></row><row><cell>Random</cell><cell></cell><cell>35.6</cell><cell></cell><cell>31.4</cell></row><row><cell cols="2">Relative-Loc [13]</cell><cell>40.0</cell><cell></cell><cell>35.0</cell></row><row><cell cols="2">Rotation-Pred [19]</cell><cell>40.0</cell><cell></cell><cell>34.9</cell></row><row><cell>NPID [50]</cell><cell></cell><cell>39.4</cell><cell></cell><cell>34.5</cell></row><row><cell>MoCo [24]</cell><cell></cell><cell>40.9</cell><cell></cell><cell>35.5</cell></row><row><cell>MoCo v2 [8]</cell><cell></cell><cell>40.9</cell><cell></cell><cell>35.5</cell></row><row><cell>SimCLR [7]</cell><cell></cell><cell>39.6</cell><cell></cell><cell>34.6</cell></row><row><cell>BYOL [22]</cell><cell></cell><cell>40.3</cell><cell></cell><cell>35.1</cell></row><row><cell cols="2">truncated triplet</cell><cell>41.7</cell><cell></cell><cell>36.2</cell></row><row><cell>supervised</cell><cell></cell><cell>40.0</cell><cell></cell><cell>34.7</cell></row><row><cell>Method</cell><cell cols="2">AP50 Box</cell><cell cols="2">AP Box</cell><cell>AP75 Box</cell></row><row><cell>Random</cell><cell>59.0</cell><cell></cell><cell></cell><cell>32.8</cell><cell>31.6</cell></row><row><cell>Relative-Loc [13]</cell><cell>80.4</cell><cell></cell><cell></cell><cell>55.1</cell><cell>61.2</cell></row><row><cell>Rotation-Pred [19]</cell><cell>80.9</cell><cell></cell><cell></cell><cell>55.5</cell><cell>61.4</cell></row><row><cell>NPID [50]</cell><cell>80.0</cell><cell></cell><cell></cell><cell>54.1</cell><cell>59.5</cell></row><row><cell>MoCo [24]</cell><cell>81.4</cell><cell></cell><cell></cell><cell>56.0</cell><cell>62.2</cell></row><row><cell>MoCo v2 [8]</cell><cell>82.0</cell><cell></cell><cell></cell><cell>56.6</cell><cell>62.9</cell></row><row><cell>SimCLR [7]</cell><cell>79.4</cell><cell></cell><cell></cell><cell>51.5</cell><cell>55.6</cell></row><row><cell>BYOL [22]</cell><cell>81.0</cell><cell></cell><cell></cell><cell>51.9</cell><cell>56.5</cell></row><row><cell>truncated triplet</cell><cell>82.6</cell><cell></cell><cell></cell><cell>56.9</cell><cell>63.8</cell></row><row><cell>supervised</cell><cell>81.6</cell><cell></cell><cell></cell><cell>54.2</cell><cell>59.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison with state-of-the-art methods on SYSU-30k.</figDesc><table><row><cell>supervision</cell><cell>method</cell><cell>rank-1</cell></row><row><cell></cell><cell>DARI [44]</cell><cell>11.2</cell></row><row><cell>Transfer learning</cell><cell>DF</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Effect of avoiding over-clustering.</figDesc><table><row><cell cols="2">Training epoch</cell><cell>event</cell><cell>0</cell><cell>180</cell></row><row><cell>k = 1</cell><cell></cell><cell cols="3">Pr(?|A) 0.1538 0.9656 Pr(?|B) 0.1618 0.9948</cell></row><row><cell>k = 5</cell><cell></cell><cell cols="3">Pr(?|A) 0.1167 0.2105 Pr(?|B) 0.1230 0.2132</cell></row><row><cell>k = 52</cell><cell></cell><cell cols="3">Pr(?|A) 0.1220 0.0110 Pr(?|B) 0.1288 0.0233</cell></row><row><cell cols="4">Table 7. Impact of margin.</cell></row><row><cell>Margin</cell><cell cols="4">C = ?0.3 C = ?1.2 C = ?100</cell></row><row><cell>Top-1 accuracy</cell><cell></cell><cell>28.3</cell><cell>29.8</cell><cell>30.0</cell></row><row><cell></cell><cell cols="3">Table 8. Impact of rank-k.</cell></row><row><cell>Rank-k</cell><cell></cell><cell cols="3">rank-1 rank-5 rank-52</cell></row><row><cell cols="2">Top-1 accuracy</cell><cell>28.9</cell><cell>29.5</cell><cell>30.0</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">although the class labels are unknown.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The Bernoulli Distribution model is simple yet can be justified by the experiment (Section 5). Meanwhile,<ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b36">35]</ref> used elegant mathematical boundedness tools to show the generalization ability of contrastive learning. A more elegant justification using boundedness tools is welcome.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">Many methods (e.g., HSA [51] / MoCo v2<ref type="bibr" target="#b8">[8]</ref> / SimCLR<ref type="bibr" target="#b7">[7]</ref> / SeLa<ref type="bibr" target="#b1">[2]</ref> / DeepCluster<ref type="bibr" target="#b4">[5]</ref>) can benefit significantly from multi-crop augmentation, but BYOL<ref type="bibr" target="#b22">[22]</ref> cant't.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">After running plenty of experiments, our practical experience shows that 20 epochs of training are enough for ablation studies.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensor-Flow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Josh Levenberg</title>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Fernanda Vi?gas</publisher>
		</imprint>
	</monogr>
	<note>Oriol Vinyals. Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-labelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">Markus</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A corrective view of neural networks: Representation, memorization and learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Bresler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Nagaraj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Learning Theory</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="848" to="901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Are all negatives created equal in contrastive instance discrimination? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiffany</forename><forename type="middle">Tianhui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Frankle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<idno>abs/2010.06682</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Proceedings, Part XIV</title>
		<meeting>Part XIV<address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="139" to="156" />
		</imprint>
	</monogr>
	<note>European Conference</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020, virtual conference online</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<editor>Hugo Larochelle, Marc&apos;Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin</editor>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep feature learning with relative distance comparison for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2993" to="3003" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-12-08" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification: Clustering and finetuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenggang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michela</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loris</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6926" to="6936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks with layer-wise adaptive rate scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo?vila</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Boosting contrastive selfsupervised learning with false negative cancellation. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><surname>Khademi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia, MM &apos;14</title>
		<meeting>the ACM International Conference on Multimedia, MM &apos;14<address><addrLine>Orlando, FL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6706" to="6716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas</title>
		<meeting><address><addrLine>NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contrastive learning with hard negative samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021, virtual</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-09" />
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fifth International Conference (ICML 2008)</title>
		<meeting><address><addrLine>Helsinki, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial-temporal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peigen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="page" from="8933" to="8940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Smoothing adversarial domain attack and p-memory reconsolidation for cross-domain person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10568" to="10577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DARI: distance metric and representation integration for person verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<editor>Dale Schuurmans and Michael P. Wellman</editor>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3611" to="3617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Grammatically recognizing images with tree convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Rajesh Gupta</title>
		<imprint>
			<publisher>Yan Liu</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Aditya</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prakash</surname></persName>
		</author>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Virtual Event, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="903" to="912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weakly supervised person re-id: Differentiable graphical learning and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangrun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xujie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengtao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Neural Networks and Learning Systems (T-NNLS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
		<editor>Susanne Boll, Kyoung Mu Lee, Jiebo Luo, Wenwu Zhu, Hyeran Byun, Chang Wen Chen, Rainer Lienhart, and Tao Mei</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-batch memory for embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6387" to="6396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/tensorpack/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Unsupervised feature learning via non-parametric instancelevel discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1805.01978</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Hierarchical semantic aggregation for contrastive representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno>abs/2012.02733</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Local convolutional neural networks for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference, MM 2018</title>
		<editor>Susanne Boll, Kyoung Mu Lee, Jiebo Luo, Wenwu Zhu, Hyeran Byun, Chang Wen Chen, Rainer Lienhart, and Tao Mei</editor>
		<meeting><address><addrLine>Seoul, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1074" to="1082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yew-Soon</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6687" to="6696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openselfsup</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/OpenSelfSup" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reidentification by relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="653" to="668" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangcong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo, ICME 2018</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
