<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Egocentric Video-Language Pretraining</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">Qinghong</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Jinpeng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">Zhongcong</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difei</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongcheng</forename><surname>Tu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Zhao</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengfei</forename><surname>Cai</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongfa</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Bristol</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">King Abdullah University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Tencent Data Platform</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Show Lab</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Egocentric Video-Language Pretraining</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-text downstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person videotext datasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-text pretraining dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a large variety of human daily activities.</p><p>(ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-text contrastive learning to the egocentric domain by mining egocentricaware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the recent interest boom in computer vision and natural language processing, Video-Language Pretraining (VLP) has prevailed, which aims to learn strong and transferable video-language representation for powering a broad spectrum of video-text downstream tasks, such as video-text retrieval <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, video question answering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>, and video captioning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. The success of VLP mainly stems from the availability of large-scale open-world video-text datasets <ref type="bibr" target="#b9">[10]</ref>, which subsume a large number of videos sourced from the Web (e.g., YouTube) and pair videos with associated textual information. For instance, HowTo100M <ref type="bibr" target="#b9">[10]</ref> collects 134K hours of instructional videos accompanied by noisy narrations yielded from Automatic Speech Recognition (ASR). WebVid-2M <ref type="bibr" target="#b2">[3]</ref> scrapes 2.5M descriptive videos with well-formed long captions.</p><p>Despite reaching an impressive data scale, videos in those existing video-text pretraining datasets are often of 3rd-person views and may have been edited before posting on the Web. Yet, there is a noticeable domain gap between the existing video-text pretraining datasets and 1st-person view videos such as those videos captured by wearable cameras or smart glasses. Egocentric video has received increasing interests from the academia (e.g., activity recognition <ref type="bibr" target="#b10">[11]</ref>, activity anticipation <ref type="bibr" target="#b11">[12]</ref>, and video summarization <ref type="bibr" target="#b12">[13]</ref>) and industry (various applications in robotics and augmented reality).  <ref type="bibr" target="#b15">[16]</ref> cooking 176 14K 14K ActivityNet Captions <ref type="bibr" target="#b6">[7]</ref> action 849 100K 100K WebVid-2M <ref type="bibr" target="#b2">[3]</ref> diverse 13K 2.5M 2.5M HowTo100M <ref type="bibr" target="#b9">[10]</ref> instructional 134K 136M 136M 3rd-person view Charades-Ego <ref type="bibr" target="#b16">[17]</ref> home 34 30K 30K UT-Ego <ref type="bibr" target="#b17">[18]</ref> diverse 37 11K 11K Disneyworld <ref type="bibr" target="#b18">[19]</ref> disneyland 42 15K 15K EPIC-KITCHENS-100 <ref type="bibr" target="#b19">[20]</ref> kitchen 100 90K 90K EgoClip diverse 2.9K 3.8M 3.8M 1st-person view However, due to such a domain gap, directly transferring the existing VLP models to egocentric downstream tasks cannot fully unleash the potential of large-scale pretraining approaches, which we have confirmed in the later experimental section. To bridge this gap, we are motivated to develop Egocentric VLP models, which can greatly benefit various egocentric video downstream applications.</p><p>However, existing egocentric video datasets are of small scale and domain-specific, making Egocentric VLP prohibitive. As illustrated in Tab. 1, the formerly largest egocentric video dataset EPIC-KITCHENS-100 <ref type="bibr" target="#b13">[14]</ref> focuses on kitchens scenarios and its size is far smaller than those of the 3rd-person pretraining sets WebVid-2M <ref type="bibr" target="#b2">[3]</ref> and HowTo100M <ref type="bibr" target="#b9">[10]</ref>. Fortunately, with the recent introduction of the massive-scale egocentric video dataset Ego4D <ref type="bibr" target="#b14">[15]</ref>, it becomes possible to unlock Egocentric VLP. Ego4D consists of 3, 670 hours of videos with manually annotated narrations from 74 worldwide locations, covering a large variety of daily-life scenarios and activities.</p><p>In this work, roused by the favorable scale and diversity of Ego4D, we make a significant effort to pave the way for Egocentric VLP with the following steps: (i) To address the aforementioned issue of lacking a suitable large-scale egocentric video-language pretraining dataset, we create a video-text pretraining dataset EgoClip which contains a total of 3.8M clean 1st-person clip-text pairs selected from Ego4D and covers diverse human daily activities.</p><p>(ii) To make full use of EgoClip for video-text representation learning, we propose a novel video-text contrastive objective EgoNCE to address unique challenges in egocentric pretraining datasets.</p><p>(iii) We create a development benchmark i.e., Egocentric Multiple-Choices-Question, dubbed EgoMCQ, which contains 39K questions created from Ego4D and focuses on evaluating video-text alignment. In contrast to other downstream benchmarks, EgoMCQ has a less discrepancy from EgoClip, powering us to accurately validate and quickly iterate our designs of EgoClip and EgoNCE.</p><p>(iv) We conduct extensive experiments to demonstrate the superiority of Egocentric VLP by transferring our pretrained representation to five egocentric downstream benchmarks and achieving state-of-the-art performance: 59.4% nDCG on video-text retrieval of EPIC-KITCHENS-100 <ref type="bibr" target="#b13">[14]</ref> 1 , 32.1% mAP on action recognition of Charades-Ego <ref type="bibr" target="#b16">[17]</ref>, and significant boosts over three Ego4D challenges 2 : natural language query, moment query and object state change classification.  the development set EgoMCQ. We use EgoClip to pretrain a VLP model with the EgoNCE loss and then evaluate on EgoMCQ. According to the feedback, we iteratively refine our designs of (a) and <ref type="bibr">(b)</ref>. We then transfer the pretrained model to downstream tasks relevant to the egocentric domain. modality <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. For example, Frozen <ref type="bibr" target="#b2">[3]</ref> employs two separate transformers to encode video and text features and aligns them by video-text InfoNCE <ref type="bibr" target="#b28">[29]</ref>. In our work, we adopt the Frozen <ref type="bibr" target="#b2">[3]</ref> but extend its InfoNCE to EgoNCE via positive and negative sampling for egocentric-friendly pretraining.</p><p>Egocentric Video Datasets. Egocentric videos, collected by participants using wearable cameras, offer a natural perspective of people's daily activities and raise a range of challenging research topics <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b29">30]</ref>. Several egocentric video datasets have been developed in decades, e.g., <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31]</ref>. However, since the collection of egocentric videos is expensive, previous egocentric datasets tend to be small-scale and domain-specific. These limitations hinder 1st-person view research and fail to match the progress of 3rd-person counterparts, such as VLP <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b2">3]</ref>. Recently, a massive egocentric video dataset Ego4D <ref type="bibr" target="#b14">[15]</ref> has been released, which consists of 3, 670 hours of videos collected by 931 people from 74 worldwide locations in 9 different countries, where most videos are accompanied by narrations, audio, 3D meshes, and more. Furthermore, Ego4D introduces a suite of new challenging benchmarks (e.g., Natural language query and moment query) to fully explore the 1st-person visual experience. With this step-changing dataset and benchmarks, Ego4D would lead to a new research surge on egocentric visual perception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EgoClip: An Egocentric Video-Language Pretraining Dataset</head><p>Data curation. For our EgoClip dataset, we source data from Ego4D <ref type="bibr" target="#b14">[15]</ref>, which contains 9, 645 untrimmed videos of varying lengths from 5 sec to 7 hrs. From these videos, most are associated with dense timestamp-level narrations assigned by two different annotators, describing the camera wearer's activities and interactions with objects. For example, the narration "#C C puts the scrapper down." corresponds to video content that occurred at 3.70s, where "#C" refers to the camera-wearer. Notably, narrations in Ego4D are well-aligned with the videos, both temporally and visually. Prior pretraining datasets are characterized by a much greater level of temporal misalignment between the video and text (e.g., HowTo100M <ref type="bibr" target="#b9">[10]</ref> narrations are scraped from ASR, yielding sentences misaligned or even unrelated to video content). We first filter Ego4D videos with missing narrations (7.4% of the total video duration) and exclude videos that belong to the validation and test sets of the Ego4D benchmark challenge <ref type="bibr" target="#b14">[15]</ref> (a further 23.9% of the total video duration). Next, we retain textual annotation from both narrators in EgoClip, allowing us to consider narration diversity when pairing video and text for pretraining purposes. Finally, we adopt several criteria to filter the video and textual narrations, further reducing noise (detailed steps are provided in Supplementary B.1). Overall, this procedure yields 2.9K hours of videos with 3.85 million narrations which cover 2927 hours of video from 129 different scenarios. EgoClip has 21.9 clips per minute with an average clip length of 1.0 seconds and a standard deviation of 0.9 seconds (the longest clip is up to 60s). Additional analyses are included in the Supplementary B.3.</p><p>Creation of clip-text pairs. Clip-text pairs are the common data format for VLP, but are usually not present in untrimmed video datasets with only a weak matching between narrations captions and videos. This was first discussed in HowTo100M <ref type="bibr" target="#b9">[10]</ref>, which pairs subtitles to video clips with corresponding time intervals to produce noisy pairs. This is not suitable for Ego4D since each narration is annotated with a single timestamp rather than an interval. Thus, we design a contextual variable-length clip pairing strategy. Formally, narrations per video in Ego4D are organized as a sequence of sentences {T 0 , ? ? ? , T n } with exact timestamps {t 0 , ? ? ? , t n }, indicating an event i described by T i happened in the moment t i . For a narration T i with timestamp t i , we pair a clip V i with following start and end timepoints:</p><formula xml:id="formula_0">[t start i , t end i ] = [t i ? ? i /2?, t i + ? i /2?],<label>(1)</label></formula><p>which represents a window centered around the timestamp t i with temporal duration equal to ? i /?. ? i is an adjustable parameter equal to the average temporal distance between pairs of consecutive narrations, i.e., n?1 j=0 (t j+1 ? t j )/n. We compute ? i on a per video basis. Conversely, ? is a scale factor computed as the average of all ? i across all videos in the EgoClip (? = 4.9 seconds). Intuitively, Eq. 1 is derived from three observations: (i) Centering t i helps involve prior information about the event i; (ii) ? i measures the clip duration according to its scenario, such as longer clips watching television (352.9 seconds) v.s. shorter clips harvesting crops (0.9 seconds); (iii) ? controls the context granularity of clips (e.g., a large ? pays more attention to rapid, atomic actions). We ablate these design choices in our experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Video-Language Pretraining Model</head><p>To efficiently transfer video-language representation to egocentric downstream tasks (e.g., video-text retrieval on EPIC-KITCHENS-100 <ref type="bibr" target="#b19">[20]</ref>), We prefer the dual-encoder (discussed in Sec. 2) as our VLP model architecture. In particular, we emphasize devising a general pretraining objective EgoNCE to adapt the existing VLP model to the egocentric domain (e.g., EgoClip).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Architecture: Dual-encoder Pipeline</head><p>We choose Frozen <ref type="bibr" target="#b2">[3]</ref> as our pretraining architecture. Frozen <ref type="bibr" target="#b2">[3]</ref> design encompasses an elegant and simple dual encoder strategy (one per modality) which has favorable characteristics (e.g., indexability and efficiency <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>). Note that this allows us to use our pretrained network in single-modality tasks (e.g., video-only tasks). In practice, the video encoder adopts the TimeSformer <ref type="bibr" target="#b31">[32]</ref> architecture, while the text encoder builds upon DistillBERT <ref type="bibr" target="#b32">[33]</ref>. However, our approach is not limited to the encoder's design (e.g., the video backbone can be replaced by SlowFast <ref type="bibr" target="#b33">[34]</ref> or Video Swin <ref type="bibr" target="#b34">[35]</ref>). In the rest of the paper we adopt this notation: (V i , T i ) represents the video-text input to the model, while v i and t i are used to identify the video and text embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EgoNCE: An Egocentric-friendly Pretraining Objective</head><p>A common pretraining objective for the dual-encoder VLP is InfoNCE <ref type="bibr" target="#b28">[29]</ref>, where the matching visual-text pairs in the batch are treated as positives while all other pairwise combinations in the batch are regarded as negatives. Formally, within a batch B = {1, ? ? ? , N }, InfoNCE is computed by the sum of the video-to-text loss L v2t and text-to-video loss L t2v . For simplicity, we only formulate L v2t , whereas L t2v is defined in a symmetric way:</p><formula xml:id="formula_1">L v2t = 1 |B| i?B log exp(v T i t i /? ) j?B exp(v T i t j /? ) ,<label>(2)</label></formula><p>where the i-th video embedding v i and j-th text embedding t j are L 2 normalized features, and ? is a temperature factor.</p><p>However, this simple objective performs not well on large-scale video-text datasets like HowTo100M <ref type="bibr" target="#b9">[10]</ref> due to the serious misalignment between the two modalities of data. Therefore, <ref type="bibr" target="#b35">[36]</ref> proposes MIL-NCE which treats temporal nearest captions as positive samples.</p><p>In this work, our 1st-person human daily activity dataset, i.e. EgoClip, presents two unique challenges compared to the existing 3rd-person view video-text datasets: Challenge (i): The same action often occurs in different scenarios (e.g., "unlock the phone" could happen when "lying in bed" or "walking outdoors"). Challenge (ii): Often, different actions appearing in the same scenario tend to have indistinguishable visual differences (e.g., when "working in front of the laptop", "typing on the keyboard" or "moving the mouse" have similar feature representations).</p><p>Evaluation on the text-video retrieval task is unreliable due to duplications #C C closes the refrigerator. #C C closes the fridge #C C closes the lower part of the fridge #C C closes the refrigerator.</p><p>Retrieval result: Top clips are not GT but shall be considered as correct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>Text query: #C C closes the refrigerator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-video</head><p>Intra-video  To overcome these two unique challenges, we propose a novel EgoNCE training objective which takes into account two simple yet efficient sampling strategies based on the vanilla InfoNCE.</p><p>Action-aware Positive Sampling. In this work, we make a reasonable assumption that the critical elements in linking visual actions to textual narrations are verbs and objects mentioned in the narrations (e.g., "drinking coffee" and "opening fridge"). Following this assumption, we can devise a clever method to address challenge (i). Specifically, for each narration, we identify its nouns and verbs and merge synonym words based on the Ego4D taxonomy dictionary <ref type="bibr" target="#b14">[15]</ref>, a thesaurus recording meaningful nouns/verbs in Ego4D narrations. Then, batch samples that shared at least one noun and at least one verb are treated as positive samples. At last, for the sample i, we define its positive samples set within batch B as P i = {j ? B | noun(j) ? noun(i) = ?, verb(j) ? verb(i) = ?}.</p><p>Scene-aware Negative Sampling. To address challenge (ii), we consider different actions in the same scenario as hard negative samples. Specifically, for each video clip i, we sample an adjacent clip i ? N (i), which is close to i in time within the same video. We augment the original batch B with such hard negative samples and each sample i in B has its negative counterparts i . Hence the batch is updated as</p><formula xml:id="formula_2">B = {1, 2, ? ? ? N B , 1 , 2 , ? ? ? , N N (B) }.</formula><p>With these two sampling strategies, our new pretraining objective EgoNCE can be formulated as:</p><formula xml:id="formula_3">L ego v2t = 1 | B| i? B log k?Pi exp(v T i t k /? ) j?B exp(v T i t j /? ) + exp(v T i t j /? ) .<label>(3)</label></formula><p>Here the item in purple corresponds to our proposed action-aware positive samples and blue corresponds to our proposed scene-aware negative samples. EgoNCE provides a general extension to adapt the existing VLP models for video-text pretraining datasets in the egocentric domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EgoMCQ: A Benchmark for Egocentric VLP Development</head><p>The need for a development benchmark. We find that most egocentric benchmarks are domainspecific and focus on single-modality tasks (see Tab. 1). However, our purpose is to exploit Ego4D's diversity to learn rich video-text representations. Hence, to validate our design choices of the pretraining dataset (e.g., EgoClip), and model (e.g., EgoNCE), it is essential to measure performance on a benchmark highly aligned with the pretraining task. Therefore, we propose EgoMCQ, a new egocentric benchmark for reliable and fast developments of Egocentric VLP.</p><p>Data source. We start from the Ego4D data excluded from constructing the EgoClip, which mainly covers the validation set of the Ego4D challenge benchmarks. Additionally, to assure that the scene is not visible during pretraining, we manually remove videos that share multiple views with the videos in EgoClip. To ensure diversity, we randomly select one annotator's narration for each video. We follow the same clip pairing strategy as Eq. 1 to be consistent with the data format of EgoClip.</p><p>Benchmarking task design. To determine the task for development, we first consider video-text retrieval since it highly aligns with the VLP pretraining objective. However, as depicted in the top half of <ref type="figure" target="#fig_1">Fig. 2</ref>, for an action (e.g., close the refrigerator), there are substantial duplicates or semantically similar captions in Ego4D. This can cause issues in retrieval evaluation <ref type="bibr" target="#b36">[37]</ref> making model training unreliable. A straightforward approach to prevent this is deduplication (dedup), but it is challenging to devise a dedup criterion and perform well in the retrieval settings of a "one-to-whole validation set". Therefore, we select the Multiple-Choice Questions (MCQ) task for development since repetitions are highly unlikely given a small number of answers.</p><p>Grouping strategies. To set up the MCQ task, a naive construction randomly groups five video clips to form options for a question. But we find randomly grouping is not challenging since options are highly likely to come from different videos and vary widely in content. We redefine this basic setting as "inter-video" and ensure that the five clips originate from different videos, aiming to distinguish instances from different scenarios (the left-bottom of <ref type="figure" target="#fig_1">Fig. 2</ref>). Furthermore, we propose a more challenging setting "intra-video" by grouping five continuous clips together.This setting is regarded as a specific form of video-text localization focused on fine-grained context clues, such as hand interaction (the right-bottom of <ref type="figure" target="#fig_1">Fig. 2</ref>). Dedup is performed within five options for each question for reliable assessment (see Supp. C.1) and we adopt accuracy as the EgoMCQ metric.</p><p>Statistics. We finalize 39K questions covering 198K narrations with 468 hours of video, where the "inter-video" has 24K questions covering 290.3 hours of videos. And the "intra-video" has 15K questions and covers 178.3 hours of videos. The average duration among the five options is 34.2 seconds (More statistics of EgoMCQ are shown in Supplementary C.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We assess our Egocentric VLP along two directions: (i) We conduct an extensive analysis to explore key components of Egocentric VLP (e.g., EgoClip, EgoNCE, and EgoMCQ); (ii) we transfer our pretrained model to various downstream tasks to validate the quality of our video-text representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Benchmarks and Settings</head><p>We evaluate our VLP model on five egocentric benchmarks, spanning video-text tasks and pure video tasks, across three different datasets. We briefly describe each task below.</p><p>Multi-Instance Retrieval of EPIC-KITCHENS-100. This task is modelled as a video-text retrieval which considers the semantic overlap between different videos narrations, where multiple videos may correspond to the same narration. The training set contains 67.2K clips and validation set contains 9.7K clips. The evaluation metrics are mean Average Precision (mAP) and the normalized Discounted Cumulative Gain (nDCG).</p><p>Natural Language Query of Ego4D Challenges. The Natural Language Query task is modelled as a natural language grounding problem <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Given a language query and a video, the task aims at localizing the temporal interval within the video, in which the answer is deducible. The training set contains 11.3K queries annotated from 1K clips for this task, while the validation contains 3.9K queries collected from 0.3K clips. The evaluation metric is Recall@K for IoU=? (R@K-IoU=?) <ref type="bibr" target="#b37">[38]</ref> where ? is a threshold. We evaluate for K?{1, 5} and ??{0.3, 0.5}.</p><p>Action Recognition of Charades-Ego. This dataset has 64K instances, spanning 1st-person and 3rd-person views and covering 157 activity categories for training. We train and evaluate only on the 1st-person videos. The validation set contains 847 videos for classification and each video belongs to multiple classes. The evaluation metric is mAP.</p><p>Moment Query of Ego4D Challenges. The Moment Query task is a video-only task modelled as Temporal Action Localization <ref type="bibr" target="#b10">[11]</ref>. Given a particular high-level activity category, the task solution consists of retrieving all the possible temporal windows where the activity occurs. The training set  Implementation Details. Our codebase is based on the official Frozen 3 one and retains the same settings unless specified. During pretraining, we sample 4 frames for each clip, and use the Adam optimizer <ref type="bibr" target="#b40">[41]</ref> with a learning rate of 3?10 ?5 . To select the best method we pretrain our architecture for 10 epochs and use the best performing model on the EgoMCQ benchmark. Pretraining takes two days on 32 A100 GPUs (1, 536 GPU hrs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Studies</head><p>Ablation of the strategy used when creating EgoClip. We validate our proposed strategies, i.e., Eq.1 in Tab. 2, by comparing the following variants: (a) fixed length ?, start at timestamp; (b) fixed length ?, center at timestamp; (c) variable clip, start and end by adjacent timestamps; (d) our proposed strategy, scaled by 2; (e) our proposed strategy, scaled by 4; (f) our proposed strategy.</p><p>We consider that a good pretraining dataset creation strategy should satisfy: (1) the VLP model trained on EgoClip should be able to well distinguish instances in EgoMCQ with the same data format;</p><p>(2) the VLP model pretrained on EgoClip with the specific clip creation strategy should perform well on public downstream tasks (e.g., video-text retrieval on <ref type="bibr" target="#b19">[20]</ref> and zero-shot for efficiency).</p><p>We draw several conclusions from Tab. 2: (i) The performance of EgoMCQ is well aligned with the zero-shot result on EPIC-KITCHENS-100, especially minor gain on downstream but noticeable on EgoMCQ, which means EgoMCQ provides valid feedback and is suitable as a development set.</p><p>(ii) Under the same clip length ?, (b) surpassing <ref type="bibr">(a)</ref> proves that centering at timestamp includes prior information is helpful. (iii) Variable-length clips make a big difference, as shown in (c) and <ref type="bibr">(d)</ref>.  Notably, with our designed ? i , (d) outperforms (b) with a similar average clip length, which validates our key idea of "contextual varied clip length". (iv) Based on <ref type="bibr">(d)</ref>, <ref type="bibr">(e)</ref>, and (f), we found a proper scale factor greater than 1 is preferred, which helps focus on a large of instantaneous actions densely labeled by Ego4D <ref type="bibr" target="#b14">[15]</ref>. These ablation studies demonstrate the effectiveness of our proposed EgoClip creation strategy and EgoMCQ for development.   <ref type="bibr">(a)</ref> or nouns (b) for positive selection degrades the accuracy performance with respect to naive InfoNCE. However, we successfully push the performance beyond the baseline results when considering both verbs and nouns jointly <ref type="bibr">(c)</ref>. Moreover, we notice that merely selecting negatives within the same video leads to better performance. In particular, we obtain the best performance for temporally "hard negatives" (f). Finally, we pick the optimal settings from positive and negative sides and combine them together for (g) EgoNCE and reach the best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparisons with State-of-the-arts</head><p>Multi-Instance Retrieval. In Tab. 4, we report both zero-shot and fine-tuning evaluation results. In the zero-shot setting, pretraining with EgoClip (3.8M), despite being smaller in scale, still outperforms CC3M+WebVid-2M (5.5M) and HowTo100M (136M), validating the unique benefit of pretraining on egocentric data. When fine-tuned with 4 frames (rows 5-9), EgoClip pretraining maintains a margin over the best baseline CC3M+WebVid-2M, further verifying the viewpoint domain gap within fine-tuning. Lastly, we increase the sample frames of our finalized model as well as the best competitor CC3M+WebVid-2M pretraining to 16 (rows 10-11). As expected, performance gains accompany the frame increase. We deem that notable benefits come from better temporal modeling for frequent action in the 1st-person view. Overall, our pretraining model outperforms the best baseline (JPoSE) by 1.0 mAP and 5.9% nDCG while requiring fewer frames and input modalities.</p><p>Natural Language Query. We report validation results on Tab. 5. We adopt the same baselines as introduced in <ref type="bibr" target="#b14">[15]</ref>, namely: 2DTAN <ref type="bibr" target="#b43">[44]</ref> and VSLNet <ref type="bibr" target="#b44">[45]</ref>, and substitute the SlowFast-BERT features with our video and language representations. We observe a large boost in performance offered by our pretrained model on all metrics. Notably, we improve R@1 for IoU=0.3 from 5.45 to 10.84, despite our video branch not being pre-trained on Kinetics400. Besides, we significantly surpass VLP pretrained on CC3M+WebVid-2M and HowTo100M. We believe that this increase is due   to the egocentric data availability and the video-text interaction learned from large-scale pretraining. Please see Supplementary E.5 for the test set results.</p><p>Action Recognition. We conduct action recognition on Charades-Ego, where categories are short phrases like "Holding some clothes". Thus this task can be solved as a video-text retrieval by leveraging the text representation. We present the result in Tab. 6 under zero-shot and fine-tuning settings. In zero-shot settings, our model outperforms two supervised baselines, which validates the stronger generalization of jointly learning video-text features. After fine-tuning (rows 5-9), our model surpasses all VLP counterparts and improves over the state-of-the-art classifier Ego-Exo by 2.0% with fewer sampled frames, which shows the superior advantage of joint video-text representations.   To the best of our knowledge, this work is the pioneering work to unlock Egocentric VLP. (i) We devise a principled data curation and create EgoClip, an egocentric large-scale text-video pretraining dataset with 3.8M clip-text pairs well-chosen from Ego4D. (ii) We exploit the particular characteristics of egocentric videos and devise EgoNCE with meaningful sampling strategies for effective egocentric pretraining. (iii) We create EgoMCQ, an egocentric video-language benchmark close to the pretraining set to support efficient exploration and development of EgoClip and EgoNCE. Finally, we further demonstrate the strong representation of our egocentric pretraining on five tasks across three datasets. We believe that our EgoClip, EgoMCQ and EgoNCE would greatly benefit the egocentric video community, laying a good foundation for the new research trend of egocentric VLP.</p><p>Limitations. Our pretraining approach does not take into account the long-term temporal dependencies in long Ego4D videos. We leave this for future work. Societal impact. Egocentric VLP learns real-world perception knowledge that may contribute to practical applications such as augmented reality and robotics. However, Ego4D videos collected by participants may contain users' privacy and unintended biases, so should be used cautiously. We refer the readers to the Ego4D paper about further privacy and societal impacts. In our work, we study the video-language pretraining in a specific yet significant domain -the 1st-person view, which is motivated by the release of the Ego4D dataset. However, there is a long way to pave from the Ego4D dataset to Egocentric VLP, which consists of the pretraining dataset, development set, model designs, and transferability evaluation. Since they are not as fully explored as their third-person counterparts, thus we pioneer them by ourselves and conduct a systematic study toward the egocentric video-language pretraining -the contribution of our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgements</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pretraining dataset</head><p>Despite the merits of Ego4D, it has not been proposed for video-language pretraining, and cannot be directly used as its untrimmed videos, no direct video-text pairs, and noisy data. We thus see our clear distinction and contribution in proposing a successful approach to curate a pretraining dataset, our proposed EgoClip. Notably, It is also non-trivial to figure out what is the best way of curating Ego4D to create a pretraining dataset EgoClip, e.g., our pairing approach outperforms the naive strategy with a large margin in the development set, which requires substantial design and experimental validations. We add a Tab. 9, as an extension of Tab. 1, to clearly show their difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Ego? Domain Dur (hrs) # Clips # Texts</head><p>Ego4D <ref type="bibr" target="#b14">[15]</ref> (untrimmed) diverse 3.6K -5.0M EgoClip (well-curated from Ego4D) diverse 2.9K 3.8M 3.8M <ref type="table">Table 9</ref>: Comparison of EgoClip and Ego4D dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Development set</head><p>In the 1st-person domain, there is lacking a satisfactory benchmark that good aligns with pretraining data diversity and focuses on video-text alignment. Therefore, we propose a new development set i.e. EgoMCQ to power rapid design of video-text pretraining i.e. its pretraining dataset and model pretraining objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Model designs</head><p>We select Frozen as the baseline because its elegant and scalable dual-encoder architecture is representative in state-of-the-art VLP methods. Besides, corresponding to MIL-NCE <ref type="bibr" target="#b22">[23]</ref> built on top of the 3rd-person domain's HowTo100M <ref type="bibr" target="#b9">[10]</ref>, we aim to explore a general pretraining objective i.e., EgoNCE to learn rich video-text representations in 1st-person domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Transferability evaluation</head><p>Extensive experiments and promising results demonstrate the effectiveness and necessity of Egocentric VLP, which will greatly benefit the egocentric community. Note that Ego4D has not been used previously for any downstream tasks on other datasets. This is also where our work makes significant value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Construction details and statistics of EgoClip pretraining dataset B.1 Data filtering</head><p>After we source video-text data for EgoClip, we adopt the following criteria to further reduce noise:</p><p>(i) We select double-sized stereo videos (1.3% videos dur) and keep half per video for a normal size.</p><p>(ii) We discard videos with an aspect ratio greater than 2 (0.4% videos dur).</p><p>(iii) We filter narrations with unsure tags (4.0% texts) e.g. "#C C washes #unsure in sink".</p><p>(iv) We remove narrations less than 3 words (0.9% texts), since such narrations generally cannot be deduced from the video, e.g., "#C C speaks", "#C C looks".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Data compression</head><p>The Ego4D videos are untrimmed, which tend to be very long (average 24 mins and max to 7 hrs) and have large resolution (e.g., 1920 ? 1080, 1440 ? 1080), so it is impossible to adopt untrimmed videos as model input due to heavy data loading. Therefore we propose to compress them:</p><p>(i) We first resize all videos with short size 256.</p><p>(ii) Chunk each resized video into several segments, which are up to 10 min in length.</p><p>During pretraining, given the start and end time points of a clip, we only load the segment that this clip belongs to, rather than the whole video. To this end, we are able to perform efficient end-to-end pretraining with raw RGB videos as model input. One epoch of pretraining 3.8M video-text pairs costs 6 hrs on 32 V100 GPUs (192 GPU hrs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Data analysis</head><p>Geographic diversity. We present the distribution of EgoClip clips source in <ref type="figure" target="#fig_3">Fig. 3</ref>, which covers worldwide 13 institutions from 9 different countries <ref type="bibr" target="#b14">[15]</ref>, including: Europe (UK, Italy); Asia (India, Japan, Singapore, Kingdom of Saudi Arabia); America (USA, Colombia); Africa (Rwanda). Therefore, our created pretraining dataset inherited the good geographic as well as participants diversities of Ego4D (More details can be found in "Supp. C. Demographics" in Ego4D paper <ref type="bibr" target="#b14">[15]</ref>).   <ref type="figure">Figure 5</ref>: Scenario distribution of EgoMCQ Scenario diversity. We have statistics the scenario distribution of EgoClip in <ref type="figure">Fig. 4</ref>, which covers 129 human daily scenarios e.g., household (cooking, cleaning), outdoor (shopping, hiking), workplace (at desk, on a laptop), leisure (playing board games), etc. Notably, this distribution is long tailed, where the largest scenario "Crafting/knitting/sewing/drawing/painting" includes 622K (11.1%) and the smallest scenario "Hair and Makeup stylist" contains 35 instances.</p><p>Clip analysis. We present the statistics on the created clips in EgoClip. <ref type="figure" target="#fig_5">Fig. 6 (a)</ref> shows the distribution of clip frequency over the 2.9K pretraining set videos (For each video, we calculate two frequencies from two annotators respectively). The varying clip frequencies are mainly dependent on manual narrations that are annotated based on the video scenarios and activities. There have average 13.4 clips per minute of video, maximize to 175.8 narrations / minute and minimize to 0.06 narrations / minute. Our clip creation strategy Eq. (1) takes this characteristic into account by estimating clip length based on the frequency of the video that the clip belongs. <ref type="figure" target="#fig_5">Fig. 6 (b)</ref> displays the distribution of clip duration. The average duration is 0.98 seconds with a standard deviation of 0.95 seconds, and 69.5% of clips are less than 1.0 seconds in length, due to the massive atomic instantaneous actions densely labeled by Ego4D. Besides, the clip might be max to 65.36 seconds, which corresponding to the scenario that "a people walking in a forest".   <ref type="figure" target="#fig_5">Fig. 6 (c)</ref>, we present the distribution of narration words length. The average words length of EgoClip narration is 9.39. Notably, the EgoClip narrations cover 116 verbs and 555 nouns, where we merge the semantically synonyms words, e.g., the nouns of "handkerchief","napkin","serviette","tissue","wipe" both belong to "napkin". Each narration of EgoClip have 1.84 nouns and 0.87 verbs on average.  We further display the distribution of the top 50 most frequently verbs and nouns of EgoClip in <ref type="figure" target="#fig_6">Fig. 7</ref>. The most common nouns is "napkin", which appeared in 1.0M (27.06%) clips.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Construction details and statistics of EgoMCQ benchmark C.1 Data deduplication</head><p>To ensure repetitions do not appear in five options, we devise a deduplication strategy. Initially, we use Sentence-BERT to extract sentence-level embeddings of narrations and set a manual threshold to remove repetitions. But in this way, it is hard to control the fine-grained diversity between narrations, e.g., two narrations "#C C closes the refrigerator with his left hand." and "#C C opens the refrigerator with his left hand." only differ in one word. These two sentences have a high score in sentence-level similarity, but are entirely different in semantic meanings. We hope to keep them and let the model distinguish them, especially in our intra-video setting.</p><p>Therefore, we propose to extract the first verb and the first noun of each narration and use them to define a tag for each narration. The narrations shared with the same verb and the noun will be assigned the same tag. We also consider the words synonyms (based on Ego4D taxonomy dictionary <ref type="bibr" target="#b14">[15]</ref>). For instance, "#C C take the phone" and "#C C pick the cellphone" are semantically same in verb and noun thus will be assigned the same tag. Then the narrations shared with the same tag are treated as repetitions, we only keep one of them and sample a new one until the tags of the five options are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Multiple-views removing</head><p>We first select videos from NUS/Minnesota/Georgia Tech/Indiana sources, which contribute to the multi-camera video data. Then, based on the metadata of the video (i.e. times when videos were collected), we observed that videos collected in the same timeframe tend to be multi-views of the same recording, so we manually group these videos into the same split to ensure the same scene does not appear in another split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Data analysis</head><p>We finalize 39K questions covering 198K narrations with 468 hours of video, where the "inter-video" has 24K questions covering 290.3 hours of videos. And the "intra-video" has 15K questions and covers 178.3 hours of videos. The average duration among the five options is 34.2 seconds.</p><p>Geographic diversity. We present the geographic diversity of EgoMCQ in <ref type="figure" target="#fig_0">Fig. 10</ref>, which covers 13 institutions and is align with the geographic diversity of EgoClip. Scenario diversity. In <ref type="figure">Fig. 5</ref>, we present the scenario distribution of EgoMCQ, which covers 74 scenario. The largest scenario "Cooking" includes 49K (15.3%) clips and the smallest scenario "Bus" contains 6 instances. EgoMCQ covers 71% of scenarios in EgoClipand has other 3 scenarios not appear in EgoClip. EgoMCQ is close to EgoClip both in terms of geography and scene diversity, making it a good development set for EgoClip pretraining.</p><p>Verbs and Nouns. EgoMCQ covers 198K narrations and each narration contains 3.15 nouns and 0.97 verbs in average. In <ref type="figure" target="#fig_0">Fig. 11</ref>, we display the top 50 most frequently verb and nouns of EgoMCQ. The mostly common noun is "hand", covering 86K (36.2%) instances and the mostly frequently verb is "pick", which covers 28K (12.0%) instances.  Visualization. In <ref type="figure">Fig. 9</ref>, we display examples of both the intra and inter settings of EgoMCQ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Technical details of our VLP model</head><p>In this section, we present more technical details of our VLP model, mainly architecture and pretraining objective.</p><p>D.1 Architecture: Frozen-in-time <ref type="bibr" target="#b2">[3]</ref> Video Encoder. The video encoder is built upon Timesformer <ref type="bibr" target="#b31">[32]</ref>, a convolution-free video backbone that divides space-time attention in an efficient manner. Take a RGB video clip V i ? R T ?3?H?W with T frames and resolution H ? W as input, the clip is first divided into M ? N patches p ? R M ?N ?3?P ?P with size of P ? P , where N = HW/P 2 . Next, patches p are linearly embed as a token sequence z ? R M N ?D with D dimension. Then, the learned temporal embeddings E s ? R N ?D and spatial positional embeddings E t ? R N ?D are added to each input token. Besides, a learnable CLS token is concatenated at the beginning of the token sequence. Finally, these token sequences are fed into Timesformer and output the CLS token of the last block, which is further projected to a d dimension embedding by a linear layer to form the final clip representation</p><formula xml:id="formula_4">v i ? R d .</formula><p>Text Encoder. The text encoder is built upon DistillBERT <ref type="bibr" target="#b32">[33]</ref>, which has 40% less parameters than BERT while also preserves over 95% performance, thus is efficient. Taking a sentence T i as input, first tokenize it as a sequence of tokens and feed it into DistillBERT. Similar to the video encoder, the CLS token of DistillBERT's output is projected as t i ? R d for the final text representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Pretraining objective: EgoNCE</head><p>To supplement the Eq. 2 and Eq. 3, we first formulate the complete form of InfoNCE:</p><formula xml:id="formula_5">L = L v2t + L t2v = 1 |B| i?B log exp(v T i t i /? ) j?B exp(v T i t j /? ) + 1 |B| i?B log exp(t T i v i /? ) j?B exp(t T i v j /? )<label>(4)</label></formula><p>and our EgoNCE extends the above as Eq. 5 via two sampling strategies:</p><formula xml:id="formula_6">L ego = L ego v2t + L ego t2v = 1 | B| i? B log k?Pi exp(v T i t k /? ) j?B exp(v T i t j /? ) + exp(v T i t j /? ) + 1 | B| i? B log k?Pi exp(t T i v k /? ) j?B exp(t T i v j /? ) + exp(t T i v j /? ) .<label>(5)</label></formula><p>For positive sampling (the numerator term), we pre-extract the nouns and verbs for each narration T i before pretraining and define two word vectors w n i ? {0, 1} K1 and w v i ? {0, 1} K2 to encode the appearing nouns and verbs in sentence, where K 1 and K 2 denote the number of nouns and verbs in EgoClip (Refer to Sec B narration analysis). During pretraining, for another instance j within batch, we calculate the s ij = (w n i ) T w n j ? (w v i ) T w v j , if s ij &gt; 0, we regard instance j is one of the positive sample j ? P i of instance i. Notably, the positive sampling space P would cover B when working with the negative sampling strategy.</p><p>For negative sampling (the denominator term), each time we sample an instance i, we sample an instance i ? V i in the same video and close in time (less than 1 min) to generate the negative sample i ? N (i) of instance i. Notably, in this way, the actual instance within the batch | B| = 2N will be double the batch size |B| = N . In practice, we have to halve the batch size due to GPU memory limitations. Under halving the batch size, random sampling doesn't help in our method, which can be concluded by comparing baseline InfoNCE and variants <ref type="bibr">(d)</ref> in Tab. 3 of the main body, where the batch size of the latter is half of the former. Despite this, our proposed sampling strategy (f) can successfully improve the pretraining effect beyond baseline.</p><p>In contrast to the conventional negative sampling from the same video <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b51">52]</ref>, we specifically design our temporally adjacent negative sampling strategy to focus on the frequent appearance changes in egocentric videos, which has not been explored in previous approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional experimental details and results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Implementation details</head><p>Following the settings of official Frozen <ref type="bibr" target="#b2">[3]</ref>, the video encoder is initialized with ViT <ref type="bibr" target="#b52">[53]</ref> weights trained on ImageNet-21K with sequence dimension D = 768. The text encoder is based on huggingface's distilbert-base-uncased. The dimension of common feature space is set as 256, and the temperature parameter is set to 0.05. During pretraining, each video is resized to 224 ? 224 as input with sample frames number 4 and batch size 512. We use the Adam optimizer with a learning rate of 3 ? 10 ?5 with a total epoch of 10. When transferring to downstream tasks, we select the checkpoints with the best score on EgoMCQ benchmark i.e. average accuracy of inter-video and intra-video settings by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Downstream settings</head><p>We present the setting details of the downstream tasks we evaluated. For a fair comparison, for VLPs variants pretrained on different datasets, we use the same settings on downstream tasks, such as the fine-tuning objective.</p><p>EPIC-KITCHENS-100 Multi-Instance Retrieval. In this task, after we finalize video-text pretraining, we continue to fine-tune the VLP model and keep most settings of pretraining (e.g., input resolution, learning rate). Notably, we set the training epoch as 100 and replace the training objective as Multi-instance Maxmargin loss in Eq. 6, which is same as the baseline method JPoSE <ref type="bibr" target="#b42">[43]</ref>. The reason for this is that in this task a narration may be jointly associated with multiple clips, so multi-instance learning mechanism can better handle such a situation. And this dataset also provides the action label to calculate the correlation c ij between two clip-text pairs (i, j), which supports the implementation of Multi-instance Maxmargin loss.</p><formula xml:id="formula_7">L = (i,j,k)?? max ? + v T i t j ? v T i t k + ? + t T i v j ? t T i v k ,<label>(6)</label></formula><p>where ? = {(i, j, k) |j ? i + , k ? i ? } is a triple, which indicates a positive instance j and a negative instance k for i. In our setting, we define the positive set as i + = {j|c ij &gt; 0.1} and the negative as the remains sample within batch. The ? is a margin factor and we set it as 0.2.</p><p>Charades-Ego Action Recognition. In this task, the textual categories are short phrases like "Holding some clothes". Thus, we regard this task as a kind of video-text retrieval by leveraging the text representation and using the InfoNCE as fine-tuning objective. We set the epoch number as 10 and keep other parameters unchanged.</p><p>Ego4D Natural Language Query This task is a kind of video-text localization and is hard to perform end-to-end training (since a clip might long to 1200 seconds). The baseline method <ref type="bibr" target="#b44">[45]</ref> takes 2304 dim SlowFast features (1.87 fps, with Kinetics 400 pretrained) and 768 dim BERT features as input. Therefore, we propose to replace the baseline input features as features of pretrained VLP video and text encoders to evaluate the pretraining effectiveness. We extract the features with the same fps 1.87 and sampling frame number 4. In fine-tuning stage, we keep the default setting of <ref type="bibr" target="#b44">[45]</ref>.</p><p>Ego4D Moment Query This task is a video-only task: temporal action localization. Similar to Natural Language Query task, we replace the input Slowfast features of baseline VSGN <ref type="bibr" target="#b48">[49]</ref> with VLP video features for evaluation. The extraction details are the same as Natural Language Query.</p><p>Ego4D Object State Change Classification This is an action classification task, we sample each clip with 16 frames as input and use the cross-entropy as fine-tuning objective. The epoch is set as 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 VLP Evaluation on EgoMCQ</head><p>In Tab. 10, we display EgoMCQ evaluation result of Frozen pretrained on different video-text datasets.  As shown, pretraining with EPIC-KITCHENS-100 dataset (1st-person view, 67.2K pairs) reach comparable performance with HowTo100M pretraining (3rd-person view, 136M noisy pairs), which demonstrates the major domain gaps. Besides, Frozen with CC3M+WebVid-2M pretraining reach significant improvement on the intra-video setting, but minor in inter-video. We speculate this due to CC3M+WebVid-2M dataset covering a wide range of appearance information but still less exploration in the fine-grained action e.g. human-object interaction.  <ref type="figure" target="#fig_0">Fig. 12</ref>, we display training curves of EPIC-KITCHENS-100 video-text retrieval under different video-text pretraining, which also includes a baseline without video-text pre-training. We can found that: Variants with video-text pretraining have a faster rise in performance. Except for HowTo100M, which is similar to variant without video-text pretraining. Especially with EgoClip for egocentric pretraining, the VLP model achieves nearly convergent performance with only a small number of epochs (less than 20). With EgoNCE as pretraining objective, this positive effect is further enhanced.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 Results on test set of Natural Language Query</head><p>In Tab. 11, we found the similar conclusions in test set of Natural Language Query, pretraining with EgoClip and EgoNCE reach the optimum performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Video   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.7 Visualization</head><p>To intuitively understand the effect of egocentric pre-training, in <ref type="figure" target="#fig_0">Fig. 13</ref>, we compare the EPIC-KITCHENS-100 video-text retrieval results between our pre-training (EgoClip w/ EgoNCE) and CC3M+WebVid-2M pre-training, both fine-tuning with 16 frames. The numbers after each narration represent the correlation scores between the query and the retrieval result, with 1 being the best.  <ref type="figure" target="#fig_0">Figure 13</ref>: Visualization of EPIC-KITCHENS-100 video-text retrieval. Given the same text query, we compare the Top-5 results of 1st-person pretraining and 3rd-person pretraining.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>F e e d b a c k o f M o d e l d e s i gFigure 1 :</head><label>1</label><figDesc>Our Egocentric VLP includes: (a) the pretraining set EgoClip, (b) the VLP model, and (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Design of the Egocentric VLP development set. Top: An illustration of why the task of text-video retrieval is not suitable; Bottom: Two settings of EgoMCQ. Left-bottom: The "intervideo" setting, each question contains 5 clips from different videos. Right-bottom: The "intra-video" setting, each question contains 5 contiguous clips from the same video, making it more challenging.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>This project is supported by the National Research Foundation, Singapore under its NRFF Award NRF-NRFF13-2021-0008, and Mike Zheng Shou's Start-Up Grant from NUS. The computational work for this article was partially performed on resources of the National Supercomputing Centre, Singapore. Michael Wray and Dima Damen are supported by EPSRC UMPIRE (EP/T004991/1). Mattia Soldan and Bernard Ghanem are supported by the King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research through the Visual Computing Center (VCC) funding, as well as, the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI). Thanks to Tencent Data Platform for the support of computing resources. Our work is built upon the Ego4D dataset, and we greatly appreciate the contributions and efforts of the Ego4D community.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Institution distribution of EgoClip</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Clip and narration distribution of EgoClipNarration analysis. In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Verbs and nouns distributions of EgoClip's narrations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>( a )</head><label>a</label><figDesc>Inter-video setting (a) #C C takes off the pancake (b) #C C puts the pancake in the hotpot (c) #C C picks the flour solution (d) #C C scoops the solution with the spoon (d) #C C pours the solution in the pan (a) #C C holds the sisal fiber (b) #C C pulls the sisal fiber (c) #C C holds the plant (d) #C C ties the plants (d) #C C uproots a plant from the ground</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>( b )Figure 9 :</head><label>b9</label><figDesc>Intra-video setting Visualization of EgoMCQ under two settings. Left are the text questions; Right are the five candidate clips for each question and the text below as clip's narrations. The correct clip's narrations is highlighted in green and the wrong in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Institution distributions of EgoMCQ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Verbs and nouns distributions of EgoMCQ's narration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>E. 4 Figure 12 :</head><label>412</label><figDesc>Training Curves of EPIC-KITCHENS-100 video-text retrieval Training Curves of EPIC-KITCHENS-100 video-text retrieval In</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of our proposed EgoClip pretraining dataset against the mainstream videolanguage datasets (top) and egocentric datasets (bottom).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>[t i ?? i /2, t i +? i /2]</figDesc><table><row><cell>Clip creation strategy</cell><cell>Clip's length (s) Avg ? Std</cell><cell cols="4">EgoMCQ Acc (%) Zero-shot T?V Retrieval [20] Inter-video Intra-video mAP (avg) nDCG (avg)</cell></row><row><cell>(a) [t i , t i +?]</cell><cell>5.0 ? 0.0</cell><cell>87.66</cell><cell>39.72</cell><cell>19.6</cell><cell>12.3</cell></row><row><cell>(b) [t i ??/2, t i +?/2]</cell><cell>5.0 ? 0.0</cell><cell>89.23</cell><cell>41.68</cell><cell>20.6</cell><cell>13.7</cell></row><row><cell>(c) [t i?1 , t i+1 ]</cell><cell>10.0 ? 38.2</cell><cell>88.13</cell><cell>40.62</cell><cell>20.6</cell><cell>13.7</cell></row><row><cell cols="2">(d) 4.9 ? 4.7</cell><cell>89.74</cell><cell>44.82</cell><cell>21.1</cell><cell>14.5</cell></row><row><cell>(e) [t i ?? i /4, t i +? i /4]</cell><cell>2.4 ? 2.4</cell><cell>90.23</cell><cell>49.67</cell><cell>21.9</cell><cell>15.3</cell></row><row><cell>(f) [t i ?? i /2?, t i +? i /2?]</cell><cell>1.0 ? 0.9</cell><cell>89.36</cell><cell>51.51</cell><cell>22.1</cell><cell>15.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Results on our development set EgoMCQ and video-text retrieval on EPIC-KITCHENS-100 when using different strategies in the creation of EgoClip, where t i , ?, ? i are defined in Eq. 1.In all experiments, we bold the best results and underlined the second best results. contains 13.6K instances from 1.5K clips, while the validation set contains 4.3K instances from 0.5K clips. The evaluation metrics are mAP and R@K-IoU=? for K?{1, 5} and ??{0.3, 0.5, 0.7}.Object State Change Classification (OSCC) of Ego4D Challenges. This OSCC task is modelled as an (N+1)-way classification aiming to identify an object's state change in a given video. The training and val. sets contain 41K and 28K clips, respectively. The evaluation metric is accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>EgoNCE sampling strategy ablation. We evaluate accuracy performance on our development benchmark EgoMCQ.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Effect of EgoNCE. In this section, we evaluate the effect of the proposed sampling strategies for the EgoNCE objective (Eq. 3) on EgoMCQ and compare against a vanilla InfoNCE loss (Eq. 2). We ablate several configurations for positive and negative sampling strategies. The sampling strategy for positive pairs exploits language cues, while negative pairs rely on temporal, visual cues. Given a text-video pair, we regard other text-video pairs as positive if the textual narrations: (a) share at</figDesc><table><row><cell>Methods</cell><cell cols="2">Vis Enc Input # Frames</cell><cell>Vis-text PT</cell><cell cols="2">mAP (%) V?T T?V</cell><cell>Avg</cell><cell cols="2">nDCG (%) V?T T?V</cell><cell>Avg</cell></row><row><cell>Random</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>5.7</cell><cell>5.6</cell><cell>5.7</cell><cell>10.8</cell><cell>10.9</cell><cell>10.9.</cell></row><row><cell>MI-MM</cell><cell>S3D [42]</cell><cell>32</cell><cell>HowTo100M</cell><cell>34.8</cell><cell>23.6</cell><cell>29.2</cell><cell>47.1</cell><cell>42.4</cell><cell>44.7</cell></row><row><cell>MME [43]</cell><cell>TBN  ? [14]</cell><cell>25</cell><cell>-</cell><cell>43.0</cell><cell>34.0</cell><cell>38.5</cell><cell>50.1</cell><cell>46.9</cell><cell>48.5</cell></row><row><cell>JPoSE [43]</cell><cell>TBN  ? [14]</cell><cell>25</cell><cell>-</cell><cell>49.9</cell><cell>38.1</cell><cell>44.0</cell><cell>55.5</cell><cell>51.6</cell><cell>53.5</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>-</cell><cell>38.8</cell><cell>29.7</cell><cell>34.2</cell><cell>50.5</cell><cell>48.3</cell><cell>49.4</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>HowTo100M</cell><cell>39.2</cell><cell>30.1</cell><cell>34.7</cell><cell>50.7</cell><cell>48.7</cell><cell>49.7</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>CC3M+WebVid-2M</cell><cell>41.2</cell><cell>31.6</cell><cell>36.4</cell><cell>52.7</cell><cell>50.2</cell><cell>51.4</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>EgoClip</cell><cell>44.5</cell><cell>34.7</cell><cell>39.6</cell><cell>55.7</cell><cell>52.9</cell><cell>54.3</cell></row><row><cell>Frozen+EgoNCE</cell><cell>Raw Videos</cell><cell>4</cell><cell>EgoClip</cell><cell>45.1</cell><cell cols="3">35.3 40.2 56.2</cell><cell cols="2">53.5 54.8</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>16</cell><cell>CC3M+WebVid-2M</cell><cell>45.8</cell><cell>36.0</cell><cell>40.9</cell><cell>57.2</cell><cell>54.3</cell><cell>55.8</cell></row><row><cell>Frozen+EgoNCE</cell><cell>Raw Videos</cell><cell>16</cell><cell>EgoClip</cell><cell>49.9</cell><cell cols="3">40.1 45.0 60.9</cell><cell cols="2">57.9 59.4</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>HowTo100M</cell><cell>6.8</cell><cell>6.3</cell><cell>6.5</cell><cell>11.6</cell><cell>12.8</cell><cell>12.2</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>CC3M+WebVid-2M</cell><cell>8.6</cell><cell>7.4</cell><cell>8.0</cell><cell>14.5</cell><cell>14.6</cell><cell>14.5</cell></row><row><cell>Frozen</cell><cell>Raw Videos</cell><cell>4</cell><cell>EgoClip</cell><cell>17.9</cell><cell>13.1</cell><cell>15.5</cell><cell>23.0</cell><cell>21.2</cell><cell>22.1</cell></row><row><cell>Frozen+EgoNCE</cell><cell>Raw Videos</cell><cell>4</cell><cell>EgoClip</cell><cell>19.4</cell><cell cols="3">13.9 16.6 24.1</cell><cell cols="2">22.0 23.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Performance of the EPIC-KITCHENS-100 Multi-Instance Retrieval. Note that TBN ? feature<ref type="bibr" target="#b13">[14]</ref> is a combination of three modalities: RGB, Flow and Audio. Conversely, our approach only relies on RGB input. The grey highlighted rows correspond to zero-shot evaluation.</figDesc><table /><note>least one noun, (b) share at least one verb, and (c) share at least a verb-noun pair. Conversely, we define the following heuristics for negative sampling: (d) a random text-video pair from EgoClip, (e) a text-video pair from the same video, and (f) a text-video pair within 1 minute from the given video-text pair annotation timestamp. Tab. 3 shows that using solely verbs</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc></figDesc><table><row><cell>Methods</cell><cell>Vis Enc</cell><cell># Frames</cell><cell>Vis-Text PT</cell><cell>Train / FT Data</cell><cell>mAP (%)</cell></row><row><cell>Actor [46]</cell><cell>ResNet-152</cell><cell>25</cell><cell>-</cell><cell>Charades-Ego (1st + 3rd)</cell><cell>20.0</cell></row><row><cell>SSDA [47]</cell><cell>I3D</cell><cell>32</cell><cell>-</cell><cell>Charades-Ego (1st + 3rd)</cell><cell>23.1</cell></row><row><cell>I3D [47]</cell><cell>I3D</cell><cell>32</cell><cell>-</cell><cell>Charades-Ego (1st).</cell><cell>25.8</cell></row><row><cell>Ego-Exo [48]</cell><cell>SlowFast (ResNet-101)</cell><cell>32</cell><cell>-</cell><cell>Charades-Ego (1st)</cell><cell>30.1</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell>-</cell><cell>Charades-Ego (1st)</cell><cell>28.8</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell>HowTo100M</cell><cell>Charades-Ego (1st)</cell><cell>28.3</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell cols="2">CC3M+WebVid-2M Charades-Ego (1st)</cell><cell>30.9</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell>EgoClip</cell><cell>Charades-Ego (1st)</cell><cell>31.2</cell></row><row><cell>Frozen+EgoNCE</cell><cell>TimeSformer</cell><cell>16</cell><cell>EgoClip</cell><cell>Charades-Ego (1st)</cell><cell>32.1</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell>HowTo100M</cell><cell>-</cell><cell>9.2</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell cols="2">CC3M+WebVid-2M -</cell><cell>20.9</cell></row><row><cell>Frozen</cell><cell>TimeSformer</cell><cell>16</cell><cell>EgoClip</cell><cell>-</cell><cell>23.6</cell></row><row><cell>Frozen+EgoNCE</cell><cell>TimeSformer</cell><cell>16</cell><cell>EgoClip</cell><cell>-</cell><cell>25.0</cell></row></table><note>Recall for several IoUs on the NLQ task's val. set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Performance of the action recognition on the Charades-Ego dataset (a first-person test set). The grey highlighted rows correspond to zero-shot evaluation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>58.43 25.16 46.18 15.36 25.81 9.10 5.76 3.41 6.03 VSGN [49] Frozen HowTo100M 31.40 52.61 22.28 41.29 13.41 23.21 9.83 6.72 3.84 6.72 VSGN [49] Frozen CC3M+WebVid-2M 32.08 56.40 23.46 43.81 13.73 23.77 9.83 6.40 3.86 6.58 VSGN [49] Frozen EgoClip 40.06 63.71 29.59 48.32 17.41 26.33 15.90 10.54 6.19 10.69 VSGN [49] Frozen+EgoNCE EgoClip 40.43 65.67 30.14 51.98 19.06 29.77 16.63 11.45 6.57 11.39</figDesc><table><row><cell>Methods</cell><cell cols="2">Video Pre-extracted Features</cell><cell>IoU=0.3</cell><cell>IoU=0.5</cell><cell>IoU=0.7</cell><cell>mAP (%) @ IoU</cell></row><row><cell></cell><cell>Vis Enc</cell><cell>Vis-text PT</cell><cell cols="3">R@1 R@5 R@1 R@5 R@1 R@5</cell><cell>0.1</cell><cell>0.3</cell><cell>0.5 Avg</cell></row><row><cell cols="2">VSGN [49] SlowFast</cell><cell>-</cell><cell>33.45</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>Recall and mAP metrics for several IoUs on the Moment Query task's val. set.</figDesc><table><row><cell>Methods</cell><cell>Vis-Text PT</cell><cell>Acc. (%)</cell></row><row><cell>Always Positive</cell><cell>-</cell><cell>48.1</cell></row><row><cell>Bi-d LSTM [50]</cell><cell>ImageNet</cell><cell>65.3</cell></row><row><cell>I3D (ResNet-50) [51]</cell><cell>-</cell><cell>68.7</cell></row><row><cell>Frozen</cell><cell>-</cell><cell>70.3</cell></row><row><cell>Frozen</cell><cell>HowTo100M</cell><cell>71.7</cell></row><row><cell>Frozen</cell><cell>CC3M+WebVid-2M</cell><cell>71.5</cell></row><row><cell>Frozen</cell><cell>EgoClip</cell><cell>73.4</cell></row><row><cell>Frozen+EgoNCE</cell><cell>EgoClip</cell><cell>73.9</cell></row></table><note>Moment Query. This task investigates the quality of video-only features. We extract video features and provide them as input to the VSGN model [49]. We report the validation results in Tab. 7, We find that our features achieves the best performance over SlowFast features with an increase of 4.66% in Avg mAP. Moreover, we maintain better performance with respect to 3rd-person large-scale pretraining datasets. This demonstrates that the 1st-person VLP model also learns competitive video representations. Please see the Supplementary E.6 for the test set results.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Accuracy metric on the Object State Change Classification task's val. set.</figDesc><table><row><cell>Object State Change Classification. We report the</cell></row><row><cell>validation results on Tab. 8. Once again, our model</cell></row><row><cell>achieves the best performance of all baselines, 2.4%</cell></row><row><cell>than CC3M+WebVid-2M counterparts, which indi-</cell></row><row><cell>cates our visual representations are able to focus on</cell></row><row><cell>the fine-grained clues related to state changes.</cell></row><row><cell>Summary of EgoNCE. From the above experimen-</cell></row><row><cell>tal results, Frozen pretrained on EgoClip with the</cell></row><row><cell>EgoNCE objective brings a consistent improvement</cell></row><row><cell>over the InfoNCE on all downstream tasks, which</cell></row><row><cell>comprehensively demonstrates the effect of EgoNCE,</cell></row><row><cell>as well as the decision from EgoMCQ.</cell></row></table><note>7 Conclusion, Limitations, and Societal Impacts.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 10 :</head><label>10</label><figDesc>Results of VLPs pretrained on different datasets in EgoMCQ</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 11 :</head><label>11</label><figDesc>Recall for several IoU on the NLQ task's test set.E.6 Results on test set of Moment QueryWe further display the test set results of Moment Query in Tab. 12, pretraining with EgoClip and EgoNCE reach the best performance, 3.78% on R@1 and 4.65% on Avg mAP over the baseline.</figDesc><table><row><cell>Methods</cell><cell cols="2">Video-text Pre-extrated Features</cell><cell cols="2">IoU=0.5 mAP(%)IoU</cell></row><row><cell></cell><cell>Vis-text Enc</cell><cell>Vis-text PT</cell><cell>R@1</cell><cell>Avg</cell></row><row><cell>VSGN</cell><cell>SlowFast</cell><cell>-</cell><cell>24.25</cell><cell>5.68</cell></row><row><cell>VSGN</cell><cell>Frozen</cell><cell>HowTo100M</cell><cell>18.06</cell><cell>5.28</cell></row><row><cell>VSGN</cell><cell>Frozen</cell><cell>CC3M+WebVid-2M</cell><cell>19.74</cell><cell>5.95</cell></row><row><cell>VSGN</cell><cell>Frozen</cell><cell>EgoClip</cell><cell>27.98</cell><cell>9.78</cell></row><row><cell>VSGN</cell><cell>Frozen+EgoNCE</cell><cell>EgoClip</cell><cell>28.03</cell><cell>10.33</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 12 :</head><label>12</label><figDesc>Recall and mAP metrics on the MQ task's test set.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/m-bain/frozen-in-time</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We present the following items in the supplemental material: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1728" to="1738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1645" to="1653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densecaptioning events in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="706" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reconstruction network for video captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bairui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7622" to="7631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">When will you do what?-anticipating temporal occurrences of activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Yazan Abu Farha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5343" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A user attention model for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Fei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lie</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Jiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingjing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MM</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="533" to="542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Epic-fusion: Audiovisual temporal binding for egocentric action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5492" to="5501" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ego4d: Around the world in 3,000 hours of egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Westbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Chavis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jackson</forename><surname>Hamburger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="18995" to="19012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards automatic learning of procedures from web instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Charades-ego: A large-scale dataset of paired third and first person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.09626</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong Jae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joydeep</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1346" to="1353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alircza</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1226" to="1233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5659" to="5667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Object-aware video-language pre-training for retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3313" to="3322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Temporal localization of moments in video collections with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12763</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Thinking fast and slow: Efficient text-to-visual retrieval with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9826" to="9836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1807</biblScope>
		</imprint>
	</monogr>
	<note>Representation learning with contrastive predictive coding. arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Assistq: Affordance-centric question-driven task completion for egocentric assistant</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benita</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">You</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Weixian</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><forename type="middle">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Delving into egocentric actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="287" to="295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Is space-time attention all you need for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Video swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="3202" to="3211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On semantic similarity in video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Localizing moments in video with temporal language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">TALL: Temporal Activity Localization via Language Query</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Jiyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhenheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<meeting><address><addrLine>Nevatia, Ram</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Vlg-net: Videolanguage graph matching network for video grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattia</forename><surname>Soldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sisi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesper</forename><surname>Tegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3224" to="3234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple parts-of-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning 2d temporal adjacent networks for moment localization with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12870" to="12877" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Span-based localizing network for natural language video localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6543" to="6554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Actor and observer: Joint modeling of first and third-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7396" to="7404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised domain adaptation for action recognition from drones</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1717" to="1726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ego-exo: Transferring visual representations from third-person to first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6943" to="6953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video self-stitching graph network for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><forename type="middle">K</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13658" to="13667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bidirectional lstm networks for improved phoneme classification and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Fern?ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICANN</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="799" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Detecting moments and highlights in videos via natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11846" to="11858" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">In Fig. 8, we visualize some clip-text pairs created by our strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visualizations</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<title level="m">#C C ties the vegetable with a band</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">#C C moves the right hand</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">#C C picks the chopsticks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">#C C cuts the apple with a knife</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">#C C draws on a book</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">#C C stretches his left hand</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visualization of EgoClip clip-text pairs. We sample five frames uniformly for each clip and take its narration as its caption</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
