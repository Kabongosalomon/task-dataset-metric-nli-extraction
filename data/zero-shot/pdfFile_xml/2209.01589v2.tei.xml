<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CONSISTENT TARGETS PROVIDE BETTER SUPERVISION IN SEMI-SUPERVISED OBJECT DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-10-03">October 3, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjiang</forename><surname>Wang</surname></persName>
							<email>wangxinjiang@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Yang</surname></persName>
							<email>xyang@u.nus.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilong</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Litong</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Fang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Lyu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sensetime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sensetime Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CONSISTENT TARGETS PROVIDE BETTER SUPERVISION IN SEMI-SUPERVISED OBJECT DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-10-03">October 3, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, we dive deep into the inconsistency of pseudo targets in semi-supervised object detection (SSOD). Our core observation is that the oscillating pseudo targets undermine the training of an accurate semi-supervised detector. It not only injects noise into student training but also leads to severe overfitting on the classification task. Therefore, we propose a systematic solution, termed Consistent-Teacher , to reduce the inconsistency. First, adaptive anchor assignment (ASA) substitutes the static IoU-based strategy, which enables the student network to be resistant to noisy pseudo bounding boxes; Then we calibrate the subtask predictions by designing a 3D feature alignment module (FAM-3D). It allows each classification feature to adaptively query the optimal feature vector for the regression task at arbitrary scales and locations. Lastly, a Gaussian Mixture Model (GMM) dynamically revises the score threshold of the pseudo-bboxes, which stabilizes the number of ground-truths at an early stage and remedies the unreliable supervision signal during training. Consistent-Teacher provides strong results on a large range of SSOD evaluations. It achieves 40.0 mAP with ResNet-50 backbone given only 10% of annotated MS-COCO data, which surpasses previous baselines using pseudo labels by around 3 mAP. When trained on fully annotated MS-COCO with additional unlabeled data, the performance further increases to 47.2 mAP. Our code will be open-sourced soon.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? Mean-Teacher: Inconsistent Assignment and Drifting Pseudo Labels We compare the training losses between the Mean-Teacher and our Consistent-Teacher . In Mean-Teacher, inconsistent pseudo targets lead to overfitting on the classification branch, while regression losses become difficult to converge. In contrast, our approach sets consistent optimization objectives for the students, effectively balancing the two tasks and preventing overfitting. (Right) Snapshots for the dynamics of pseudo labels and assignment. The Green and Red bboxes refer to the ground-truth and pseudo bbox for the polar bear. Red dots are the assigned anchor boxes for the pseudo label. The heatmap indicates the dense confidence score predicted by the teacher (brighter the larger). The nearby board is finally misclassified as a polar bear in the baseline while our adaptive assignment prevents overfitting.</p><p>prediction could also affect the assignment results and the regression target dramatically. In general, inconsistent targets inject substantial noise into the student network and may even lead to severe overfitting on unlabeled images.</p><p>Common two-stage <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and single-stage <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> SSOD networks adopt static criteria for anchor assignment, e.g. IOU score or centerness. The static assignment methods are sensitive to noise in the bounding boxes (bboxes) predicted by the teacher. To illustrate this, we train a single-stage detector with standard IoU-based assignment on MS-COCO %10 data. As shown in <ref type="figure" target="#fig_0">Fig. ( 1)</ref>, a small change in the teacher's output results in strong noise in the boundaries of pseudo-bboxes, which associate the erroneous targets to an unrelated but nearby object under static IoU-based assignment. It is due to the fact that the high-responding anchors in the teacher network may not necessarily be assigned positive in the student network. Consequently, the network overfits as it produces inconsistent label to neighboring objects. The overfitting is also observed in the classification loss curve on unlabeled images 1 .</p><p>A second cause for inconsistency lies in the tuples of classification and regression labels (c i , bbox i ) in SSOD. Typically, only the classification score is used to filter the pseudo labels. However, the classification score does not necessarily reflect the quality of its bbox <ref type="bibr" target="#b5">[6]</ref>. The misalignment between c i and bbox i also accounts for the oscillation in the pseudo-bbox boundaries, which further exacerbates the inconsistency caused by static assignment in SSOD.</p><p>The hard threshold scheme in common SSOD methods also causes temporal inconsistencies in pseudo labels. Traditional SSOD methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> usually adopt a hard threshold on top of the confidence score to distinguish pseudo-bboxes for student training. However, the hard threshold, as a hyper-parameter, not only needs to be carefully tuned for each model-task combination, but should also be dynamically adjusted in accordance with model's capability at different time-steps. In the Mean-Teacher <ref type="bibr" target="#b15">[16]</ref> training paradigm, the number of pseudo-bboxes may increase from too few to too many under hard threshold scheme, which incurs inefficient and biased training for the student.</p><p>Therefore, we propose Consistent-Teacher in this study to address the inconsistency issues. First, we find that a simple replacement of the static IOU-based anchor assignment by cost-aware adaptive sample assignment (ASA) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> greatly alleviates the inconsistency in dense pseudo targets. During each training step, we calculate the matching cost between each pseudo-bbox with the student network's predictions. Only feature points with lowest costs are assigned as positive. It reduces the mismatch between the teacher's high-response features and the student's assigned positive pseudo targets, which inhibits overfitting.</p><p>Then, we calibrate the classification and regression tasks such that high teacher's classification confidence c j provides a good proxy of the tuple (c j , bbox j ) quality, which reduces the oscillation in pseudo-bbox boundaries and make consistent targets for the student network. Inspired by TOOD <ref type="bibr" target="#b18">[19]</ref>, we propose a 3-D feature alignment module (FAM-3D) that allows classification feature to sense and adopt the best feature in its neighborhood for regression. Different from the single scale searching, FAM-3D reorders the features pyrimad for regression across scales as well as locations.</p><p>In this way, a unified confidence score accurately measures the quality of classification and regression with the improved alignment module, and ultimately brings consistent pseudo-targets for the student in SSOD.</p><p>As for the temporal inconsistency in pseudo-bboxes, we apply Gaussian Mixture Model (GMM) to generate an adaptive threshold for each category at training time. We consider the confidence scores of each class as the weighted sum of positive and negative distributions and predict the parameters of each Gaussian with maximum likelihood estimation. It is expected that the model is able to adaptively infer the optimal threshold at different training steps so as to stabilize the number of positive samples.</p><p>The proposed Consistent-Teacher greatly surpasses current SSOD methods. Consistent-Teacher reaches 40.0 mAP with 10% of labeled data on MS-COCO, which is3 mAP ahead of the state-of-the-art <ref type="bibr" target="#b10">[11]</ref>. When using the 100% labels together with extra unlabeled MS-COCO data, the performance is further boosted to 47.2 mAP. The effectiveness of Consistent-Teacher is also testified on other ratios of labeled data and on other datasets as well. Concretely, the paper contributes in the following aspects.</p><p>? We provide the first in-depth investigation for the inconsistency target problem in object detection under semi-supervised situation, which incurs severe overfitting issues.</p><p>? We introduce the adaptive sample assignment to stabilize the matching between noisy pseudo-bboxes and anchors, leading to robust training objective for the student.</p><p>? We develop a 3-D feature alignment module (FAM-3D) to calibrate the classification confidence and regression quality, which helps stabilize pseudo-bbox boundaries of high confidence scores.</p><p>? We adopt GMM to flexibly determine the threshold for each class during training. The adaptive threshold evolves through time and reduces the temporal consistencies for SSOD.</p><p>? Consistent-Teacher achieves compelling improvement on a wide range of evaluations and serves as a new solid baseline for SSOD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Semi-supervised object detection (SSOD). It is a common practice for SSOD to generate pseudo bounding boxes using a teacher model and expect the student detectors to make consistent prediction on augmented input samples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. Two-stage detectors <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> have been dominant in traditional SSOD methods while single-stage detectors also shown the advantages for its simplicity and higher performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>. In this study, we adopt a single-stage framework yet a different path and focus on the inconsistency problem in SSOD. To resolve the inconsistency issues, we design the adaptive anchor assignment, feature alignment and GMM-based threshold to improve the label quality.</p><p>Label assignment in object detection. Defining positive and negative sample <ref type="bibr" target="#b23">[24]</ref> plays a substantial role in object detection. Typical Anchor-based or anchor-free detectors either adopt hard IoU thresholding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref> or the centerness prior <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> as the assigning criterion. In contrast, modern detectors have been shfting to adaptive assignment strategies. <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17]</ref> For example, PAA <ref type="bibr" target="#b38">[39]</ref> adaptively differentiates the positive anchors and negative ones by fitting the anchor scores distribution. OTA <ref type="bibr" target="#b16">[17]</ref> treats the label assignment as an optimal transport problem so that the assignment cost is minimized.</p><p>However, these assignment methods are all carried out in fully-supervised settings, whereas we find that the static assignment induces new inconsistency issues and accumulates error in SSOD. We show that a simple cost-ware adaptive assignment stabilize the label noisy and greatly benefits the SSOD task.</p><p>3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consistent-Teacher</head><p>In this section, we elaborate on how our Consistent-Teacher works to to address the SSOD inconsistencies. It is composed of three key modules, namely Adaptive Sample Assignment, 3D Feature Alignment Module and Gaussian Mixture based thresholding. The full pipeline is in <ref type="figure">Figure 2</ref>.  <ref type="figure">Figure 2</ref>: The pipeline of Consistent-Teacher . We design three modules to address the inconsistency in SSOD, where GMM dynamically determines the threshold; 3D feature alignment calibrates regression quality; Adaptive assignment assigns anchor based on matching cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Baseline SSOD Detector</head><p>We adopt a general SSOD paradigm as our baseline, namely a Mean-Teacher <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16]</ref> pipeline with a RetinaNet <ref type="bibr" target="#b27">[28]</ref> object detector. The teacher model is an exponential moving average <ref type="bibr" target="#b15">[16]</ref> of a student detector. Unlabeled images first go through weak augmentations and are fed into the teacher detector to generate pseudo bboxes. Pseudo-bboxes are then used as supervision for the student network, whose unlabeled images are strongly jittered. In the meantime, student detector take the labeled images as input to learn discriminative representation for both classification and regression. Given a labeled set D L = {x l i , y l i } N with N samples and an unlabeled set D U = {x u j } M with M samples, we maintain a teacher detector f t (?; ? t ) and a student detector f s (?; ? s ) that minimize the loss</p><formula xml:id="formula_0">L = 1 N i L cls f s (T (x l i ); ? s ), y l i + L reg f s (T (x l i ); ? s ), y l i + ? u 1 M j L cls f s (T (x u j ); ? s ),? u j + L reg f s (T (x u j ); ? s ),? u j ,<label>(1)</label></formula><p>where T and T stands for weak and strong image transformations, y = {y l = (c l , bbox l )} L l=1 is the g including L bboxes with classification label c l .? = f t (T (x); ? t ) is the pseudo-bboxes generated by the teacher model. Teacher parameter is updated as ? t ? (1 ? ?)? t + ?? s . ? u is a weighting parameter. To ensure a fair comparison, Focal Loss <ref type="bibr" target="#b27">[28]</ref> and GIoU loss <ref type="bibr" target="#b39">[40]</ref> are set for L cls and L reg for all models in this study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consistent Adaptive Sample Assignment (ASA).</head><p>Each anchor in RetinaNet is assigned as positive only if its IOU with ground truth (GT) bbox is larger than a threshold. As described in Sec. 1, the static IOU-based assignment could assign irrelevant anchors as positive under noise in pseudo-bbox boundaries . Therefore, we propose to adopt Adaptive Sample Assignment (ASA). Specifically, a matching cost between each anchor 2 and ground truths (also including pseudo bboxes) is calculated <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41]</ref>, and anchors with lowest matching costs are assigned as positive. Given an anchor i, the cost between each GT y l and the prediction p i from the anchor considers the quality of classification, regression and prior information, as seen in <ref type="figure">Fig. 2</ref>, and is calculated as</p><formula xml:id="formula_1">C ij = L cls (p i , y l ) + ? reg L reg (p i , y l ) + ? dist C dist ,<label>(2)</label></formula><p>where ? reg and ? dist are weighting parameters. C dist measures the center distance between anchor i and GT bbox y l , which acts as a small (? dist ? 0.001) center prior to stabilize training. After sorting out the matching cost for each GT y l , anchors with top K lowest costs are assigned as positive. Since the assignment is made in accordance with the model's detection quality, noise in pseudo-bboxes would have a negligible impact on feature points assignment. Therefore, the optimization target would be more consistent, as seen in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">BBox consistency via 3-D Feature Alignment Module (FAM-3D).</head><p>In common SSOD frameworks, pseudo bboxes are generated purely according to classification scores. A highconfidence prediction, however, does not always guarantee accurate bbox localization <ref type="bibr" target="#b5">[6]</ref>. It again contributes to the noise in the pseudo-bbox. Therefore, inspired by TOOD <ref type="bibr" target="#b18">[19]</ref>, we introduce a 3-D Feature Alignment Module (FAM-3D) to calibrate the bbox localization with the classification confidence. It allows each classification feature to adaptively cast about the optimal feature vector for regression task.</p><p>Assume the feature pyramid is P, we would like to construct a re-sampling function P ? s(P) to rearrange the feature map to conduct the regression task, such that P better aligns with the classification features. Different from the single-scale feature re-sampling in <ref type="bibr" target="#b18">[19]</ref>, we extend the process to multi-scale feature space, considering the fact that the optimal features for classification and regression could be at different scales <ref type="bibr" target="#b42">[42]</ref>.</p><p>Our feature alignment is realized via a sub-branch in the detection head that predicts the 3-D offset with the feature pyramid for regression. As illustrated in <ref type="figure">Fig. 2</ref>, we add one extra CONV 3?3 (RELU(CONV 1?1 )) layer at different FPN levels and estimates an offset vector d = (d 0 , d 1 , d 2 ) ? R 3 for each regression prediction. P is then re-ordered using the predicted offsets in two steps</p><formula xml:id="formula_2">P (i, j, l) ? P(i + d 0 , j + d 1 , l) (3) P (i, j, l) ? P (i , j , l + d 2 ),<label>(4)</label></formula><p>where Eq. 3 is to conduct feature offset in a 2-D space and Eq. 4 is the offset across different scales. i, j represent the planer feature coordinates while l indexes an FPN layer. In Eq. 4, i and j are the rescaled coordinates of i and j at different FPN levels. Notably, the extra CONV layers increases the computational cost slightly (? 1%), but significantly improves the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Temporal consistency using Gaussian Mixture Model (GMM)</head><p>Previous works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref> require a static hyperparameter ? for pseudo bboxes filtering. It fails to take into account that the model's prediction confidence varies across categories and iterations, which makes inconsistent target and has a profound effect on performance <ref type="bibr" target="#b14">[15]</ref>. Furthermore, tuning the threshold on different datasets is tedious.</p><p>Our goal is to find a way to automatically distinguish the positive from negative pseudo-bboxes. Specifically, we hypothesize that the score prediction s c in each category c is sampled from a Gaussian mixture (GMM) distribution P(s c ) on all unlabeled data with two modalities, positive and negative. (see the score distribution in the subfigure of <ref type="figure">Fig. 2</ref>)</p><formula xml:id="formula_3">P(s c ) = w c n N (s c |? c n , (? c n ) 2 ) + w c p N (s c |? c p , (? c p ) 2 ),<label>(5)</label></formula><p>where N (?, ? 2 ) denotes a Gaussian distribution, w c n , ? c n , (? c n ) 2 and w c p , ? c p , (? c p ) 2 represent the weight, mean and variance of negative and positive modalities, respectively. Expectation-Maximization (EM) algorithm is then used to infer the posterior P(pos|s c , ? c p , (? c p ) 2 ) which is the probability that a detection should be set as the pseudo-target for the student, and the adaptive score threshold is determined as</p><formula xml:id="formula_4">? c = argmax s c P(pos|s c , ? c p , (? c p ) 2 )<label>(6)</label></formula><p>In practice, we maintain a prediction queue of size N (N ? 200) for each class to fit GMM. Considering that the score distribution from a single-stage detector is strongly imbalanced as the majority of prediction is negative, only the top K = k (s k ) number of predictions are stored in a queue. The EM algorithm only accounts for ? 7% training time increase. The threshold can then be adaptively determined w.r.t. the model's performance at different training stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we first evaluate our solution on a series of SSOD benchmarks, and then validate the effectiveness of each components through extensive ablation studies. Datasets and Evaluation Setup. we conduct comprehensive experiment on the MS-COCO 2017 <ref type="bibr" target="#b43">[43]</ref> benchmark and PASCAL VOC datasets <ref type="bibr" target="#b44">[44]</ref>.</p><p>We include three evaluation protocols: (1) COCO-PARTIAL: We randomly sample 1%/2%/5%/10% of the images in train2017 as labeled data and treat the rest as unlabeled data. We report the AP 50:95 3 results on the val2017 as the evaluation metrics. (2) COCO-ADDITION: We use the full train2017 as labeled set and include the official unlabeled set unlabel2017 as unlabeled set. The trained models are evaluated on val2017. (3) VOC-PARTIAL: We utilize the VOC2007 trainval set as the labeled data and make use of the VOC2012 trainval as our unlabeled data. The final model is verified on VOC2007 test set using both AP 50 and AP 50:95 following <ref type="bibr" target="#b2">[3]</ref>. In addition, some of the model improvements are also evaluated on the standard fully-supervised COCO-1x training <ref type="bibr" target="#b27">[28]</ref> to compare the relative benefits on semiand fully-supervised regimes. Implementation Details. To ensure a fair comparison, all detectors are trained on 8 GPUs with 5 images per GPU (1 labeled and 4 unlabeled images) similar to <ref type="bibr" target="#b5">[6]</ref>. The detectors are optimized using SGD with a constant learning rate of 0.01, momentum of 0.9 and weight decay of 0.0001. The unlabeled data weight is ? U = 2. No learning rate decay is applied. In COCO-PARTIAL and VOC-PARTIAL evaluation, we train the detectors for 180K iterations, whereas we increase the training time on COCO-ADDITION to 720K for better convergence. The teacher model is updated through EMA with a momentum of 0.9995. We follow the same data prepossessing and augmentation pipeline in <ref type="bibr" target="#b5">[6]</ref>. We adopt RetinaNet <ref type="bibr" target="#b27">[28]</ref> with ResNet-50 <ref type="bibr" target="#b26">[27]</ref> backbone as our baseline. ImageNet <ref type="bibr" target="#b45">[45]</ref>-pretrained model is used as initialization.</p><p>We compare our Consistent-Teacher with numerous prevailing SSOD approaches including CSD <ref type="bibr" target="#b3">[4]</ref>, STAC <ref type="bibr" target="#b2">[3]</ref>, Instant Teaching <ref type="bibr" target="#b6">[7]</ref>, Humble Teacher <ref type="bibr" target="#b22">[23]</ref>, Unbiased Teacher v1 and v2 <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9]</ref>, Soft Teacher <ref type="bibr" target="#b5">[6]</ref>, ACRST <ref type="bibr" target="#b46">[46]</ref>, DSL <ref type="bibr" target="#b14">[15]</ref>, S4OD <ref type="bibr" target="#b13">[14]</ref>, Dense Teacher <ref type="bibr" target="#b10">[11]</ref> and PseCo <ref type="bibr" target="#b9">[10]</ref>. In addition, we implement a baseline method where students are trained using labeled and pseudo-labeled data, and the teacher is updated through EMA of student. We name it the Mean-Teacher baseline <ref type="bibr" target="#b15">[16]</ref>. The default confidence threshold is set as 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Troubleshooting the Inconsistency Problems in SSOD</head><p>In the first step, we provide a thorough analysis to justify inconsistencies in SSOD, and how our solution addresses them.</p><p>Inconsistency Leads to Noisy Labels. We plot the mAP of the pseudo bboxes against the GT targets on unlabeled data in <ref type="figure" target="#fig_3">Figure 3</ref>(Left axis). It stands for the quality for the labels. In addition, the inconsistency is measured, which is an accumulation of the mismatch between the pseudo-bboxes of two consecutive teacher checkpoints (Right axis). Please refer to the Appendix 1.1 for the full formulation.</p><p>According to <ref type="figure" target="#fig_3">Figure 3</ref>(Right axis), while the Mean-Teacher suffers from unfavorable large inconsistencies during training, Consistent-Teacher significantly reduces the target discrepancy at different time steps. Consequently, our model enjoys continuous improvement overtime, therefore provides high quality labels for its student, as shown in <ref type="figure" target="#fig_3">Figure 3</ref>(Left axis).  Inconsistency Leads to Dynamic Definition of Targets. <ref type="figure">Figure 4</ref> plots the number of pseudo GTs per image on the unlabeled data using different thresholding schedules. Notably, it reveals a critical problems that, with static confidence thresholds ? = 0.4, 0.5, 0.6, the number of pseudo label keeps going up as detector becomes more confident. GMM-based approach, on the other hand, adaptively adjust the best threshold according to the model capacity, with a nearly constant number of GTs, which reduces temporal inconsistency.</p><p>In <ref type="figure" target="#fig_5">Figure 5</ref>, we plot the estimated threshold curve obtained by GMM on COCO 1%/5%/10%. The value steadily increases as training proceeds. Further more, with less labeled samples, GMM sets higher confident threshold in accordance with more overfitting issues. Typical static threshold setting is incapable to address the inconsistency in learning targets, while GMM provides a gratifying solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inconsistency Introduces Classification-Regression</head><p>Misalignment. It is a well-known problem in object detection that, the classification score may not fully reflect the regression quality <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>. It deters the essence of SSOD since we rely heavily on the prediction score to filter labels. <ref type="figure">Figure 6</ref> visualizes the confidence-IOU heatmap of all predicted bounding boxes on the COCO val2017. For each predicted bbox, we plot the confidence of the maximum category and its maximum IOU with the GT boxes in the corresponding class. As highlighted in the red squares, Mean-Teacher predicts low-confidence but high-IOU bboxes. On the other hand, our model generates predictions that are concentrated in high-confidence, high-IOU region. Consistent-Teacher gives rise to more calibrated predictions.   <ref type="figure">Figure 6</ref>: Heatmap of predicted bboxes confidence and its IOU score with GTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-supervised Object Detection</head><p>In this section, we compare our method with previous state-of-the-arts under COCO-PARTIAL, VOC-PARTIAL and COCO-ADDITION evaluation protocol.</p><p>COCO-PARTIAL Results. <ref type="table">Table 1</ref> systematically compares mAP of all aforementioned semi-supervised detectors trained with COCO 1%/2%/5%/10% labels. We first note that the simple Mean Teacher baseline with RetinaNet detector constitutes a strong method for SSOD. It achieves an mAP of 35.5 on COCO 10% experiments without sophisticated data re-weighting strategy or pseudo labeling selection methods. More surprisingly, Consistent-Teacher achieves remarkable progress over current methods on 2%/5%/10% experiments. It scores 36.1 and 40.0 mAP on COCO 5%/10% data, largely surpassing the best-performed model Dense Teacher by ? 3.1 and ? 3 mAP. <ref type="table">Table 1</ref>: COCO-PARTIAL comparison with other semi-supervised detector on val2017. The results for two-stage (upper half) and single-stage (lower half) detectors are listed separately. We also report the Faster-RCNN and RetinaNet performance trained on labeled data only. All models adopt ResNet50 with FPN as backbone. We highlight the previous best record with underline.  <ref type="table" target="#tab_2">Table 3</ref>. Again, we notice that our Consistent-Teacher makes outstanding improvements over its counterparts. Our method shows an improvement of 2.2 absolute mAP compared with the latest state-of-the-art <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>COCO-addition Results. Now we would like to push our model to its limits by taking the full COCO train train2017 as labeled data and additional unlabel2017 as unlabeled data. As shown in <ref type="table" target="#tab_1">Table 2</ref>, in the case of COCO-ADDITION, our model achieves 47.20 mAP, surpassing all previous state-of-the-arts.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this section, we validate the effectiveness of our 3 major designs on the MS-COCO dataset.</p><p>Adaptive Sample Assignment. We first examine the effect of ASA strategy. To enable a fair comparison between all assigners, we utilize the Mean Teacher with fixed confidence threshold 0.4 and unlabeled weight of 2 as our baseline and replace its IOU-based assignment with our proposed ASA. Since the adaptive assignment is also applicable to the supervised scenario, we further experiment on the supervised MS-COCO with the standard 1? (12 epochs) training setting. It is notable that, as shown in <ref type="table" target="#tab_3">Table 4</ref>, robust sample assignment plays a pivotal role in SSOD. By specializing the assignment policy on semi-supervised tasks, our ASA achieves 38.50 mAP on COCO 10%, with an improvement of 3 mAP compared with the heuristic matching cost using IOU. Another finding is that, the performance benefits from ASA is almost doubled on SSOD (3.0 mAP) than on the fully supervised setting (1.7 mAP). It suggests our proposed ASA is particularly beneficial in the evaluation of the SSOD tasks, as also seen in <ref type="figure" target="#fig_0">Fig. 1</ref> of its ability to suppress the confirmation bias in SSOD.</p><p>3D Feature Alignment Module. To testify the effectiveness of FAM, we first replace the FAM-3D as a 2-D counterpart, which is adopted in <ref type="bibr" target="#b18">[19]</ref>. <ref type="table" target="#tab_4">Table 5</ref> provides the ablative study of our method with different FAM structure. We observe that the FAM-3D surpasses the setting without feature alignment by 1.0 mAP and FAM-2D by 0.4 mAP on COCO 10% evaluation, with negligible parameters and FLOPs. It is shown that, by automatically estimating the best 3D feature location for classification and regression, the semi-supervised detector are better calibrated to identify high quality pseudo-labels.</p><p>GMM. We testify the detector performance with or without the GMM-based pseudo-labelling. We replace it with a hard confidence threshold ? ? (0.2, 0.3, 0.4, 0.5, 0.6, 0.9). <ref type="figure">Figure 7</ref> illustrates the test mAP on val2017. Notice that the detetor is highly sensitive to confidence threshold, with the optimal constant threshold at 0.4. By fitting the distribution of confidence, GMM dynamically adjusts the threshold for selecting pseudo-labels. This allows our model to gain more accuracy and stable supervision signal than a fixed threshold, achieving the final performance of 40.00 mAP with 0.5 mAP improvement on 10% labeled data. GMM is also higher than the model using hard threshold (0.4) at different ratios of labeled data as well, as illustrated in <ref type="figure">Figure 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future work</head><p>In this paper, we systematically investigate the inconsistency problems in semi-supervised object detection, where the pseudo boxes may be highly inaccurate and vary greatly at different stages of training. To alleviate the aforementioned problem, we present a simple yet effective semi-supervised object detector termed Consistent-Teacher . We introduce adaptive anchor assignment which selects the positive anchor with lowest matching costs and FAM which regress the 3-D feature pyramid offsets that aligns classification and regression tasks. To solve the temporal inconsistency in pseudo-bboxes, we leverage GMM to dynamically adjust the threshold for self-training. Through integration of three designs, our Consistent-Teacher is able to simultaneously obtain a robust anchor assignment with consistent pseudo-bboxes, outperforming the state-of-the-art methods by a large margin on a series of SSOD benchmarks.</p><p>In this supplementary material, we provide additional experimental quantitative results, model size comparison, as well as bounding boxes visualization to further support the effectiveness of our proposed Consistent-Teacher . In addition, we delineate more experimental details, implementation information, and hyper-parameter settings of our method. Our code is also attached for your reference.</p><p>1 More details in Consistent-Teacher 1.1 Inconsistency measurement.</p><p>Inconsistency refers to the fact that the pseudo boxes may be highly inaccurate and vary greatly at different stages of training. Therefore, we measure the pseudo-bboxes variation across different training steps. Specifically, we store the checkpoints every 4000 training steps. We then run inference using these checkpoints on a subset with 5000 images from the unlabeled set. The prediction output from the previous checkpoint is then set as GT and we evaluate the mAP of the current checkpoint with the previous predictions. Therefore, a higher mAP implies a more consistent pseudo targets. Then the inconsistency is measured by accumulating 1 ? mAP for these checkpoints to reflect the accumulated effect of noisy targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Verify the Inconsistency in SSOD</head><p>Assignment Inconsistency under Noisy Pseudo Labels. To illustrate that the conventional IOU-based or heuristic label assignment is problematic in SSOD, we intentionally inject random noise to the ground-truth bounding boxes and testify the assignment consistency by quantifying the assignment IOU (A-IOU) of clean and noisy assignments. Suppose a bounding box b = (x 1 , y 1 , x 2 , y 2 ) is assigned to a set of k anchors A = {a 1 , . . . , a k }. We add Gaussian noise to its coordinate with a noise ratio ?, so that b = (</p><formula xml:id="formula_5">x 1 + x1 ? w, y 1 + y1 ? h, x 2 + x2 ? w, y 2 + y2 ? h)</formula><p>, in which w and h are width and height of the box. x1 , y1 , x2 , y2 are sampled from a normal distribution N (0, ?). The perturbed box b is matched to a new set of l anchors A = {a 1 , . . . , a l }. The A-IOU is computed as the intersection-of-union between A and A . The higher A-IOU score suggests the assignment is more robust to label noise.</p><p>We testify the assignment consistency under two scenario. First, we calculate the assignment IOU with different degrees of noise ratio ? ? {0.1, 0.2, . . . , 0.5} using the final model. Second, we would like to investigate how the assignment consistency change through training. We report the A-IOU at different time of training with a constant ? = 0.1. We compare our ASA with IOU-based assigner <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> and ATSS assigner <ref type="bibr" target="#b23">[24]</ref> with Mean Teacher RetinaNet baseline on COCO 10%. All modules except for the assignment are kept the same to provide a fair comparison. For both evaluations, we randomly select 1000 images from val2017 to compute the A-IOU. <ref type="figure" target="#fig_8">Figure 9</ref> visualize the mean?std A-IOU between clean and noisy label at different training time and different noise ratio ?. In <ref type="figure" target="#fig_8">Figure 9</ref>(a), both ATSS and our ASA provides higher A-IOU compared with the broadly applied IOU-based assignment. However, ATSS is still based on heuristic matching rule between label and anchor boxes. ASA, instead, steadily improves itself as the detector becomes more accurate. In <ref type="figure" target="#fig_8">Figure 9</ref>(b), we see that IOU-based assignment fails to maintain the initial assignment when the large magnitude of noise is introduced in the labels. Given the noisy nature of pseudo label in SSOD, our experiment suggests that IOU-based assignment is incapable of maintaining the assignment consistency in SSOD. In contrast, our ASA strategy still performs well under server noise scenario. This experiment supports our argument that the proposed consistent assignment strategy is robust to label noise in SSOD.</p><p>[b] Classification and Regression Inconsistency. We unveil the regression and classification mismatch problem in SSOD by identifying the mismatch between the high-score and high-IOU predictions. We obtain the confidence-IOU pairs on val2017 using Consistent-Teacher and Mean Teacher RetinaNet when trained on COCO 10% data, and analyze the correlation between the two variables. We apply linear regression and measures the standard error to reflect the correlation between confidences and IOUs. Smaller error indicates higher correlation. <ref type="table" target="#tab_5">Table 6</ref> provides the LR standard error for Consistent-Teacher and Mean Teacher RetinaNet. The right scatter figure displays the confidence-IOU of Mean Teacher. We observe clear cls-reg misalignment on semi-supervised detectors: numerous low-confident predictions possess high IOU score. It indicates that classification confidence does not provides a strong enough clue for an accurate regression result, which give rise to erroneous pseudo-label noise during training. The high LR error of 0.109 with Mean teacher also demonstrates this point. On the contrary, our Consistent-Teacher largely elinimates the mismatch between the two tasks with a lower LR error of 0.080. It supports our arguments that Consistent-Teacher can align the classification and regression sub-tasks and reduce the mismatch in SSOD.</p><p>3 Semi-supervised detection results visualization <ref type="bibr" target="#b2">3</ref>.0.1 Qualitative comparison with Baseline.</p><p>We further compare the baseline Mean Teacher RetinaNet with our Consistent-Teacher by visualizing the predicted bounding boxes on val2017 under the COCO 10% protocol. In <ref type="figure" target="#fig_0">Figure 10</ref>, we plot the predicted and ground-truth bounding boxes in Violet and Orange respectively, alongside with the false positive bboxes highlighted in Red.</p><p>There are 3 general properties that we could observed in our demonstration.</p><p>1. First, Consistent-Teacher fits the situation of crowded object localization better, whereas Mean Teacher often mistakes the intersection of two overlapped objects as a new instance. For example, in the scenes of zebras or sheep, Mean Teacher often gives a false positive output in the overlapping area of the two objects, while Consistent-Teacher largely resolves the inaccurate positioning problem through the adaptive anchor selection mechanism.</p><p>2. Secondly, we see that under the semi-supervised setting, the Mean Teacher RetinaNet would either predict the wrong class for the correct location or regress an inaccurate bounding box despite its high classification confidence. For example, birds are sometimes misidentified as airplanes even when the localization is accurate. It is mainly attributed to the inconsistency of classification and regression tasks, i.e. the features required for regression may not be optimal for classification. Consistent-Teacher effectively discriminates similar categories using the FAM-3D to select the features dynamically.</p><p>3. Third, Consistent-Teacher embraces higher recall since it is capable of detecting small or crowded instances which Mean Teacher fails to point out. For example, Consistent-Teacher discovers most of the hot dogs on the grill while Mean Teacher neglects most of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.0.2">Good cases and Failure cases.</head><p>We provides more examples to showcase the good and failure examples produced by Consistent-Teacher on COCO val2017 in <ref type="figure" target="#fig_0">Figure 11</ref> and <ref type="figure" target="#fig_0">Figure 12</ref>. Although our proposed method achieved gratifying performance on a series of SSOD benchmarks, we can still point out its deficiencies in <ref type="figure" target="#fig_0">Figure 12</ref>. First, the trained detector lacks robustness to     some out-of-distribution samples, for example, cartoon characters on street signs are recognized as real people, and reflections in mirrors are recognized as objects. Second, our detection performance is poor for some classes with small sizes, such as toothbrushes, hair dryers, etc. Third, Consistent-Teacher also tends to treat parts of the object as a whole, such as the head of the giant panda as a separate animal (in the lower left corner), and the dial of a clock as the entire clock (on the right of the panda).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Data Augmentations.</head><p>We use the same data augmentations as described in Soft Teacher <ref type="bibr" target="#b5">[6]</ref>, including a labeled data augmentation in <ref type="table" target="#tab_8">Table 7</ref>, a weak unlabeled augmentation in <ref type="table" target="#tab_9">Table 8</ref> and a strong unlabeled augmentation in <ref type="table">Table 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We implement our Consistent-Teacher based on MMdetection 4 framework with the data prepossessing code from the open-sourced Soft-Teacher 5 and google ssl-detection <ref type="bibr" target="#b5">6</ref> . We train our detectors on 8 NVIDIA Tesla V100 GPUs. It takes approximately 3 days for an 180K training. Each GPU contains 1 labeled image a and 4 unlabeled images. The source code is attached in a separate zip file. Randomly horizontally flip a image with probability of p. p = 0.5 OneOf Select one of the transformation in a transformation set T . T = TransAppearance Randomly horizontally flip a image with probability of p. p = 0.5 <ref type="table">Table 9</ref>: Strong data augmentation for unlabeled image.  Invert all pixels above a threshold value T . T ? U (0, 1) RandColor</p><p>Adjust the color balance of image. C = 0 returns a black&amp;white image, C = 1 returns the original image.</p><p>C ? U (0.05, 0.95)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandContrast</head><p>Adjust the contrast of image. C = 0 returns a solid grey image, C = 1 returns the original image.</p><p>C ? U (0.05, 0.95)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandBrightness</head><p>Adjust the brightness of image. C = 0 returns a black image, C = 1 returns the original image.</p><p>C ? U (0.05, 0.95)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandSharpness</head><p>Adjust the sharpness of image. C = 0 returns a blurred image, C = 1 returns the original image.</p><p>C ? U (0.05, 0.95)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RandPolarize</head><p>Reduce each pixel to C bits. C ? U (4, 8) </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of inconsistency problem in SSOD on COCO 10% evaluation. (Left)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Consistent-Teacher improves the training consistency in SSOD. (Left axis) mAP on the unlabeled set at different time. (Right axis) The inconsistency of pseudo labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 GMMFigure 4 :</head><label>64</label><figDesc>Number of pseudo labels/image with threshold schedules on COCO 10%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>GMM threshold dynamics along training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Ablative study of GMM-based pseudo-label filtering. Each value represents the mAP score on COCO 10% data. Ablation of GMM at different labeled data ratio on COCO. Models are compared to baselines with a hard threshold 0.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Anchor assignment IOU dynamics along training (b) Anchor assignment IOU with different noise ratio</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Assignment IOU score between ground-truth and the noisy bounding boxes (a) at different time of training and (b) using different noise ratio.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative comparison on the COCO%10 evaluation. The bounding boxes in Orange is the ground-truth, and Violet refers to the prediction. Red highlights the false positive predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Good detection results for the COCO%10 evaluation. The bounding boxes in Orange is the ground-truth, and Violet refers to the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 12 :</head><label>12</label><figDesc>Failure detection results for the COCO%10 evaluation. The bounding boxes in Orange is the ground-truth, and Violet refers to the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>to a the height of h randomly sampled from h ? U (hmin, hmax), while keeping the height-width ratio unchanged. hmin = 400, hmax = 1200 in MS-COCO hmin = 480, hmax = 800 in PASCAL-VOC RandomFlip Randomly horizontally flip a image with probability of p. p = 0.5 OneOf Select one of the transformation in a transformation set T . T = TransAppearance OneOf Select one of the transformation in a transformation set T . T = TransGeo RandErase Randomly selects K rectangle region of size ?h ? ?w in an image and erases its pixels with random values, where (h, w) are height and width of the original image. K ? U (1, 5) ? ? U (0, 0.2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>In addition to the COCO evaluations, we compare our proposed model against other SSOD approaches on VOC0712 datasets in</figDesc><table><row><cell>Method</cell><cell cols="4">1% COCO 2% COCO 5% COCO 10% COCO</cell></row><row><cell>Labeled Only</cell><cell>9.05</cell><cell>12.70</cell><cell>18.47</cell><cell>23.86</cell></row><row><cell>CSD</cell><cell>10.51</cell><cell>13.93</cell><cell>18.63</cell><cell>22.46</cell></row><row><cell>STAC</cell><cell>13.97</cell><cell>18.25</cell><cell>24.38</cell><cell>28.64</cell></row><row><cell>Instant Teaching</cell><cell>18.05</cell><cell>22.45</cell><cell>26.75</cell><cell>30.40</cell></row><row><cell>Humble teacher</cell><cell>16.96</cell><cell>21.72</cell><cell>27.70</cell><cell>31.61</cell></row><row><cell>Unbiased Teacher</cell><cell>20.75</cell><cell>24.30</cell><cell>28.27</cell><cell>31.50</cell></row><row><cell>Soft Teacher</cell><cell>20.46</cell><cell>-</cell><cell>30.74</cell><cell>34.04</cell></row><row><cell>ACRST</cell><cell>26.07</cell><cell>28.69</cell><cell>31.35</cell><cell>34.92</cell></row><row><cell>PseCo</cell><cell>22.43</cell><cell>27.77</cell><cell>32.50</cell><cell>36.06</cell></row><row><cell>Labeled Only</cell><cell>10.22</cell><cell>13.80</cell><cell>19.40</cell><cell>24.10</cell></row><row><cell>Unbiased Teacher v2</cell><cell>22.71</cell><cell>26.03</cell><cell>30.08</cell><cell>32.61</cell></row><row><cell>DSL</cell><cell>22.03</cell><cell>25.19</cell><cell>30.87</cell><cell>36.22</cell></row><row><cell>Dense Teacher</cell><cell>22.38</cell><cell>27.20</cell><cell>33.01</cell><cell>37.13</cell></row><row><cell>S4OD</cell><cell>20.10</cell><cell>-</cell><cell>30.00</cell><cell>32.90</cell></row><row><cell>Mean-Teacher</cell><cell>20.40</cell><cell>26.00</cell><cell>30.40</cell><cell>35.50</cell></row><row><cell>Consistent-Teacher</cell><cell>25.30</cell><cell>30.40</cell><cell>36.10</cell><cell>40.00</cell></row><row><cell>VOC-PARTIAL Results.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>COCO-ADDITION experimental results on val2017 with unlabel2017 as unlabeled set. Note that 1? represents 90K training iterations, and N? represents N?90K training iterations.</figDesc><table><row><cell>Method</cell><cell>AP50:95</cell></row><row><cell>CSD(3?)</cell><cell>40.20 ???38.82 -1.38</cell></row><row><cell>STAC(6?)</cell><cell>39.48 ???39.21 -0.27</cell></row><row><cell>Unbiased Teacher(3?)</cell><cell>40.20 ? ?? ?41.30 +1.10</cell></row><row><cell>ACRST(3?)</cell><cell>40.20 ? ?? ?42.79 +2.59</cell></row><row><cell>Soft Teacher(16?)</cell><cell>40.90 ? ?? ?44.50 +3.70</cell></row><row><cell>DSL(2?)</cell><cell>40.20 ? ?? ?43.80 +3.60</cell></row><row><cell>PseCo(8?)</cell><cell>41.00 ? ?? ?46.10 +5.10</cell></row><row><cell>Dense Teacher(8?)</cell><cell>41.24 ? ?? ?46.12 +4.88</cell></row><row><cell>Consistent-Teacher (8?)</cell><cell>40.50 ? ?? ?47.20 +6.70</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>VOC-PARTIAL experimental results comparison with other semi-supervised detector on VOC07 labeled and VOC12 unlabeled set.</figDesc><table><row><cell>Method</cell><cell>AP50</cell><cell>AP50:95</cell></row><row><cell>Labeled Only</cell><cell>72.63</cell><cell>42.13</cell></row><row><cell>CSD</cell><cell>74.70</cell><cell>-</cell></row><row><cell>STAC</cell><cell>77.45</cell><cell>44.64</cell></row><row><cell>ACRST</cell><cell>78.16</cell><cell>50.12</cell></row><row><cell>Instant Teaching</cell><cell>79.20</cell><cell>50.00</cell></row><row><cell>Humble Teacher</cell><cell>80.94</cell><cell>53.04</cell></row><row><cell>Unbiased Teacher</cell><cell>77.37</cell><cell>48.69</cell></row><row><cell>Unbiased Teacher v2</cell><cell>81.29</cell><cell>56.87</cell></row><row><cell>Mean-Teacher</cell><cell>77.02</cell><cell>53.61</cell></row><row><cell>Consistent-Teacher</cell><cell>81.00</cell><cell>59.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparisons between IoU-based and our adaptive anchor assignment on COCO.</figDesc><table><row><cell>Assignment</cell><cell>AP 1? 50:95</cell><cell>AP 10% 50:95</cell></row><row><cell>IOU-based</cell><cell>38.4</cell><cell>35.50</cell></row><row><cell>our ASA</cell><cell>40.1(+1.7)</cell><cell>38.50(+3.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Ablation Study on detection head structure. We compare the performance, model size and FLOPs on different head structures on COCO 10% and standard 1? evaluation. FLOPs are measured on the input image size of 1280 ? 800.</figDesc><table><row><cell>Method</cell><cell cols="3">FLOPs (G) AP 1? 50:95 AP 10% 50:95</cell></row><row><cell cols="2">Ours w/o FAM 205.21</cell><cell>40.1</cell><cell>38.5</cell></row><row><cell cols="2">Ours w FAM-2D 205.70</cell><cell cols="2">40.4(+0.3) 39.1(+0.6)</cell></row><row><cell cols="2">Ours w FAM-3D 208.49</cell><cell cols="2">40.7(+0.6) 39.5(+1.0)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Classification and Regression inconsistency analysis using IOU-Confidence linear regression (LR) error. We also provide the Mean Teacher IOU-Confidence plot on the right.</figDesc><table><row><cell></cell><cell>LR Standard Error</cell></row><row><cell>Mean Teacher</cell><cell>0.109</cell></row><row><cell>Consistent-Teacher</cell><cell>0.080</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>The Microsoft Common Objects in Context (MS-COCO) is a large-scale object detection, segmentation, key-point detection, and captioning dataset. We use COCO2017 in our experiments for SSOD, which includes 118K training and 5K validation images along with bounding boxes of 80 object categories.</figDesc><table><row><cell>4 Experiment and Hyper-parameter settings</cell></row><row><cell>4.1 Datasets and data prepossessing.</cell></row><row><cell>4.1.1 MS-COCO 2017.</cell></row><row><cell>4.1.2 PASCAL VOC 2007-2012.</cell></row></table><note>The PASCAL Visual Object Classes (VOC) dataset contains 20 object categories alongside with pixel-level segmentation annotations, bounding box annotations, and object class annotations. The official VOC 2007 trainval set is adopted as the labeled set with 5011 images and the 11540 images from VOC 2012 trainval set is used as unlabeled data in this study. We evaluate on the VOC 2007 test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Data augmentation for labeled image training. RandomResize Resize the image to a the height of h randomly sampled from h ? U (hmin, hmax), while keeping the height-width ratio unchanged. hmin = 400, hmax = 1200 in MS-COCO hmin = 480, hmax = 800 in PASCAL-VOC RandomFlip</figDesc><table><row><cell>Transformation</cell><cell>Description</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Weak data augmentation for unlabeled image. to a the height of h randomly sampled from h ? U (hmin, hmax), while keeping the height-width ratio unchanged.</figDesc><table><row><cell>Transformation</cell><cell>Description</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Appearance transformations, called TransAppearance.</figDesc><table><row><cell>Transformation</cell><cell>Description</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Geometric transformations, called TransGeo. Rotates the image by ? degrees. ? ? U (?30 ? , 30 ? ) RanShear X Shears the image along the horizontal axis with rate R. R ? U (?0.480, 0.480) RanShear Y Shears the image along the vertically axis with rate R.</figDesc><table><row><cell>Transformation</cell><cell>Description</cell></row></table><note>R ? U (?0.480, 0.480)</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">All GT bboxes on unlabeled data are only used to calculate the loss value but not for updating the parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The anchor definition in this study generalizes to feature points in anchor-free detectors.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">AP50:95 is interchangable with mAP in this study.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/open-mmlab/mmdetection 5 https://github.com/microsoft/SoftTeacher 6 https://github.com/google-research/ssl_detection/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interpolation-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsung</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11602" to="11611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal self-ensembling teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouyang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunlin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhao</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A simple semisupervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-end semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3060" to="3069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Instant-teaching: An end-to-end semi-supervised object detection framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4081" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Label matching semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binbin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shicai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="14381" to="14390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbiased teacher v2: Semi-supervised object detection for anchor-free and anchor-based detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="9819" to="9828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pseco: Pseudo labeling and consistency training for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16317</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Dense teacher: Dense pseudo-labels for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.02541</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10687" to="10698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pseudo-labeling and confirmation bias in deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">S4od: Semi-supervised learning for single-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runbo</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04492</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dense learning based semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binghui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="4815" to="4824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01780</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ota: Optimal transport assignment for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osamu</forename><surname>Yoshie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yolox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.08430</idno>
		<title level="m">Exceeding yolo series in 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tood: Task-aligned one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjian</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3510" to="3519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2207.03337</idno>
		<title level="m">Factorizing knowledge in neural networks</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving object detection with selective self-supervised self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfeng</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="589" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-uncertainty guided multi-phase learning for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4568" to="4577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Humble teachers teach better students for semisupervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3132" to="3141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Kaiming He, and Jian Sun. R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A scale-aware yolo model for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Lagani?re</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Visual Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9627" to="9636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Foveabox: Beyound anchor-based object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="7389" to="7398" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Freeanchor: Learning to match anchors for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiple anchor learning for visual object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10206" to="10215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhang</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03496</idno>
		<title level="m">Autoassign: Differentiable label assignment for dense object detection</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Probabilistic anchor assignment with iou prediction for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee Seok</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="355" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Semi-supervised object detection with adaptive class-rebalancing self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.05031</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
