<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Visual Representation Learning by Synchronous Momentum Grouping</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
							<email>pangbo@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyi</forename><surname>Li</surname></persName>
							<email>liyaoyi@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">HuaWei Technologies Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Cai</surname></persName>
							<email>caijiai1@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="institution">HuaWei Technologies Co</orgName>
								<address>
									<settlement>Ltd</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
							<email>lucewu@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Visual Representation Learning by Synchronous Momentum Grouping</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a genuine group-level contrastive visual representation learning method whose linear evaluation performance on ImageNet surpasses the vanilla supervised learning. Two mainstream unsupervised learning schemes are the instance-level contrastive framework and clustering-based schemes. The former adopts the extremely fine-grained instance-level discrimination whose supervisory signal is not efficient due to the false negatives. Though the latter solves this, they commonly come with some restrictions affecting the performance. To integrate their advantages, we design the SMoG method. SMoG follows the framework of contrastive learning but replaces the contrastive unit from instance to group, mimicking clustering-based methods. To achieve this, we propose the momentum grouping scheme which synchronously conducts feature grouping with representation learning. In this way, SMoG solves the problem of supervisory signal hysteresis which the clustering-based method usually faces, and reduces the false negatives of instance contrastive methods. We conduct exhaustive experiments to show that SMoG works well on both CNN and Transformer backbones. Results prove that SMoG has surpassed the current SOTA unsupervised representation learning methods. Moreover, its linear evaluation results surpass the performances obtained by vanilla supervised learning and the representation can be well transferred to downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the era of adopting deep learning and data-driving as the mainstream framework <ref type="bibr" target="#b32">[33]</ref>, the quality of the learned representation largely determines the performance of models on a majority of tasks <ref type="bibr" target="#b74">[75]</ref>. For a long time, people utilize supervised tasks and adopt a large amount of annotations to train models and get good representations. Nevertheless, this simple and efficient scheme faces the problems of expensive and time-consuming annotating costs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b56">57]</ref>, and non-ideal generalization performance caused by bias from the annotation information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b50">51]</ref>. To solve these problems, people extensively study the unsupervised representation learning method <ref type="bibr" target="#b57">[58]</ref>, including the generative models ? Cewu Lu is the corresponding author. arXiv:2207.06167v1 [cs.CV] 13 Jul 2022 providing expressive latent features <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b54">55]</ref> and self-supervised methods with different pretext tasks <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b47">48]</ref>. However, there are still performance gaps between these methods and the supervised scheme. In recent years, unsupervised representation learning achieves great improvements, SOTA contrastive learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b20">21]</ref> based unsupervised methods reduce the performance gap to less than 3%. We continue the study of the unsupervised representation learning, stand on the shoulder of previous contrastive and clustering-based methods, and finally, push the performance of unsupervised methods above the level of the vanilla supervised scheme. <ref type="figure">Fig. 1</ref>: Group contrastive vs. instance contrastive learning. Compared with instance contrasting (red parts), group contrasting (blue part) learns representations through higher-level semantics, which can reduce the chance that the already learned similar instances are still treated as negative pairs (false negatives). The significant reduction of contrast elements after the grouping process also makes global contrasting easier to calculate. The colorized instances or groups denote the positive pairs and the grey ones are negatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instances Groups</head><p>Traditional contrastive learning methods adopt instance discrimination as the pretext task. This kind of refinement of classification treats every sample as a category to conduct discrimination. Thus, it will introduce many false-negative pairs, leading to inefficient supervisory signals. And another problem is that the accurate instance discrimination requires pair-wise comparison on the entire dataset. However, with limited resources, most implementations compromise to adopt a subset of the comparison with a large batch size or memory bank <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b66">67]</ref>, which further decreases the efficiency of learning.</p><p>Previous clustering-based methods relax instance discrimination into group discrimination to solve these problems. The core task of clustering is how to split instances into groups. They try to adopt K-means clustering method <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b70">71]</ref> or optimal transport <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6]</ref> method to achieve this, but due to the limitations they introduce (asynchronous two-stage training or local equipartition respectively), they don't show an advantage in terms of performance.</p><p>Here, we integrate the advantages of instance contrastive and clusteringbased methods to the newly proposed SMoG method which follows the instance contrastive framework, and extend it to group-level (see <ref type="figure">Fig. 1</ref>). One major design is the momentum grouping scheme that allows gradient propagating through groups to synchronously conduct the instance grouping process together with the representation learning. It is the first method to apply contrasting directly on groups instead of instances, since no gradient can propagate through group fea-tures in previous methods, With it, SMOG avoids the false negatives in instance contrasting and wipes off limiting factors in previous clustering-based methods.</p><p>The proposed SMoG method is effective and pretty simple. We evaluate it on several standard self-supervised benchmarks and multiple downstream tasks. In particular, under the linear protocol, it achieves 76.4% top-1 accuracy on ImageNet with a standard ResNet-50 <ref type="bibr" target="#b24">[25]</ref>, which surpasses the vanilla supervised-level performance for the first time! Extensive experiments show that SMoG works robustly and we observe consistent SOTA performance under linear (+0.8%), semi-supervised classification (+2%), detection, and segmentation tasks. Besides, SMoG works well on both CNN and Transformer backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Handcraft Pretext Task</head><p>Adopting pretext tasks to provide training supervisory signals is an effective unsupervised representation learning scheme. By now, well studied pretext tasks include jigsaw solving <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>, colorization <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b72">73]</ref>, denoising <ref type="bibr" target="#b61">[62]</ref>, inpainting <ref type="bibr" target="#b52">[53]</ref>, super-resolution <ref type="bibr" target="#b33">[34]</ref>, and patch position prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>. In addition, for video visual inputs with temporal dimension, sequential-related pretext tasks are proved useful, such as ordering frames <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b65">66]</ref>, motion estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b38">39]</ref>, and further frame estimation <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b63">64]</ref>. Due to the specificity of these tasks, the learned representation often carry a certain bias, leading to relatively limited performances on downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Instance Discrimination Method</head><p>Present mainstream contrastive learning methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b51">52]</ref> learn representations by instance discrimination which treats each image (pixel) in a training set as its own category. This scheme achieves state-of-the-art performances on downstream tasks. Its common training direction provided by In-foNCE <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b21">22]</ref> or its variations is to maximize the mutual information <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b26">27]</ref>, which requires a large number of negative pairs to get good performances <ref type="bibr" target="#b7">[8]</ref>. Adopting a large batch size is a straightforward method, but it consumes lots of resources. To solve this, MoCo <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b66">[67]</ref> propose to utilize memory structures. Some latest designs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7]</ref> conduct contrasting without negative samples by an asymmetry Siamese structure or normalization techniques. Recent work <ref type="bibr" target="#b36">[37]</ref> proposes self-supervised learning with the Hilbert-Schmidt Independence Criterion, which yields a new understanding of InfoNCE. While RELIC <ref type="bibr" target="#b46">[47]</ref> improves the generalization performance through an invariance regularizer under the causal framework. NNCLR <ref type="bibr" target="#b14">[15]</ref> adopts the nearest neighbour from the dataset as the positives. UniGrad <ref type="bibr" target="#b58">[59]</ref> provides a unified contrastive formula through gradient analysis. Instance discrimination, as an excessive fine-grained classification setting implies some problems like false-negative pairs <ref type="bibr" target="#b75">[76]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Group Discrimination Method</head><p>Our work follows the group discrimination scheme. DeepClustering <ref type="bibr" target="#b4">[5]</ref> adopts the K-means clustering method to get the groups and learn features from them in an iterative manner. However, the two-step training scheme (clustering, learning) leads to delayed supervisory signal, against effective representation learning. ODC <ref type="bibr" target="#b70">[71]</ref> and CoKe <ref type="bibr" target="#b53">[54]</ref> shortens the two-step circulation period but does not solve the delay problem. SeLa <ref type="bibr" target="#b2">[3]</ref> and SwAV <ref type="bibr" target="#b5">[6]</ref> treat the grouping problem as pseudo-labelling and solve it as a optimal transport task. But, to avoid degeneracy, they add an equipartition constraint, decreasing the validity of grouping. Our work stands on the shoulders of these works, solves the mentioned problems, and finally achieves the level of vanilla supervised learning for the first time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>Present SOTA methods commonly adopt the instance contrastive learning, apply a two-stream Siamese network structure, and take InfoNCE or its variants as the loss function to conduct the contrasting. This is an eminent and robust structure that has been proved by many models. However, there are two potential problems: 1) In terms of the problem setting, instance discrimination is too finegrained and the learned representation goes against the downstream tasks that rely on the high-level semantics to some extent, because for high-level semantics, instance-level contrasting introduces many false-negative pairs which damage the quality of representation learning. 2) In terms of technical practice, contrastive learning based on InfoNCE needs a large number of negative pairs to improve the upper bound of the theoretical performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref>, which poses a challenge on computing resources or needs specific model designs.</p><p>To deal with the above two problems, previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b64">65]</ref> propose the clustering-based method: adopting groups as negatives to reduce false-negatives and the total number of negative pairs. However, these methods introduce some limitations and lose the advantages of contrastive methods: the always-updating optimization signal and totally unrestricted feature distribution. To integrate the advantages of contrastive and clustering-based method, we propose to contrast in the group unit and design the first group-level contrastive learning algorithm Synchronous Momentum Grouping (SMoG) which inherits and develops current contrastive learning techniques, and pushes the performance of unsupervised methods to the level of vanilla supervised scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Group-Level Contrastive Learning</head><p>In general, given a dataset X = {x 1 , x 2 , ...x n } with n instances and a visual model f ? with parameter set ? which maps each instance to a vector representation f ? (x), the pipeline of the instance contrastive learning framework can be expressed as:</p><formula xml:id="formula_0">L i = ?log exp(sim(f ? (x a i ),f ? (x b i )/? ) xj ? =x a i ,xj ?Y?X exp(sim(f ? (x a i ),f ? (x j ))/? ) ,<label>(1)</label></formula><p>where sim(u, v) is commonly instantiated as the normalized inner product,f ? is just f ? or a variant of f ? such as its momentum updated version or the version without the last few layers (predictor). x a i and x b i are two different augmented views of x i . In theory, x j should come from the training set X, but in practical, taking computing complexity into account, x j is commonly selected from a much smaller subset Y ? X.</p><p>In order to extend this framework to the group level, we need to first define and generate a certain number of groups with group feature g, attach each instance to a certain group, allow gradients propagating through groups, and update them synchronously during training:</p><formula xml:id="formula_1">c a i , {g} ? ?(f ? (x a i )|{g}),</formula><p>where ? is the group assigning function that takes in instance and group features, generates instances' corresponding group c a i , and updates the groups {g}. Thus, c i = g k means that instance x i is attached to group k. With the group-level features, we can derive the group-level contrastive learning framework as:</p><formula xml:id="formula_2">L i = ?log exp(sim(c a i , c b i )/? ) gj ?G exp(sim(c a i , g j )/? ) ,<label>(2)</label></formula><p>where G is the set of group features. Since the number of groups is a certain small value, we can conduct group contrasting globally with all the group pairs.</p><p>Group Feature Instance Feature Gathering Force</p><formula xml:id="formula_3">= = = = Fig. 2:</formula><p>The left part illustrates the group contrastive target described by Eq. 2. As the training progresses, preliminary formed meaningful representations have a large chance to make x a i and x b i belong to the same group. In this case, the gathering force (the green arrow) cannot make them get closer. Thus, we modify it to the version in Eq. 3 illustrated in the right part, where the push and pull force required by contrastive learning can always take effect.</p><p>The algorithm of gaining group features (?) will be detailed introduced in the next section. Here, we emphasize that besides well expressing a group of similar instances without extra limitation, a qualified ? also needs to make sure every group feature c i is updated synchronously with the instance feature f ? (x i ) since the gradients need to be back-propagated through c i to f ? (x i ) for training the parameters which means c i can replace the latest f ? (x i ) to participate in contrasting. This is the core difference from the previous clusteringbased method, since their group features gained by clustering cannot back-propagate the gradient, and directly contrasting group features cannot train the network.</p><p>Intuitively, the newly designed group contrastive learning method aims at directly adjusting the distribution of group features, and since gradients can propagate through group features to instance features, this algorithm can grad- ually learn the instance representation. However, unfortunately, it has flaws in the second half of the training process. With the meaningful representations gradually formed, c a i and c b i tend to be the same group feature. This will make the supervisory signal fail to gather similar instances and get them closer (see <ref type="figure">Fig. 2</ref>). To solve this problem, considering that c a i is the combination of a group of f ? (x), we change the group contrastive learning loss to:</p><formula xml:id="formula_4">Momentum ? ? Momentum ( ) ? ( ) a 1</formula><formula xml:id="formula_5">L i = ?log exp(sim(f ? (x a i ), c b i )/? ) gj ?G exp(sim(f ? (x a i ), g j )/? ) .<label>(3)</label></formula><p>When c a i ? = c b i Eq. 3 can be seen as the "stochastic-batch" version of Eq. 2, which splits an intact loss into several batches, similar to SGD and GD. When c a i = c b i , Eq. 3 can still gather instances, which solves the problem. This formulation looks similar to previous clustering-based methods, but remember that we still conduct group contrasting instead of instance classification that previous methods do, since group features c i and g j can propagate the gradient, thus has the same status with f ? (x a i ) in contrasting and directly guide the learning direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Synchronous Momentum Grouping</head><p>SMoG's network structure and training settings are identical to conventional contrastive learning frameworks as <ref type="figure">Fig. 3</ref> shows. That is generating x a i and x b i through two different groups of augmentation t a , t b ? T , and gaining their features through a Siamese network. Following MoCo <ref type="bibr" target="#b22">[23]</ref>, we adopt a momentum network:</p><formula xml:id="formula_6">f ? ? ? * f ? + (1 ? ?) * f ? ,</formula><p>where ? is the momentum ratio and here, f ? denotes layers that also exist inf ? . So far, SMoG is consistent with the typical contrastive learning methods. By adding the group assigning function ?, we can get the complete SMoG method.</p><p>Generating Group Features As mentioned in last section, group-level contrasting optimizes instance features through group features. Thus, group features must represent the latest instance ones to propagate gradients, namely, c i needs to be synchronously updated with f ? (x i ) and calculated by a differentiable function. Directly adopting conventional global clustering as ? is not feasible to synchronously update c i with f ? (x i ) at each iteration due to the computing cost. Thus, we simply modify it to the momentum grouping scheme, generating c i through an iterative algorithm synchronized with representation learning.</p><p>Before the training start, we initialize the l group features {g 1 , ..., g k , ..., g l } randomly or using a clustering method such as k-means. Then along with the training process, we update g k and get c i in each iteration by:</p><formula xml:id="formula_7">c i = argmin g k (sim(f ? (x i ), g k )) g k ? ? * g k + (1 ? ?) * mean ct=g k f ? (x t ),<label>(4)</label></formula><p>where ? is the momentum ratio, x t comes from a mini-batch, and we omit the normalization. Importantly, this mechanism does not introduce extra limitation. The momentum grouping scheme assigns each instance to the closest group and iteratively updates the group features in a momentum manner. In this way, the group features are always the latest representative of the instance visual features and more importantly, for each iteration, the updated g k is adopted to conduct contrasting and the gradient can be back-propagated through g k to f ? (x i ). This is the main difference from previous clustering-based methods.</p><p>Avoiding Collapse The momentum grouping scheme ensures that the group features g k always represent the latest learned instance representations. However, because the algorithm is based on the iterative updating with a local subset of instances (a batch), at the early training stage, it might be unstable. The scale of the groups may be unbalanced or all the instances collapse into few groups. To deal with this, we periodically apply an extra grouping process on a cached relatively large feature set to relocate the groups. There will be long intervals among groupings which are light and quick, thus, the overhead can be ignored.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Compared with Previous Clustering Methods</head><p>Group discrimination is not a first proposed concept. Previous clustering based methods DeepClustering <ref type="bibr" target="#b4">[5]</ref> and SwAV <ref type="bibr" target="#b5">[6]</ref> also conduct it and in terms of the loss function, the three algorithms have a similar form. The main difference lies on the method to generate the groups and the way to utilize them. Our SMoG aims at conducting the contrasting among the groups like Eq. 2 shows. This is impossible for previous clustering-based methods since their group features have to be detached out and cannot back-propagate the gradients to the parameters. Thus, they do not contrast the groups but classify the instances into groups. To achieve the group contrasting, we propose the momentum grouping scheme which allows us directly contrasting the groups and propagate gradients. Although we modify the final loss to Eq. 3 for better performance, our SMoG is still a group-level contrastive algorithm, since the group features directly guide the optimization direction. This is the reason we call our method "grouping contrasting" instead of "clustering" to distinguish them. SMoG conducts contrasting among groups instead of classifying instances based on clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation Details</head><p>Augmentation. We adopt the asymmetrical augmentation method used in BYOL <ref type="bibr" target="#b20">[21]</ref>, where there are two augmentation schemes for the two streams of the Siamese network. The two schemes adopt the same color jittering but one has a stronger Gaussian blur and the other one has stronger solarization. Since the contrasting loss we adopt is also asymmetrical (contrasting among instance and group features), the two streams of the Siamese network are not assigned to a fixed augmentation scheme, instead, they adopt one of them in an alternate manner. In this way, the two streams can generate instance features under the same distribution so that they can be mapped to the same group feature set. SMoG Setting. Like many previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b67">68]</ref>, both f ? andf ? have a backbone and a projection head. Andf ? has an extra prediction head stacked on the projection head. The backbone's output is adopted as the learning representation. The two kinds of head are both a two-layer MLP and their hidden layer is followed by a BatchNorm <ref type="bibr" target="#b27">[28]</ref> and an activation function (ReLU for ResNet and GELU <ref type="bibr" target="#b25">[26]</ref> for SwinTransformer <ref type="bibr" target="#b39">[40]</ref>). The output layer of projection head also has BN. The dimension of the hidden layer is 2048, while for the output layer, it is 128 for CNN and 256 for Transformer. The Siamese's momentum ratio ? is 0.999. In experiments, we group all the instances into 3k groups and the group features g are initialized with the K-means algorithm. The momentum ratio ? of grouping follows a linear schedule from 1.0 to 0.99. To avoid collapse, we reset the group features every 300 iterations with K-means on cached features of the past 300 iterations andf ? is synchronously reset with the parameters of f ? .</p><p>Pre-Training Details. We pre-train the models on ImageNet dataset <ref type="bibr" target="#b11">[12]</ref>. For CNN backbones, we train with the LARS <ref type="bibr" target="#b68">[69]</ref> optimizer with 4096 mini-batchsize on 64 GPUS (when adopting ResNet50). The base learning rate is set to lr = 0.3 ? batchsize/256, following first a 10-epoch warm-up and then the cosine scheduler. The weight decay and temperature ? are set to 10 ?6 and 0.1. For Transformer based backbones, we adopt the Adamw optimizer <ref type="bibr" target="#b41">[42]</ref>. The base learning rate and weight decay are 5e ?4 ?batchsize/2048 and 0.05. Other settings are the same with CNN backbones. For efficient training, we also adopt the multi-crop training scheme <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> with two large views (224?224) and 4 small views (96?96). When adopting multi-crop, the scale of the random crops are [0.2, 1.0] and [0.05, 0.2] respectively for large and small views. We evaluate SMoG firstly on the standard benchmark on Ima-geNet with both CNN and Transformer backbones. We then compare the performance on several downstream tasks and give a detailed ablation study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Linear Evaluation</head><p>Following the standard benchmarks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b67">68]</ref>, we first evaluate the representation of ResNet-50 <ref type="bibr" target="#b24">[25]</ref> and Swin Transformer Tiny <ref type="bibr" target="#b39">[40]</ref> trained with the proposed SMoG by the linear protocol: linear classification on frozen features. Results on ResNet-50 is shown in Tab. 1 and we can see that previous group-based methods have no advantages over instance contrastive ones (about -2% top-1 accuracy) due to the introduced restrictions. While the proposed SMoG improves the best performance of group-based methods by 2.3% top-1 accuracy and achieves the SOTA performances among all the unsupervised representation methods. More importantly, after adopting the multi-crop training strategy <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, for the first time, the unsupervised representation of ResNet-50 surpasses the performance of the vanilla supervised one (+ 0.3%) on ImageNet.</p><p>We adopt Swin-Transformer to evaluate the performance of SMoG on Transformer backbones. Results are shown in Tab. 2. For the two-view setting, most Transformer-based backbones perform worse than the CNNs with similar parameters. But Transformers benefit more from the multi-view training. We hold the opinion that the selfattention, a global operator, needs more data to train. Thus, stronger augmentation leads to better performances. SMoG, in the two-view setting, achieves SOTA performances compared to DINO (+2%), MoCo (+2%), EsViT (+4%), and MOBY (-0.5%). Note that, for fair comparison, we report the performance of EsViT with only L V . In the multi-crops setting, SMoG achieves the supervised performance without Mixup <ref type="bibr" target="#b71">[72]</ref> augmentation. Since mixup needs labels, it is not straightforwardly suitable for the unsupervised setting. This comparison reveals a landmark progress in contrastive learning. Large Architectures Tab. 3 shows the results on several varients of ResNet-50 with larger widths <ref type="bibr" target="#b30">[31]</ref>. The performance increases with a similar trend of supervised method and previous unsupervised framework. And it is worth noting that in previous works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, with larger architectures, their gap with the supervised learning decreases, but it is a pity that we do not observe an increasing superiority of SMoG over the supervised one. In <ref type="figure">Fig. 4</ref>, we demonstrate that on both CNN and Transformer, SMoG is comparable with the vanilla supervised method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-Supervised Fine-Tune Evaluation</head><p>Next, we evaluate the proposed SMoG through semi-supervised fine-tuning the unsupervised representation on a subset of training data of ImageNet. We follow the protocol adopted in <ref type="bibr" target="#b7">[8]</ref> and the 1% and 10% labeled splits of ImageNet we adopt are the fixed ones provided in <ref type="bibr" target="#b7">[8]</ref>.    We further transfer the unsupervised representations of ResNet-50 learned with the proposed SMoG to several downstream tasks, namely semantic segmentation, object detection, and instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Transfer to Other Vision Tasks</head><p>We first evaluate SMoG on Cityscapes <ref type="bibr" target="#b10">[11]</ref> and VOC-2012 <ref type="bibr" target="#b15">[16]</ref> semantic segmentation tasks. For fair comparison, we align all the methods with the same training recipe of FCN <ref type="bibr" target="#b40">[41]</ref>. Results of mean accuracy and mean IoU are provided in Tab. 5, we can see that nearly all the unsupervised representations outperform the conventional supervised one which reveals that in this downstream task, unsupervised representation is already a better choice. And again, SMoG performs better than the original SOTAs on Cityscapes (+1.1 mIoU) and VOC-2012 (+1.4 mIoU) datasets.</p><p>Then, we evaluate on object detection and instance segmentation tasks on COCO <ref type="bibr" target="#b37">[38]</ref> dataset. Similarly, all the unsupervised representations are transferred with the same fine-tuning recipe of Mask-RCNN <ref type="bibr" target="#b23">[24]</ref>. We provide AP results in Tab. 6. The same with semantic segmentation, on these two downstream tasks, our unsupervised pre-training representations are better than the supervised one. And still, SMoG improves the current SOTA results, +0.7 AP on object detection and +1.1 AP on instance segmentation. We provide ablation studies on key components of SMoG. ResNet-50 is adopted. Network is trained for 100 epochs without multi-crop. The representation is evaluated under the linear protocol.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Grouping quality To evaluate the grouping quality, in <ref type="figure" target="#fig_2">Fig. 5</ref>, we provide the density distribution of group entropy. All the three groupdiscrimination methods (our SMoG, SwAV, and DeepClus-teringv2) have 3k groups. For each group, its entropy is i ?p i * log(p i ) where p i is the ratio of instances that belong to class i (data annotation) in this group. A lower entropy means a group has more unitary semantics and is more meaningful. Thus, more groups with low entropy indicate higher grouping quality. From <ref type="figure" target="#fig_2">Fig. 5</ref>, we can see SMoG has twice more low-entropy groups than the two baselines, proving its superiority.</p><p>Momentum ratio ? of group features From Tab. 7a, we can see that the linearly decreasing schedule for ? is much better than the fixed schedule. This is because our momentum grouping scheme updates the group features with only a small part of features (a batch) in each iteration and at the beginning of training, the feature distribution changes drastically, a small ? will lead to unstable groups. Thus, we need a large ? at the beginning to solve the problem and the linear schedule is a straightforward solution. After adopting the linearly decreasing schedule, our SMoG is not sensitive to the final value of ? and we adopt 0.99 as the default value for all the experiments on both CNN and Transformer.  Avoiding collapse Still, because the group feature updating process cannot access the global feature distribution, there is a possibility of collapse during training. Thus, we adopt the periodical clustering (pd) trick to avoid this. The first two rows of Tab. 7b prove its necessity. We can see that it successfully makes the backbone learn useful representations but the performance is not good enough. We believe this is because the group features are always generated and updated based on the instance features generated by only f ? , leading to misalignment with features fromf ? . Thus, we also resetf ? with the parameters of f ? after each clustering. The integrated pd withf ? reset leads to ideal performance. And surprisingly, we find that only resettingf ? without the clustering can avoid the collapse too. We think the sudden change off ? feature distribution also helps to avoid the network gradually falling into degeneration. <ref type="table">Table 7</ref>: Ablation study on SMoG. We adopt ResNet50 as the backbone and train the unsupervised algorithm on ImageNet for 100 epochs without multicrop training strategy. We report the linear evaluation results (Top-1 accuracy).  Number of groups In Tab. 7c, we evaluate the influence of the number of groups used in SMoG under the linear protocol. The results show that SMoG is not sensitive to the group number. Even if we tune it in a wide range (1k?30k), the performance is maintained at a stable level (? 0.2), as long as there are enough groups (300 groups are too few to perform well). This is consistent with the conclusion of SwAV <ref type="bibr" target="#b5">[6]</ref>. More groups increase the computation time consuming of the momentum grouping algorithm (?) since it needs node communication to synchronize the group features among all the nodes. Thus, we adopt 3K as the default setting for all the experiments.</p><p>Method for updating group features In Tab. 7d, we evaluate different operators for updating the group features. We consider 4 operators here: 1) Randomly select (RS): randomly select a group of latest instance features and adopt them as the group features. 2) Adopt latest (AL): g k ? mean ct=g k f ? (x t ). 3) Averaging update (AU): g k ? g k + (1/n) * (mean ct=g k f ? (x t ) ? g k ), where n is the total number of instances belonging to group k, including ones in current iteration. 4) Momentum update (MU): the proposed method described in Eq. 4. As the random baseline, RS's poor performance shows us that the grouping algorithm plays an important role in our SMoG method. AL also does not perform well, which implies that the local grouping method will not provide an efficient training signal for representation learning. AU follows the sequential k-means algorithm and achieves a relatively great performance. But compared to our MU, it is not the best choice for conducting grouping together with the representation learning. The specifically designed MU which gives new features more weights is more suitable for the group-level contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We extend the recently popular contrastive learning to group level with the proposed SMoG and push the performance of unsupervised representation under linear protocol to the vanilla supervised level. SMoG synchronously conducts the representation learning and grouping process to effectively achieve the grouplevel contrasting. We hope the newly designed group-level contrastive learning will be useful for the community to further develop visual unsupervised method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :</head><label>5</label><figDesc>Entropy of each groups. SMoG produces groups with much lower entropy which represents a better grouping quality in terms of high-level semantics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) Momentum ratio ?. Linearly decreasing schedule performs better. Tricks dealing with collapse. The periodical clustering is necessary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Pipeline of our SMoG vs. conventional contrastive method. Like typical contrastive learning methods, SMoG gets instance features from two augmentation views of the same image by a Siamese structure. Rather than directly contrasting the instance features, SMoG works at the group level. Specifically, it dynamically splits the already seen instances into groups, assigns the new instance to the closest group (the red one in thefigure), and adopts the group feature which replaces the instance one to conduct the contrasting. Synchronous with the representation learning, group features are updated by the new instances in a momentum manner. The yellow dashed lines show the back-propagation paths, and we can see that the group feature can also back-propagate gradients.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>( )</cell></row><row><cell></cell><cell>Groups</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Assign a group</cell><cell>Momentum Update</cell><cell>asting</cell><cell>Contr</cell><cell>? ( ) Contrasting</cell></row><row><cell></cell><cell cols="2">Update groups</cell><cell></cell><cell></cell></row><row><cell>a Network to get representation</cell><cell>b Our SMoG</cell><cell></cell><cell></cell><cell></cell><cell>c Conventional Contrastive Method</cell></row><row><cell>Fig. 3:</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Linear protocol results on Ima-geNet. ResNet50 is adopted. ? denotes the model adopts the multi-crop training strategy. "acc" means accuracy.</figDesc><table><row><cell>model</cell><cell cols="4">epoch batchsize top1 acc top5 acc</cell></row><row><cell>supervised</cell><cell>100ep</cell><cell>256</cell><cell>76.1</cell><cell>92.7</cell></row><row><cell cols="2">instance contrastive method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ReSSL [74]</cell><cell>200</cell><cell>256</cell><cell>69.6</cell><cell>-</cell></row><row><cell>MoCoV2 [23]</cell><cell>800</cell><cell>256</cell><cell>71.1</cell><cell>90.1</cell></row><row><cell>SimSiam [9]</cell><cell>800</cell><cell>256</cell><cell>71.3</cell><cell>-</cell></row><row><cell cols="2">InfoMin Aug. [60] 800</cell><cell>256</cell><cell>73.0</cell><cell>91.1</cell></row><row><cell>MoCoV3 [10]</cell><cell>400</cell><cell>4096</cell><cell>73.1</cell><cell>-</cell></row><row><cell>MoCoV3</cell><cell>800</cell><cell>4096</cell><cell>73.8</cell><cell>-</cell></row><row><cell>BYOL [21]</cell><cell>400</cell><cell>4096</cell><cell>73.2</cell><cell>-</cell></row><row><cell>BYOL</cell><cell>800</cell><cell>4096</cell><cell>74.3</cell><cell>91.6</cell></row><row><cell cols="2">Barlow Twins [70] 1000</cell><cell>2048</cell><cell>73.2</cell><cell>91.0</cell></row><row><cell>RELIC [47]</cell><cell>1000</cell><cell>4096</cell><cell>74.8</cell><cell>92.2</cell></row><row><cell>SSL-HSIC [37]</cell><cell>1000</cell><cell>4096</cell><cell>74.8</cell><cell>92.2</cell></row><row><cell cols="2">group-based method</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DeepCluster</cell><cell>400</cell><cell>256</cell><cell>52.2</cell><cell>-</cell></row><row><cell>ODC [71]</cell><cell>400</cell><cell>256</cell><cell>57.6</cell><cell>-</cell></row><row><cell>PCL [36]</cell><cell>200</cell><cell>256</cell><cell>67.6</cell><cell>-</cell></row><row><cell>DeepClusterV2</cell><cell>400</cell><cell>4096</cell><cell>70.2</cell><cell>-</cell></row><row><cell>SwAV</cell><cell>400</cell><cell>4096</cell><cell>70.1</cell><cell>-</cell></row><row><cell>CoKe [54]</cell><cell>800</cell><cell>1024</cell><cell>72.2</cell><cell>-</cell></row><row><cell>SMoG</cell><cell>400</cell><cell>2048</cell><cell>73.6</cell><cell>91.3</cell></row><row><cell>SMoG</cell><cell>800</cell><cell>4096</cell><cell>74.5</cell><cell>91.9</cell></row><row><cell>with multi-crop</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SwAV ?</cell><cell>400</cell><cell>4096</cell><cell>74.6</cell><cell>-</cell></row><row><cell>SwAV ?</cell><cell>800</cell><cell>4096</cell><cell>75.3</cell><cell>-</cell></row><row><cell>DC-v2 ?</cell><cell>800</cell><cell>4096</cell><cell>75.2</cell><cell>-</cell></row><row><cell>DINO  ? [7]</cell><cell>800</cell><cell>4096</cell><cell>75.3</cell><cell>-</cell></row><row><cell>UniGrad  ? [59]</cell><cell>800</cell><cell>4096</cell><cell>75.5</cell><cell>-</cell></row><row><cell>NNCLR  ? [15]</cell><cell>1000</cell><cell>4096</cell><cell>75.6</cell><cell>92.4</cell></row><row><cell>SMoG ?</cell><cell>400</cell><cell>4096</cell><cell>76.4</cell><cell>93.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Linear protocol results on ImageNet. ? denotes adopting the multi-crop training strategy. The throughput (im/s) is calculated on a NVIDIA V100 GPU with 128 samples per forward. We report performances without Mixup and standard ones for supervised methods.</figDesc><table><row><cell>model</cell><cell cols="4">backbone throughput param top1 acc</cell></row><row><cell cols="2">supervised SwinT</cell><cell>808</cell><cell cols="2">28 77.8 / 81.3</cell></row><row><cell cols="2">Supervised DeiT-S/16</cell><cell>1007</cell><cell cols="2">21 77.5 / 79.8</cell></row><row><cell>MoBY</cell><cell>DeiT-S/16</cell><cell>1007</cell><cell>21</cell><cell>72.8</cell></row><row><cell>MoBY</cell><cell>SwinT</cell><cell>808</cell><cell>28</cell><cell>75.0</cell></row><row><cell cols="2">MoCoV3 DeiT-S/16</cell><cell>1007</cell><cell>21</cell><cell>72.5</cell></row><row><cell cols="2">MoCoV3 ViT-B/16</cell><cell>312</cell><cell>85</cell><cell>76.7</cell></row><row><cell>DINO</cell><cell>DeiT-S/16</cell><cell>1007</cell><cell>21</cell><cell>72.5</cell></row><row><cell>DINO  ?</cell><cell>DeiT-S/16</cell><cell>1007</cell><cell>21</cell><cell>77.0</cell></row><row><cell cols="2">EsViT [35] SwinT</cell><cell>808</cell><cell>28</cell><cell>70.5</cell></row><row><cell>EsViT  ?</cell><cell>SwinT</cell><cell>808</cell><cell>28</cell><cell>77.0</cell></row><row><cell>SMoG</cell><cell>SwinT</cell><cell>808</cell><cell>28</cell><cell>74.5</cell></row><row><cell>SMoG  ?</cell><cell>SwinT</cell><cell>808</cell><cell>28</cell><cell>77.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Linear protocol results on ImageNet with larger backbones. We experiment on wider ResNet. The pre-training details are the same with the ResNet-50 ?1.</figDesc><table><row><cell>model</cell><cell cols="3">backbone param top1 acc top5 acc</cell></row><row><cell cols="2">Res50 (x2) 188 Supervised Res50 (x4) 375</cell><cell>77.8 78.9</cell><cell>93.8 94.5</cell></row><row><cell>BYOL</cell><cell>Res50 (x2) 188 Res50 (x4) 375</cell><cell>77.4 78.6</cell><cell>93.6 94.2</cell></row><row><cell>SwAV</cell><cell>Res50 (x2) 188 Res50 (x4) 375</cell><cell>77.3 77.9</cell><cell>--</cell></row><row><cell>SMoG</cell><cell>Res50 (x2) 188 Res50 (x4) 375</cell><cell>78.0 79.0</cell><cell>93.9 94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Semi-supervised results on ImageNet.</figDesc><table><row><cell>As shown in Tab. 4,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SMoG consistently outper-forms all the previous meth-ods in both 1% and 10% set-tings on ResNet-50 ?1 and ?2. Besides, we also fine-tune the unsupervised rep-resentation on the full Im-ageNet. With SMoG pre-training, ResNet-50 achieves 78.3% top-1 accuracy under</cell><cell cols="4">method resnet-50 ?1 Supervised SimCLR BYOL SwAV Barlow Twins 55.0 69.7 -79.2 89.3 -Top-1 Acc (%) Top-5 Acc (%) 1% 10% 100% 1% 10% 100% 25.4 56.4 76.1 48.4 80.4 92.9 48.3 65.6 76.0 75.5 87.8 93.1 53.2 68.8 77.7 78.4 89.0 93.9 53.9 70.2 -78.5 89.9 -SSL-HSIC 52.1 67.9 77.2 77.7 88.6 93.6 NNCLR 56.4 69.8 -80.7 89.3 -</cell></row><row><cell>the standard training recipe,</cell><cell>SMoG</cell><cell cols="3">58.0 71.2 78.3 81.6 90.5 94.2</cell></row><row><cell>surpassing the directly super-</cell><cell>resnet-50 ?2</cell><cell></cell><cell></cell><cell></cell></row><row><cell>vised learning by 2.2%. Simi-</cell><cell>Supervised</cell><cell>-</cell><cell>-77.8 -</cell><cell>-93.8</cell></row><row><cell>larly, the ResNet-50 ? 2 also</cell><cell>SimCLR</cell><cell cols="3">58.5 71.7 -83.0 91.2 -</cell></row><row><cell>outperforms the directly su-</cell><cell>BYOL</cell><cell cols="3">62.2 73.5 -84.1 91.7 -</cell></row><row><cell>pervised training by 2.4%.</cell><cell>SMoG</cell><cell cols="3">63.6 74.4 80.2 85.6 92.4 95.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Transfer learning results on semantic segmentation task. We fine-tune the representations on VOC2012 and Cityscapes dataset. The segmentation model is FCN with ResNet-50.</figDesc><table><row><cell>model</cell><cell>Cityscapes VOC-2012 mIoU mAcc mIoU mAcc</cell></row><row><cell cols="2">Supervised 73.83 82.56 73.59 83.74</cell></row><row><cell cols="2">MoCoV2 74.30 83.37 70.86 80.37</cell></row><row><cell>SwAV</cell><cell>74.80 83.01 74.97 84.27</cell></row><row><cell>BYOL</cell><cell>74.90 83.73 74.76 84.37</cell></row><row><cell>SMoG</cell><cell>76.03 83.97 76.22 85.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Transfer learning results on object detection and instance segmentation tasks. we adopt COCO as the fine-tuning dataset. Mask RCNN with ResNet-50-FPN is the detection and segmentation model. We report the AP metrics.</figDesc><table><row><cell></cell><cell>COCO det</cell><cell cols="3">COCO instance seg.</cell></row><row><cell>method</cell><cell cols="3">AP bb AP bb 50 AP bb 75 AP mk AP mk 50</cell><cell>AP mk 75</cell></row><row><cell>Rand Init</cell><cell>31.0 49.5 33.2</cell><cell>28.5</cell><cell>46.8</cell><cell>30.4</cell></row><row><cell>Supervised</cell><cell>38.9 59.6 42.7</cell><cell>35.4</cell><cell>56.5</cell><cell>38.1</cell></row><row><cell>InsDis [67]</cell><cell>37.4 57.6 40.6</cell><cell>34.1</cell><cell>54.6</cell><cell>36.4</cell></row><row><cell>PIRL [45]</cell><cell>38.5 57.6 41.2</cell><cell>34.0</cell><cell>54.6</cell><cell>36.2</cell></row><row><cell>MoCoV2</cell><cell>39.4 59.9 43.0</cell><cell>35.8</cell><cell>56.9</cell><cell>38.4</cell></row><row><cell>SwAV</cell><cell>38.5 60.4 41.4</cell><cell>35.4</cell><cell>57.0</cell><cell>37.7</cell></row><row><cell>DC-v2 [6]</cell><cell>38.3 60.3 41.3</cell><cell>35.4</cell><cell>56.7</cell><cell>38.0</cell></row><row><cell>BYOL</cell><cell>39.4 59.9 43.0</cell><cell>35.8</cell><cell>56.8</cell><cell>38.5</cell></row><row><cell cols="2">Barlow Twins 39.2 58.7 42.6</cell><cell>34.3</cell><cell>55.4</cell><cell>36.5</cell></row><row><cell>SMoG</cell><cell cols="2">40.1 61.6 43.7 36.9</cell><cell>58.7</cell><cell>39.3</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This work was supported by the National Key R&amp;D Program of China (2021ZD0110700), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), Shanghai Qi Zhi Institute, and SHEITC (018-RGZN-02046).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Labelling unlabelled videos from scratch with multi-modal self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.13662</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05371</idno>
		<title level="m">Self-labelling via simultaneous clustering and representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<title level="m">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Exploring simple siamese representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="15750" to="15758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">An empirical study of training self-supervised visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2104</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2051" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14548</idno>
		<title level="m">With a little help from my friends: Nearest-neighbor contrastive learning of visual representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to detect and track visible and occluded body joints in a virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Instaboost: Boosting instance segmentation via probability map guided copy-pasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="682" to="691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3636" to="3645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revitalizing cnn attentions via transformers in self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05340</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<title level="m">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="297" to="304" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1413" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1920" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6874" to="6883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Efficient self-supervised vision transformers for representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09785</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<title level="m">Prototypical contrastive learning of unsupervised representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Self-supervised learning with kernel dependence maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pogodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.08320</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Switchable temporal propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De Mello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="87" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Shuffle and learn: unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="527" to="544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.07922</idno>
		<title level="m">Representation learning via invariant causal mechanisms</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Boosting self-supervised learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vinjimoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9359" to="9367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Unsupervised representation for semantic segmentation by implicit cycle-attention contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Unsupervised visual representation learning by online constrained k-means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11527</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and variational inference in deep latent gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PMLR</publisher>
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Correlation field for boosting 3d object detection in structured scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Human trajectory prediction with momentary observation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page" from="6467" to="6476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Exploring the equivalence of siamese self-supervised learning via a unified gradient framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.05141</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243</idno>
		<title level="m">What makes for good views for contrastive learning? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Self-supervised learning of motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="98" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02612</idno>
		<title level="m">Generating videos with scene dynamics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning by cross-level instancegroup discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12586" to="12595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning and using the arrow of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8052" to="8060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via nonparametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<title level="m">Self-supervised learning with swin transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<title level="m">Scaling sgd batch size to 32k for imagenet training</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03230</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Online deep clustering for unsupervised representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6688" to="6697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<title level="m">mixup: Beyond empirical risk minimization. ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Ressl: Relational self-supervised learning with weak augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.09282</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yamins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6002" to="6012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
