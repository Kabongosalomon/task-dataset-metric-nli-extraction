<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">From Coarse to Fine: Robust Hierarchical Localization at Large Scale</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Edouard</forename><surname>Sarlin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Dymczyk</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Systems Lab</orgName>
								<orgName type="institution">ETH Z?rich</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Sevensense Robotics AG</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">From Coarse to Fine: Robust Hierarchical Localization at Large Scale</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Robust and accurate visual localization is a fundamental capability for numerous applications, such as autonomous driving, mobile robotics, or augmented reality. It remains, however, a challenging task, particularly for large-scale environments and in presence of significant appearance changes. State-of-the-art methods not only struggle with such scenarios, but are often too resource intensive for certain real-time applications. In this paper we propose HF-Net, a hierarchical localization approach based on a monolithic CNN that simultaneously predicts local features and global descriptors for accurate 6-DoF localization. We exploit the coarse-to-fine localization paradigm: we first perform a global retrieval to obtain location hypotheses and only later match local features within those candidate places. This hierarchical approach incurs significant runtime savings and makes our system suitable for real-time operation. By leveraging learned descriptors, our method achieves remarkable localization robustness across large variations of appearance and sets a new state-of-the-art on two challenging benchmarks for large-scale localization. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The precise 6-Degree-of-Freedom (DoF) localization of a camera within an existing 3D model is one of the core computer vision capabilities that unlocks a number of recent applications. These include autonomous driving in GPS-denied environments <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref> and consumer devices with augmented reality features <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref>, where a centimeter-accurate 6-DoF pose is crucial to guarantee reliable and safe operation and fully immersive experiences, respectively. More broadly, visual localization is a key component in computer vision tasks such as Structure-from-Motion (SfM) or SLAM. This growing range of applications of visual localization calls for reliable operation both indoors and outdoors, irrespective of the weather, illumination, or seasonal changes.</p><p>Robustness to such large variations is therefore critical, along with limited computational resources. Maintaining a model that allows accurate localization in multiple con- <ref type="bibr" target="#b0">1</ref> Code available at https://github.com/ethz-asl/hf_net ditions, while remaining compact, is thus of utmost importance. In this work, we investigate whether it is actually possible to robustly localize in large-scale changing environments with constrained resources of mobile devices. More specifically, we aim at estimating the 6-DoF pose of a query image w.r.t. a given 3D model with the highest possible accuracy.</p><p>Current leading approaches mostly rely on estimating correspondences between 2D keypoints in the query and 3D points in a sparse model using local descriptors. This direct matching is either robust but intractable on mobile <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b42">43]</ref>, or optimized for efficiency but fragile <ref type="bibr" target="#b28">[29]</ref>. In both cases, the robustness of classical localization methods is limited by the poor invariance of hand-crafted local features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref>. Recent features emerging from convolutional neural networks (CNN) exhibit unrivalled robustness at a low compute cost <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b33">34]</ref>. They have been, however, only recently <ref type="bibr" target="#b52">[52]</ref> applied to the visual localization problem, and only in a dense, expensive manner. Learned sparse descriptors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref> promise large benefits that remain yet unexplored in localization.</p><p>Alternative localization approaches based on image retrieval have recently shown promising results in terms of robustness and efficiency, but are not competitive in terms of accuracy. The benefits of an intermediate retrieval step have been demonstrated earlier <ref type="bibr" target="#b41">[42]</ref>, but fall short of reach-ing the scalability required by city-scale localization.</p><p>In this paper, we propose to leverage recent advances in learned features to bridge the gap between robustness and efficiency in the hierarchical localization paradigm. Similar to how humans localize, we employ a natural coarseto-fine pose estimation process which leverages both global descriptors and local features, and scales well with large environments <ref type="figure" target="#fig_0">(Figure 1</ref>). We show that learned descriptors enable unrivaled robustness in challenging conditions, while learned keypoints improve the efficiency in terms of compute and memory thanks to their higher repeatability. To further improve the efficiency of this approach, we propose a Hierarchical Feature Network (HF-Net), a CNN that jointly estimates local and global features, and thus maximizes the sharing of computations. We show how such a compressed model can be trained in a flexible way using multitask distillation. By distilling multiple state-of-the-art predictors jointly into a single model, we obtain an incomparably fast, yet robust and accurate, localization. Such heterogenous distillation is applicable beyond visual localization to tasks that require both multimodal expensive predictions and computational efficiency. Overall, our contributions are as follows:</p><p>-We set a new state-of-the-art in several public benchmarks for large-scale localization with an outstanding robustness in particularly challenging conditions;</p><p>-We introduce HF-Net, a monolithic neural network which efficiently predicts hierarchical features for a fast and robust localization;</p><p>-We demonstrate the practical usefulness and effectiveness of multitask distillation to achieve runtime goals with heterogeneous predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section we review other works that relate to different components of our approach, namely: visual localization, scalability, feature learning, and deployment on resource constrained devices.</p><p>6-DoF visual localization methods have traditionally been classified as either structure-based or image-based. The former perform direct matching of local descriptors between 2D keypoints of a query image and 3D points in a 3D SfM model <ref type="bibr" target="#b51">[51,</ref><ref type="bibr" target="#b55">55,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b52">52]</ref>. These methods are able to estimate accurate poses, but often rely on exhaustive matching and are thus compute intensive. As the model grows in size and perceptual aliasing arises, this matching becomes ambiguous, impairing the robustness of the localization, especially under strong appearance changes such as daynight <ref type="bibr" target="#b43">[44]</ref>. Some approaches directly regress the pose from a single image <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22]</ref>, but are not competitive in term of accuracy <ref type="bibr" target="#b46">[46]</ref>. Image-based methods are related to image retrieval <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b57">57]</ref> and are only able to provide an approximate pose up to the database discretization, which is not sufficiently precise for many applications <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">52]</ref>. They are however significantly more robust than direct local matching as they rely on the global image-wide information. This comes at the cost of increased compute, as state-of-the-art image retrieval is based on large deep learning models.</p><p>Scalable localization often deals with the additional compute constrains by using features that are inexpensive to extract, store, and match together <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref>. These improve the runtime on mobile devices but further impair the robustness of the localization, limiting their operations to stable conditions <ref type="bibr" target="#b28">[29]</ref>. Hierarchical localization <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b41">42]</ref> takes a different approach by dividing the problem into a global, coarse search followed by a fine pose estimation. Recently, <ref type="bibr" target="#b41">[42]</ref> proposed to search at the map level using image retrieval and localize by matching hand-crafted local features against retrieved 3D points. As we discuss further in Section 3, its robustness and efficiency are limited by the underlying local descriptors and heterogeneous structure.</p><p>Learned local features have recently been developed in attempt to replace hand-crafted descriptors. Dense pixel-wise features naturally emerge from CNNs and provide a powerful representation used for image matching <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b39">40]</ref> and localization <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b43">44]</ref>. Matching dense features is however intractable with limited computing power. Sparse learned features, composed of keypoints and descriptors, provide an attractive drop-in replacement to their handcrafted counterparts and have recently shown outstanding performance <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b17">18]</ref>. They can easily be sampled from dense features, are fast to predict and thus suitable for mobile deployment. CNN keypoint detections have also been shown to outperform classical methods, although they are notably difficult to learn. SuperPoint <ref type="bibr" target="#b13">[14]</ref> learns from selfsupervision, while DELF <ref type="bibr" target="#b35">[36]</ref> employs an attention mechanism to optimize for the landmark recognition task.</p><p>Deep learning on mobile. While learning some building blocks of the localization pipeline improves performance and robustness, deploying them on mobile devices is a non-trivial task. Recent advances in multi-task learning allow to efficiently share compute across tasks without manual tuning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">50]</ref>, thus reducing the required network size. Distillation <ref type="bibr" target="#b19">[20]</ref> can help to train a smaller network <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b60">60</ref>] from a larger one that is already trained, but is usually not applied in a multi-task setting.</p><p>To the best of our knowledge, our approach is the first of its kind that combines advances in the aforementioned fields to optimize for both efficiency and robustness. The proposed method seeks to leverage the synergies of these algorithms to deliver a competitive large-scale localization solution and bring this technology closer to real-time, online applications with constrained resources.  <ref type="figure">Figure 2</ref>. The hierarchical localization with HF-Net is significantly simpler than concurrent approaches <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b51">51]</ref>, yet more robust, accurate, and efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Hierarchical Localization</head><p>We aim at maximizing the robustness of the localization while retaining tractable computational requirements. Our method is loosely based on the hierarchical localization framework <ref type="bibr" target="#b41">[42]</ref>, which we summarize here.</p><p>Prior retrieval. A coarse search at the map level is performed by matching the query with the database images using global descriptors. The k-nearest neighbors (NN), called prior frames, represent candidate locations in the map. This search is efficient given that there are far fewer database images than points in the SfM model.</p><p>Covisibility clustering. The prior frames are clustered based on the 3D structure that they co-observe. This amounts to finding connected components, called places, in the covisibility graph that links database images to 3D points in the model.</p><p>Local feature matching. For each place, we successively match the 2D keypoints detected in the query image to the 3D points contained in the place, and attempt to estimate a 6-DoF pose with a PnP <ref type="bibr" target="#b24">[25]</ref> geometric consistency check within a RANSAC scheme <ref type="bibr" target="#b15">[16]</ref>. This local search is also efficient as the number of 3D points considered is significantly lower in the place than in the whole model. The algorithm stops as soon as a valid pose is estimated.</p><p>Discussion. In the work of <ref type="bibr" target="#b41">[42]</ref>, a large state-of-the-art network for image retrieval, NetVLAD <ref type="bibr" target="#b1">[2]</ref>, is distilled into a smaller model, MobileNetVLAD (MNV). This helps to achieve given runtime constraints while partly retaining the accuracy of the original model. The local matching step is however based on SIFT <ref type="bibr" target="#b27">[28]</ref>, which is expensive to compute and generates a large number of features, making this step particularly expensive. While this method exhibits good performance in small-scale environments, it does not scale well to larger, denser models. Additionally, SIFT is not competitive with recent learned features, especially under large illumination changes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>. Lastly, a significant part of the computation of local and global descriptors is redundant, as they are both based on the image low-level clues. The heterogeneity of hand-crafted features and CNN image retrieval is thus computationally suboptimal and could be critical on resource-constrained platforms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed Approach</head><p>We now show how we address these issues and achieve improved robustness, scalability, and efficiency. We first motivate the use of learned features with a homogeneous network structure, and then detail the architecture in Section 4.1 and our novel training procedure in Section 4.2.</p><p>Learned features appear as a natural fit for the hierarchical localization framework. Recent methods like Su-perPoint <ref type="bibr" target="#b13">[14]</ref> have shown to outperform popular baseline like SIFT in terms of keypoint repeatability and descriptor matching, which are both critical for localization. Some learned features are additionally significantly sparser than SIFT, thus reducing the number of keypoints to be matched and speeding up the matching step. We show in Section 5.1 that a combination of state-of-the-art networks in image retrieval and local features naturally achieves state-of-the-art localization. This approach particularly excels in extremely challenging conditions, such as night-time queries, outperforming competitive methods by a large margin along with a smaller 3D model size.</p><p>While the inference of such networks is significantly faster than computing SIFT on GPU, it still remains a large computational bottleneck for the proposed localization system. With the goal of improving the ability to localize online on mobile devices, we introduce here a novel neural network for hierarchical features, HF-Net, enabling an efficient coarse-to-fine localization. It detects keypoints and computes local and global descriptors in a single shot, thus maximizing sharing of computations, but retaining performance of a larger baseline network. We show in <ref type="figure">Figure 2</ref> its application within the hierarchical localization framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">HF-Net Architecture</head><p>Convolutional neural networks intrinsically exhibit a hierarchical structure. This paradigm fits well the joint predictions of local and global features and comes at low additional runtime costs. The HF-Net architecture ( <ref type="figure">Figure 3</ref>) is composed of a single encoder and three heads predicting: i) keypoint detection scores, ii) dense local descriptors and iii) a global image-wide descriptor. This sharing of computation is natural: in state-of-the-art image retrieval networks, the global descriptors are usually computed from the aggregation of local feature maps, which might be useful to predict local features.</p><p>The encoder of HF-Net is a MobileNet <ref type="bibr" target="#b40">[41]</ref> backbone, a popular architecture optimized for mobile inference. Similarly to MNV <ref type="bibr" target="#b41">[42]</ref>, the global descriptor is computed by  <ref type="figure">Figure 3</ref>. HF-Net generates three outputs from a single image: a global descriptor, a map of keypoint detection scores, and dense keypoint descriptors. All three heads are trained jointly with multitask distillation from different teacher networks. a NetVLAD layer <ref type="bibr" target="#b1">[2]</ref> on top of the last feature map of MobileNet. For the local features, the SuperPoint <ref type="bibr" target="#b13">[14]</ref> architecture is appealing for its efficiency, as it decodes the keypoints and local descriptors in a fixed non-learned manner. This is much faster than applying transposed convolutions to upsample the features. It predicts dense descriptors which are fast to sample bilinearly, resulting in a runtime independent from the number of detected keypoints. On the other hand, patch-based architectures like LF-Net <ref type="bibr" target="#b37">[38]</ref> apply a Siamese network to image patches centered at all keypoint locations, resulting in a computational cost proportional to the number of detections.</p><p>For its efficiency and flexibility, we thus adopt the Super-Point decoding scheme for keypoints and local descriptors. The local feature heads branch out from the MobileNet encoder at an earlier stage than the global head, as a higher spatial resolution is required to retain spatially discriminative features, local features are on a lower semantic level than image-wide descriptors <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training Process</head><p>Data scarcity. Local and global descriptors are often trained with metric learning using ground truth positive and negative pairs of local patches and full images. These ground truth correspondences are particularly difficult to obtain at the scale required to train large CNNs. While global supervision naturally emerges from local correspondences, there is currently no such dataset that simultaneously i) exhibits a sufficient perceptual diversity at the global image level, e.g. with various conditions such as day, night, seasons, and ii) contains ground truth local correspondences between matching images. These correspondences are often recovered from the dense depth <ref type="bibr" target="#b37">[38]</ref> computed from an SfM model <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b49">49]</ref>, which is intractable to build at the scale required by image retrieval.</p><p>Data augmentation. Self-supervised methods that do not rely on correspondences, such as SuperPoint, require heavy data augmentation, which is key to the invariance of the local descriptor. While data augmentation often captures well the variations in the real world at the local level, it can break the global consistency of the image and make the learning of the global descriptor very challenging.</p><p>Multi-task distillation is our solution to this data problem. We employ distillation to learn the representation directly from an off-the-shelf trained teacher model. This alleviates the above issues, with a simpler and more flexible training setup that allows the use of arbitrary datasets, as infinite amount of labeled data can be obtained from the inference of the teacher network. Directly learning to predict the output of the teacher network additionally eases the learning task, allowing to directly train a smaller student network. We note an interesting similarity with SuperPoint, whose detector is training by bootstrapping, supervised by itself through the different training runs. This process could also be referred as self-distillation, and shows the effectiveness of distillation as a practical training scheme.</p><p>The supervision of local and global features can originate from different teacher networks, resulting in a multitask distillation training that allows to leverage state-of-theart teachers. Recent advances <ref type="bibr" target="#b22">[23]</ref> in multi-task learning enable a student s to optimally copy all teachers t 1,2,3 without any manual tuning of the weights that balance the loss:</p><formula xml:id="formula_0">L = e ?w1 ||d g s ? d g t1 || 2 2 + e ?w2 ||d l s ? d l t2 || 2 2 + 2e ?w3 CrossEntropy(p s , p t3 ) + i w i ,<label>(1)</label></formula><p>where d g and d l are global and local descriptors, p are keypoint scores, and w 1,2,3 are optimized variables. More generally, our formulation of the multi-task distillation can be applied to any application that requires multiple predictions while remaining computationally efficient, particularly in settings where ground truth data for all tasks is expensive to collect. It could also be applied to some hand-crafted descriptors deemed too compute-intensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we present experimental evaluations of the building blocks of HF-Net and of the network as a whole. We want to prove its applicability to large-scale localization problems in challenging conditions while remaining computationally tractable. We first perform in Section 5.1 a thorough evaluation of current top-performing classical and learning-based methods for local feature detection and description. Our goal is to explain how these insights influenced the design choices of HF-Net presented in Section 5.2. We then evaluate in Section 5.3 our method on challenging large-scale localization benchmarks <ref type="bibr" target="#b43">[44]</ref> and demonstrate the advantages of the coarse-to-fine localization paradigm. To address our real-time localization focus, we conclude with runtime considerations in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Local Features Evaluation</head><p>We start our evaluation by investigating the performance of local matching methods under different settings on two datasets, HPatches <ref type="bibr" target="#b3">[4]</ref> and SfM <ref type="bibr" target="#b37">[38]</ref>, that provide dense ground truth correspondences between image pairs for both 2D and 3D scenes.</p><p>Datasets. HPatches <ref type="bibr" target="#b3">[4]</ref> contains 116 planar scenes containing illumination and viewpoint changes with 5 image pairs per scene and ground truth homographies. SfM is a dataset built by <ref type="bibr" target="#b37">[38]</ref> composed of photo-tourism collections collected by <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b53">53]</ref>. Ground truth correspondences are obtained from dense per-image depth maps and relative 6-DoF poses, computed using COLMAP <ref type="bibr" target="#b47">[47]</ref>. We select 10 sequences for our evaluation and for each randomly sample 50 image pairs with a given minimum overlap. A metric scale cannot be recovered with SfM reconstruction but is important to compute localization metrics. We therefore manually label each SfM model using metric distances measured in Google Maps.</p><p>Metrics. We compute and aggregate pairwise metrics defined by <ref type="bibr" target="#b13">[14]</ref> over all pairs for each dataset. For the detectors, we report the repeatability and localization error of the keypoint locations. Both are important for visual localization as they can impact the number of inlier matches, the reliability of the matches, but also the quality of the 3D model. We compute nearest neighbor matches between descriptors and report the mean average precision and the matching score. The former reflects the ability of the method to reject spurious matches. The latter assesses the quality of the detector and the descriptor together. We also compute the recall of pose estimation, either a homography for HPatches or a 6-DoF pose for the SfM dataset, with thresholds of 3 pixels and 3 meters, respectively.</p><p>Methods. We evaluate the classical detectors Difference of Gaussian (DoG) and Harris <ref type="bibr" target="#b16">[17]</ref> and the descriptor Root-SIFT <ref type="bibr" target="#b2">[3]</ref>. For the learning-based methods, we evaluate the detections and descriptors of SuperPoint <ref type="bibr" target="#b13">[14]</ref> and LF-Net <ref type="bibr" target="#b11">[12]</ref>. We additionally evaluate a dense version of DOAP <ref type="bibr" target="#b17">[18]</ref> and the feature map conv3_3 of NetVLAD <ref type="bibr" target="#b1">[2]</ref> and use SuperPoint detections for both. More details are provided in the appendix.</p><p>Detectors. We report the results in <ref type="table" target="#tab_1">Table 1</ref>. Harris exhibits the highest repeatability but also the highest localization error. Conversely, DoG is less repeatable but has the lowest error, likely due to the multi-scale detection and pixel refinement. SuperPoint seems to show the best trade-off between repeatability and error. Descriptors. DOAP outperforms SuperPoint on all metrics on the SfM dataset, but cannot be evaluated on HPatches as it was trained on this dataset. NetVLAD shows good pose estimation but poor matching precision on SfM, which is disadvantageous when the number of keypoints is limited or the inlier ratio important, e.g. for localization. Overall, it stands that learned features outperform hand-crafted ones. Interestingly, SuperPoint descriptors perform poorly when extracted from Harris detections, although the latter is also a corner detector with high repeatability. This hints that learned descriptors can be highly coupled with the corresponding detections.</p><p>LF-Net and SIFT, both multi-scale approaches with subpixel detection and patch-based description, are outperformed by dense descriptors like DOAP and SuperPoint. A simple representation trained with the right supervision can thus be more effective than a complex and computationalheavy architecture. We note that SuperPoint requires significantly fewer keypoints to estimate a decent pose, which is highly beneficial for runtime-sensitive applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Motivated by the results presented in Section 5.1, this section briefly introduces the design and implementation of HF-Net. Below, we explain our choices of the distillation teacher models, training datasets and improvements to the baseline 2D-3D local matching.</p><p>Teacher models. We evaluate the impact of the two best descriptors, DOAP and SuperPoint, on the localization in Section 5.3. Results show that the latter is more robust to day-night appearance variations, as its training set included low-light data. We eventually chose it as the supervisor teacher network for the descriptor head of HF-Net. The global head is supervised by NetVLAD.</p><p>Training data. In this work, we target urban environments in both day and night conditions. To maximize the performance of the student model on this data, we select training data that fits this distribution. We thus train on 185k images from the Google Landmarks dataset <ref type="bibr" target="#b35">[36]</ref>, containing a wide variety of day-time urban scenes, and 37k images from the night and dawn sequences of the Berkeley Deep Drive dataset <ref type="bibr" target="#b58">[58]</ref>, composed of road scenes with motion blur. We found the inclusion of night images in the training dataset to be critical for the generalization of the global retrieval head to night queries. For example, a network trained on day-time images only would easily confuse a night-time dark sky with a day-time dark tree. We also train with photometric data augmentation but use the targets predicted on the clean images.</p><p>Efficient hierarchical localization. Sarlin et al. <ref type="bibr" target="#b41">[42]</ref> identified the local 2D-3D matching as the bottleneck of the pipeline. Our system significantly improves on the efficiency of their approach: i) Spurious local matches are filtered out using a modified ratio test that only applies if the first and second nearest neighbor descriptors correspond to observations of different 3D points, similarly to <ref type="bibr" target="#b34">[35]</ref>, thus retaining more matches in highly covisible areas. ii) Learned global and local descriptors are normalized and matched with a single matrix multiplication on GPU. Additional implementation details and hyperparameters are provided in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Large-scale Localization</head><p>Under the light of the local evaluation, we now evaluate our hierarchical localization on three challenging largescale benchmarks introduced by <ref type="bibr" target="#b43">[44]</ref>.</p><p>Datasets. Each dataset is composed of a sparse SfM model built with a set of reference images. The Aachen Day-Night dataset <ref type="bibr" target="#b45">[45]</ref> contains 4,328 day-time database images from a European old town, and 824 and 98 queries taken in day and night conditions respectively. The RobotCar Seasons dataset <ref type="bibr" target="#b29">[30]</ref> is a long-term urban road dataset that spans multiple city blocks. It is composed of 20,862 overcast reference images and a total of 11,934 query images taken in multiple conditions, such as sun, dusk, and night. Lastly, the CMU Seasons dataset <ref type="bibr" target="#b4">[5]</ref> was recorded in urban and suburban environments over a course of 8.5 km. It contains 7,159 reference images and 75,335 query images recorded in different seasons. This dataset is of significantly lower scale as the queries are localized against isolated submodels containing around 400 images each.</p><p>Large scale model construction. SfM models built with COLMAP <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b49">49]</ref> using RootSIFT are provided by the dataset authors. These are however not suitable when localizing with methods based on different feature detectors. We thus build new 3D models with keypoints detected by Su-perPoint and HF-Net. The process is as follows: i) we perform 2D-2D matching between reference frames using our features and an initial filtering ratio test; ii) the matches are further filtered within COLMAP using two-view geometry; iii) 3D points are triangulated using the provided ground truth reference poses. Those steps result in a 3D model with the same scale and reference frame as the original one.</p><p>Comparison of model quality. The HF-Net Aachen model contains fewer 3D points (685k vs 1,899k for SIFT) and fewer 2D keypoints per image (2,576 vs 10,230 for SIFT). However, a larger ratio of the original 2D keypoints is matched (33.8% vs 18.8% for SIFT), and each 3D point is on average observed from more reference images. Matching a query keypoint against this model is thus more likely to succeed, showing that our feature network produces 3D models more suitable for localization.</p><p>Methods. We first evaluate our hierarchical localization based on learned features extracted by NetVLAD <ref type="bibr" target="#b1">[2]</ref> and SuperPoint <ref type="bibr" target="#b13">[14]</ref>. Named NV+SP, it uses the most powerful predictors available. We then evaluate a more efficient localization with global descriptors and local features computed by HF-Net. We also consider several localization baselines evaluated by the benchmark authors. Active Search (AS) <ref type="bibr" target="#b42">[43]</ref> and City Scale Localization (CSL) <ref type="bibr" target="#b51">[51]</ref> are both 2D-3D direct matching methods representing the current state-of-the-art in terms of accuracy. Den-seVLAD <ref type="bibr" target="#b56">[56]</ref> and NetVLAD <ref type="bibr" target="#b1">[2]</ref> are image retrieval approaches that approximate the pose of the query by the pose of the top retrieved database image. The recentlyintroduced Semantic Match Consistency (SMC) <ref type="bibr" target="#b55">[55]</ref> relies on semantic segmentation for outlier rejection. It assumes known gravity direction and camera height and, for the RobotCar dataset, was trained on the evaluation data using ground truth semantic labels. We introduce an additional baseline, NV+SIFT, that performs hierarchical localization with RootSIFT as local features, and is an upper bound to the MNV+SIFT method of <ref type="bibr" target="#b41">[42]</ref>.</p><p>Results. We report the pose recall at position and orientation thresholds different for each sequence, as defined by the benchmark <ref type="bibr" target="#b43">[44]</ref>. <ref type="table">Table 3</ref> shows the localization results for the different methods. Cumulative plots for the three most challenging sequences are presented in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>Localization with NV+SP. On the Aachen dataset, NV+SP is competitive on day-time queries and outperforms all methods for night-time queries, where the performance drop w.r.t. the day is significantly smaller than for direct matching methods, which suffer from the increased ambiguity of the matches. On the RobotCar dataset, it performs similarly to other methods on the dusk sequence, where the accuracy tends to saturate. In the more challenging sequences, image retrieval methods tend to work better than direct matching approaches, but are far outperformed by  <ref type="table">Table 3</ref>. Evaluation of the localization on the Aachen Day-Night, RobotCar Seasons, and CMU Seasons datasets. We report the recall [%] at different distance and orientation thresholds and highlight for each of them the best and second-best methods. X+Y denotes hierarchical localization with X (Y) as global (local) descriptors. SMC is excluded from the comparison for RobotCar as it uses extra semantic data. NV+SP in both fine-and coarse-precision regimes. On the difficult CMU dataset, NV+SP achieves an outstanding robustness compared to all baselines, including the most recent SMC. Overall, NV+SP sets a new state-of-the-art on the CMU dataset and on the challenging sequences of the Aachen and RobotCar datasets. The superior performance in both fine-and coarse-precision regimes shows that our approach is both more accurate and more robust.</p><p>Comparison with NV+SIFT. We observe that NV+SIFT consistently outperforms AS and CSL, although all methods are based on the same RootSIFT features. This shows that our hierarchical approach with a coarse initial prior brings significant benefits, especially in challenging conditions where image-wide information helps disambiguate matches. It thus provides a better outlier rejection than complex domain-specific heuristics used in AS and CSL. The superiority of NV+SP highlights the simple gain of learned features like SuperPoint. On the Aachen night and Robot-Car dusk sequences, which are the easiest ones, NV+SIFT performs marginally better than NV+SP for the fine threshold. This is likely due to the lower localization accuracy of the SuperPoint keypoints, as highlighted in Section 5.1, since DoG performs a subpixel refinement.</p><p>Localization with HF-Net. On most sequences, HF-Net performs similarly to its upper bound NV+SP, with a recall drop of 2.6% on average. We show qualitative results in <ref type="figure" target="#fig_3">Fig-ure 5</ref> and in the appendix. In the RobotCar night sequences, HF-Net is significantly worse than NV+SP. We attribute this to the poor performance of the distilled global descriptors on blurry low-quality images. This highlights a clear limitation of our approach: on large, self-similar environment, the model capacity of HF-Net becomes the limiting factor. A complete failure of the global retrieval directly translates into a failure of the hierarchical localization. Ablation study. In <ref type="table">Table 4</ref>, we evaluate the influence of different predictors within the hierarchical localization framework. Comparing NV+SP with NV+HF, we note that local HF-Net features perform better than the SuperPoint model that was used to train them. This demonstrates the benefits of multi-task distillation, where the supervision signal from the global teacher can improve intermediate features and help local descriptors. We also observe that the localization with DOAP is significantly worse at night, which might be due to the complex augmentation schemes Su-perPoint is based on. Finally, the comparison of HF-Net with NV+HF-Net reveals that HF-Net global descriptors have a somewhat limited capacity compared to the original NetVLAD and are limiting the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Runtime Evaluation</head><p>As our propose localization solution was developed keeping the computational constraints in mind, we analyze its runtime and compare it with baselines presented in Section 5.3. These were measured on a PC equipped with an Intel Core i7-7820X CPU (3.60GHz) CPU, 32GB of RAM and an NVIDIA GeForce GTX 1080 GPU. <ref type="table">Table 5</ref>   <ref type="table">Table 5</ref>. Timings [ms] for the different steps of hierarchical localization: feature extraction, global search, covisibility clustering, local matching, and pose estimation with PnP. Feature extraction with SIFT or CNN and matching of learned descriptors are performed on the GPU, and other operations on the CPU. We highlight the fastest method for each sequence. Localizing with HF-Net is 10 times faster than with AS, the fastest method available.</p><p>Hierarchical localization. Timings of NV+SP and HF-Net show that our coarse-to-fine approach scales well to large environments. The global search is fast, and only depends on the number of images used to build the model. It successfully reduces the set of potential candidate correspondences and enables a tractable 2D-3D matching. This strongly depends on the SfM model -the denser the covisibility graph is, the more 3D points are retrieved and matched per prior frame, which increases the runtime. NV+SIFT is therefore prohibitively slow, as its SfM model is much denser, especially on Aachen. NV+SP significantly improves on it, as the sparser SfM model yield clusters with fewer 3D points. The inference of NetVLAD and SuperPoint however accounts for 75% of its runtime, and is thus, as previously mentioned, the bottleneck. HF-Net mitigates this issues with an inference 7 times faster.</p><p>Existing approaches. CSL and SMC are not listed in <ref type="table">Table 5</ref> as they both require several tens of seconds per query, and are thus three orders of magnitude slower than our fastest method. AS improves on this, but is still slower, especially in case of a low success rate, such as on Robot-Car night. Overall, our localization system based on HF-Net can run at 20 FPS on very large-scale environments. It is 10 times faster than AS, designed for efficiency, and is much more accurate on all datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we have presented a method for visual localization that is at the same time robust, accurate, and runs in real-time. Our system follows a coarse-to-fine localization paradigm. First, it performs a global image retrieval to obtain a set of database images, which are subsequently clustered into places using the covisibility graph of a 3D SfM model. We then perform local 2D-3D matching within the candidate places to obtain an accurate 6-DoF estimate of the camera pose.</p><p>A version of our method is based on existing neural networks for image retrieval and feature matching. It outperforms state-of-the-art localization approaches on several large-scale benchmarks that include day-night queries and substantial appearance variations across weather conditions and seasons. We then improve its efficiency by proposing HF-Net, a novel CNN that computes keypoints as well as global and local descriptors in a single shot. We demonstrate the effectiveness of multitask distillation to train it in a flexible manner while retaining the original performance. The resulting localization systems runs at more than 20 FPS at large scale and offers an unparalleled robustness in challenging conditions.</p><p>A. HF-Net Implementation A.1. Network Architecture HF-Net is built on top of a MobileNetV2 <ref type="bibr" target="#b40">[41]</ref> encoder with depth multiplier 0.75. The local heads are identical to the original SuperPoint <ref type="bibr" target="#b13">[14]</ref> and branch off at the layer 7. The global head is composed of a NetVLAD layer <ref type="bibr" target="#b1">[2]</ref> and a dimensionality reduction, implemented as a multiplication with a learnable matrix, in order to match the dimension of the target teacher descriptor. The global head is appended to the MobileNet layer 18. The detailed architecture is shown in <ref type="figure">Figure 6</ref>.  <ref type="figure">Figure 6</ref>. Detail of the HF-Net architecture, consisting of a Mo-biletNet encoder and three heads predicting a global descriptor, a dense local descriptor map, and keypoint scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Training details</head><p>The images from both Google Landmarks <ref type="bibr" target="#b35">[36]</ref> and Berkeley Deep Drive <ref type="bibr" target="#b58">[58]</ref> are resized to 640 ? 480 and converted to grayscale. We found RGB to be detrimental to the performance of the local feature heads, most likely because of the limited bandwidth of the encoder. As photometric data augmentation, we apply, in a random order, Gaussian noise, motion blur in random directions, and random brightness and contrast changes.</p><p>The losses of the global and local descriptors are the L2 distances with their targets. For the keypoints, we apply the cross-entropy with the target probabilities (soft labels). We found hard labels to perform poorly, likely due to their sparsity and the smaller size of the student network. The three losses are aggregated using the multi-task learning scheme of Kendall et al. <ref type="bibr" target="#b22">[23]</ref>.</p><p>The MobileNet layers are initialized with weights pretrained on ImageNet <ref type="bibr" target="#b12">[13]</ref>. The network is implemented with Tensorflow <ref type="bibr" target="#b0">[1]</ref> and trained for 85k iterations with the RMSProp optimizer <ref type="bibr" target="#b54">[54]</ref> and a batch size of 32. We use an initial learning rate of 10 ?3 , which is successively divided by ten at iterations 60k and 80k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Local Feature Evaluation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Setup</head><p>The images of both HPatches <ref type="bibr" target="#b3">[4]</ref> and SfM <ref type="bibr" target="#b37">[38]</ref> datasets are resized so that their largest dimension is 640 pixels. The metrics are computed on image pairs and follow the definitions of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>. A keypoint k 1 in an image is deemed correct if its reprojectionk 1 in a second image lies within a given distance threshold to a second detected keypoint k 2 . Additionally, k 1 is matched correctly if it is correct and if k 2 is its nearest neighbor in the descriptor space.</p><p>For HPatches, we detect 300 keypoints for both keypoint and descriptor evaluations, and set = 3 pixels. The homography is estimated using the OpenCV function findHomography and considered accurate if the average reprojection error of the image corners is lower than 3 pixels. For the SfM dataset, due to the extensive texture, 1000 keypoints are detected. The keypoint and descriptor metrics use correctness thresholds of 3 and 5, respectively. The 6-DoF pose is estimated with the function solvePnPRansac, and deemed correct if its ground truth is within distance and orientation thresholds of 3 m and 1 ? , respectively.</p><p>For DoG, Harris <ref type="bibr" target="#b16">[17]</ref>, and SIFT <ref type="bibr" target="#b27">[28]</ref>, we use the implementations of OpenCV. For SuperPoint <ref type="bibr" target="#b13">[14]</ref> and LF-Net <ref type="bibr" target="#b37">[38]</ref>, we use the implementations provided by the authors. For NetVLAD, we use the implementation of <ref type="bibr" target="#b11">[12]</ref> and the original model trained on Pittsburgh30k. Dense descriptors are obtained by normalizing the feature map conv3_3 before the ReLU activation. For DOAP <ref type="bibr" target="#b17">[18]</ref>, we use the trained model provided by the authors. As we are mostly interested in dense descriptors for run-time efficiency, we disable the spatial transformer and enable padding in the last layer, thus producing a feature map four times smaller than the input image. We found the model trained on HPatches with spatial transformer to give the best results and thus only evaluate DOAP on the SfM dataset. As a post-processing, we apply Non-Maximum Suppression (NMS) with a radius of 4 to both Harris and Super-Point. Sparse descriptors are sampled from the dense maps of SuperPoint, NetVLAD, and DOAP using bilinear interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Qualitative Results</head><p>We show in <ref type="figure" target="#fig_4">Figures 7 and 8</ref>   <ref type="table">Table 6</ref>. We also report the track length, i.e. the number of observation per 3D point, as defined by <ref type="bibr" target="#b48">[48]</ref>. The metrics for the CMU dataset are aggregated over the models of the slices corresponding to the urban and suburban environments. For SIFT, some metrics cannot be computed on the CMU model as the keypoints that are not matched were not provided.  <ref type="table">Table 6</ref>. Statistics of 3D models built with SIFT and HF-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Implementation Details</head><p>We now provide additional details regarding the implementation of our hierarchical localization pipeline. For all datasets, we reduce the dimensionality of the global descriptors predicted by both NetVLAD and HF-Net to 1024 dimensions using PCA, whose parameters are learned on the reference images, independently for each dataset. A total of 10 prior frames are retrieved and clustered. Due to limits on the GPU memory, features are extracted on images downsampled such that their largest dimension is 960 pixels for Aachen and Robotcar, and 1024 for CMU. For both SuperPoint and HF-Net, NMS with radius 4 is applied to the detected keypoints in the query image and 2k of them are retained. When performing local matching, our modified ratio test uses a threshold of 0.9. PnP+RANSAC uses a threshold on the reprojection error of 10 pixels for Aachen, 5 pixels for CMU (due to the lower image size), and 12 pixels for RobotCar (due to the lower keypoint localization accuracy of SuperPoint and HF-Net). The estimated pose is deemed correct when the number of inliers is larger than a threshold, whose value is 12 for Aachen and CMU, and 15 for Robotcar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Evaluation Process</head><p>The method and baselines introduced in this work are evaluated on all three datasets by the benchmark's authors <ref type="bibr" target="#b43">[44]</ref>, who also generated the plots shown in the main paper. For Active Search <ref type="bibr" target="#b42">[43]</ref>, City Scale Localization <ref type="bibr" target="#b51">[51]</ref>, DenseVLAD <ref type="bibr" target="#b56">[56]</ref>, and NetVLAD <ref type="bibr" target="#b1">[2]</ref>, we use the evaluation reported in the paper introducing the benchmark.</p><p>The evaluation of Semantic Match Consistency <ref type="bibr" target="#b55">[55]</ref> (SMC) is the one reported in the original paper. We do not directly compare this method to the ones introduced in the present work, nor to the benchmark baselines, as SMC assumes a known camera height, and, more importantly, relies on a semantic segmentation CNN which was trained on the evaluation dataset of RobotCar. We emphasize that our HF-Net never encountered any test data during training, and that it was evaluated on the three datasets using the same trained model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4. Qualitative Results</head><p>Visual results of HF-Net on the Aachen Day-Night, RobotCar Seasons, and CMU Seasons datasets are shown in Figures 9, 10, and 11, respectively. We additionally show a comparison with NV+SIFT in <ref type="figure" target="#fig_0">Figure 12</ref>.   . Localization with HF-Net on Aachen night. For each image pair, the left image is the query and the right image is the retrieved database image with the most inlier matches, as returned by PnP+RANSAC. We show challenging successful queries (left), failed queries due to an incorrect global retrieval (center), and failed queries due to incorrect or insufficient local matches (right). <ref type="figure" target="#fig_0">Figure 10</ref>. Localization with HF-Net on RobotCar night and night-rain. For each image pair, the left image is the query and the right image is the retrieved database image with the most inlier matches, as returned by PnP+RANSAC. We show challenging successful queries (left), failed queries due to an incorrect global retrieval (center), and failed queries due to insufficient local matches (right). <ref type="figure" target="#fig_0">Figure 11</ref>. Localization with HF-Net on CMU suburban. For each image pair, the left image is the query and the right image is the retrieved database image with the most inlier matches, as returned by PnP+RANSAC. We show challenging successful queries (left), failed queries due to an incorrect global retrieval (center), and failed queries due to insufficient local matches (right). <ref type="figure" target="#fig_0">Figure 12</ref>. Comparison between HF-Net and NV+SIFT on Aachen night, with one query for which HF-Net returns the correct location but NV+SIFT fails. We show the matches with one retrieved database image, labeled by PnP+RANSAC as inliers (green) and outliers (red). We show the inliers of HF-Net (left), all the matches of HF-Net (center), and all the matches of NV+SIFT (right). HF-Net generates significantly fewer matches than SIFT, thus reducing the computational footprint of the local matching. At the same time, more of its matches are inliers, increasing the robustness of the localization. The higher inlier ratio reduces the number of required RANSAC iterations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Hierarchical localization. A global search first retrieves candidate images, which are subsequently matched using powerful local features to estimate an accurate 6-DoF pose. This two-step process is both efficient and robust in challenging situations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Cumulative distribution of position errors for the Aachen night (left), RobotCar night-all (center) and CMU suburban (right) datasets. On Aachen, HF-Net and NV+SP have similar performance and outperform approaches based on global retrieval and on feature matching. On RobotCar, HF+Net performs worse than NV+SP, which suggests a limitation of the distillation process. On CMU, the hierarchical localization shows a significant boost over other methods, particularly for small distance thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Successful localization with HF-Net on the Aachen Day-Night dataset. We show two queries (left) and the retrieved database images with the most inlier matches (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results on the HPatches dataset. Keypoints (green if repeatable, red if not repeatable, blue if not visible in the other image) and inlier matches are shown for SIFT (left), SuperPoint (center) and HF-Net (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results on the SfM dataset for SIFT (left), SuperPoint (center) and HF-Net (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9</head><label>9</label><figDesc>Figure 9. Localization with HF-Net on Aachen night. For each image pair, the left image is the query and the right image is the retrieved database image with the most inlier matches, as returned by PnP+RANSAC. We show challenging successful queries (left), failed queries due to an incorrect global retrieval (center), and failed queries due to incorrect or insufficient local matches (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>HPatches SfM Rep. MLE Rep. MLE DoG 0.307 0.94 0.284 1.20 Harris 0.535 1.14 0.510 1.46 SuperPoint 0.495 1.04 0.509 1.45 LF-Net 0.460 1.13 0.454 1.44 Evaluation of the keypoint detectors. We report the repeatability (rep.) and mean localization error (MLE).</figDesc><table><row><cell></cell><cell cols="2">HPatches</cell><cell></cell><cell>SfM</cell></row><row><cell cols="5">(detector / descriptors) Homography MS mAP Pose MS mAP</cell></row><row><cell>Root-SIFT</cell><cell>0.681</cell><cell cols="3">0.307 0.651 0.700 0.199 0.236</cell></row><row><cell>LF-Net</cell><cell>0.629</cell><cell cols="3">0.305 0.572 0.676 0.221 0.207</cell></row><row><cell>SuperPoint</cell><cell>0.810</cell><cell cols="3">0.441 0.846 0.794 0.418 0.488</cell></row><row><cell>Harris / SuperPoint</cell><cell>0.669</cell><cell cols="3">0.448 0.737 0.684 0.404 0.397</cell></row><row><cell>SuperPoint / DOAP</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.838 0.448 0.554</cell></row><row><cell>SuperPoint / NetVLAD</cell><cell>0.788</cell><cell cols="3">0.419 0.798 0.800 0.374 0.423</cell></row></table><note>Table 2. Evaluation of the local descriptors. The matching score (MS) and mean Average Precision (mAP) are reported, in addition to the homography correctness for HPatches and the pose accuracy for the SfM dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>/ 83.7 / 96.6 19.4 / 30.6 / 43.9 44.7 / 74.6 / 95.9 25.0 / 46.5 / 69.1 0.5 / 1.1 / 3.4 1.4 / 3.0 / 5.2 55.2 / 60.3 / 65.1 20.7 / 25.9 / 29.9 CSL 52.3 / 80.0 / 94.3 24.5 / 33.7 / 49.0 56.6 / 82.7 / 95.9 28.0 / 47.0 / 70.4 0.2 / 0.9 / 5.3 0.9 / 4.3 / 9.1 36.7 / 42.0 / 53.1 8.6 / 11.7 / 21.1 DenseVLAD 0.0 / 0.1 / 22.8 0.0 / 2.0 / 14.3 10.2 / 38.8 / 94.2 5.7 / 16.3 / 80.2 0.9 / 3.4 / 19.9 1.1 / 5.5 / 25.5 22.2 / 48.7 / 92.8 9.9 / 26.6 / 85.2 NetVLAD 0.0 / 0.2 / 18.9 0.0 / 2.0 / 12.2 7.4 / 29.7 / 92.9 5.7 / 16.5 / 86.7 0.2 / 1.8 / 15.5 0.5 / 2.7 / 16.4 17.4 / 40.3 / 93.2 7.7 / 21.0 / 80./ 88.1 / 93.1 30.6 / 43.9 / 58.2 55.6 / 83.5 / 95.3 46.3 / 67.4 / 90.9 4.1 / 9.1 / 24.4 2.3 / 10.2 / 20.5 63.9 / 71.9 / 92.8 28.7 / 39.0 / 82.1 NV+SP (ours) 79.7 / 88.0 / 93.7 40.8 / 56.1 / 74.5 54.8 / 83.0 / 96.2 51.7 / 73.9 / 92.4 6.6 / 17.1 / 32.2 5.2 / 17.0 / 26.6 91.7 / 94.6 / 97.7 74.6 / 81.6 / 91.4 HF-Net (ours) 75.7 / 84.3 / 90.9 40.8 / 55.1 / 72.4 53.9 / 81.5 / 94.2 48.5 / 69.1 / 85.7 2.7 / 6.6 / 15.8 4.7 / 16.8 / 21.8 90.4 / 93.1 / 96.1 71.8 / 78.2 / 87.1</figDesc><table><row><cell></cell><cell cols="2">Aachen</cell><cell></cell><cell cols="2">RobotCar</cell><cell></cell><cell>CMU</cell><cell></cell></row><row><cell></cell><cell>day</cell><cell>night</cell><cell>dusk</cell><cell>sun</cell><cell>night</cell><cell>night-rain</cell><cell>urban</cell><cell>suburban</cell></row><row><cell>distance [m]</cell><cell>.25/.50/5.0</cell><cell>0.5/1.0/5.0</cell><cell>.25/.50/5.0</cell><cell>.25/.50/5.0</cell><cell>.25/.50/5.0</cell><cell>.25/.50/5.0</cell><cell>.25/.50/5.0</cell><cell>.25/.50/5.0</cell></row><row><cell>orient. [deg]</cell><cell>2/5/10</cell><cell>2/5/10</cell><cell>2/5/10</cell><cell>2/5/10</cell><cell>2/5/10</cell><cell>2/5/10</cell><cell>2/5/10</cell><cell>2/5/10</cell></row><row><cell>AS</cell><cell cols="8">57.3 5</cell></row><row><cell>SMC</cell><cell>-</cell><cell>-</cell><cell cols="6">(53.8 / 83.0 / 97.7) (46.7 / 74.6 / 95.9) (6.2 / 18.5 / 44.3) (8.0 / 26.4 / 46.4) 75.0 / 82.1 / 87.8 44.0 / 53.6 / 63.7</cell></row><row><cell>NV+SIFT</cell><cell>82.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>detected keypoints and their corresponding matches on the HPatches and SfM datasets, respectively.</figDesc><table><row><cell>C. Large-scale Localization</cell></row><row><cell>C.1. Model Quality</cell></row><row><cell>Extended statistics of models built with SIFT and HF-</cell></row><row><cell>Net for the Aachen Day-Night, RobotCar Seasons, and</cell></row><row><cell>CMU Seasons datasets, are provided in</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank the reviewers for their valuable comments, Torsten Sattler for helping to evaluate the localization, and Eduard Trulls for providing support for the SfM dataset.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We provide here additional experiment details and qualitative results.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: a system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mart?n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="265" to="283" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petr</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HPatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding how camera configuration and environmental conditions affect appearance-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aayush</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hernan</forename><surname>Badino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles (IV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to localize using a lidar intensity map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Ioan Andrei Barsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Geometry-aware learning of maps for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Vizard: Reliable visual localization for autonomous vehicles in urban outdoor environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>B?rki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Schaupp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Dymczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Dub?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesar</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04343</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BRIEF: Binary robust independent elementary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Calonder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Strecha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">GradNorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Universal Correspondence Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Siddharth Choudhary, and Davide Scaramuzza. Data-efficient decentralized visual SLAM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titus</forename><surname>Cieslewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SuperPoint: Self-supervised interest point detection and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning for Visual SLAM at CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical metric learning and matching for 2d and 3d geometric correspondences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><forename type="middle">E</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc-Huy</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Zeeshan</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Vernaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert C</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A combined corner and edge detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Alvey vision conference</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Local descriptors optimized for average precision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Reconstructing the World in Six Days</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Heinly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrique</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">From structure-from-motion point clouds to fast location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold</forename><surname>Irschara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel tracking and mapping for small ar workspaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Mixed and Augmented Reality</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A novel parametrization of the perspective-three-point problem for a direct computation of absolute camera position and orientation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Kneip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">BRISK: Binary robust invariant scalable keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Leutenegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margarita</forename><surname>Chli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><forename type="middle">Y</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient global 2d-3d matching for camera localization in a large-scale 3d map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">10</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Get out of my lab: Large-scale, real-time visual-inertial localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lynen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">A</forename><surname>Hesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RSS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Chris Linegar, and Paul Newman. 1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pascoe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Shady dealings: Robust, long-term visual localisation using illumination invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Mcmanus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Churchill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">D</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Scalable 6-DOF localization on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sven</forename><surname>Middelberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Untzelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seqslam: Visual route-based navigation for sunny summer days and stormy winter nights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gordon</forename><forename type="middle">F</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wyeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiia</forename><surname>Mishchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mods: Fast and robust method for two-view matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with attentive deep local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andre</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Self-supervised learning of geometrically stable features through probabilistic introspection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Novotny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">LF-Net: Learning local features from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Ono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang Moo</forename><surname>Yi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Neighbourhood consensus networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">MobileNetV2: inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Leveraging deep visual descriptors for hierarchical efficient localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Sarlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Debraine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dymczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning (CoRL)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient &amp; effective prioritized matching for large-scale image-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Safari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<imprint>
			<publisher>Fredrik Kahl</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Benchmarking 6DOF outdoor visual localization in changing conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Image retrieval for image-based localization revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leif</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Understanding the limitations of cnn-based absolute camera pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qunjie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taixe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Comparative evaluation of hand-crafted and learned local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pixelwise view selection for unstructured multi-view stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lutz Sch?nberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Multi-task learning as multi-objective optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fredrik Kahl, and Magnus Oskarsson. City-scale localization for cameras with known vertical direction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linus</forename><surname>Sv?rm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olof</forename><surname>Enqvist</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">InLoc: Indoor visual localization with dense matching and view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hajime</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.01817</idno>
		<title level="m">The new data in multimedia research</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="26" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Semantic match consistency for long-term visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Toft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Stenborg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Hammarstrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Brynte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><surname>Kahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiko</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masatoshi</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">PlaNet -photo geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Bdd100k: A diverse driving video database with scalable annotation tooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vashisht</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04687</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
