<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><forename type="middle">Bok</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seanie</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woo</forename><forename type="middle">Tae</forename><surname>Jeong</surname></persName>
							<email>wtjeong@42maru.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghwan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
							<email>sjhwang82@kaist.ac.kr</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaist</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inc</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">South</forename><surname>Korea</surname></persName>
						</author>
						<title level="a" type="main">Generating Diverse and Consistent QA pairs from Contexts with Information-Maximizing Hierarchical Conditional VAEs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the most crucial challenges in question answering (QA) is the scarcity of labeled data, since it is costly to obtain question-answer (QA) pairs for a target text domain with human annotation. An alternative approach to tackle the problem is to use automatically generated QA pairs from either the problem context or from large amount of unstructured texts (e.g. Wikipedia). In this work, we propose a hierarchical conditional variational autoencoder (HCVAE) for generating QA pairs given unstructured texts as contexts, while maximizing the mutual information between generated QA pairs to ensure their consistency. We validate our Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE) on several benchmark datasets by evaluating the performance of the QA model (BERT-base) using only the generated QA pairs (QA-based evaluation) or by using both the generated and human-labeled pairs (semisupervised learning) for training, against stateof-the-art baseline models. The results show that our model obtains impressive performance gains over all baselines on both tasks, using only a fraction of data for training. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Extractive Question Answering (QA) is one of the most fundamental and important tasks for natural language understanding. Thanks to the increased complexity of deep neural networks and use of knowledge transfer from the language models pretrained on large-scale corpora <ref type="bibr">(Peters et al., 2018;</ref><ref type="bibr" target="#b0">Devlin et al., 2019;</ref><ref type="bibr">Dong et al., 2019)</ref>, the stateof-the-art QA models have achieved human-level performance on several benchmark datasets <ref type="bibr">(Rajpurkar et al., 2016</ref><ref type="bibr">(Rajpurkar et al., , 2018</ref>. However, what is also * Equal contribution 1 The generated QA pairs and the code can be found at https://github.com/seanie12/Info-HCVAE Paragraph (Input) Philadelphia has more murals than any other u.s. city, thanks in part to the 1984 creation of the department of recreation's mural arts program, . . . The program has funded more than 2,800 murals Q1 which city has more murals than any other city? A1 philadelphia Q2 why philadelphia has more murals? A2 the 1984 creation of the department of recreation's mural arts program Q3 when did the department of recreation' s mural arts program start ? A3 1984 Q4 how many murals funded the graffiti arts program by the department of recreation? A4 more than 2,800 <ref type="table">Table 1</ref>: An example of QA pairs generated with our framework. The paragraph is an extract from Wikipedia provided by <ref type="bibr">Du and Cardie (2018)</ref>. For more examples, please see <ref type="bibr">Appendix D.</ref> crucial to the success of the recent data-driven models, is the availability of large-scale QA datasets. To deploy the state-of-the-art QA models to real-world applications, we need to construct high-quality datasets with large volumes of QA pairs to train them; however, this will be costly, requiring a massive amount of human efforts and time.</p><p>Question generation (QG), or Question-Answer pair generation (QAG), is a popular approach to overcome this data scarcity challenge. Some of the recent works resort to semi-supervised learning, by leveraging large amount of unlabeled text (e.g. Wikipedia) to generate synthetic QA pairs with the help of QG systems <ref type="bibr">(Tang et al., 2017;</ref><ref type="bibr">Yang et al., 2017;</ref><ref type="bibr">Tang et al., 2018;</ref><ref type="bibr">Sachan and Xing, 2018)</ref>. However, existing QG systems have overlooked an important point that generating QA pairs from a context consisting of unstructured texts, is essentially a one-to-many problem. Sequence-tosequence models are known to generate generic sequences <ref type="bibr">(Zhao et al., 2017a)</ref> without much variety, as they are trained with maximum likelihood estimation. This is highly suboptimal for QAG since the contexts given to the model often contain richer information that could be exploited to generate multiple QA pairs. To tackle the above issue, we propose a novel probabilistic deep generative model for QA pair generation. Specifically, our model is a hierarchical conditional variational autoencoder (HCVAE) with two separate latent spaces for question and answer conditioned on the context, where the answer latent space is additionally conditioned on the question latent space. During generation, this hierarchical conditional VAE first generates an answer given a context, and then generates a question given both the answer and the context, by sampling from both latent spaces. This probabilistic approach allows the model to generate diverse QA pairs focusing on different parts of a context at each time.</p><p>Another crucial challenge of the QG task is to ensure the consistency between a question and its corresponding answer, since they should be semantically dependent on each other such that the question is answerable from the given answer and the context. In this paper, we tackle this consistency issue by maximizing the mutual information <ref type="bibr">(Belghazi et al., 2018;</ref><ref type="bibr">Hjelm et al., 2019;</ref><ref type="bibr">Yeh and Chen, 2019)</ref> between the generated QA pairs. We empirically validate that the proposed mutual information maximization significantly improves the QA-pair consistency. Combining both the hierarchical CVAE and the InfoMax regularizer together, we propose a novel probabilistic generative QAG model which we refer to as Information Maximizing Hierarchical Conditional Variational AutoEncoder (Info-HCVAE). Our Info-HCVAE generates diverse and consistent QA pairs even from a very short context (see <ref type="table">Table 1</ref>).</p><p>But how should we quantitatively measure the quality of the generated QA pairs? Popular evaluation metrics (e.g. <ref type="bibr">BLEU (Papineni et al., 2002)</ref>, <ref type="bibr">ROUGE (Lin and Hovy, 2002)</ref>, METEOR (Banerjee and Lavie, 2005)) for text generation only tell how similar the generated QA pairs are to Ground-Truth (GT) QA pairs, and are not directly correlated with their actual quality <ref type="bibr">(Nema and Khapra, 2018;</ref><ref type="bibr">Zhang and Bansal, 2019)</ref>. Therefore, we use the QA-based Evaluation (QAE) metric proposed by <ref type="bibr">Zhang and Bansal (2019)</ref>, which measures how well the generated QA pairs match the distribution of GT QA pairs. Yet, in a semi-supervised learning setting where we already have GT labels, we need novel QA pairs that are different from GT QA pairs for the additional QA pairs to be truly effective. Thus, we propose a novel metric, Reverse QAE (R-QAE), which is low if the generated QA pairs are novel and diverse.</p><p>We experimentally validate our QAG model on SQuAD v1.1 <ref type="bibr">(Rajpurkar et al., 2016</ref><ref type="bibr">), Natural Questions (Kwiatkowski et al., 2019</ref><ref type="bibr">), and Trivi-aQA (Joshi et al., 2017</ref><ref type="bibr">) datasets, with both QAE and R-QAE using BERT-base (Devlin et al., 2019</ref> as the QA model. Our QAG model obtains high QAE and low R-QAE, largely outperforming stateof-the-art baselines using a significantly smaller number of contexts. Further experimental results for semi-supervised QA on the three datasets using the SQuAD as the labeled dataset show that our model achieves significant improvements over the state-of-the-art baseline (+2.12 on SQuAD, +5.67 on NQ, and +1.18 on Trivia QA in EM).</p><p>Our contribution is threefold:</p><p>? We propose a novel hierarchical variational framework for generating diverse QA pairs from a single context, which is, to our knowledge, the first probabilistic generative model for questionanswer pair generation (QAG). ? We propose an InfoMax regularizer which effectively enforces the consistency between the generated QA pairs, by maximizing their mutual information. This is a novel approach in resolving consistency between QA pairs for QAG. ? We evaluate our framework on several benchmark datasets by either training a new model entirely using generated QA pairs (QA-based evaluation), or use both ground-truth and generated QA pairs (semi-supervised QA). Our model achieves impressive performances on both tasks, largely outperforming existing QAG baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Question and Question-Answer Pair Generation Early works on Question Generation (QG) mostly resort to rule-based approaches <ref type="bibr">(Heilman and Smith, 2010;</ref><ref type="bibr">Lindberg et al., 2013;</ref><ref type="bibr">Labutov et al., 2015)</ref>. However, recently, encoder-decoder based neural architectures <ref type="bibr">(Du et al., 2017;</ref><ref type="bibr">Zhou et al., 2017)</ref> have gained popularity as they outperform rule-based methods. Some of them use paragraph-level information <ref type="bibr">(Du and Cardie, 2018;</ref><ref type="bibr">Song et al., 2018;</ref><ref type="bibr">Liu et al., 2019;</ref><ref type="bibr">Zhao et al., 2018;</ref><ref type="bibr">Kim et al., 2019;</ref><ref type="bibr">Sun et al., 2018)</ref> as additional information. Reinforcement learning is a popular approach to train the neural QG models, where the reward is defined as the evaluation metrics <ref type="bibr">(Song et al., 2017;</ref><ref type="bibr">Kumar et al., 2018)</ref>, or the QA accuracy/likelihood <ref type="bibr">(Yuan et al., 2017;</ref><ref type="bibr">Hosking and Riedel, 2019;</ref><ref type="bibr">Zhang and Bansal, 2019)</ref>. State-ofthe-art QG models <ref type="bibr" target="#b0">(Alberti et al., 2019;</ref><ref type="bibr">Dong et al., 2019;</ref><ref type="bibr">Chan and Fan, 2019)</ref> use pre-trained language models. Question-Answer Pair Generation (QAG) from contexts, which is our main target, is a relatively less explored topic tackled by only a few recent works <ref type="bibr">(Du and Cardie, 2018;</ref><ref type="bibr" target="#b0">Alberti et al., 2019;</ref><ref type="bibr">Dong et al., 2019)</ref>. To the best of our knowledge, we are the first to propose a probabilistic generative model for end-to-end QAG; Yao et al.</p><p>(2018) use VAE for QG, but they do not tackle QAG. Moreover, we effectively resolve the QApair consistency issue by maximizing their mutual information with an InfoMax regularizer <ref type="bibr">(Belghazi et al., 2018;</ref><ref type="bibr">Hjelm et al., 2019;</ref><ref type="bibr">Yeh and Chen, 2019)</ref>, which is another contribution of our work.</p><p>Semi-supervised QA with QG With the help of QG models, it is possible to train the QA models in a semi-supervised learning manner to obtain improved performance.  <ref type="bibr">(Serban et al., 2017;</ref><ref type="bibr">Zhao et al., 2017b;</ref><ref type="bibr">Park et al., 2018;</ref><ref type="bibr">Du et al., 2018;</ref><ref type="bibr">Qiu et al., 2019)</ref>, and machine translation <ref type="bibr">(Zhang et al., 2016;</ref><ref type="bibr">Su et al., 2018;</ref><ref type="bibr">Deng et al., 2018)</ref>. In this work, we propose a novel hierarchical conditional VAE framework with an InfoMax regularization for generating a pair of samples with high consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to generate diverse and consistent QA pairs to tackle the data scarcity challenge in the extractive QA task. Formally, given a context c which contains M tokens, c = (c 1 , . . . , c M ), we want to generate QA pairs (x, y) where x = (x 1 , . . . , x N ) is the question containing N tokens and y = (y 1 , . . . , y L ) is its corresponding answer containing L tokens. We aim to tackle the QAG task by learning the conditional joint distribution of the question and answer given the context, p(x, y|c), from which we can sample the QA pairs:</p><formula xml:id="formula_0">(x, y) ? p(x, y|c)</formula><p>We estimate p(x, y|c) with a probabilistic deep generative model, which we describe next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Conditional VAE</head><p>We propose to approximate the unknown conditional joint distribution p(x, y|c), with a variational autoencoder (VAE) framework (Kingma and Welling, 2014). However, instead of directly learning a common latent space for both question and answer, we model p(x, y|c) in a hierarchical conditional VAE framework with a separate latent space for question and answer as follows:</p><formula xml:id="formula_1">p ? (x, y|c) = zx zy p ? (x|z x , y, c)p ? (y| z x , z y , c)? p ? (z y |z x , c)p ? (z x |c)dz x</formula><p>where z x and z y are latent variables for question and answer respectively, and the p ? (z x |c) and p ? (z y |z x , c) are their conditional priors following an isotropic Gaussian distribution and a categorical distribution ( <ref type="figure" target="#fig_0">Figure 1-(a)</ref>). We decompose the latent space of question and answer, since the answer is always a finite span of context c, which can be modeled well by a categorical distribution, while a continuous latent space is a more appropriate choice for question since there could be unlimited valid questions from a single context. Moreover, we design the bi-directional dependency flow of joint distribution for QA. By leveraging hierarchical structure, we enforce the answer latent variables  to be dependent on the question latent variables in p ? (z y |z x , c) and achieve the reverse dependency by sampling question x ? p ? (x|z x , y, c). We then use a variational posterior q ? (?) to maximize the Evidence Lower Bound (ELBO) as follows (The complete derivation is provided in Appendix A):</p><formula xml:id="formula_2">log p ? (x, y|c) ? E zx?q ? (zx|x,c) [log p ? (x|z x , y, c)] + E zy?q ? (zy|zx,y,c) [log p ? (y|z y , c)] ? D KL [q ? (z y |z x , y, c)||p ? (z y |z x , c)] ? D KL [q ? (z x |x, c)||p ? (z x |c)] =: L HCVAE</formula><p>where ?, ?, and ? are the parameters of the generation, posterior, and prior network, respectively. We refer to this model as a Hierarchical Conditional Variational Autoencoder (HCVAE) framework. <ref type="figure" target="#fig_1">Figure 2</ref> shows the directed graphical model of our HCVAE. The generative process is as follows:</p><formula xml:id="formula_3">1. Sample question L.V.: z x ? p ? (z x | c) 2. Sample answer L.V.: z y ? p ? (z y | z x , c) 3. Generate an answer: y ? p ? (y | z y , c) 4. Generate a question: x ? p ? (x | z x , y, c)</formula><p>Embedding We use the pre-trained word embedding network from <ref type="bibr">BERT (Devlin et al., 2019)</ref> for posterior and prior networks, whereas the whole BERT is used as a contextualized word embedding model for the generative networks. For the answer encoding, we use a binary token type id of BERT. Specifically, we encode all context tokens as 0s, except for the tokens which are part of answer span (highlighted words of context in <ref type="figure" target="#fig_0">Figure 1</ref>-(a) or -(c)), which we encode as 1s. We then feed the sequence of the word token ids, token type ids, and position ids into the embedding layer to encode the answer-aware context. We fix all the embedding layers in HCVAE during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prior Networks</head><p>We use two different conditional prior networks p ? (z x |c), p ? (z y |z x , c) to model context-dependent priors (the dashed lines in <ref type="figure" target="#fig_0">Figure 1-(a)</ref>). To obtain the parameters of isotropic Gaussian N (?, ? 2 I) for p ? (z x |c), we use a bidirectional LSTM (Bi-LSTM) to encode the word embeddings of the context into the hidden representations, and then feed them into a Multi-Layer Perceptron (MLP). We model p ? (z y |z x , c) following a categorical distribution Cat(?), by computing the parameter ? from z x and the hidden representation of the context using another MLP.</p><p>Posterior Networks We use two conditional posterior networks q ? (z x |x, c), q ? (z y |z x , y, c) to approximate true posterior distributions of latent variables for both question x and answer y. We use two Bi-LSTM encoders to output the hidden representations of question and context given their word embeddings. Then, we feed the two hidden representations into MLP to obtain the parameters of Gaussian distribution, ? and ? (upper right corner in <ref type="figure" target="#fig_0">Figure 1-(a)</ref>). We use the reparameterization trick <ref type="bibr">(Kingma and Welling, 2014)</ref> to train the model with backpropagation since the stochastic sampling process z x ? q ? (z x |x, c) is nondifferentiable. We use another Bi-LSTM to encode the word embedding of answer-aware context into the hidden representation. Then, we feed the hidden representation and z x into MLP to compute the parameters ? of categorical distribution (lower right corner in <ref type="figure" target="#fig_0">Figure 1-(a)</ref>). We use the categorical reparameterization trick with gumbel-softmax <ref type="bibr">(Maddison et al., 2017;</ref><ref type="bibr">Jang et al., 2017)</ref> to enable backpropagation through sampled discrete latent variables. Answer Generation Networks Since we consider extractive QA, we can factorize p ? (y|z y , c) into p ? (y s |z y , c) and p ? (y e |z y , c), where y s and y e are the start and the end position of an answer span (highlighted words in <ref type="figure" target="#fig_0">Figure 1-(b)</ref>), respectively. To obtain MLE estimators for both, we first encode the context c into the contextualized word embedding of E c = {e c 1 , . . . , e c M } with the pre-trained BERT. We compute the final hidden representation of context and the latent variable z y with a heuristic matching layer (Mou et al., 2016) and a Bi-LSTM:</p><formula xml:id="formula_4">f i = [e c i ; z y ; |e c i ? z y |; e c i z y ] ? ? h i = ? ??? ? LSTM([f i , ? ? h i?1 ]) ? ? h i = ? ??? ? LSTM([f i , ? ? h i+1 ]) H = [ ? ? h i ; ? ? h i ] M i=1</formula><p>where z y is linearly transformed, and H ? R dy?M is the final hidden representation. Then, we feed H into two separate linear layers to predict y s and y e . Question Generation Networks We design the encoder-decoder architecture for our QG network by mainly adopting from our baselines <ref type="bibr">(Zhao et al., 2018;</ref><ref type="bibr">Zhang and Bansal, 2019)</ref>. For encoding, we use pre-trained BERT to encode the answer-specific context into the contextualized word embedding, and then use a two-layer Bi-LSTM to encode it into the hidden representation (in <ref type="figure" target="#fig_0">Figure 1-(c)</ref>). We apply a gated self-attention mechanism <ref type="bibr">(Wang et al., 2017)</ref> to the hidden representation to better capture long-term dependencies within the context, to obtain a new hidden representation? ? R dx?M . The decoder is a two-layered LSTM which receives the latent variable z x as an initial state. It uses an attention mechanism <ref type="bibr">(Luong et al., 2015)</ref> to dynamically aggregate? at each decoding step into a context vector of s j , using the j-th decoder hidden representation d j ? R dx (in <ref type="figure" target="#fig_0">Figure 1-(c)</ref>). Then, we feed d j and s j into MLP with maxout activation <ref type="bibr">(Goodfellow et al., 2013)</ref> to compute the final hidden representationd j as follows:</p><formula xml:id="formula_5">d 0 = z x , d j = LSTM([e x j?1 , d j?1 ]) r j =? T W a d j , a j = softmax(r j ), s j =?a ? d j = MLP([ d j ; s j ])</formula><p>where z x is linearly transformed, and e x j is the j-th question word embedding. The probability vector over the vocabulary is computed as p(x j | x &lt;j , z x , y, c) = softmax(W ed j ). We initialize the weight matrix W e as the pretrained word embedding matrix and fix it during training. Further, we use the copy mechanism <ref type="bibr">(Zhao et al., 2018)</ref>, so that the model can directly copy tokens from the context. We also greedily decode questions to ensure that all stochasticity comes from the sampling of the latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Consistent QA Pair Generation with Mutual Information Maximization</head><p>One of the most important challenges of the QAG task is enforcing consistency between the generated question and its corresponding answer. They should be semantically consistent, such that it is possible to predict the answer given the question and the context. However, neural QG or QAG models often generate questions irrelevant to the context and the answer (Zhang and Bansal, 2019) due to the lack of the mechanism enforcing this consistency. We tackle this issue by maximizing the mutual information (MI) of a generated QA pair, assuming that an answerable QA pair will have high MI.</p><p>Since an exact computation of MI is intractable, we use a neural approximation. While there exist many different approximations <ref type="bibr">(Belghazi et al., 2018;</ref><ref type="bibr">Hjelm et al., 2019)</ref>, we use the estimation proposed by Yeh and Chen (2019) based on Jensen-Shannon Divergence:</p><formula xml:id="formula_6">MI(X; Y ) ? E x,y?P [log g(x, y)] + 1 2 Ex ,y?N [log(1 ? g(x, y))] + 1 2 E x,??N [log(1 ? g(x,?))] =: L Info</formula><p>where E P and E N denote expectation over positive and negative examples. We generate negative examples by shuffling the QA pairs in the minibatch, such that a question is randomly associated with an answer. Intuitively, the function g(?) acts like a binary classifier that discriminates whether QA pair is from joint distribution or not. We empirically find that the following g(?) effectively achieves our goal of consistent QAG: In all experiments, we always set the ? as 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>Stanford Question Answering Dataset v1.1 (SQuAD) <ref type="bibr">(Rajpurkar et al., 2016)</ref>. This is a reading comprehension dataset consisting of questions obtained from crowdsourcing on a set of Wikipedia articles, where the answer to every question is a segment of text or a span from the corresponding reading passage. We use the same split used in Zhang and Bansal (2019) for the fair comparison. Natural Questions (NQ) <ref type="bibr">(Kwiatkowski et al., 2019)</ref>. This dataset contains realistic questions from actual user queries to a search engine, using Wikipedia articles as context. We adapt the dataset provided from MRQA shared task <ref type="bibr">(Fisch et al., 2019)</ref> and convert it into the extractive QA format. We split the original validation set in half, to use as validation and test for our experiments.</p><p>TriviaQA <ref type="bibr">(Joshi et al., 2017)</ref>. This is a reading comprehension dataset containing question-answerevidence triples. The QA pairs and the evidence (contexts) documents are authored and uploaded by Trivia enthusiasts. Again, we only choose QA pairs of which answers are span of contexts.</p><p>HarvestingQA 2 This dataset contains top-ranking 10K Wikipedia articles and 1M synthetic QA pairs generated from them, by the answer span extraction and QG system proposed in <ref type="bibr">(Du and Cardie, 2018)</ref>. We use this dataset for semi-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setups</head><p>Implementation Details In all experiments, we use BERT-base (d = 768) (Devlin et al., 2019) as the QA model, setting most of the hyperparameters as described in the original paper. For both HCVAE and Info-HCVAE, we set the hidden dimensionality of the Bi-LSTM to 300 for posterior, prior, and answer generation networks, and use the dimensionality of 450 and 900 for the encoder and the decoder of the question generation network. We set the dimensionality of z x as 50, and define z y to be set of 10-way categorical variables z y = {z 1 , . . . , z 20 }.</p><p>For training the QA model, we fine-tune the model for 2 epochs. We train both the QA model and Info-HCVAE with Adam optimizer <ref type="bibr">(Kingma and Ba, 2015)</ref> with the batch size of 32 and the initial learning rate of 5 ? 10 ?5 and 10 ?3 respectively. For semi-supervised learning, we first pre-train BERT on the synthetic data for 2 epochs and fine-tune it on the GT dataset for 2 epochs. To prevent posterior collapse, we multiply 0.1 to the KL divergence terms of question and answer <ref type="bibr">(Higgins et al., 2017)</ref>. For more details of the datasets and experimental setup, please see Appendix C. Baselines We experiment two variants of our model against several baselines: For the baselines, we use the same answer spans extracted by the answer extraction system (Du and Cardie, 2018).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Quantitative Analysis</head><p>QAE and R-QAE One of crucial challenges with generative models is a lack of a good quantitative evaluation metric. We adopt QA-based Evaluation (QAE) metric proposed by Zhang and Bansal <ref type="formula">(2019)</ref>    data covers larger distribution than the human annotated training data, R-QAE will be lower. However, note that having a low R-QAE is only meaningful when the QAE is high enough since trivially invalid questions may also yield low R-QAE.</p><p>Results We compare HCVAE and Info-HCVAE with the baseline models on SQuAD, NQ, and Triv-iaQA. We use 10% of Wikipedia paragraphs from HarvestingQA (Du and Cardie, 2018) for evaluation. <ref type="table" target="#tab_3">Table 2</ref> shows that both HCVAE and Info-HCVAE significantly outperforms all baselines by large margin in QAE on all three datasets, while obtaining significantly lower R-QAE, which shows that our model generated both high-quality and diverse QA pairs from the given context. Moreover, Info-HCVAE largely outperforms HCVAE, which demonstrates the effectiveness of our InfoMax regularizer for enforcing QA-pair consistency. <ref type="figure" target="#fig_3">Figure 3</ref> shows the accuracy as a function of number of QA pairs. Our Info-HCVAE outperform all baselines by large margins using orders of magnitude smaller number of QA pairs. For example, Info-HCVAE achieves 61.38 points using 12K QA pairs, outperforming Semantic-QG that use 10 times larger number of QA pairs. We also report   the score of x T Wy as an approximate estimation of mutual information (MI) between QA pairs generated by each method in <ref type="table" target="#tab_4">Table 3</ref>; our Info-HCVAE yields the largest value of MI estimation. Ablation Study We further perform an ablation study to see the effect of each model component.</p><p>We start with the model without any latent variables, which is essentially a deterministic Seq2Seq model (denoted as Baseline in <ref type="table" target="#tab_6">Table 4</ref>). Then, we add in the question latent variable (+Q-latent) and then the answer latent variable (+A-latent), to see the effect of probabilistic latent variable modeling and hierarchical modeling respectively. The results in <ref type="table" target="#tab_6">Table 4</ref> shows that both are essential for improving both the quality (QAE) and diversity (R-QAE) of the generated QA pairs. Finally, adding in the In-foMax regularization (+InfoMax) further improves the performance by enhancing the consistency of the generated QA pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>Human Evaluation As a qualitative analysis, we first conduct a pairwise human evaluation of the QA pairs generated by our Info-HCVAE and Maxout-QG on 100 randomly selected paragraphs. Specifically, 20 human judges performed blind quality assessment of two sets of QA pairs that are presented in a random order, each of which contained two to five QA pairs. Each set of QA pairs is evalu-   ated in terms of the overall quality, diversity, and consistency between the generated QA pairs and the context. The results in <ref type="table" target="#tab_8">Table 5</ref> show that the QA pairs generated by our Info-HCVAE is evaluated to be more diverse and consistent, compared to ones generated by the baseline models.</p><p>One-to-Many QG To show that our Info-HCVAE can effectively tackle one-to-many mapping problem for question generation, we qualitatively analyze the generated questions for given a context and an answer from the SQuAD validation set. Specifically, we sample the question latent variables multiple times using the question prior network p ? (z x | c), and then feed them to question generation networks p ? (x | z x , y, c) with the answer. The example in <ref type="table" target="#tab_9">Table 6</ref> shows that our Info-HCVAE generates diverse and semantically consistent questions given an answer. We provide more qualitative examples in Appendix D.</p><p>Latent Space Interpolation To examine if Info-HCVAE learns meaningful latent space of QA pairs, we qualitatively analyze the QA pairs generated by interpolating between two latent codes of it on SQuAD training set. We first encode z x from two QA pairs using posterior networks of q ? (z x |x, c), and then sample z y from interpolated values of z x using prior networks p ? (z y |z x , c) to generate corresponding QA pairs. <ref type="table" target="#tab_11">Table 7</ref> shows that the semantic of the QA pairs generated smoothly transit from one latent to another with high diversity and consistency. We provide more qualitative examples  in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Semi-supervised QA</head><p>We now validate our model in a semi-supervised setting, where the model uses both the ground truth labels and the generated labels to solve the QA task, to see whether the generated QA pairs help improve the performance of a QA model in a conventional setting. Since such synthetic datasets consisting of generated QA pairs may inevitably contain some noise <ref type="bibr">(Zhang and Bansal, 2019;</ref><ref type="bibr">Dong et al., 2019;</ref><ref type="bibr" target="#b0">Alberti et al., 2019)</ref>, we further refine the QA pairs by using the heuristic suggested by <ref type="bibr">Dong et al. (2019)</ref>, to replace the generated answers whose F1 score to the prediction of the QA model trained on the human annotated data is lower than a set threshold. We select the threshold of 40.0 for the QA pair refinement model via cross-validation on the SQuAD dataset, and used it for the experiments. Please see Appendix C for more details. SQuAD We first perform semi-supervised QA experiments on SQuAD using the synthetic QA pairs generated by our model. For the contexts, we use both the paragraphs in the original SQuAD (S) dataset, and the new paragraphs in the Harvest-ingQA dataset (H). Using Info-HCVAE, we generate 10 different QA pairs by sampling from the latent spaces (denoted as S?10). For the baseline, we use Semantic-QG (Zhang and Bansal, 2019) with the beam search size of 10 to obtain the same number of QA pairs. We also generate new QA pairs  using different portions of paragraphs provided in HarvestingQA (denoted as H?10%-H?100%), by sampling one latent variable per context. <ref type="table" target="#tab_13">Table 8</ref> shows that our framework improves the accuracy of the BERT-base model by 2.12 (EM) and 1.59 (F1) points, significantly outperforming Semantic-QG. NQ and TriviaQA Our model is most useful when we do not have any labeled data for a target dataset. To show how well our QAG model performs in such a setting, we train the QA model using only the QA pairs generated by our model trained on SQuAD and test it on the target datasets (NQ and TriviaQA). We generate multiple QA pairs from each context of the target dataset, sampling from the latent space one to ten times (denoted by N?1-10 or T?1-10 in <ref type="table" target="#tab_15">Table 9</ref>). Then, we fine-tune the QA model pretrained on the SQuAD dataset with the generated QA pairs from the two datasets. Table 9 shows that as we augment training data with larger number of synthetic QA pairs, the performance of the QA model significantly increases, significantly outperforming the QA model trained on SQuAD only. Yet, models trained with our QAG still largely underperform models trained with human labels, due to the distributional discrepancy between the source and the target dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed a novel probabilistic generative framework for generating diverse and consistent questionanswer (QA) pairs from given texts. Specifically, our model learns the joint distribution of question and answer given context with a hierarchically conditional variational autoencoder, while enforcing consistency between generated QA pairs by maximizing their mutual information with a novel In-  </p><formula xml:id="formula_7">? D KL [q ? (z x |x, c)||p ? (z x |c)] + E q ? (zy|zx,y,c) [log p ? (y|z y , c)] ? D KL [q ? (z y |z x , y, c)||p ? (z y |z x , c)]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Datatset</head><p>The statistics and the data resource are summarized in <ref type="table" target="#tab_17">Table 10</ref>.   beam size of 10 for decoding. We also evaluate the Maxout-QG model on our SQuAD validation set with <ref type="bibr">BLEU4 (Papineni et al., 2002)</ref>, and get 15.68 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection of Threshold for Replacement</head><p>As mentioned in our paper, we use the threshold of 40.0 selected via cross-validation of the QA model performance, using both the full SQuAD and Harvest-ingQA dataset for QAG. The detailed selection processes are as follows: 1) train QA model on only human annotated data, 2) compute F1 score of generated QA pairs, and 3) if the F1 score is lower than the threshold, replace the generated answer with the prediction of QA model. We investigate the optimal value of threshold among [20.0, 40.0, 60.0, 80.0] using our validation set of SQuAD. <ref type="table" target="#tab_18">Table 11</ref> shows the results of cross-validation on the validation set. The optimal value of 40.0 is used for semisupervised experiments on Natural Questions and TriviaQA. For fully unlabeled semi-supervised experiments on Natural Questions and TriviaQA, the QA model is only trained on SQuAD and used to replace the synthetic QA pairs (denoted in our paper as N?1-10, T?1-10).</p><p>Semi-supervised learning For the semisupervised learning experiment on SQuAD, we follow Zhang and Bansal (2019)'s split for a fair comparison. Specifically, we receive the unique IDs for QA pairs from the authors and use exactly the same validation and test set as theirs. For the Natural Questions and TriviaQA experiments, we use our own split as mentioned in the above. We generate QA pairs from the paragraphs of Wikipedia extracted by Du and Cardie (2018) and train BERT-base QA model with the synthetic data for two epochs. Then we further train the model with human-annotated training data for two more epochs. The catastrophic forgetting reported in Zhang and Bansal (2019) does not occur in our cases. We use Adam optimizer (Kingma and Ba, 2015) with batch size 32 and follow the learning rate scheduling as described in <ref type="bibr" target="#b0">(Devlin et al., 2019)</ref> with initial learning rate 2 ? 10 ?5 and 3 ? 10 ?5 for synthetic and human annotated data, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Qualitative Examples</head><p>The qualitative examples in <ref type="table" target="#tab_3">Table 12</ref>, 13, 14 are shown in the next page. Paragraph-2 Victoria is the centre of dairy farming in Australia. It is home to 60% of Australia's 3 million dairy cattle and produces nearly two-thirds of the nation's milk, almost 6.4 billion litres. The state also has 2.4 million beef cattle, with more than 2.2 million cattle and calves slaughtered each year. In 200304, Victorian commercial fishing crews and aquaculture industry produced 11,634 tonnes of seafood valued at nearly $109 million. . . .  <ref type="table" target="#tab_3">Table 12</ref>: Examples of QA pairs generated by our Info-HCVAE. We sample multiple latent variables from p ? (?), and feed them to generation networks. All the paragraphs are from validation set of SQuAD.</p><p>Paragraph-1 Notre Dame is known for its competitive admissions, with the incoming class enrolling in fall 2015 admitting 3,577 from a pool of 18,156 (19.7%). The academic profile of the enrolled class continues to rate among the top 10 to 15 in the nation for national research universities. . . . 1,400 of the 3,577 (39.1% ) were admitted under the early action plan.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ori1</head><p>Q where does notre dame rank in terms of academic profile among research universities in the us? A the top 10 to 15 in the nation Gen Q where does the academic profile of notre dame rank?</p><p>A the top 10 to 15</p><p>Q what was the rate of the incoming class enrolling in the fall of 2015?</p><p>A 3,577 from a pool of 18,156 (19.7%)</p><p>Q how many students attended notre dame? A 3,577</p><p>Ori2 Q what percentage of students at notre dame participated in the early action program? A 39.1%</p><p>Paragraph-2 . . . begun as a one-page journal in September 1876, the scholastic magazine is issued twice monthly and . . . In 1987, when some students believed that the observer began to show a . . . In spring 2008 an undergraduate journal for political science research, beyond politics, made its debut.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ori1</head><p>Q when did the scholastic magazine of notre dame begin publishing? A september 1876 Paragraph-3 As at most other universities, notre dame's students run a number of news media outlets. The nine student -run outlets include . . . , and several magazines and journals. . . . . the dome yearbook is published annually. . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ori1</head><p>Q what is the daily student paper at notre dame called? A the observer Gen Q how many student media outlets are there at notre dame? A nine student -run outlets include three Q what type of media is the student paper at notre dame? A a number of news media Q how often is the dome published? A annually Q how many magazines are published at notre dame ?</p><p>A several Ori2 Q how many student news papers are found at notre dame ? A three <ref type="table" target="#tab_6">Table 14</ref>: QA pairs generated by interpolating between two latent codes encoded by our posterior networks. Ori1 and Ori2 are from training set of SQuAD.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The conceptual illustration of the proposed HCVAE model encoding and decoding question and its corresponding answer jointly. The dashed line refers to the generative process of HCVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The directed graphical model for HCVAE. The gray and white nodes denote observed and latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>g(x, y) = sigmoid(x T Wy) where x = 1 N id i and y = 1 L j? j are summarized representations of question and answer, respectively. Combined with the ELBO, the final objective of our Info-HCVAE is as follows: max ? L HCVAE + ?L Info where ? includes all the parameters of ?, ?, ? and W, and ? controls the effect of MI maximization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>QAE vs. # of QA pairs (log-scaled) on SQuAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Paragraph- 1</head><label>1</label><figDesc>Near Tamins-Reichenau the Anterior Rhine and the Posterior Rhine join and form the Rhine. . . . This section is nearly 86km long, and descends from a height of 599m to 396m. It flows through a wide glacial alpine valley known as the Rhine Valley (German: Rheintal). Near Sargans a natural dam, only a few metres high, . . . The Alpine Rhine begins in the most western part of the Swiss canton of Graubnden, . . . Q-1: how long is the rhine? A-1: 86km long Q-2: how large is the dam? A-2: a few metres high Q-3: where does the anterior rhine and the posterior rhine join the rhine? A-3: Tamins-Reichneau Q-4: what type of valley does the rhine flows through? A-4: glacial alpine Q-5: what is the rhine valley in german? A-5: Rheintal Q-6: where deos the alpine rhine begin? A-7: Swiss canton of Graubnden</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Q- 1 :</head><label>1</label><figDesc>what industry produced 11,63 million tonnes of seafood in 2003-04 ? A-1: aquaculture Q-2: what type of cattle is consumed in Victoria? A-2: beef Q-3: in what year did victorian commercial fishing and aquaculture industry produce a large amount of seafood? A-3: 200304 Q-4: how many cattle and calves each year are slaughtered annually? A-4: 2.2 million Q-5: how much of the nation's milk is produced by the dairy? A-5: two-thrids Paragraph-3 A teacher's role may vary among cultures. Teachers may provide instruction in literacy and numeracy, craftsmanship or vocational training, the arts, religion, civics, community roles, or life skills. Q-1: what do a teacher's role vary? A-1: culture Q-2: what do teachers provide instruction in? A-2: vocational training Q-3: what is one thing a teacher may provide instruction for? A-3: community roles Q-4: what is one of the skills that teachers provide in? A-4: life skills</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>to measure the quality of QA pair. QAE is obtained by first training the QA model on the synthetic data, and then evaluating the QA model with human annotated test data. However, QAE only measures how well the distribution of synthetic QA pairs matches the distribution of GT QA pairs, and does not consider the diversity of QA pairs. Thus, we propose Reverse QA-based Evaluation (R-QAE), which is the accuracy of the QA model trained on the human-annotated QA pairs, evaluated on the generated QA pairs. If the synthetic 62.49/78.24 Semantic-QG 60.49/71.81 74.23/88.54 HCVAE 69.46/80.79 37.57/61.24 Info-HCVAE 71.18/81.51 38.80/60.73</figDesc><table><row><cell>Method</cell><cell>QAE (?)</cell><cell>R-QAE (?)</cell></row><row><cell cols="2">SQuAD (EM/F1)</cell><cell></cell></row><row><cell cols="3">Natural Questions (EM/F1)</cell></row><row><cell cols="3">Harvesting-QG 27.91/41.23 49.89/70.01</cell></row><row><cell>Maxout-QG</cell><cell cols="2">30.98/44.96 49.96/70.03</cell></row><row><cell>Semantic-QG</cell><cell cols="2">30.59/45.29 58.42/79.23</cell></row><row><cell>HCVAE</cell><cell cols="2">31.45/46.77 32.78/55.12</cell></row><row><cell>Info-HCVAE</cell><cell cols="2">37.18/51.46 29.39/53.04</cell></row><row><cell cols="2">TriviaQA (EM/F1)</cell><cell></cell></row><row><cell cols="3">Harvesting-QG 21.32/30.21 29.75/47.73</cell></row><row><cell>Maxout-QG</cell><cell cols="2">24.58/34.32 31.56/49.92</cell></row><row><cell>Semantic-QG</cell><cell cols="2">27.54/38.25 37.45/58.15</cell></row><row><cell>HCVAE</cell><cell cols="2">30.20/40.88 34.41/48.16</cell></row><row><cell>Info-HCVAE</cell><cell cols="2">35.45/44.11 21.65/37.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>QAE and R-QAE results on three datasets. All results are the performances on our test set.</figDesc><table><row><cell cols="4">Harvest Maxout Semantic HCVAE -QG -QG -QG</cell><cell>Info-HCVAE</cell></row><row><cell>111.74</cell><cell>114.58</cell><cell>112.94</cell><cell>113.89</cell><cell>117.41</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The results of mutual information estimation. The results are based on QA pairs generated from H?10%.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>QAE and R-QAE results of the ablation study on SQuAD dataset. All the results are the performances on our test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>The results of human judgement in terms of diversity, consistency, and overall quality on the generated QA pairs.Paragraph The scotland act 1998 which was passed by and given royal assent by queen Elizabeth ii on 19 november 1998, governs functions and role of the scottish parliament and delimits its legislative competence . . .</figDesc><table><row><cell>GT what act sets forth the functions of the scottish</cell></row><row><cell>parliament?</cell></row><row><cell>O-1 which act was passed in 1998?</cell></row><row><cell>O-2 which act governs role of the scottish parliament?</cell></row><row><cell>O-3 which act was passed by queen Elizabeth ii?</cell></row><row><cell>O-4 which act gave the scottish parliament the</cell></row><row><cell>responsibility to determine its legislative policy?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Examples of one-to-many mapping of our Info- HCVAE. The answer is highlighted by pink. GT denotes the ground-truth question. O-denotes questions generated by Info-HCVAE.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Paragraph ... Atop the main building' s gold dome is a golden statue of the virgin mary. ... Next to the main building is the basilica of the sacred heart. Immediately behind the basilica is the grotto, ... a marian place of prayer and reflection. ... At the end of the main drive ..., is a simple, modern stone statue of mary.</figDesc><table><row><cell>Ori1</cell><cell>Q what is the grotto at notre dame? A a marian place of prayer and reflection</cell></row><row><cell></cell><cell>Q where is the grotto at?</cell></row><row><cell></cell><cell>A a marian place of prayer and reflection</cell></row><row><cell></cell><cell>Q what place is behind the basilica of prayer?</cell></row><row><cell></cell><cell>A grotto</cell></row><row><cell>Gen</cell><cell>Q what is next to the main building at</cell></row><row><cell></cell><cell>notre dame?</cell></row><row><cell></cell><cell>A the basilica of the sacred heart</cell></row><row><cell></cell><cell>Q what is at the end of the main drive?</cell></row><row><cell></cell><cell>A stone statue of mary</cell></row><row><cell></cell><cell>Q what sits on top of the main building at</cell></row><row><cell>Ori2</cell><cell>notre dame?</cell></row><row><cell></cell><cell>A a golden statue of the virgin mary</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>QA pairs generated by interpolating between two latent codes encoded by our posterior networks. Ori1 and Ori2 are from training set of SQuAD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>The results of semi-supervised QA experiments on SQuAD. All the results are the performances on our test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9 :</head><label>9</label><figDesc>The result of semi-supervised QA experiments on level question-answer pairs from wikipedia. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018. Xinya Du, Junru Shao, and Claire Cardie. 2017. Learning to ask: Neural question generation for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017. Yanghoon Kim, Hwanhee Lee, Joongbo Shin, and Kyomin Jung. 2019. Improving neural question generation using answer separation. In Proceedings of the AAAI Conference on Artificial Intelligence. Lili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan, and Zhi Jin. 2016. Natural language inference by tree-based convolution and heuristic matching. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016. Linfeng Song, Zhiguo Wang, and Wael Hamza. 2017. A unified query-based generative model for question generation and question answering. CoRR.</figDesc><table><row><cell>Natural Questions and TriviaQA dataset. All results are the</cell></row><row><cell>performance on our test set.</cell></row><row><cell>foMax regularizer. To our knowledge, ours is the</cell></row><row><cell>first successful probabilistic QAG model. We eval-</cell></row><row><cell>uated the QAG performance of our model by the</cell></row><row><cell>accuracy of the BERT-base QA model trained us-</cell></row><row><cell>ing the generated questions on multiple datasets,</cell></row><row><cell>on which it largely outperformed the state-of-the-</cell></row><row><cell>art QAG baseline (+6.59-10.69 in EM), even with</cell></row><row><cell>a smaller number of QA pairs. We further vali-</cell></row><row><cell>dated our model for semi-supervised QA, where it</cell></row><row><cell>improved the performance of the BERT-base QA</cell></row><row><cell>model on the SQuAD by 2.12 in EM, significantly</cell></row><row><cell>outperforming the state-of-the-art model. As fu-</cell></row><row><cell>ture work, we plan to extend our QAG model to a</cell></row><row><cell>meta-learning framework, for generalization over</cell></row><row><cell>diverse datasets.</cell></row></table><note>Jiachen Du, Wenjie Li, Yulan He, Ruifeng Xu, Lidong Bing, and Xuan Wang. 2018. Variational autoregres- sive decoder for neural response generation. In Pro- ceedings of the 2018 Conference on Empirical Meth- ods in Natural Language Processing. EMNLP 2018.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>SQuAD We tokenize questions and contexts with WordPiece tokenizer from BERT. To fairly compare our proposed methods with the existing semi-</figDesc><table><row><cell>Datasets</cell><cell>Train (#)</cell><cell cols="2">Valid (#) Source</cell></row><row><cell>SQuAD</cell><cell>86,588</cell><cell>10,507</cell><cell>Crowd-sourced questions from Wikipedia paragraph</cell></row><row><cell cols="2">Natural Questions 104,071</cell><cell>12,836</cell><cell>Questions from actual userfor searching Wikipedia paragraph</cell></row><row><cell>TriviaQA</cell><cell>74,160</cell><cell>7,785</cell><cell>Question and answer pairs authored by trivia enthusaists from the Web</cell></row><row><cell>HarvestQA</cell><cell cols="2">1,259,691 -</cell><cell>Generated by neural networks from top-ranking 10,000 Wikipedia articles</cell></row><row><cell></cell><cell></cell><cell></cell><cell>supervised QA, we follow Zhang and Bansal</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(2019)'s split, which divides original development</cell></row><row><cell></cell><cell></cell><cell></cell><cell>set from SQuAD v1.1 (Rajpurkar et al., 2016) into</cell></row><row><cell></cell><cell></cell><cell></cell><cell>new validation set and test set. We adopt most of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the codes from Wolf et al. (2019) for preprocessing</cell></row><row><cell></cell><cell></cell><cell></cell><cell>data, training, and evaluating the BERT-base QA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>model.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Natural Questions Other than the original Natu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ral Questions (Kwiatkowski et al., 2019) dataset,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>we use subset of the dataset provided by MRQA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>shared task (Fisch et al., 2019) for extractive QA.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>As semi-supervised setting with SQuAD, we split</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the validation set provided from MRQA into half</cell></row><row><cell></cell><cell></cell><cell></cell><cell>for validation set and the others for test set. All</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the tokens from question and context are tokenized</cell></row><row><cell></cell><cell></cell><cell></cell><cell>with WordPiece tokenizer from BERT. We gener-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ate QA pairs from context not containing html tag,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and evaluate QA model with the official MRQA</cell></row><row><cell></cell><cell></cell><cell></cell><cell>evaluation scripts.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>TriviaQA For TriviaQA (Joshi et al., 2017), we</cell></row><row><cell></cell><cell></cell><cell></cell><cell>also use the training set from MRQA shared task,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>and divide the development set from MRQA into</cell></row><row><cell></cell><cell></cell><cell></cell><cell>half for validation set and the other for test set. All</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the tokens from question and context are tokenized</cell></row><row><cell></cell><cell></cell><cell></cell><cell>with WordPiece tokenizer from BERT. For evalu-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ation, we follow the MRQA's official evaluation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>procedure.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>HarvestingQA 3 We use paragraphs from Harvest-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ingQA dastaset (Du and Cardie, 2018) to generate</cell></row><row><cell></cell><cell></cell><cell></cell><cell>QA pairs for QA-based Evaluation (QAE) and Re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>verse QA-based Evaluation (R-QAE). For the base-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>line QG models such as Maxout-QG and Semantic-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>QG, we use the same answer spans from the dataset.</cell></row><row><cell></cell><cell></cell><cell></cell><cell>For the experiments of Maxout-QG baseline, we</cell></row><row><cell></cell><cell></cell><cell></cell><cell>train the model and generate new questions from</cell></row><row><cell></cell><cell></cell><cell></cell><cell>the context and answer, while the questions gener-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ated by Semantic-QG are provided by the authors</cell></row><row><cell></cell><cell></cell><cell></cell><cell>(Zhang and Bansal, 2019).</cell></row><row><cell></cell><cell></cell><cell></cell><cell>C Training Details</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Maxout-QG We use Adam (Kingma and Ba, 2015)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>optimizer with the batch size of 64 and set the</cell></row><row><cell></cell><cell></cell><cell></cell><cell>initial learning rate of 10 ?3 . We always set the</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>The statistics and the data source of SQuAD, Natural Questions, TriviaQA, and HarvestingQA.</figDesc><table><row><cell>Replace</cell><cell>EM</cell><cell>F1</cell></row><row><cell>F1 ? 0.0</cell><cell>82.4</cell><cell>89.39</cell></row><row><cell cols="3">F1 ? 20.0 83.11 89.65</cell></row><row><cell cols="3">F1 ? 40.0 83.32 89.79</cell></row><row><cell cols="3">F1 ? 60.0 83.20 89.78</cell></row><row><cell cols="3">F1 ? 80.0 83.09 89.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>The effect of F1-based replacement strategy in semi-supervised setting of SQuAD+H?100%. All results are the performance on validation set of Zhang and Bansal (2019).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>Gen Q when was the scholastic magazine published? A 1876 Q in what year did notre dame get its liberal newspaper? A 1987 Q how often is the scholastic magazine published ? A twice Ori2 Q in what year did notre dame begin its undergraduate journal ? A 2008</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/xinyadu/ harvestingQA</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/xinyadu/ harvestingQA</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Derivation of Variational Lower Bound</head><p>Theorem. If we assume conditional independence of y and z x , i.e., p ? (y|z x , z y , c) = p ? (y|z y , c),</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Synthetic QA corpora generation with roundtrip consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Andor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Super bowl 50 was an american football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24 10 to earn their third super bowl title</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Some clergy offer healing services, while exorcism is an occasional practice by some clergy in the united methodist church in Africa</title>
	</analytic>
	<monogr>
		<title level="m">GT which NFL team represented the AFC at super bowl 50? Ours-1 what team did the American Football Conference represent? Ours-2 who won the 2015 American Football Conference? Ours-3 which team defeated the carolina panthers? Ours-4 who defeated the panthers in 2015? Ours-5 what team defeated the carolina panthers in the 2015 season? Ours-6 who was the champion of the American Football League in the 2015 season? Ours-7 what team won the 2015 American Football Conference? Paragraph-2</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ours-1 in what country do some clergy in the united methodist church take place? Ours-2 in what country is exorcism practice an occasional practice? Ours-3 use of exorcism is an occasional practice in what country? Ours-4 is exorcism usually an occasional practice in what country? Paragraph-3 . . . , the city was the subject of a song , &quot;walking into fresno</title>
	</analytic>
	<monogr>
		<title level="m">GT in what country does some clergy in the umc occasionally practice exorcism</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Ours-2 &quot;walking into fresno&quot; was written by whom? Ours-3 the song &quot;walking into fresno&quot; was written by whom? Table 13: Examples of one-to-many mapping of our Info-HCVAE. Answers are highlighted by pink. We sample multiple question latent variables from p ? (z x | c), and feed them to question generation networks with a fixed answer. GT denotes ground-truth question, and Seq2Seq denotes question generated by Maxout-QG. All the paragraphs</title>
	</analytic>
	<monogr>
		<title level="m">GT who wrote &quot;walking in fresno&quot;? Ours-1 who wrote &quot;walking into fresno</title>
		<imprint/>
	</monogr>
	<note>ground truth questions, and answers are from validation set of SQuAD</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
