<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EAGAN: Efficient Two-stage Evolutionary Architecture Search for GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohao</forename><surname>Ying</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Southern California</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Gao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">The Hong Kong University of Science and Technology (Guangzhou)</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution">The Hong Kong University of Science and Technology</orgName>
								<address>
									<settlement>Hong Kong SAR</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EAGAN: Efficient Two-stage Evolutionary Architecture Search for GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Generative adversarial networks (GANs) have proven successful in image generation tasks. However, GAN training is inherently unstable. Although many works try to stabilize it by manually modifying GAN architecture, it requires much expertise. Neural architecture search (NAS) has become an attractive solution to search GANs automatically. The early NAS-GANs search only generators to reduce search complexity but lead to a sub-optimal GAN. Some recent works try to search both generator (G) and discriminator (D), but they suffer from the instability of GAN training. To alleviate the instability, we propose an efficient two-stage evolutionary algorithm-based NAS framework to search GANs, namely EAGAN. We decouple the search of G and D into two stages, where stage-1 searches G with a fixed D and adopts the many-to-one training strategy, and stage-2 searches D with the optimal G found in stage-1 and adopts the one-to-one training and weightresetting strategies to enhance the stability of GAN training. Both stages use the non-dominated sorting method to produce Pareto-front architectures under multiple objectives (e.g., model size, Inception Score (IS), and Fr?chet Inception Distance (FID)). EAGAN is applied to the unconditional image generation task and can efficiently finish the search on the CIFAR-10 dataset in 1.2 GPU days. Our searched GANs achieve competitive results (IS=8.81?0.10, FID=9.91) on the CIFAR-10 dataset and surpass prior NAS-GANs on the STL-10 dataset (IS=10.44?0.087, FID=22.18). Source code: https://github.com/marsggbo/EAGAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b10">[11]</ref> have obtained remarkable achievements on image generation tasks. A GAN consists of two networks (i.e., generator (G) and discriminator (D)) that contest with each other in a zero-sum game. G learns to generate semantic images from real data distributions, while D distinguishes real data from generated data. Since G and D have conflicting optimization objectives, GAN training is unstable and prone to collapse. Therefore, many ? ?: Equal contributions. ?: Corresponding author (xwchu@ust.hk). arXiv:2111.15097v2 [cs.CV] 12 Jul 2022 efforts have been made to manually enhance architectures of GANs <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3]</ref>, but this requires much professional knowledge. Recently, neural architecture search (NAS) has proven to be effective in automatically finding superior models in various tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>, including GANs. The early NAS-GAN works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref> search only generator with a fixed discriminator to reduce search difficulty, but this may lead to a sub-optimal GAN. Although some recent works have searched both G and D, they suffer from the instability of GAN training. For example, AdversarialNAS <ref type="bibr" target="#b8">[9]</ref>, which is the first gradient-based NAS-GAN, proposes an adversarial loss function to search G and D simultaneously, but the architectures of G and D are deeply coupled, which increases search complexity and the instability of GAN training. A subsequent gradient-based NAS-GAN work <ref type="bibr" target="#b31">[32]</ref> also demonstrates that simultaneously searching both G and D hampers the search of optimal GANs. DGGAN <ref type="bibr" target="#b24">[25]</ref> alleviates instability by progressively growing G and D but takes 580 GPU days to search on the CIFAR-10 dataset <ref type="bibr" target="#b19">[20]</ref>.</p><p>In this paper, we propose an efficient two-stage Evolutionary Architecture search framework for Generative Adversarial Networks (EAGAN) on the unconditional image generation task. First, to alleviate the instability of GAN training during the search, we decouple the search of G and D into two stages. In stage-1, we fix the architecture of discriminator and search only generators. All generators are paired with the same discriminator, i.e., the candidate generators and the fixed discriminator are in a many-to-one relationship. In stage-2, the best generator of stage-1 is used to provide supervision signals for searching discriminators. Specifically, in stage-2, we create multiple copies of the best generator architecture of stage-1, and each generator copy is paired with a different discriminator and trained independently. Thus, the generators and candidate discriminators of stage-2 are in a one-to-one relationship. Because we indirectly evaluate the discriminators of stage-2 via IS (Inception Score <ref type="bibr" target="#b30">[31]</ref>) and FID (Fr?chet Inception Distance <ref type="bibr" target="#b14">[15]</ref>) based on generators, the one-to-one strategy has a potential problem, i.e., if some generators have mode collapse at some time, then subsequently searched discriminators paired with these generators will be evaluated unfairly. To solve this problem, we propose the weight-resetting strategy, where all generators inherit the weights of the best generator of the previous search round before a new search round starts. The results in Sec. 5.3 show that our simple yet effective weight-resetting strategy can stabilize GAN searching. We summarize our contributions as follows.</p><p>1. We greatly reduce the instability of GAN training by decoupling the search of generator and discriminator into two stages, where stage-1 and stage-2 adopt the many-to-one and one-to-one training strategy, respectively. 2. We propose the weight-resetting strategy, which is simple yet effective to avoid mode collapse when searching discriminators in stage-2 and ensure fair evaluations of different discriminators. 3. EAGAN is efficient and takes 1.2 GPU days on the CIFAR-10 dataset to finish searching GANs. EAGAN achieves competitive results on the CIFAR-10 dataset and outperforms the prior NAS-GANs on the STL-10 dataset <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Generative Adversarial Network (GAN)</head><p>Generative Adversarial Networks (GANs) are first proposed in <ref type="bibr" target="#b10">[11]</ref> and have been widely used in the various generation and synthesis tasks. A GAN comprises a generator (G) that generates plausible new data and a discriminator (D) that distinguishes the generator's fake data from real data. Suppose D and G are parameterized by ? and ?, respectively, their loss functions are defined as</p><formula xml:id="formula_0">L D (?, ?) = ?E x?p data (x) [log D ? (x)] ? E z?p(z) [log(1 ? D ? (G ? (z)))]<label>(1)</label></formula><formula xml:id="formula_1">L G (?, ?) = E z?p(z) [log(1 ? D ? (G ? (z)))]<label>(2)</label></formula><p>where p data is the real data distribution and p z is a prior distribution. In other words, G and D play a min-max game with value function V , formulated below</p><formula xml:id="formula_2">min G max D V (G, D) =E x?p data [log D(x)] + E z?pz [log(1 ? D(G(z)))]<label>(3)</label></formula><p>The mix-max optimization incurs that GAN training suffers from multiple instability issues, such as mode collapse and gradient vanishing. To alleviate these problems, many efforts have been made <ref type="bibr" target="#b1">[2]</ref> from the perspective of loss functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b15">16]</ref>, normalization and constraint <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>, conditional techniques <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18]</ref>, and validation methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b14">15]</ref>. Besides, architecture enhancements have been proven effective to improve GANs performance in many works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Neural Architecture Search (NAS)</head><p>NAS aims at automatic architecture design and has achieved remarkable results in various fields <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14]</ref>. It can be formulated as a bilevel optimization problem as below</p><formula xml:id="formula_3">? * = arg min ? L val (?|w * ) s.t. w * = arg min w L train (w|?)<label>(4)</label></formula><p>where L train and L val indicate the training and validation loss; w and ? indicate the weight and architecture of neural network. This process aims to select the architecture ? * performing best on the validation set, conditioned on the optimal network weights w on the training set. There are mainly four approaches in NAS: 1) Reinforcement learning (RL) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b27">28]</ref> based methods train an RNN controller to generate neural networks; 2) Gradient-based methods <ref type="bibr" target="#b23">[24]</ref> apply softmax function to relax the discrete search space, allowing differential optimization of architectures; 3) Surrogate model-based optimization (SMBO) <ref type="bibr" target="#b22">[23]</ref> builds a surrogate model of the objective function to predict the searched model's performance, which can substantially improve search efficiency; 4) Evolutionary algorithm (EA) based methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38]</ref> maintain and evolve a large population of neural architectures to produce the Pareto-front architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Type search D? Multi-objective? Evaluation Metric(s) AGAN <ref type="bibr" target="#b34">[35]</ref> RL ? ? IS AutoGAN <ref type="bibr" target="#b9">[10]</ref> ? ? IS E2GAN <ref type="bibr" target="#b32">[33]</ref> ? ? IS+FID ? DEGAS <ref type="bibr" target="#b6">[7]</ref> Gradient ? ? Loss AdversarialNAS <ref type="bibr" target="#b8">[9]</ref> ? ? Loss AlphaGAN <ref type="bibr" target="#b31">[32]</ref> ? ? Loss EGAN <ref type="bibr" target="#b33">[34]</ref> EA ? ? Loss EAS-GAN <ref type="bibr" target="#b21">[22]</ref> x x Loss COEGAN <ref type="bibr" target="#b4">[5]</ref> ? ? FID (G); Loss (D) EAGAN ? ? Pareto-front(IS,FID,#size) ? </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">NAS for GANs</head><p>Due to the great success of NAS in searching neural networks, many works have also applied NAS to search GANs, summarized in <ref type="table">Table.</ref> 1. AGAN <ref type="bibr" target="#b34">[35]</ref> and Au-toGAN <ref type="bibr" target="#b9">[10]</ref> are among the first RL-based NAS methods to search GANs, but they only use IS as the reward to guide the search. E2GAN <ref type="bibr" target="#b32">[33]</ref> is rewarded by a linear combination of IS and FID. However, to avoid the notorious instability of GAN training, these early NAS-GAN methods only search generator (G) with a fixed discriminator (D) architecture, resulting in a sub-optimal GAN. Adversar-ialNAS <ref type="bibr" target="#b8">[9]</ref> proposes to search G and D simultaneously in a differentiable way. However, it results in highly coupled architectures of G and D. The ablation study in <ref type="bibr" target="#b31">[32]</ref> has demonstrated that simultaneously searching G and D would potentially increase the negative impact of inferior discriminators and hinder finding the optimal GANs. Liu et al. <ref type="bibr" target="#b24">[25]</ref> propose to progressively grow the architectures of G and D in an alternating fashion, but this is only a remedy to alleviate the issue of architecture coupling and causes huge computational costs (580 GPU days on the CIFAR-10 [20] dataset). COEGAN <ref type="bibr" target="#b4">[5]</ref> is very relevant to our work, which also uses an evolutionary algorithm to search G and D in two separate groups of architectures (called populations), but the two populations' architectures are coupled during the search. To reduce the search difficulty, CO-EGAN only explores a simple search space and experiments on a small dataset (MNIST <ref type="bibr" target="#b20">[21]</ref>). The final results show that COEGAN fails to outperform the previous human-designed GANs. In summary, since coupling G and D is not conducive to searching for the optimal GAN, we decouple them into two stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Weight-sharing based Neural Architecture Search</head><p>The early NAS methods first retrain the searched models from scratch and then evaluate their performance <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b29">30]</ref>, which obtains accurate evaluation but con-sumes huge resources, e.g., <ref type="bibr" target="#b29">[30]</ref> took 3,150 GPU days to search. To improve search efficiency, the weight-sharing strategy <ref type="bibr" target="#b27">[28]</ref> was proposed to allow all subnets to share weights within a super network, so they can be evaluated without retraining by inheriting the weights from SuperNet. In our work, we also adopt the weight-sharing method to search generators and discriminators from SuperNet-G N G and SuperNet-D N D , respectively. To simplify the notations, we use N to refer to both N G and N D . Denote the loss of the i-th subnet N i as L i , and the weights of N as W . The gradients of SuperNet loss L with respect to W is</p><formula xml:id="formula_4">? W L = 1 N N i=1 ? Wi L i = 1 N N i=1 ?L i ?W i (5)</formula><p>where W i is the weights of N i , and N is the total number of subnets. However, it is not practical to accumulate all subnets' gradients in each batch. An alternative way is to use mini-batch subnets to update weights W . In our experiments, we find that randomly sampling one subnet (i.e., N = 1) per batch can also work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Search Space</head><p>To ensure a fair comparison, we use the same search space as in <ref type="bibr" target="#b8">[9]</ref> since it also searches both generators and discriminators. The search space is given in <ref type="figure" target="#fig_0">Fig. 1</ref>. SuperNet-G N G comprises a fully-connected (FC) layer and three Up-Cells. Each cell contains five ordered nodes (0-4), where node 0 is the output of the previous cell. There are multiple candidate operations between two nodes, each represented by an edge, and only one operation will be activated (solid edge). The edges E G0 and E G1 indicate up-sampling operations. The rest edges (E G2 to E G6 ) are normal operations, where "None" indicates no connection between two nodes. We encode each edge by a one-hot sequence. For example, [0,1,0] for edge E G0 indicates that the bilinear interpolation operation is activated. SuperNet-D N D comprises three Down-Cells and an FC layer. The Down-Cell is the inverted structure of the Up-Cell. The edges E D0 to E D4 are normal operations, and E D5 and E D6 are down-sampling operations. Thus, searching the architecture of G and D is transformed into searching a set of one-hot sequences.</p><formula xml:id="formula_5">FC Up Cell Up Cell Up Cell Down Cell Down Cell Down Cell x SuperNet-G SuperNet-D z FC Up-sampling operations Nearest Neighbor Interpolation Bilinear Interpolation Transposed Conv3x3 Normal operations Conv1x1 (dilation=1) Conv3x3 (dilation=1) Conv5x5 (dilation=1) None Skip-connection Conv3x3 (dilation=2) Conv5x5 (dilation=2) Down-sampling operations Average pooling Max pooling Conv3x3 (dilation=1) Conv5x5 (dilation=1) Conv3x3 (dilation=2) Conv5x5 (dilation=2) SuperNets Cells Candidate Operations E G0 E G1 E G5 E G6 0 1 2 3 4 ... ... ... ... ... ... ... E G3 E G4 E G2 ... ... ... ... ... 0 2 1 3 4 ... ... E D0 E D1 E D2 E D4 E D5 E D3 E D6</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methods</head><p>EAGAN comprises two stages, each having two steps: weights training and architecture evolution. The many-to-one and one-to-one training strategies tailored for two stages are detailed in Sec. 4.1 and Sec. 4.2, respectively. Sec. 4.3 describes the steps for evolving architectures, which is the same in both stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Stage-1: Searching Generator</head><p>Many-to-One GAN Training. As shown in <ref type="figure">Fig. 2 (left)</ref>, in stage-1, we search generators (G) with a fixed discriminator (D) that has 0.91M parameters and the same architecture as that of <ref type="bibr" target="#b8">[9]</ref>. We adopt the many(G)-to-one(D) training strategy. Specifically, the fixed discriminatorD is denoted by architecture and weights variables, i.e.,D ? (?, wD). During each round, we produce P candidate generators to form the population-G A G , where all candidate generators share the weights W G of SuperNet-G, and each candidate G i is parameterized with architecture and weights variables, i.e., G i ? (? i , w Gi ), where w Gi = W G (? i ). We then pair each candidate generator with the fixed discriminatorD to form P GANs, i.e., {(G 1 ,D), ..., (G P ,D)}. Stage-1 can be formalized as below</p><formula xml:id="formula_6">? * = arg min ?i {V val ? i | w * Gi , w * D ,? , i ? {1, ..., P }} (6) s.t. w * Gi = arg min w G i E z?p(z) log 1 ?D (G i (z)) (7) w * D = arg max wD P i=1 E x?p data (x) [logD(x)] + E z?p(z) [log(1 ? D(G i (z)))]<label>(8)</label></formula><p>where the inner (Eq. <ref type="formula">(7)</ref>? <ref type="formula" target="#formula_6">(8)</ref>) is to optimize weights of P GANs on the training set via the many-to-one strategy, and the outer (Eq. <ref type="formula">(6)</ref>) is to obtain the optimal architecture of G according to the value function on the validation set (i.e., V val ). The inner and outer optimizations are solved by iterative procedures, outlined in Alg. 1. These P GANs share the same discriminator and are trained for multiple epochs for each round. To get a fair comparison between generators, for each training batch, we uniformly draw a generator from P candidate generators and train it with the fixed discriminator (lines 4 to 10 in Alg. 1). The many-to-one training mechanism can bring two benefits. First, the fixed discriminatorD is trained with various generators, which can be viewed as an ensemble method to some extent, avoiding thatD is over-fitted and much stronger than generators. Second, different generators are trained with the same discriminator, so we can fairly compare the performance of these generators to find the optimal one. Besides, a generator with mode collapse will not interfere with other generators because the selection step will eliminate it from the population (see Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Stage-2: Searching Discriminator</head><p>After stage-1, we obtain an optimal generator G * with architecture ? * . In stage-2, we use it to guide searching discriminators (D). There are two major challenges in searching D: the lack of evaluation metrics for discriminators and the instability of GAN training. Next, we describe our approaches to these two challenges.</p><p>One-to-One GAN Training. Unlike generators, discriminators are difficult to be assessed directly. For example, the accuracy of discriminators does not reflect the overall performance of GANs, as high accuracy may indicate that generators are too weak to fool discriminators, and low accuracy may indicate that generator has mode collapse, with no way to analyze the real cause. Some works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b4">5]</ref> use the reconstructed loss (e.g., Eq. (1)) to monitor discriminator, but the loss is not a reliable monitor metric as GAN training is a dynamic equilibrium process. An alternative solution is to indirectly assess the discriminator via IS and FID metrics calculated based on a generator, so we cannot simply imitate the training strategy of stage-1 (e.g., many(D)-to-one(G)) in stage-2; otherwise, all discriminators are paired with the same generator and not comparable. To this end, we propose the one-to-one training strategy. Specifically, we create P copies of G * , each paired with a candidate discriminator from population-D A D . Thus, we obtain P GANs, i.e.,</p><formula xml:id="formula_7">{(G i , D i ), i ? {1, ..., P }}, where G i ? (? * , w Gi )</formula><p>and D i ? (? i , w Di ). Each GAN is independently trained as a regular GAN via Eq. (1)?(3). Therefore, stage-2 can be formalized as follows</p><formula xml:id="formula_8">? * = arg min ?i {V val ? i | w * Gi , w * Di , ? * , i ? {1, ..., P }} (9) s.t. w * Gi , w * Di = min Gi max Di E x?p data (x) [log D i (x)] + E z?p(z) [log(1 ? D i (G i (z)))]<label>(10)</label></formula><p>Weight-resetting. The second challenge of stage-2 is that the one-to-one training strategy does not fully guarantee a fair comparison between different discriminators. Since P generators are trained independently, each generator will have different weights after a round of one-to-one training, presented with different colors (see <ref type="figure">Fig. 2 (right)</ref>). If some generators have mode collapse due to combination with unsuitable discriminators, then subsequent discriminators paired with these generators will obtain unfair and biased estimation. To alleviate this problem, we propose the weight-resetting strategy, which is to first copy the weights of best generator in the current round, and then initialize all generators in the next round with the copied weights. In the first round, all generators are initialized with the weights of G * found in stage-1. In summary, the one-toone training strategy allows each discriminator to be paired with an independent generator, and the weight-resetting strategy ensures a fair comparison between different discriminators and alleviates the instability of GAN training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Architecture Evolution</head><p>As shown in <ref type="figure">Fig. 2</ref>, after weights training, stage-1 and stage-2 perform the same steps to evolve generators and discriminators, respectively. To simplify notations, we use N , N i , and A to denote the SuperNet, the i-th subnet, and population, of candidate generators (stage-1) and discriminators (stage-2), respectively.</p><p>Selection. This step is equivalent to Eq. (6) of stage-1 and Eq. (9) of stage-2. In our work, we use IS <ref type="bibr" target="#b30">[31]</ref> and FID <ref type="bibr" target="#b14">[15]</ref> metrics to evaluate the performance of individual (i.e., subnet). FID is inversely correlated with IS, so we adopt the non-dominated sorting strategy <ref type="bibr" target="#b5">[6]</ref> as the value function to produce the Paretofront individuals during each round. An individual N i is said to be dominated by another individual N j when Eq. (11) satisfies.</p><formula xml:id="formula_9">F k (N i ) ? F k (N j ) ?k ? {1, . . . , K} F k (N i ) &gt; F k (N j ) ?k ? {1, . . . , K}<label>(11)</label></formula><p>where F k indicates the objective (e.g., FID, and 1 IS 6 ). We split the population with P individuals into a number of disjoint subsets (or ranks) ? = {? 0 , ? 1 , ...} by comparing the number of times each individual being dominated by other individuals, where the length of ? and each subset may be different for each search round. After non-dominated sorting, individuals in the same subset are regarded as equally important and better than those in a larger rank. For example, the individuals in the subset ? 0 outperform all other subsets of individuals. Finally, we sequentially select P 2 individuals from lower to higher ranks. Crossover&amp;Mutation. As detailed in Sec. 3.2, the architecture of each subnet is encoded by a set of one-hot sequences, where the one-hot sequence indicates an edge and the position of 1 indicates the candidate operation acti-vated on that edge. Thus, the basic unit of crossover and mutation is the one-hot sequence. We set P 2 Pareto-front individuals obtained from the selection step as parents. Then, we repeatedly perform crossover and mutation on these parents with probabilities of 0.3 and 0.5, respectively, until we generate P 2 new individuals. For crossover, we randomly choose two parents and exchange a single one-hot sequence (i.e., an edge). For mutation, we also randomly choose the one-hot sequence of an edge and change the position of 1 on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Implementation Settings</head><p>Datasets. Following the previous NAS-GANs <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>, we search on the CIFAR-10 <ref type="bibr" target="#b19">[20]</ref> and evaluate on both CIFAR-10 and STL-10 <ref type="bibr" target="#b3">[4]</ref> datasets. CIFAR-10 has 50,000 training images and 10,000 test images with 32?32 resolutions. STL-10 has 100,500 images with 96?96 resolutions, but we resize them to 48?48.</p><p>Warm-up Stage. We set up a warm-up stage before the start of stage-1 and stage-2 to ensure a fair competition for all candidate subnets. Specifically, all candidate operations in search space are activated uniformly and trained equally. The warm-up stage has 50 epochs. After that, we randomly sample P subnets to form the first round of population.</p><p>Two-stage Search. For both stage-1 and stage-2, we use the hinge loss <ref type="bibr" target="#b25">[26]</ref> and Adam optimizer <ref type="bibr" target="#b18">[19]</ref> with an initial learning rate of 0.0002. The total number of search rounds is 18, each containing 10 epochs. The noise data is sampled from the Gaussian distribution. A population of P = 32 individuals is trained and evolved during each round. The batch sizes for generator and discriminator are 40 and 80, respectively. Besides, we adopt a low-fidelity evaluation strategy, i.e., the number of images used to calculate FID and IS is reduced to 5,000, which greatly reduces the evaluation time and keeps the performance of the searched architectures. Stage-1 and stage-2 take 0.8 and 0.4 GPU days, respectively.</p><p>Fully-train Stage. After the two-stage search, we fully train the bestperforming GAN (G * , D * ) from scratch. For the CIFAR-10 dataset, the batch size and learning rate are the same as the search stage, but the total number of training epochs is 600. For the STL-10 dataset, the batch size and the learning rate are 128 and 0.0003 for the generator, and 64 and 0.0002 for the discriminator, respectively. Following the previous NAS-GAN works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, we generate 50,000 images to calculate IS and FID metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results and Analysis</head><p>Search only Generator (EAGAN-G). Our searched generator G * is shown in <ref type="figure" target="#fig_3">Fig. 3</ref>. Note that the generators for the CIFAR-10 (G C with 7.14M parameters) and STL-10 (G S with 11.55M parameters) datasets have the same architecture but different input channels, so their sizes are different. We can see that 1) bilinear operation is preferred for up-sampling, which is also observed in previous NAS-GANs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref>; 2) there are 6 "None" operations and 3 "skip-connect" operations among 15 total normal operations, and the normal convolution with kernel size 3 ? 3 is preferred, which is probably because the low-resolution images do not need complicated convolutions to generate. The results in <ref type="table">Table. 2</ref> show that, compared with AdversarialNAS <ref type="bibr" target="#b8">[9]</ref>, our EAGAN can find a better generator with similar time overhead, given the same search space and fixed discriminator. Specifically, our discovered generator achieves a highly competitive FID (10.14) and IS (8.76?0.09) on the CIFAR-10 dataset. In terms of IS, there is a certain gap between NAS-GANs and BigGAN <ref type="bibr" target="#b2">[3]</ref> because BigGAN additionally introduces category information as input into the generator's architecture, while NAS-GANs only receive noise data as input. Besides, our generator G S achieves remarkable results (IS 10.02?0.11, FID=23.34) on the STL-10 dataset, showing an excellent transferability. Search both Generator and Discriminator (EAGAN-GD1). In stage-2, we use the best generator G * found in stage-1 to help search a set of Paretofront discriminators, from which we select the optimal discriminators for the CIFAR-10 (D C with 0.91M parameters) and STL-10 (D S with 1.58M parameters) datasets, respectively, shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. We can see a subtle difference (marked in red) between them, i.e., D S prefers convolutions with a larger kernel size (5 ? 5), while D C selects skip-connection and a smaller convolution. A possible reason is that the resolution of STL-10 (48?48) is larger than CIFAR-10 (32?32), so it needs a larger kernel size to obtain larger receptive fields.</p><p>After two-stage search, we retrain two GANs (i.e., (G C , D C ) and (G C , D S )) on the CIFAR-10 and STL-10 datasets, respectively, and report their results in <ref type="table">Table.</ref> 2. We can see that none of existing NAS-GANs can guarantee to find excellent GANs in both search scenarios: (a) searching only generators; and (b) searching both generators and discriminators. For example, AdverearialNAS <ref type="bibr" target="#b8">[9]</ref> performs poorly (IS=7.86?0.08, FID=24.04) in scenario (a), and AlphaGAN <ref type="bibr" target="#b31">[32]</ref> suffers from instability in scenario (b), as its performance drops significantly from (IS=8.89?0.09, FID=10.35) in scenario (a) to (IS=8.70?0.11, FID=15.56) in scenario (b). However, our EAGAN performs well in both search scenarios, and the discriminators searched in stage-2 can further improve the performance of the optimal generator discovered in stage-1. Specifically, we achieve a competitive IS value (8.81?0.10) and the best FID (9.91) on the CIFAR-10 dataset. Besides,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Search Method  <ref type="table">Table 2</ref>. Results on the CIFAR-10 and STL-10 datasets. ? indicates searching both generators (G) and discriminators (D). our EAGAN achieves remarkable performance (IS=10.44?0.08, FID=22.18) on the STL-10 dataset, which outperforms the existing NAS-searched GANs. In <ref type="figure" target="#fig_5">Fig. 5</ref>, we present 50 images randomly generated by generators trained on the CIFAR-10 and the STL-10 datasets without cherry-picking, respectively. The generated images are of rich diversity and high quality.</p><p>(a) CIFAR-10 (b) STL-10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>Search G or D first? EAGAN searches G first and then searches D. What about search D first? Our experiments show that searching D first in stage-1 will make the searched D much stronger than candidate G in stage-2, which in turn causes the gradients of G to vanish. Thus, we should search G first. Initialize different D in stage-1. Our above experiment (i.e., EAGAN-GD1) uses the discriminator of <ref type="bibr" target="#b8">[9]</ref> in stage-1. We further implement two experiments to explore the effect of initializing different D in stage-1. EAGAN-GD2 uses a simple network with 0.92M parameters, comprising five normal convolutions and a linear layer, as the initial D in stage-1. EAGAN-GD3 is to repeat the two-stage search several times, i.e., the optimal D of the previous stage-2 is set as the initial D of the next stage-1. From <ref type="table">Table.</ref> 2, we can see that both EAGAN-GD2 and EAGAN-GD3 achieve competitive results on the CIFAR-10 and STL-10 datasets, indicating that EAGAN does not require strong prior knowledge to design the initial state of D and that searching once is sufficient to find good models, balancing search overhead and model performance.</p><p>Decoupled vs. Coupled. To validate the effectiveness of our decoupled search method, we perform a coupled search experiment as the baseline, i.e., the architectures of G and D are evolved simultaneously for each search round. <ref type="figure">Fig. 6</ref> presents the learning curves of the baseline and our EAGAN, which shows that coupled search is unstable as it fluctuates throughout the search. In contrast, the overall performance of our decoupled search is better and significantly improved, especially in stage-2 of searching discriminators. Besides, the decoupled search also fluctuates in stage-1 due to the competition among candidate generators incurred by the weight-sharing strategy, and how to address the negative impact of weight-sharing is still an open problem <ref type="bibr" target="#b36">[37]</ref>.</p><p>Weight-resetting Strategy. We conduct another experiment on the CIFAR-10 dataset, which differs from our EAGAN only in that the weights of P generators in stage-2 are continuously and independently trained without weightresetting (WR) strategy. <ref type="figure">Fig. 7</ref> presents the learning curves with and without the WR strategy in stage-2, which shows that our proposed WR strategy can effectively enhance the stability of GAN training and obtain better IS and FID scores in stage-2 of searching discriminators. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion &amp; Future Work</head><p>This paper proposes an efficient two-stage evolutionary algorithm-based NAS framework to search GANs, namely EAGAN. We demonstrate that decoupling the search of the generator and discriminator into two stages can significantly improve the stability of searching GANs via the GAN training strategies (manyto-one and one-to-one) tailored for both stages and the weight-resetting strategy. EAGAN is very efficient and takes 1.2 GPU days to finish the search on CIFAR-10. Our searched GANs achieve competitive performance (IS and FID) on the CIFAR-10 dataset and outperform previous NAS-GANs on the STL-10 dataset. We believe our work deserves more in-depth study and may benefit other potential fields. For example, our decoupled paradigm and tailored training strategies are well suited for large-scale parallel search when architectures require adversarial training. Further, we shall investigate reducing the interference of weight-sharing in search and explore high-resolution generative tasks. Acknowledgements. Thanks to the NVIDIA AI Technology Center (NVAITC) for providing the GPU cluster to support our work. BH was supported by the NSFC Young Scientists </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Overview of search space. EG0 and EG1 are up-sampling operations, ED5 and ED6 are down-sampling operations, and the other edges are normal operations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Stage- 2 :Fig. 2 .</head><label>22</label><figDesc>search D using the best G Two-stage pipeline of EAGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 G 10 UpdateG 14 A 16 GPD 28 A</head><label>110141628</label><figDesc>EAGAN.Input: SuperNet-G NG, SuperNet-D ND, population-G AG, population-D AD, population size P = |AG| = |AD|, multi-objective set F, total search rounds R, each round contains E epochs of training. Output: G * and D * 1D ? (?, wD) ? Initialize a discriminator with weights wD and fixed architecture?; ), i ? {1, ..., P }} ? Initialize P GANs that share the same discriminator; 4 for r=0:R ? 1 do5 for e=0:E ? 1 do 6 for batch x = {x1, ..., xm} in training set do 7 Sample noise data z = {z1, ..., zm}; , i ? {1, ..., P }; 9 Update weights ofD via Eq. (8); ? Select Pareto-front generators under F based on validation set; * ? (? * , wG * ) ? the best generator with architecture ? * and weights wG * ; } ? Warm-up(G * , ND); 18 {(Gi, D (0) i ), i ? {1, ..., P }} ? Initialize P GANs, where Gi is a copy of G * ; 19 for r=0:R ? 1 do 20 for e=0:E ? 1 do 21 for batch x = {x1, ..., xm} in training set do 22 Sample noise data z = {z1, ..., zm}; 23 Uniformly sample a GAN (Gi, D (r) i ) from P GANs; 24 Update weights of Gi and D ? Select Pareto-front discriminators under F based on validation set; the generator weights of the best GAN; 30 wG 1 = ... = wG P = wG * ? Weight-resetting; 31 end 32 D * ? (? * , wD * ) ? the best discriminator with architecture ? * and weights wD * ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of the searched generator (GC = GS = G * ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The searched discriminators on CIFAR-10 (top) and STL-10 (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>The generated images by EAGAN in random without cherry-picking.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .Fig. 7 .</head><label>67</label><figDesc>Learning curves when generators and discriminators are coupled/decoupled. The dashed line indicates the boundary between the two decoupled stages of EAGAN. Learning curves with and without (W/O) the weight-resetting (WR) strategy in stage-2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fund No. 62006202, Guangdong Basic and Applied Basic Research Foundation No. 2022A1515011652, RGC Early Career Scheme No. 22200720, RGC Research Matching Grant Scheme No. RMGS2022 11 02 and HKBU CSD Departmental Incentive Grant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of our EAGAN and the existing NAS-GAN methods. The third column indicates whether the method supports searching discriminators. ? indicates a linear combination of metrics. ? indicates the Pareto-front of multiple metrics.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>55+0.73 8.69?0.10 10.53 10.14?0.11 24.22</figDesc><table><row><cell></cell><cell></cell><cell>GPU</cell><cell cols="2">CIFAR-10</cell><cell>STL-10</cell></row><row><cell></cell><cell></cell><cell>Days</cell><cell>IS?</cell><cell>FID?</cell><cell>IS?</cell><cell>FID?</cell></row><row><cell>DCGANs [29]</cell><cell></cell><cell></cell><cell cols="2">6.64?0.14 37.7</cell><cell>-</cell><cell>-</cell></row><row><cell>WGAN-GP [12]</cell><cell></cell><cell></cell><cell cols="2">7.86?0.07 29.3</cell><cell>-</cell><cell>-</cell></row><row><cell>Progressive GAN [17]</cell><cell></cell><cell></cell><cell cols="2">8.80?0.05 18.33</cell><cell>-</cell><cell>-</cell></row><row><cell>SN-GAN [26]</cell><cell>Manual</cell><cell>-</cell><cell cols="4">8.22?0.05 21.7 9.16?0.12 40.1</cell></row><row><cell>ProbGAN [13]</cell><cell></cell><cell></cell><cell>7.75</cell><cell cols="3">24.60 8.87?0.09 46.74</cell></row><row><cell>Improv MMD GAN[36]</cell><cell></cell><cell></cell><cell>8.29</cell><cell>16.21</cell><cell>9.34</cell><cell>37.63</cell></row><row><cell>BigGAN [3]</cell><cell></cell><cell></cell><cell>9.22</cell><cell>14.73</cell><cell>-</cell><cell>-</cell></row><row><cell>AGAN [35]</cell><cell></cell><cell>1200</cell><cell cols="4">8.29?0.09 30.5 9.23?0.08 52.7</cell></row><row><cell>AutoGAN [10] E2GAN [33]</cell><cell>RL</cell><cell>2 0.3</cell><cell cols="4">8.55?0.10 12.42 9.16?0.12 31.01 8.51?0.13 11.26 9.51?0.09 25.35</cell></row><row><cell>DEGAS [7]</cell><cell></cell><cell cols="5">1.167 8.37?0.08 12.01 9.71?0.11 28.76</cell></row><row><cell>AlphaGAN [32]</cell><cell></cell><cell>0.13</cell><cell cols="4">8.98?0.09 10.35 10.12?0.13 22.43</cell></row><row><cell>AlphaGAN [32] ?</cell><cell>Gradient</cell><cell>-</cell><cell cols="2">8.70?0.11 15.56</cell><cell>-</cell><cell>-</cell></row><row><cell>AdversarialNAS [9]</cell><cell></cell><cell>1</cell><cell cols="4">7.86?0.08 24.04 8.52?0.05 38.85</cell></row><row><cell>AdversarialNAS [9] ?</cell><cell></cell><cell>1</cell><cell cols="4">8.74?0.07 10.87 9.63?0.19 26.98</cell></row><row><cell>DGGAN [25]</cell><cell>Heuristic</cell><cell>580</cell><cell cols="2">8.64?0.06 12.10</cell><cell>-</cell><cell>-</cell></row><row><cell>EGAN [34] EAS-GAN [22]</cell><cell>EA</cell><cell>1.25 1</cell><cell cols="2">6.9?0.09 7.45?0.08 33.2 -</cell><cell>--</cell><cell>-38.84</cell></row><row><cell>EAGAN-G</cell><cell></cell><cell>0.8</cell><cell cols="4">8.76?0.09 10.14 10.02?0.11 23.34</cell></row><row><cell>EAGAN-GD1 ? EAGAN-GD2 ?</cell><cell>EA</cell><cell cols="5">0.8+0.4 8.81?0.10 9.91 10.44?0.08 22.18 0.75+0.37 8.63?0.09 12.84 9.76?0.06 26.52</cell></row><row><cell>EAGAN-GD3 ?</cell><cell></cell><cell>1.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The higher the IS value, the better the GAN performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13076</idno>
		<title level="m">The six fronts of the generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourteenth international conference on artificial intelligence and statistics</title>
		<meeting>the fourteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coevolution of generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Louren?o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Machado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on the Applications of Evolutionary Computation (Part of EvoStar)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="473" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: Nsga-ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Deb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pratap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Meyarivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on parallel problem solving from nature</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="849" to="858" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Doveh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00606</idno>
		<title level="m">Degas: Differentiable efficient generator search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05377</idno>
		<title level="m">Neural architecture search: A survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarialnas: Adversarial neural architecture search for gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Autogan: Neural architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICCV</title>
		<meeting>the ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Improved training of wasserstein gans. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Probgan: Towards probabilistic gan with theoretical guarantees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Automl: A survey of the state-of-the-art. Knowledge-Based Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="page">106622</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS</title>
		<meeting>the NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08431</idno>
		<title level="m">Boundaryseeking generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<title level="m">Progressive growing of gans for improved quality, stability, and variation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. rep</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evolutionary architectural search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Progressive neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dynamically grown generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8680" to="8687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2642" to="2651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03268</idno>
		<title level="m">Efficient neural architecture search via parameter sharing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI</title>
		<meeting>the AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NeurIPS</title>
		<meeting>the NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Alphagan: Fully differentiable architecture search for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09134</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Off-policy reinforcement learning for efficient and effective gan architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Evolutionary generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Evolutionary Computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="921" to="934" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.11080</idno>
		<title level="m">Agan: Towards automated design of generative adversarial networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Improving MMD-GAN training with repulsive loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Halgamuge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Weight-sharing neural architecture search: A battle to shrink the optimization gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cars: Continuous evolution for efficient neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR42600.2020.00190</idno>
		<ptr target="https://doi.org/10.1109/CVPR42600.2020.00190" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR</title>
		<meeting>the CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
