<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">View-Volume Network for Semantic Scene Completion from a Single Depth Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
							<email>yuxiao.guo@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
							<email>xtong@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">View-Volume Network for Semantic Scene Completion from a Single Depth Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a View-Volume convolutional neural network (VVNet) for inferring the occupancy and semantic labels of a volumetric 3D scene from a single depth image. The VVNet concatenates a 2D view CNN and a 3D volume CNN with a differentiable projection layer. Given a single RGBD image, our method extracts the detailed geometric features from the input depth image with a 2D view CNN and then projects the features into a 3D volume according to the input depth map via a projection layer. After that, we learn the 3D context information of the scene with a 3D volume CNN for computing the result volumetric occupancy and semantic labels. With combined 2D and 3D representations, the VVNet efficiently reduces the computational cost, enables feature extraction from multi-channel high resolution inputs, and thus significantly improves the result accuracy. We validate our method and demonstrate its efficiency and effectiveness on both synthetic SUNCG and real NYU dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Reconstructing and understanding a 3D scene from its partial observations is an important technique in many robotic and vision tasks, such as indoor navigation, object retrieval, and visual reasoning. Given a single depth image captured from a 3D scene, a set of methods recently have been developed  for automatically predicting the semantic labels or completing 3D shapes of the objects in the scene using the convolutional neural networks (CNN). To achieve good performance, previous studies have revealed that both local geometric details and global 3D context of the scene need be learned in this task . The former one helps the system to identify the small objects in the scene, and the later one is used for inferring occluded objects from the scene layout. However, designing a CNN that can efficiently learn both features is a non-trivial task.</p><p>2D CNN based methods <ref type="bibr" target="#b7">[Gupta et al., 2014;</ref> take the depth input as an additional channel of RGB image and apply 2D CNNs for scene segmentation and object detection. Although these methods can fully exploit the high resolution input to generate detailed segmentation results, they ignore the 3D context information of the scene and thus cannot infer the invisible part of the scene. 3D CNN based methods <ref type="bibr">[Wu et al., 2015;</ref><ref type="bibr" target="#b14">Nguyen et al., 2016;</ref><ref type="bibr" target="#b16">Song and Xiao, 2016;</ref> convert input depth maps or point clouds into a volumetric representation and design 3D CNNs for 3D scene segmentation or object completion. However, the high computational and memory cost of the 3D CNN limits their capability for recovering object <ref type="bibr">details. Recently, Song et al.[2017]</ref> proposed SSCNet for semantic scene completion, where the system simultaneously predicts the object shapes and semantic labels from a single depth image. However, the 3D CNN used in their solution limits the input resolution and the depth of the neural networks, which leads to wrong labels and missing shape details in the results.</p><p>In this paper, we propose a cascaded convolutional neural network, named View-Volume Net (VVNet), for semantic scene completion from a single depth image. The key idea of our method is to handle the local geometric details and global 3D context with two convolutional neural networks: a 2D view CNN for extracting 2D geometric features, and a 3D volume CNN for learning 3D context of the scene. Given a single depth image captured from a 3D scene, our method first extracts a set of 2D feature maps from the input depth image with the 2D view CNN and then projects the feature maps into a 3D feature volume according to the input depth map via a projection layer. After that, the 3D volume CNN is applied to the 3D feature volume to learn the 3D context information and predicts the occupancy and semantic label of each voxel in the view frustum. Since the feature projection is differentiable, the cascaded VVNet can be trained end-toend.</p><p>The VVNet efficiently learns both geometry features and 3D context from the training dataset. The 2D view CNN avoids the high computational cost and memory consumption of the 3D CNN, which not only enables us to extract geometric features from the high-resolution input depth map, but also allows us to exploit multiple signals computed from the input depth image for feature extraction. Meanwhile, the 3D volume CNN defined over the low resolution feature volume efficiently learns the global 3D context of the scene. More-over, the VVNet provides a flexible framework for combining variant 2D and 3D CNNs. To this end, we design and evaluate a set of VVNet models with different configurations for semantic scene completion.</p><p>We train the VVNets on both synthetic SUNCG dataset and real NYU dataset and validate our design. We also compare their performance with previous methods. Experimental results demonstrate that our method outperforms the stateof-the-art methods on both datasets, with much better accuracy and three times speed up for training, as well as more than 7 times speed up for inference. With different configurations, we further offer VVNet models with different trade-offs between the training/inference cost and result accuracy. All these VVNet models achieve better accuracy than previous solutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we discuss related work and focus on methods for analyzing and completing a 3D scene from depth images or 3D point clouds, as well as the deep learning approaches that are based on hybrid 2D and 3D representations. Please refer to <ref type="bibr">[Ioannidou et al., 2017]</ref> for a survey of deep learning techniques for 3D data processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">3D Scene Analysis</head><p>A set of methods have been proposed for scene segmentation, scene completion, and object detection from an input RGBD image or depth image. 2D image-based methods regard the depth as an additional channel of the 2D RGB image and leverage manually-crafted features <ref type="bibr" target="#b6">[Gupta et al., 2013;</ref><ref type="bibr" target="#b0">Atapour-Abarghouei and Breckon, 2017]</ref> or 2D deep neural networks <ref type="bibr" target="#b7">[Gupta et al., 2014;</ref> for these scene analysis tasks. 3D volume-based approaches convert the input depth map into a volumetric representation and exploit manually crafted 3D features <ref type="bibr" target="#b16">[Ren and Sudderth, 2016]</ref> or 3D CNNs <ref type="bibr" target="#b16">[Song and Xiao, 2016]</ref> for detecting 3D objects from the input RGBD image. Although these methods can successfully detect and segment visible 3D objects and scenes in the input RGBD images, they cannot infer the scenes that are totally occluded. Instead, our method predicts semantic labeling and 3D shapes for both visible and invisible objects in a 3D scene. <ref type="bibr" target="#b13">Liu et al. [2017]</ref> introduced 3DCNN-DQN-RNN for parsing 3D point cloud of a scene. PointNet <ref type="bibr" target="#b15">[Qi et al., 2016]</ref> and PointNet++ <ref type="bibr" target="#b15">[Qi et al., 2017]</ref> develop deep learning framework on 3D point cloud for scene semantic labeling and other 3D shape analysis tasks. These methods take the 3D point cloud of whole 3D scene as the input. On the contrary, our method takes a single depth image for semantic scene completion. <ref type="bibr" target="#b3">Firman et al. [2016]</ref> inferred the occluded 3D object shapes from a single depth image via random forest. <ref type="bibr" target="#b19">Zheng et al. [2013]</ref> completed the occluded scene in the input depth image with a set of pre-defined rules and refined the completion results by physical reasoning. These methods perform scene segmentation and completion in two separate steps. <ref type="bibr">Recently, Song et al. [2017]</ref> proposed 3D SSCNet for simultaneously predicting the semantic labels and volumetric occupancy of the 3D objects from a single depth image. Although this method unifies segmentation and completion and significantly improves the result, the expensive 3D CNN limits the input volume resolution and network depth, and thus restrains its performance. By combining 2D CNN and 3D CNN, our method efficiently reduces the training and inference cost, enhances the network depth and thus significantly improves the result accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">3D Scene Completion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">3D Object Completion</head><p>A set of methods reconstruct the 3D object shape from a single depth image using 3D shape retrieval <ref type="bibr" target="#b16">[Rock et al., 2015]</ref>, Convolutional Deep Belief Network (CDBN) <ref type="bibr">[Wu et al., 2015;</ref><ref type="bibr" target="#b14">Nguyen et al., 2016]</ref>, or a 3D Generative Adversarial Networks (GAN) <ref type="bibr" target="#b19">[Yang et al., 2017]</ref>. All these methods model the input depth maps and resulting 3D shapes with a 3D volumetric representation. Although these methods can be combined with other scene segmentation methods for predicting 3D shapes of the visible object in the input depth map, they cannot be used for inferring objects that are totally occluded. Our method is designed for recovering complete 3D shapes of both visible and occluded 3D objects from a single depth image of a 3D scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Hybrid Representation for 3D Deep Learning</head><p>A set of methods <ref type="bibr" target="#b1">[Choy et al., 2016;</ref><ref type="bibr" target="#b5">Girdhar et al., 2016;</ref><ref type="bibr" target="#b10">Jimenez Rezende et al., 2016;</ref><ref type="bibr" target="#b18">Yan et al., 2016;</ref><ref type="bibr" target="#b9">H?ne et al., 2017]</ref> cascade 2D encoder and 3D decoder for reconstructing 3D object shapes from a single-view color image. Because the feature vector extracted from the 2D color image has no 3D position information, it is mapped to a low resolution 3D volume (2 3 or 4 3 ) via fully connected (FC) neural networks. In our method, the 2D features extracted from the input depth map inherit the 3D positions of the input depth map and thus can be directly mapped to a 3D volume via projection.  combined 3D GAN and 2D recurrent convolutional networks (RCN) for reconstructing high resolution 3D shapes from corrupted 3D models, where the 2D RCN takes 2D slices of the 3D volume generated by the 3D GAN as input for completing 3D shapes. Our method takes 2D depth map as the input and applies 3D volume CNN to the projected 2D features for 3D scene completion. Kalogerakis et al. <ref type="bibr">[2017]</ref> fused semantic segmentation results generated by 2D fully convolutional networks (FCN) from different views into a conditional random field defined on 3D object surface for object segmentation. Instead of directly obtaining the segmentation results in each view, the 2D View CNN in VVNet only extracts intermediate geometric features for the 3D Volume CNN to learn the 3D context information in the whole volume for scene completion.  for each voxel of the scene inside the view frustum. Here we follow  to denote the number of object classes as N and mark all empty voxels by c 0 . As illustrated in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the VVNet consists of three parts: a 2D view network for extracting 2D features from the input depth image, a feature projection layer for converting the 2D features maps into a 3D feature volume, and a 3D volume network for inferring voxel labels from the projected feature volume. In the following part of this section, we discuss the design of each part and VVNet training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">2D View Network</head><p>The 2D view network extracts 2D geometry features from the input depth map. For this purpose, our method first computes the normal map from the input depth image and then feeds both normal and depth maps to the 2D view network. As illustrated in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, we apply the residual neural network (ResNet) structure in our 2D view network design. Each ResNet block includes two convolution layers and a shortcut from input to output as shown in <ref type="figure" target="#fig_0">Figure 1</ref>(e). When a pooling layer is applied after a ResNet block, the feature map resolution is halved and the number of feature maps is doubled. For 2D ResNet used in the view network, a batch normalization layer is applied after each convolution layer. The total number of the layers in the 2D view network is determined by the resolution of the target feature maps (320 ? 240 in <ref type="figure" target="#fig_0">Figure 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Projection</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, the projection layer projects the 2D feature maps constructed by the 2D view network into a 3D feature volume. The voxel size of the feature volume is set to be twice of the average distance of the neighboring depth pixels. Because the feature volume resolution is always lower than the feature map resolution, several neighboring features will be projected into the same voxel. This is equivalent to perform a pooling operation during the projection.</p><p>To project the 2D feature maps into the feature volume, we first construct an axis-aligned volume in the viewing coordinate system as in  and then upsample the feature maps to input depth resolution with the nearest neighboring sampling. After that, we project the feature vectors of all depth pixels into the voxels of the 3D feature volume according to their viewing directions and depth values. Finally, we get the feature vector in each voxel by averaging the feature vectors projected into it. For the voxels that are not occupied by any depth image pixels, their feature vectors are zero. We found that other pooling operations (e.g. max pooling) can also be used for computing the feature vector in each voxel but have the similar affect to the result. Instead of downsampling the depth image to the feature map resolution, we apply this super-sampling scheme for feature projection so that we can avoid the holes caused by the perspective projection and large variations in the depth image. During training, we record the mapping between feature map pixels and voxels in a table for gradient back-propagation.</p><p>In our current implementation, we set the resolutions of our feature volumes to be same as the ones in the SSCNet  for a fair comparison. We set the resolution of the feature map that corresponds to the largest 3D volume (240 ? 144 ? 240) to be 640 ? 480 because for the TSDF in this resoluton, the one constructed from half-resolution depth image (320?240) is almost same as the one constructed from 640 ? 480 depth image. For the downsampled feature volumes, we scale down the resolutions of the feature maps accordingly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">3D Volume Network</head><p>After the feature projection layer, the view-dependent 2D feature maps extracted by the view CNN are converted to a viewindependent 3D feature volume. In this step, we extract the 3D context of the scene and infer the semantic label of voxels from the 3D feature volume via a 3D volume CNN. To this end, we follow the SSCNet in our 3D feature volume CNN design <ref type="figure" target="#fig_0">(Figure 1(b)</ref>), where the 3D features are first extracted by the ResNet layers and then fed into a backbone network to generate the semantic labels for all voxels. We also design a new backbone network with enlarged reception field. As shown in <ref type="figure" target="#fig_0">Figure 1(d)</ref>, the new backbone adds a new pooling layer to downsample the extracted 3D features and then applies the original backbone network to 3D features at a lower resolution. After that, we deconvolute the concatenated features to the output resolution and generate volumetric semantic labels using two convolution layers. Note that in 3D volume network, we do not apply batch normalization after each convolution layer. We denote the VVNet with the SSCNet backbone as VVNet, and the one with the new backbone as VVNetR. In Section 4, we demonstrate that our VVNetR not only reduces the computational cost for learning the 3D context, but also improves the result accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Trade-off between View and Volume Networks</head><p>The VVNet provides a flexible and general framework for combining 2D and 3D CNN for 3D scene analysis. By choosing different resolutions of the result feature maps of 2D view network, we can make different trade-offs between the depth of 2D view network and 3D volume network. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates three VVNets, which are named as VVNetR-120, VVNetR-60, and VVNetR-30, where the numbers in the name indicate the resolution of the projected feature volume. On one side, as the resolution of the projected feature volume decreases, more layers in the 3D volume CNN are replaced by the corresponding 2D view network layers, which results in less computations and smaller memory footprints in both network training and inference. On the other side, as more 3D volume network layers are replaced by the 2D view network layers, more detailed 3D context information in the scene may be lost in the projected 3D feature volume and thus leads to degradation of the result accuracy. We evaluate this trade-off in details in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Network Training</head><p>Given the training data set (i.e the depth images and ground truth volumetric object labels of 3D scene), the VVNet can be trained end-to-end. For this purpose, we use the voxel-wise softmax as the loss function as in  in the network training. To compute the loss function, we remove all empty voxels in the visible free space, outside field of view and outside the room but include all non-empty voxels and occluded empty voxels. We do not apply the data balancing scheme in  in our training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We have implemented VVNet in TensorFlow under Ubuntu 16.04 on a workstation with an Intel 6700K CPU and two NVIDIA GTX 1080Ti GPUs. We use SGD optimization in VVNet training, where we set the momentum as 0.9, learning rate as 0.01, and weight decay as 0.0005. We found that the original SSCNet is not fully trained. For a fair comparison, we ported SSCNet in TensorFlow and trained it with more iterations (150K iteration with batch size 4). We denote our SSCNet implementation as SSCNet * in the following discussions.  <ref type="bibr" target="#b5">[Guo et al., 2015]</ref>. Because some manually labeled volumes and their corresponding depth images are not well aligned in the NYU dataset, we also use NYUCAD dataset in <ref type="bibr" target="#b3">[Firman et al., 2016]</ref> in our experiments, in which the depth map is rendered from the label volume. For both NYU and NYUCAD datasets, we fine tune the VVNet models trained from SUNCG dataset with 4K iterations. After that, we test the models at every 200 iterations and pick the best one as the final result.</p><p>Error Metric For each neural network, we measure the precision, recall, and IOU of all test results as in <ref type="bibr" target="#b19">[Song et al., 2017]</ref>. The IOU measures the overlapped ratio between intersection and union of the positive prediction volume and the ground truth volume. For scene completion (SC) task , the ground truth volume includes all the occluded voxels in the view frustum. For semantic scene completion (SSC) task,  <ref type="bibr" target="#b12">Lin et al., 2013]</ref> 58.5 49.9 36.4 0.0 11.7 13.3 14.1 9.4 29.0 24.0 6.0 7.0 16.2 1.1 12.0 <ref type="bibr" target="#b4">[Geiger and Wang, 2015]</ref>   the ground truth volume includes both occluded voxels and visible surface voxels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation Test</head><p>We validate our VVNet design with a set of ablation tests on the SUNCG dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does Higher Image Resolution Help?</head><p>We downsample the depth input images to half resolution (320 ? 240) and use them for training a SSCNet * -half model and a VVNet-120-half model. As shown in <ref type="table">Table.</ref> 1, the limited TSDF resolution in SSCNet cannot preserve the geometric details in high resolution input image and thus leads to very similar results for both SSCNet * and SSCNet * -half. Note that the TSDF resolution in SSCNet cannot be increased anymore due to the large memory and computational cost of 3D CNN. On the contrary, our method can fully exploit the high resolution input. Compared to VVNet-120-half, the IOUs of the VVNet-120 for SC and SSC tasks improve 0.4% and 1.2% respectively.  Does Larger Reception Field Help? <ref type="table">Table.</ref> 1 compares the performances of VVNet-120 and VVNetR-120. For SC and SSC tasks, our new backbone with larger reception field in VVNetR-120 provides 1.9% and 5.4% IOU improvements compared to VVNet-120.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Does</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Trade-offs between View and Volume Networks</head><p>We compare the performances of VVNet models with different combinations of 2D view and 3D volume CNNs. As shown in <ref type="table">Table.</ref> 1, the IOUs of the VVNetR-60 is slightly worse than the IOUs of the VVNetR-120 (0.3% and 1.4% decreasing for SC and SSC respectively). However, the VVNetR-60 only requires half of memory footprint and 70% computational time that the VVNetR-120 needs in the training. For VVNetR-30, we observe a relatively large performance drop from the VVNetR-120. A possible reason is that the low resolution feature volume projected from the 2D features lose too much detailed 3D context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation</head><p>we test the performance of VVNet network for semantic scene completion task on all three datasets and compare our  method with other existing approaches.</p><p>SUNCG For SUNCG dataset, we compare the IOUs of VVNetR-120, SSCNet, and SSCNet * for both SC and SSC tasks. As shown in <ref type="table">Table.</ref> 1, the VVNetR-120 achieves the best performance in both SC and SSC tasks. Compared to SSCNet * , the IOUs of VVNetR-120 increase 1.4% and 5.9% for SC and SSC tasks respectively. <ref type="table">Table.</ref> 1 also lists the IOU for each object class. Note that for all object class except floor, the VVNetR-120 achieves the best results. <ref type="figure">Fig. 3</ref> illustrates the semantic scene completion results generated by different methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NYU &amp; NYUCAD</head><p>We compare our method with SSCNet * and other existing methods on both NYU and NYUCAD dataset. <ref type="table" target="#tab_4">Table 2</ref> and 4 list the performances of these methods on both datasets. For both NYU and NYUCAD datasets, our VVNet achieves the best performance among all the methods <ref type="bibr" target="#b12">[Lin et al., 2013;</ref><ref type="bibr" target="#b4">Geiger and Wang, 2015]</ref>. We believe that the small performance gap between our VVNetR models and SSCNet * on the NYU dataset is caused by the misalignment between the input and output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memory Footprint &amp; Computational Cost</head><p>Compared to the SSCNet that is based on 3D CNN, our VVNet combines the 2D CNN and 3D CNN and thus significantly reduces both memory footprint and computational cost in training and inference. <ref type="table">Table.</ref>3 compares the memory footprints and computational costs of different models for training and inference. Compared to SSCNet, the VVNetR-120 provides much better accuracy and three-times speed up for training, as well as more than 7 times speed up for inference. With 40% memory footprint of SSCNet in training, the VVNetR-60 offers 4.7 times speed up for training and more than 10 times speed up for inference at the cost of slightly degraded accuracy compared to VVNetR-120. Note that the accuracy of VVNetR-60 is still better than the SSCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduce a view-volume CNN for semantic scene completion from a single depth image. Our method concatenates a 2D view CNN and a 3D volume CNN with a projection layer and can be trained end-to-end. The VVNet provides a general and flexible framework for fusing 2D and 3D CNNs for efficient 3D learning. We validate our method on both synthetic and real datasets. Results shown that our method significantly improves the result accuracy, reduces the computational cost in training and inference, and offers variant trade-offs between the cost and accuracy. For the future work, it is interesting to explore the tradeoffs between 2D CNNs and 3D CNNs and find solutions to improve the performance of VVNet with less 3D volume layers. Another interesting direction is to extend our viewvolume framework for multiple view CNN solutions.  <ref type="figure">Figure 3</ref>: Semantic scene completion results generated by different methods for SUNCG and NYU datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The network structures of VVNet and SSCNet for semantic scene completion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Different trade-offs between view and volume networks in VVNetR design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>IoU ceil. floor wall win. chair bed sofa table tvs furn. objs. avg. SSCNet 76.3 95.2 73.5 96.3 84.9 56.8 28.2 21.3 56.0 52.7 33.7 10.9 44.3 25.4 46.4 SSCNet * 90.4 89.7 82.0 97.8 88.2 59.4 37.3 39.2 77.9 68.9 48.3 31.5 56.8 44.9 59.1 SSCNet * -half 90.5 89.5 81.9 97.8 88.0 60.8 34.8 39.8 77.5 69.5 47.8 29.8 56.0 44.8 58.8 VVNet-120-half 90.7 89.6 82.1 97.9 85.2 59.4 47.5 44.2 77.4 71.1 49.3 34.2 58.2 49.0 61.3 VVNet-120-depth 90.6 89.6 82.0 97.6 84.8 58.6 44.5 44.8 77.6 70.7 48.8 33.2 57.8 46.2 60.4 Performances of different variant VVNet design on the SUNCG dataset. half refers to the network that takes half-resolution image as input. depth refers to the network that use depth only as input.</figDesc><table><row><cell></cell><cell>scene completion</cell><cell>semantic scene completion</cell></row><row><cell cols="3">Network prec. recall VVNet-120 90.8 90.0 82.5 97.9 85.4 58.6 49.2 45.3 79.2 71.8 50.3 37.3 62.0 50.9 62.5</cell></row><row><cell>VVNetR-120</cell><cell cols="2">90.8 91.7 84.0 98.4 87.0 61.0 54.8 49.3 83.0 75.5 55.1 43.5 68.8 57.7 66.7</cell></row><row><cell>VVNetR-60</cell><cell cols="2">90.6 92.5 83.7 97.6 86.7 60.2 54.4 47.2 80.7 75.0 53.8 39.4 66.9 56.1 65.3</cell></row><row><cell>VVNetR-30</cell><cell cols="2">88.8 90.2 81.0 98.0 86.4 55.6 54.8 41.8 78.0 72.1 48.7 31.6 63.2 51.8 62.0</cell></row><row><cell cols="2">Table 1: scene completion</cell><cell>semantic scene completion</cell></row><row><cell>Method</cell><cell>prec. recall IoU</cell><cell>ceil. floor wall win. chair bed sofa table tvs furn. objs. avg.</cell></row><row><cell>[</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>The performances of different scene completion methods on the NYU dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Memory footprints and computational times of different networks for model training and inference.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Performances of different methods on NYUCAD dataset.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">View-Volume NetworkGiven a single-view depth image I d captured from a 3D scene, our VVNet outputs semantic label C = c 0 , ...c N +1</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depthcomp: Real-time depth image completion based on prior semantic scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; A</forename><surname>Breckon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Atapour-Abarghouei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Breckon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="628" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scancomplete: Large-scale scene completion and semantic segmentation for 3d scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.10215</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Structured prediction of unobserved voxels from a single depth image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Firman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5431" to="5440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint 3d object and layout inference from a single rgb-d image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang ; Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaohui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="183" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic scene completion combining colour and depth: preliminary experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Girdhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04735</idno>
		<idno>arXiv:1504.02437</idno>
	</analytic>
	<monogr>
		<title level="m">Ruiqi Guo, Chuhang Zou, and Derek Hoiem. Predicting complete 3d models of indoor scenes</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="484" to="499" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Aligning 3D models to RGB-D images of cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4731" to="4740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning advances in computer vision with 3d data: A survey</title>
		<idno type="arXiv">arXiv:1704.00710.2017</idno>
		<idno>20:1-20:38</idno>
	</analytic>
	<monogr>
		<title level="m">Elisavet Chatzilari, Spiros Nikolopoulos, and Ioannis Kompatsiaris</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">50</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
	<note>Anastasia Ioannidou</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<publisher>Melinos</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4996" to="5004" />
		</imprint>
	</monogr>
	<note>Kalogerakis et al., 2017] Evangelos Kalogerakis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">3D shape segmentation with projective convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Averkiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddhartha</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3779" to="3788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Holistic scene understanding for 3d object detection with rgbd cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1417" to="1424" />
		</imprint>
	</monogr>
	<note>Dahua Lin, Sanja Fidler, and Raquel Urtasun</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">3dcnn-dqn-rnn: A deep reinforcement learning framework for semantic parsing of large-scale 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5678" to="5687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A field model for repairing 3d shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5676" to="5684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Qi et al., 2017] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
	<note>NIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Threedimensional object detection and layout prediction using clouds of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhile</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">B</forename><surname>Sudderth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<editor>Shuran Song, Fisher Yu, Andy Zeng, Angel X Chang, Manolis Savva, and Thomas Funkhouser</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2626" to="2634" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Aditya Khosla, Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d shapenets: A deep representation for volumetric shape modeling</title>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>Shuran Song</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Perspective transformer nets: Learning single-view 3d object reconstruction without 3d supervision</title>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1696" to="1704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d object reconstruction from a single depth view with adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3127" to="3134" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
