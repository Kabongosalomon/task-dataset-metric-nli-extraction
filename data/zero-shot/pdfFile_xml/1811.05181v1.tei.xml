<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gradient Harmonized Single-stage Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
							<email>byli@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<email>yuliu@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Multimedia Laboratory</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gradient Harmonized Single-stage Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the great success of two-stage detectors, single-stage detector is still a more elegant and efficient way, yet suffers from the two well-known disharmonies during training, i.e. the huge difference in quantity between positive and negative examples as well as between easy and hard examples. In this work, we first point out that the essential effect of the two disharmonies can be summarized in term of the gradient. Further, we propose a novel gradient harmonizing mechanism (GHM) to be a hedging for the disharmonies. The philosophy behind GHM can be easily embedded into both classification loss function like cross-entropy (CE) and regression loss function like smooth-L1 (SL1) loss. To this end, two novel loss functions called GHM-C and GHM-R are designed to balancing the gradient flow for anchor classification and bounding box refinement, respectively. Ablation study on MS COCO demonstrates that without laborious hyper-parameter tuning, both GHM-C and GHM-R can bring substantial improvement for single-stage detector. Without any whistles and bells, the proposed model achieves 41.6 mAP on COCO testdev set which surpass the state-of-the-art method, Focal Loss (FL) + SL1, by 0.8. The code 1 is released to facilitate future research.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>One-stage approach is the most efficient and elegant framework for object detection. But for a long time, the performance of one-stage detectors has a large gap from that of two-stage detectors. The most challenging problem for the training of one-stage detector is the serious imbalance between easy and hard examples as well as that between positive and negative examples. The huge number of easy and background examples tend to overwhelm the training. But these problems are not existed for two-stage detectors, owing to the proposal-driven mechanism. To handle the former imbalance problem, example mining based methods such as OHEM (Shrivastava, Gupta, and Girshick 2016) are in common use, but they directly abandon most examples and the training is inefficient. For the latter imbalance, the recent work, Focal Loss <ref type="bibr" target="#b6">(Lin et al. 2017b)</ref>, has tried to address it by rectifying the cross-entropy loss function to a elaborately designed form. However, Focal Loss adopts two hyper-parameters which should be tuned with a lot of efforts. And it is a static loss which is not adaptive for the changing of data distribution, which varies along with the training process.</p><p>In this work, we first point out that the class imbalance can be summarized to the imbalance in difficulty and the imbalance in difficulty can be summarized to the imbalance in gradient norm distribution. If a positive example is wellclassified, it is an easy example and the model benefit little from it, i.e. a little magnitude of gradient will be produced by this sample. And a misclassified example should attract attention of the model no matter which class it belongs to. So if viewed globally, the large amount of negative examples tends to be easy to classify and the hard examples are usually positive. So the two kind of imbalance can be roughly summed up as attribute imbalance.</p><p>Moreover, we claim that the imbalance of examples with different attributes (hard/easy and pos/neg) can be implied by the distribution of gradient norm. The density of examples w.r.t. gradient norm, which we call as gradient density for convenient, varies largely as showed in the left of <ref type="figure" target="#fig_0">Fig.1</ref>. The examples with very small gradient norm have a quite large density which is corresponding to the large amount of easy negative examples. Although one easy example has less contribution on the global gradient than a hard example, the total contribution of the huge amount of easy examples can overwhelm the contribution of the minority of hard examples and the training process will be inefficient. Besides, we also discover that the density of examples with very large gradient norm (very hard examples) is slightly larger than the density of the medium examples. And we consider these very hard examples mostly as outliers since they exist stably even when the model is converged. The outliers may affect the stability of model since their gradients may have a large discrepancy from the other common examples.</p><p>Inspired by the analysis of gradient norm distribution, we propose a gradient harmonizing mechanism (GHM) to train the one-stage object detection model in an efficient, which focuses on the harmony of gradient contribution of different examples. The GHM first performs statistics on the number of examples with similar attributes w.r.t their gradient density and then attach a harmonizing parameter to the gradient of each example according to the density. The effect of GHM compared with CE and FL is illustrated in the right of <ref type="figure" target="#fig_0">Fig.1</ref>. Training with GHM, the huge amount of cumulated gradient produced by easy examples can be largely downweighted and the outliers can be relatively down-weighted as well. In the end, the contribution of each kind of examples will be balanced and the training can be more efficient and stable.</p><p>In practice, the modification of gradient can be equivalently implemented by reformulating the loss function, we embed the GHM into the classification loss, which is denoted as GHM-C loss. This loss function is elegantly formulated without many hyper-parameters to tune. Since the gradient density is a statistical variable depending on the examples distribution in a mini-batch, GHM-C is a dynamic loss that can adapt to the change of data distribution in each batch as well as to the updating of model. To showcase the generality of GHM, we also adopt it in the box regression branch as the form of GHM-R loss.</p><p>Experiments on the bounding box detection track of the challenging COCO benchmark show that the GHM-C loss has a large gain compared to the traditional cross-entropy loss and slightly surpasses the state-of-the-art Focal Loss. And the GHM-R loss also has better performance than the commonly used smooth L 1 loss. The combination of GHM-C and GHM-R attains a new state-of-the-art performance on COCO tes-dev set.</p><p>Our main contributions are as follows:</p><p>1. We reveal the essential principle behind the significant example imbalance in one-stage detector in term of gradient norm distribution, and propose a novel gradient harmonizing mechanism (GHM) to handle it.</p><p>2. We embed the GHM into the loss for classification and regression as GHM-C and GHM-R respectively, which rectify the gradient contribution of examples with different attributes and is robust to hyper-parameters.</p><p>3. Collaborating with GHM, we can easily train a single stage detector without any data sampling strategy and achieve the state-of-the-art result on COCO benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Object Detection: Object detection is one of the most basic and important task in the field of computer vision. Deep convolutional neural network (CNN) based methods, e.g. <ref type="bibr" target="#b8">(Ren et al. 2015;</ref><ref type="bibr" target="#b8">Redmon and Farhadi 2017;</ref>, have become more and more developed and achieved great success in recent years, owing to the significant progress of network architecture such as (Simonyan and Zisserman 2014; <ref type="bibr" target="#b4">Huang et al. 2017)</ref>. Advanced object detection frameworks can be divided into two categories: one-stage detector and two-stage detector. Most state of the art methods use two-stage detectors, e.g. <ref type="bibr" target="#b8">Ren et al. 2015;</ref><ref type="bibr" target="#b6">Lin et al. 2017a;</ref><ref type="bibr" target="#b11">Zeng et al. 2018)</ref>. They are mainly based on the Region CNN (R-CNN) architecture. These approaches first obtain a manageable number of region proposals called region of interest (RoI) from the nearly infinite candidate regions and then use the network to evaluate each RoI.</p><p>One-stage detectors have the advantage of simple structures and high speed. SSD <ref type="bibr" target="#b1">Fu et al. 2017)</ref>, <ref type="bibr">YOLO (Redmon et al. 2016;</ref><ref type="bibr" target="#b8">Redmon and Farhadi 2017;</ref><ref type="bibr" target="#b8">Redmon and Farhadi 2018)</ref> for generic object detection and RSA (Song et al. ; ) for face detection have achieved good speed/accuracy trade-off. However, they can hardly surpass the accuracy of two-stage detectors. Reti-naNet <ref type="bibr" target="#b6">(Lin et al. 2017b</ref>) is the state of the art one-stage object detector that achieve comparable performance to twostage detectors. It adopts an architecture modified from <ref type="bibr">RPN (Ren et al. 2015)</ref> and focuses on addressing the class imbalance during training.</p><p>Object Functions for Object Detector: Most detection models use cross entropy based loss function for classification <ref type="bibr" target="#b8">Ren et al. 2015;</ref><ref type="bibr" target="#b0">Dai et al. 2016;</ref><ref type="bibr" target="#b6">Lin et al. 2017a;</ref>. While onestage detectors face a problem of extreme class imbalance that two-stage detectors do not have. Earlier methods try to use hard example mining methods, e.g. <ref type="bibr" target="#b8">(Shrivastava, Gupta, and Girshick 2016;</ref><ref type="bibr" target="#b1">Felzenszwalb, Girshick, and McAllester 2010)</ref>, but they discard most examples and cannot handle the problem well. Recently the work <ref type="bibr" target="#b6">(Lin et al. 2017b</ref>) reformulate the cross-entropy loss so that easy negatives are downweighted and the hard examples are unaffected or even upweighted.</p><p>For stable training of box regression, Fast R-CNN (Girshick 2015) introduces the smooth L 1 loss. This loss reduces the impact of outliers so that the training of model can be more stable. Almost all the following works take the smooth L 1 loss as a default for box regression <ref type="bibr" target="#b8">(Ren et al. 2015;</ref><ref type="bibr" target="#b0">Dai et al. 2016;</ref><ref type="bibr" target="#b6">Lin et al. 2017a;</ref>.</p><p>The work (Imani and White 2018) tries to improve regression performance by changing the target to a distribution and using a histogram loss to calculate the K-L divergence of prediction and target. The work (Chen et al. 2017) balances multi-task losses by dynamically tuning gradient magnitude of different task branches.</p><p>Our GHM based loss harmonizes the contribution of examples on the basis of the distribution of their gradient, so that it can handle both the class imbalance and the outliers problem well. It can also adapt the weights to the changing of data distribution in each mini-batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Harmonizing Mechanism Problem Description</head><p>Similar to <ref type="bibr" target="#b6">(Lin et al. 2017b)</ref>, our efforts here are focused on classification in one-stage object detection where the classes (foreground/background) of examples are quite imbalanced. For a candidate box, let p ? [0, 1] be the probability predicted by the model and p * ? {0, 1} be its ground-truth label for a certain class. Consider the binary cross entropy loss:</p><formula xml:id="formula_0">L CE (p, p * ) = ? log(p) if p * = 1 ? log(1 ? p) if p * = 0</formula><p>(1) Let x be the direct output of the model such that p = sigmoid(x), we have the gradient with regard to x:</p><formula xml:id="formula_1">?L CE ?x = p ? 1 if p * = 1 p if p * = 0 = p ? p * (2)</formula><p>We define g as follows:</p><formula xml:id="formula_2">g = |p ? p * | = 1 ? p if p * = 1 p if p * = 0 (3)</formula><p>g equals to the norm of gradient w.r.t x. The value of g represents attribute (e.g. easy or hard) of an example and implies the example's impact on the global gradient. Although the strict definition of gradient is on the whole parameter space, which means g is a relative norm of an example's gradient, we call g as gradient norm in this paper for convenience. <ref type="figure" target="#fig_1">Fig.2</ref> shows the distribution of g from a converged onestage detection model. Since the easy negatives have a dominant number, we use log axis to display the fraction of examples to demonstrate the details of the variance of examples with different attributes. It can be seen that the number of very easy examples is extremely large, which have a great impact on the global gradient. Moreover, we can see that a converged model still can't handle some very hard examples whose number is even larger than the examples with medium difficulty. These very hard examples can be regarded as outliers since their gradient directions tends to vary largely from the gradient directions of the large amount of other examples. That is, if the converged model is forced to learn to classify these outliers better, the classification of the large number of other examples tends to be less accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gradient Density</head><p>To handle the problem of the disharmony of gradient norm distribution, we introduce a harmonizing approach with regard to gradient density. Gradient density function of training examples is formulated as Equation.4: where g k is the gradient norm of the k-th example. And</p><formula xml:id="formula_3">GD(g) = 1 l (g) N k=1 ? (g k , g)<label>(4)</label></formula><formula xml:id="formula_4">? (x, y) = 1 if y ? 2 &lt;= x &lt; y + 2 0 otherwise (5) l (g) = min(g + 2 , 1) ? max(g ? 2 , 0)<label>(6)</label></formula><p>The gradient density of g denotes the number of examples lying in the region centered at g with a length of and normalized by the valid length of the region. Now we define the gradient density harmonizing parameter as:</p><formula xml:id="formula_5">? i = N GD(g i )<label>(7)</label></formula><p>where N is the total number of examples. To better comprehend the gradient density harmonizing parameter, we can rewrite it as ? i = 1 GD(gi)/N . The denominator GD(g i )/N is a normalizer indicating the fraction of examples with neighborhood gradients to the i-th example. If the examples are uniformly distributed with regard to gradient, GD(g i ) = N for any g i and each example will have the same ? i = 1, which means nothing is changed. Otherwise, the examples with large density will be relatively down-weighted by the normalizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GHM-C Loss</head><p>We embed the GHM into classification loss by regarding ? i as the loss weight of the i-th example and the gradient density harmonized form of loss function is: <ref type="figure" target="#fig_2">Fig.3</ref> illustrates the reformulated gradient norm of different losses. Here we take the original gradient norm of CE, i.e. g = |p ? p * |, as the x-axis for convenient view since the density is calculated according to g. We can see that the curves of Focal Loss and GHM-C loss have similar trend, which implies that Focal Loss with the best hyperparameters is similar with uniform gradient harmonizing. Furthermore, GHM-C has one more merit that Focal loss ignores: down-weighting the gradient contribution of outliers. With our GHM-C loss, the huge number of very easy examples are largely down-weighted and the outliers are slightly down-weighted as well, which simultaneously addresses the attribute imbalance problem and the outliers problem. From the right figure in <ref type="figure" target="#fig_0">Fig.1</ref> we can better see that GHM-C harmonizes the total gradient contribution of different group of examples. Since the gradient density is calculated every iteration, the weights of examples are not fixed w.r.t. g (or x) like focal loss but adaptive to current state of model and mini-batch of data. The dynamic property of GHM-C loss makes the training more efficient and robust.</p><formula xml:id="formula_6">L GHM ?C = 1 N N i=1 ? i L CE (p i , p * i ) = N i=1 L CE (p i , p * i ) GD(g i )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unit Region Approximation</head><p>Complexity Analysis: The naive algorithm to calculate the gradient density values of all examples has a time complexity of O(N 2 ), which can be easily attained from Equations 4 and 8. Even parallel computed, each computing unit still bears a computation of N . And as far as we know, the best algorithm first sort the examples by gradient norm with a complexity of O(N log N ) and then use a queue to scan the examples and get their density with O(N ). This sorting based method can not gain much from parallel computing. Since N of an image in one-stage detector can be 10 5 or even 10 6 , to directly calculate the gradient density is quite time consuming. So we introduce an alternative approach to approximately attain the gradient density of examples.</p><p>Unit Region: We divide the range space of g into individual unit regions with a length of , and there are M = 1 unit regions. Let r j be the unit region with index j so that r j = [(j ? 1) , j ). Let R j denote the number of examples lying in r j . We define ind(g) = t s.t. (t ? 1) &lt;= g &lt; t , which is the index function to the unit region that g lies in.</p><p>Then we define the approximate gradient density function as:?</p><formula xml:id="formula_7">D(g) = R ind(g) = R ind(g) M<label>(9)</label></formula><p>Then we have the approximate gradient density harmonizing parameter:?</p><formula xml:id="formula_8">i = N GD(g i )<label>(10)</label></formula><p>Consider the special case where = 1: there are just one unit region and all examples lie in it, so obviously every ? i = 1 and each example keep their original gradient contribution. Finally we have the reformulated loss function:</p><formula xml:id="formula_9">L GHM ?C = 1 N N i=1? i L CE (p i , p * i ) = N i=1 L CE (p i , p * i ) GD(g i )<label>(11)</label></formula><p>From Equation. 9 we can see that the examples lying in the same unit region share the same gradient density. So we can use the algorithm of histogram statistics and the computation of all the gradient density values has a time complexity of O(M N ). And parallel computing can be applied so that each computing unit has a computation of M . In practice, we can attain good performance with quite small number of unit regions. That is M is fairly small and the calculation of loss is efficient.</p><p>EMA: Mini-batch statistics based methods usually face a problem: when many extreme data are just sampled in one mini-batch, the statistical result will be a serious noise and the training will be unstable. Exponential moving average (EMA) is a common used method to address this problem, e.g., SGD with momentum <ref type="bibr" target="#b9">(Sutskever et al. 2013)</ref> and Batch Normalization (Ioffe and <ref type="bibr" target="#b4">Szegedy 2015)</ref>. Since in the approximation algorithm the gradient densities come from the numbers of examples in the unit regions, we can apply EMA on each unit region to obtain more stable gradient densities for examples. Let R (t) j be the number of examples in the jth unit region in the t-th iteration and S (t) j be the moving averaged number. We have:</p><formula xml:id="formula_10">S (t) j = ?S (t?1) j + (1 ? ?)R (t) j (12)</formula><p>where ? is the momentum parameter. We use the averaged number S j to calculate the gradient density instead of R j :</p><formula xml:id="formula_11">GD(g) = S ind(g) = S ind(g) M<label>(13)</label></formula><p>With EMA, the gradient density will be more smooth and insensitive to extreme data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GHM-R Loss</head><p>Consider the parameterized offsets, t = (t x , t y , t w , t h ), predicted by box regression branch and the target offsets, t * = (t * x , t * y , t * w , t * h ), computed from ground-truth. The regression loss usually adopts the smooth L 1 loss function:</p><formula xml:id="formula_12">L reg = i?{x,y,w,h} SL 1 (t i ? t * i )<label>(14)</label></formula><p>where</p><formula xml:id="formula_13">SL 1 (d) = ? ? ? ? ? d 2 2? if |d| &lt;= ? |d| ? ? 2 otherwise (15)</formula><p>where ? is the division point between the quadric part and the linear part, and usually set to 1/9 in practice. Since d = t i ? t * i , the gradient of smooth L 1 loss w.r.t t i can be expressed as:</p><formula xml:id="formula_14">?SL 1 ?t i = ?SL 1 ?d = ? ? ? d ? if |d| &lt;= ? sgn(d) otherwise<label>(16)</label></formula><p>where sgn is the sign function. Note that all the examples with |d| larger than the division point have the same gradient norm | ?SL1 ?ti | = 1, which makes the distinguishing of examples with different attributes impossible if depending on the gradient norm. An alternative choice is directly using |d| as the measurement of different attributes, but the new problem is |d| can reach to infinite in theory and the unit region approximation can not be implemented.</p><p>To conveniently apply GHM on regression loss, we first modify the traditional SL 1 loss into a more elegant form:</p><formula xml:id="formula_15">ASL 1 (d) = d 2 + ? 2 ? ?<label>(17)</label></formula><p>This loss shares similar property with SL 1 loss: when d is small it approximates a quadric function (L 2 loss) and when d is large is approximate a linear function (L 1 loss). We denote the modified loss function as Authentic Smooth L 1 (ASL 1 ) loss for its good property of authentic smoothness, which means all the degrees of derivatives are existed and continuous. In contrast, the second derivative of smooth L 1 loss does not exist at the point d = ?. Furthermore, the ASL 1 loss has an elegant form of gradient w.r.t d:</p><formula xml:id="formula_16">?ASL 1 ?d = d d 2 + ? 2<label>(18)</label></formula><p>The range of the gradient is just [0, 1), so the calculation of density in unit regions for ASL 1 loss in regression is as convenient as CE loss in classification. In practice, we set ? = 0.02 for ASL 1 loss to keep the same performance with SL 1 loss. We define gr = | d ? d 2 +? 2 | as the gradient norm of ASL 1 loss and the gradient distribution of a converged model is illustrated in <ref type="figure">Fig.4</ref> We can see that there are large number of outliers. Note that the regression is only performed on the positive examples so it is reasonable for the different distribution trend between classification and regression. Above all, we can apply GHM on regression loss: The reformulated gradient contribution of SL 1 loss, ASL 1 loss and GHM-R loss in <ref type="figure">Fig.5</ref>. The x-axis adopts |d| for convenient comparison.</p><formula xml:id="formula_17">L GHM ?R = 1 N N i=1 ? i ASL 1 (d i ) = N i=1 ASL 1 (d i ) GD(gr i )<label>(19)</label></formula><p>We emphasize that in box regression not all the "easy examples" are unimportant. An easy example in classification is usually a background region with a very low predicted probability and will be definitely excluded from the final candidates. Thus the improvement of this kind of examples makes nearly no contribution to the precision. But in box regression, an easy example still has deviation from the ground truth location. Better prediction of any example will directly improve the quality of the final candidates. Moreover, advanced datasets care more about the localization accuracy. For example, COCO <ref type="bibr" target="#b6">(Lin et al. 2014</ref>) takes the average AP from the IoU threshold 0.5 to 0.95 as the metric to evaluate an algorithm. In this metric, the some of the so called easy examples (those having small errors) are also important because reducing the errors of them can directly improve the AP at high threshold (e.g. AP@IoU=0.75).</p><p>Our GHM-R loss can harmonize the contribution of easy and hard examples for box regression by up-weighting the important part of easy examples and down-weighting the outliers. Experiments show that it can attain better performance than SL 1 and ASL 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate our approach on the challenging COCO benchmark <ref type="bibr" target="#b6">(Lin et al. 2014)</ref>. For training, we follow the common used practice <ref type="bibr" target="#b6">Lin et al. 2017b</ref>) to divide the 40k validation set into a 35k subset and a 5k subset. The union of the 35k validation subset and the whole 80k training set are used for training together and denoted as trainval35k set. The 5k validation subset is denoted as minival set and our ablation study is performed on it. While our main results are reported on the test-dev set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Network Setting: We use RetinaNet <ref type="bibr" target="#b6">(Lin et al. 2017b)</ref> as network architecture and all the experiments adopt ResNet  as backbone with Feature Pyramid Network (FPN) <ref type="bibr" target="#b6">(Lin et al. 2017a</ref>) structure. Anchors use 3 scales and 3 aspect ratios for convenient comparison with focal loss. The input image scale is set as 800 pixel for all experiments. For all ablation studies, ResNet-50 is used. While the final model evaluated on test-dev adopts <ref type="bibr">ResNeXt-101 (Hu, Shen, and Sun 2017)</ref>. In contrast to focal loss, our approach doesn't need a specialized bias initialization.</p><p>Optimization: All the models are optimized by the common used SGD algorithm. We train the models on 8 GPUs with 2 images on each GPU so that the effective mini-batch size is 16. All models are trained for 14 epochs with an initial learning rate of 0.01, which is decreased by a factor 0.1 at the 9th epoch and again at the 12th epoch. We also use a weight decay parameter of 0.0001 and a momentum parameter of 0.9. The only data augmentation operation is horizontal image flipping. For the EMA used in gradient density calculation, we use ? = 0.75 for all experiments since the results are insensitive to the exact value of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GHM-C Loss</head><p>To focus on the effect of GHM-C loss function, experiments in this section all adopt smooth L 1 loss function with ? = 1/9 for the box regression branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline:</head><p>We have trained a model with the standard cross entropy loss as the baseline. The standard initialization will lead to quick divergence, so we follow focal loss <ref type="bibr" target="#b6">(Lin et al. 2017b)</ref> to initialize the bias term of the last layer to b = ? log((1 ? ?)/?) with ? = 0.01 to avoid divergence. However with the specialized initialization the loss of classification is very small, so we up-weight the classification loss by 20 to make the begging loss value reasonable (the begging classification loss value is around 1 now). But when the model converge, the classification loss is still very small and we finally obtain a model with an Average Precision (AP) of 28.6.  Comparison with Other Methods: <ref type="table">Table.</ref>4 shows the results using our loss compared with other loss functions or sampling strategy. Since the reported results on minival of models using focal loss is trained with the input image scale of 600 pixels, for fair comparison we have re-trained a focal loss using a scale of 800 pixels and keep the best parameters of focal loss. We can see our loss has slightly better performance than focal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Unit Region</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GHM-R Loss</head><p>Comparison with Other Losses: The experiments here adopt the best configuration of GHM-C loss for the classification branch. So the first baseline is the model (trained using SL 1 loss) with an AP of 35.8 showed in GHM-C loss experiments. We adopts ? = 0.02 for ASL 1 loss to get comparable results with SL 1 loss and obtain a fair baseline for GHM-R loss. <ref type="table">Table.</ref>5 shows the results of the baseline SL 1 and ASL 1 loss as well as GHM-R loss. We can see a gain of 0.7 mAP based on the ASL 1 loss.     Two-Stage Detector: GHM-R loss for regression is not limited to one-stage detectors. So we have done experiments to verify the effect on two-stage detectors. Our baseline method is faster-RCNN with Res50-FPN model using SL 1 loss for box regression. <ref type="table">Table.</ref>7 shows that GHM-R loss works for two-stage detector as well as one-stage detector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Results</head><p>We use the 32x8d FPN-ResNext101 backbone and Reti-naNet model with GHM-C loss for classification and GHM-R loss for box regression. The experiments are performed on method AP AP .5 AP .75 AP S AP M AP L SL 1 36.4 58.7 38.8 21.1 39.6 47.0 GHM-R 37.4 58.9 39.9 21.8 40.8 48.8 test-dev set. <ref type="table">Table.</ref>3 shows our main result compared with state-of-the-art methods. Our approach achieves excellent performance and outperforms focal loss in most metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion and Discussion</head><p>In this work, we focus on the two imbalance problems in single-stage detectors and summarize these two problems to the disharmony in gradient density with regard to the difficulty of samples. Two loss functions, GHM-C and GHM-R are proposed to conquer the disharmony in classification and bounding box regression respectively. Experiments show that the collaborate with GHM, the performance of single-stage detector can easily surpass modern state-of-theart two-stage detectors like FPN and Mask-RCNN with the same network backbone. Despite of the improvement of select uniform distribution to be the target, we still hold the opinion that the optimal distribution of gradient is hard to define and requires further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An illustration of gradient harmonizing mechanism. The figure in the left displays the distribution of relative gradient norm in a converged model in log scale respectively. The middle figure displays the new gradient norms after the rectification of Focal Loss (FL) and GHM-C loss, compared with the original cross-entropy (CE) loss. The right figure shows the total gradient contribution of examples w.r.t gradient norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The distribution of the gradient norm g from a converged one-stage detection model. Note that the y-axis uses log scale since the number of examples with different gradient norm can differ by orders of magnitude.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Reformulated gradient norm of different loss functions w.r.t the original gradient norm g. The y-axis uses log scale to better display the details of FL and GHM-C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>The distribution of the gradient norm gr for ASL 1 loss. Comparison of the reformulated gradient contributions of different regression losses w.r.t the value of |d|, i.e. the error to ground-truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Table.1 shows the results of varying M which is the number of unit regions. EMA is not applied here. When M is too small, the density can not have a good variation over different gradient norm and the performance is not so good. So we can gain more when M increases when M is not large. However M is not necessarily very large, when M = 30, the GHM-C loss yields a large enough improvement over baseline.MAP AP .5 AP .75 AP S AP M AP L</figDesc><table><row><cell>5 33.4 51.7</cell><cell>35.6</cell><cell>18.6 36.8 45.7</cell></row><row><cell>10 34.6 53.9</cell><cell>36.5</cell><cell>19.5 37.1 46.1</cell></row><row><cell>20 35.2 54.4</cell><cell>36.9</cell><cell>19.4 38.4 46.3</cell></row><row><cell>30 35.8 55.5</cell><cell>38.1</cell><cell>19.6 39.6 46.7</cell></row><row><cell>40 35.4 54.8</cell><cell>36.3</cell><cell>19.5 38.5 46.3</cell></row><row><cell cols="3">Table 1: Results of varying number of unit regions for GHM-</cell></row><row><cell>C loss.</cell><cell></cell><cell></cell></row><row><cell cols="3">Speed: Since our approach is a loss function, it doesn't</cell></row><row><cell cols="3">change the time for inference. For training, a small M of 30</cell></row><row><cell cols="3">is enough to attain good performance, so time consumed by</cell></row><row><cell cols="3">gradient density calculation is not long. Table.2 shows the</cell></row><row><cell cols="3">average time for each iteration during training as well as av-</cell></row><row><cell cols="3">erage precision. Here "GHM-C Standard" is implemented</cell></row><row><cell cols="3">using the original definition of gradient density and "GHM-</cell></row><row><cell cols="3">C RU" represents the implementation of region unit approxi-</cell></row><row><cell cols="3">mation algorithm. The experiments are performed on 1080Ti</cell></row><row><cell cols="3">GPUs. We can see that our region unit approximation algo-</cell></row><row><cell cols="3">rithm speed up the training by magnitudes with negligible</cell></row><row><cell cols="3">harm to performance. While compared with CE, the slow</cell></row><row><cell cols="3">down of GHM-C loss is also acceptable. Since our loss is</cell></row><row><cell cols="3">not fully GPU implemented now, there is still room for im-</cell></row><row><cell>provement.</cell><cell></cell><cell></cell></row><row><cell>method</cell><cell cols="2">AP average time per iteration (s)</cell></row><row><cell>standard CE</cell><cell>28.6</cell><cell>0.566</cell></row><row><cell cols="2">GHM-C Standard 35.9</cell><cell>13.675</cell></row><row><cell>GHM-C RU</cell><cell>35.8</cell><cell>0.824</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The comparison of training speed as well as AP.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table.6 shows the details method network AP AP 50 AP 75 AP S AP M AP L</figDesc><table><row><cell>Faster RCNN (Ren et al. 2015)</cell><cell>FPN-ResNet-101</cell><cell>36.2 59.1</cell><cell>39.0 18.2 39.0 48.2</cell></row><row><cell>Mask RCNN (He et al. 2017)</cell><cell>FPN-ResNet-101</cell><cell>38.2 60.3</cell><cell>41.7 20.1 41.1 50.2</cell></row><row><cell>Mask RCNN (He et al. 2017)</cell><cell>FPN-ResNeXt-101</cell><cell>39.8 62.3</cell><cell>43.4 22.1 43.2 51.2</cell></row><row><cell>YOLOv3 (Redmon and Farhadi 2018)</cell><cell>DarkNet-53</cell><cell>33.0 57.9</cell><cell>34.4 18.3 35.4 41.9</cell></row><row><cell>DSSD513 (Fu et al. 2017)</cell><cell>DSSD-ResNet-101</cell><cell>33.2 53.3</cell><cell>35.2 13.0 35.4 51.1</cell></row><row><cell>Focal Loss (Lin et al. 2017b)</cell><cell>RetinaNet-FPN-ResNet-101</cell><cell>39.1 59.1</cell><cell>42.3 21.8 42.7 50.2</cell></row><row><cell>Focal Loss (Lin et al. 2017b)</cell><cell cols="2">RetinaNet-FPN-ResNeXt-101 40.8 61.1</cell><cell>44.1 24.1 44.2 51.2</cell></row><row><cell>GHM-C + GHM-R (ours)</cell><cell>RetinaNet-FPN-ResNet-101</cell><cell>39.9 60.8</cell><cell>42.5 20.3 43.6 54.1</cell></row><row><cell>GHM-C + GHM-R (ours)</cell><cell cols="2">RetinaNet-FPN-ResNeXt-101 41.6 62.8</cell><cell>44.2 22.3 45.1 55.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art methods (single model) on COCO test-dev set. method AP AP .5 AP .75 AP S AP M AP L</figDesc><table><row><cell>CE</cell><cell>28.6 43.3</cell><cell>30.7</cell><cell cols="3">11.4 30.7 40.7</cell></row><row><cell cols="2">OHEM 31.1 47.2</cell><cell>33.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>FL</cell><cell>35.6 55.6</cell><cell>38.2</cell><cell cols="3">19.1 39.2 46.3</cell></row><row><cell cols="2">GHM-C 35.8 55.5</cell><cell>38.1</cell><cell cols="3">19.6 39.6 46.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of other loss functions. Note that the 'OHEM' is trained with ResNet-101 while others are trained with ResNet-50.of AP at different IoU thresholds. GHM-R loss slightly lowers the AP@IoU=0.5 but gains when the threshold is higher, which demonstrates our proposition that the so called easy examples in regression is important for accurate localization.methodAP AP .5 AP .75 AP S AP M AP L</figDesc><table><row><cell>SL 1</cell><cell>35.8 55.5</cell><cell>38.1</cell><cell>19.6 39.6 46.7</cell></row><row><cell>ASL 1</cell><cell>35.7 55.0</cell><cell>38.1</cell><cell>19.7 39.7 45.9</cell></row><row><cell cols="2">GHM-R 36.4 54.6</cell><cell>38.7</cell><cell>20.5 40.6 47.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different loss functions for regression. method AP AP .5 AP .6 AP .7 AP .8 AP .9 SL 1 35.8 55.5 51.2 43.4 31.4 11.9 ASL 1 35.7 55.0 51.1 43.5 31.5 12.1 GHM-R 36.4 54.6 51.4 44.0 32.2 13.1</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of AP at different IoU thresholds.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Comparison of regression loss functions on twostage detector.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We sincerely appreciate the technical and GPU support from Mr. Changbao Wang, Quanquan Li and Junjie Yan at Sensetime Research. And we also acknowledge the early discussion with Prof. Wanli Ouyang from University of Sydney.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02257</idno>
		<idno>Dai et al. 2016</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>R-fcn: Object detection via region-based fully convolutional networks</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade object detection with deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girshick</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Mcallester ; Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR), 2010 IEEE conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the IEEE international conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<idno type="arXiv">arXiv:1709.015077</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Mask r-cnn</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.04613</idno>
		<idno>arXiv:1502.03167</idno>
	</analytic>
	<monogr>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Improving regression performance with distributional losses</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Zoom out-and-in network with map attention decision for region proposal and object detection</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
	<note>European conference on computer vision</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<idno>abs/1409.1556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>2018 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
	<note>Sutskever et al. 2013</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Crafting gbd-net for object detection</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2109" to="2123" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
