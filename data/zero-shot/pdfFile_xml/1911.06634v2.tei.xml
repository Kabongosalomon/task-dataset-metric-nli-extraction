<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image Reflection Removal through Cascaded Refinement</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Hopcroft</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image Reflection Removal through Cascaded Refinement</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the problem of removing undesirable reflections from a single image captured through a glass surface, which is an ill-posed, challenging but practically important problem for photo enhancement. Inspired by iterative structure reduction for hidden community detection in social networks, we propose an Iterative Boost Convolutional LSTM Network (IBCLN) that enables cascaded prediction for reflection removal. IBCLN is a cascaded network that iteratively refines the estimates of transmission and reflection layers in a manner that they can boost the prediction quality to each other, and information across steps of the cascade is transferred using an LSTM. The intuition is that the transmission is the strong, dominant structure while the reflection is the weak, hidden structure. They are complementary to each other in a single image and thus a better estimate and reduction on one side from the original image leads to a more accurate estimate on the other side. To facilitate training over multiple cascade steps, we employ LSTM to address the vanishing gradient problem, and propose residual reconstruction loss as further training guidance. Besides, we create a dataset of real-world images with reflection and ground-truth transmission layers to mitigate the problem of insufficient data. Comprehensive experiments demonstrate that the proposed method can effectively remove reflections in real and synthetic images compared with state-of-the-art reflection removal methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Undesirable reflections from glass occur frequently in real-world photos. It not only significantly degrades the image quality, but also affects the performance of downstream computer vision tasks like object detection and semantic segmentation. As the reflection removal problem is ill-posed, early works primarily tackle it with multi-ple input images <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. More recently, researchers attempt to address the more common and practically significant scenario of a single input image <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>For single-image reflection removal (SIRR), researchers have observed that some handcrafted priors may help for distinguishing the transmission layer from the reflection layer in a single image. But these priors often do not generalize well to different types of reflections and scenes owing to disparate imaging conditions. In recent years, researchers apply data-driven learning to replace handcrafted priors via deep convolutional neural networks. With abundant labeled data, a network can be trained to perform effectively over a broad range of scenes. However, learning-based singleimage methods still have much room for improvement due to complications such as limited training data, disparate imaging conditions, varying scene content, limited physical understanding of this problem, and the performance limitation of various models.</p><p>In this work, inspired by the iterative structure reduction approach for hidden community detection in social networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we introduce a cascaded neural network model for transmission and reflection decomposition. <ref type="figure">Figure 1</ref> illustrates the cascade results in our model, where the transmission and reflection are progressively refined during the iterations. To the best of our knowledge, previous works on reflection removal did not utilize a cascaded refinement approach. Though some methods such as BDN <ref type="bibr" target="#b32">[33]</ref> obtain predictions over a sequence of a few sub-networks, they do not iteratively refine the estimates, but rather they conduct a short alternating optimization, e.g., by estimating the reflection from the input image and the initial transmission layer, and then estimating the transmission from the input image and the estimated reflection layer. For a cascade model on SIRR, a simple approach is to employ one network to generate a predicted transmission that serves as the auxiliary information of the next network, and continue such process with subsequent networks to iteratively improve the prediction quality. <ref type="bibr">With</ref>  vanishing gradient problem and limited training guidance at each step. To address this issue, we design a convolutional LSTM (Long Short-Term Memory) network, which saves information from the previous iteration (i.e. time step) and allows gradients to flow unchanged.</p><p>In our model, two sub-networks use identical convolutional LSTM architecture, one for transmission prediction and the other for reflection prediction. They share input information using the outputs of the previous time step to boost each others effectiveness. Here we propose a residual reconstruction loss as further training supervision at each cascade step. To simplify the reconstruction loss, we define a new concept of residual reflection, which will be described in Sec. 3.4.</p><p>Though a few real-world datasets with ground-truth have been presented <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b33">34]</ref>, the real-world data for SIRR is still insufficient due to the tremendously labor-intensive work. To help resolve the insufficiency of the real-world training data, we also collect a real dataset with densely-labeled ground truth in disparate imaging conditions and varying scenes.</p><p>Our main contributions are as follows:</p><p>? We propose a new network architecture, a cascaded network, with loss components that achieves state-ofthe-art quantitative results on real-world benchmarks for the single image reflection removal problem. ? We design a residual reconstruction loss, which can form a closed loop with the linear method for synthesizing images with reflections, to expand the influence of the synthesis method across the whole network. ? We collect a new real-world dataset containing images with densely-labeled ground-truth, which can serve as baseline data in future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Mathematically speaking, SIRR operates on a captured image I, which is generally assumed to be a linear combination of a transmission layer T and a reflection layer R. The goal is to infer a transmission layer T that is free of reflections. In this work, we focus on deep learning-based SIRR, which has produced state-of-the-art results. Previous multiple-image methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b6">7]</ref> and single-image-priors based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> are not considered here.</p><p>Due to the advantages in robustness and performance, there is an emerging interest in applying neural networks to SIRR. Fan et al. <ref type="bibr" target="#b3">[4]</ref> provide the first neural network model to solve this ill-posed problem. They propose a linear method for synthesizing images with reflection for training, and use an edge map as auxiliary information to guide the reflection removal. Wan et al. <ref type="bibr" target="#b26">[27]</ref> develop two cooperative sub-networks, which predict the transmission layer intensity and gradients concurrently. Both of these works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref> utilize edge or gradient information of the captured layer I, motivated by the idea that the reflection layers are usually not in focus and thus blurry as compared to the transmission layers. From the edge information of the captured image I, the edge map of the transmission image T is predicted and used in estimating the transmission result. Instead, BDN <ref type="bibr" target="#b32">[33]</ref> predicts reflection layers which are then used as auxiliary information in a subsequent network to estimate the transmission.</p><p>In several recent methods, improved formulations of the objective function are presented. These include the adoption of perceptual losses <ref type="bibr" target="#b10">[11]</ref> to account for both low-level and high-level image information <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b33">34]</ref>. In these works, images are fed to a deep network pre-trained on Ima-geNet, and comparisons are made based on extracted multi-stage features. Adversarial losses have also been applied, specifically to improve the realism of predicted transmission layers <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>Another direction of study focuses on datasets for training. Moving beyond improvements for the linear synthesis method in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b33">[34]</ref>, Wen et al. <ref type="bibr" target="#b30">[31]</ref> synthesize training data with learned non-linear alpha blending masks that better model the real-world imaging conditions. These masks are also used in forming a reconstruction loss that guides the prediction of transmission layers. To deal with the insufficiency of densely-labeled training data, Wei et al. <ref type="bibr" target="#b29">[30]</ref> present a technique for utilizing misaligned real-world images as the training data, as they are less burdensome to acquire than aligned images and are more realistic than synthetic images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>This work is motivated by research on hidden structures in social networks. He et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> define a set of communities as hidden structure if most of the members also belong to other stronger communities. They propose an iterative boost approach to separate a set of strong, dominant communities and another set of weak, hidden communities, and boost the detection accuracy on both sides. The key idea is that, when they detect an approximate set of dominant communities using a base algorithm, and weaken their internal connection to the average connection of the overall graph, the dominant structure is reduced to boost the detection on the set of hidden communities, and vice versa.</p><p>Under the scenario of SIRR, a useful trick is to employ sub-networks to learn auxiliary information that can facilitate transmission layer prediction. The types of auxiliary information utilized in existing works include edge information <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b26">27]</ref> and predicted reflections <ref type="bibr" target="#b32">[33]</ref>. The ideal auxiliary information would be the ground truth reflection-free version of the transmission layer, which is what we seek to predict. As this is not available at inference time, we instead use approximations to the ground-truth transmission in the form of predicted transmissions as the auxiliary information. Though certainly not as useful as the ground truth, it nevertheless provides strong guidance, especially as the transmission predictions improve. The key issue then becomes how to drive the transmission estimations closer and closer to the ground truth. Referring to the work of He et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, we regard the transmission layer as the strong, dominant structure, and the reflection layer as the weak, hidden structure. By iteratively reducing the more accurate version of the counterpart, we could extract more accurate approximations on the two layers of images.</p><p>Our model contains two sub-networks that can collaborate and boost each other's output by reducing the output of one side from the original image as effective auxiliary information for the other complementary side. Such collaborative cascaded refinement of the dominant image (transmission) and the weak image (reflection) is novel for the training of a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">General Design Principles</head><p>We use two convolutional LSTM networks to separately generate the predicted transmission layers and the predicted reflection layers. The input of each sub-network includes the outputs of both the transmission and reflection subnetworks. Besides, the outputs of the two sub-networks are combined within a reconstruction loss to supervise the whole model at each time step. The synergy between the two sub-networks leads to a mutual boost in their predictions, resulting in progressive improvements of the auxiliary information and finally accurate estimates of the transmission.</p><p>To ensure that the transmission sub-network and the reflection sub-network generate complementary outputs, we enforce a reconstruction loss where the image? synthesized from the estimated transmission and reflection is expected to match the input image I.</p><p>A related constraint is employed in RmNet <ref type="bibr" target="#b30">[31]</ref>, which synthesizes an image I from the ground-truth transmission layer with no reflection, the reflection layer used to produce reflections off the glass, and an alpha blending mask W. Thus,</p><formula xml:id="formula_0">I = W ? T + (1 ? W) ? R, where ? denotes element-wise multiplication.</formula><p>The reconstructed image? is then compared to the synthetic input image I. However, their alpha blending model only approximates the complex physical mechanisms involved in forming an actual input image with reflections, as it does not model effects such as spatially varying blurs and Gamma correction <ref type="bibr" target="#b1">[2]</ref>, which is used to correct for the differences between the way a camera captures content and the way our visual system processes light. This will limit reconstruction quality on real-world input images and consequently degrade prediction results as we found from experiments reported in <ref type="table">Table 1</ref>.</p><p>To avoid the problem that RmNet encounters, we use a scale parameter ? instead of the element-wise mask matrix W, and we directly calculate the residual reflection R by I ? ? ? T. In this way, we do not require modeling the complicated physical process involved in the formation of images with reflection, and our performance does not suffer from deficiencies in such a synthesis model. The benefit of predicting residual reflection instead of the reflection layer used to produce reflections off the glass is that image reconstruction becomes simplified as just the sum of the predicted transmission and the predicted residual reflection. Also, different from RmNet, all our linear operations are done in the linear color space, removing Gamma correction <ref type="bibr">[</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Residual Reconstruction Loss</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>The architecture of the proposed network is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>  <ref type="bibr" target="#b0">1</ref> . IBCLN consists of two sub-networks: a transmission-prediction network G T and a reflectionprediction network G R . The two sub-networks are both convolutional LSTM networks with the same architecture but different goals. The former aims to learn the transmission T while the latter aims to learn the residual reflection R, so they learn completely different weight parameters. Each sub-network consists of an encoder with 11 Convrelu blocks that extract the features from the input image, a convolutional LSTM unit <ref type="bibr" target="#b19">[20]</ref> and a decoder with 8 convolutional layers for generating the predicted transmission layer or the predicted residual reflection layer. Each convolutional layer is followed by a ReLU activation, except for the LSTM layers which are followed by a Sigmoid activation or a Tanh activation. In each sub-network, there are two skip connections between the encoder and the decoder to prevent blurred outputs. The convolutional layers and skip connections are similar to those of a contextual autoencoder <ref type="bibr" target="#b17">[18]</ref>. Different from previous works, our objective function includes the proposed residual reconstruction loss and a multi-scale perceptual loss. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates IBCLN from a different perspective. All G T illustrated in this figure is exactly the same network with the same parameters, but at different time steps in the cascade. We connect G T at adjacent time steps with convolutional LSTM units that save information from the previous time step. In the actual model, the convolutional LSTM unit is in the middle of the sub-network and connected with convolutional layers. The convolutional LSTM unit has four gates, including an input gate, a forget gate, an output gate, as well as a cell state. The cell state encodes the state information that will be fed to the next LSTM. The LSTMs output feature is fed into the next convolutional layer. More details can be found in ConvLSTM <ref type="bibr" target="#b19">[20]</ref>. At time step t, both of the sub-networks take nine channels of input, specifically a concatenation of the synthetic image I, the predicted transmissionT t?1 and residual reflection R t?1 predicted at time step t ? 1 (1 &lt; t ? N ). T 0 is set to be the synthetic image I and R 0 is set to 0.1 for all entries. The output of the transmission prediction network G T at the final time step N serves as the final result.</p><p>Many previous works consider auxiliary information to be important for predicting reflection-free transmission layers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31]</ref>, since it indicates to the network where the removal should be focused on. In our work,T t?1 and R t?1 are saved to serve as the auxiliary information of step t (1 &lt; t ? N ). The auxiliary information will improve with increasing numbers of time steps (see <ref type="figure">Figure 1</ref>). Since the predicted transmissions represent what the network can infer at a given time step, using them as auxiliary information is effective. Additionally, the predicted residual reflection is complementary to the predicted transmission in an image, so it also contains meaningful information.</p><p>Considering that the iterative process may require a long cascade, using conventional convolutional networks as the sub-networks would make the full model hard to train. This motivates our use of two convolutional LSTM networks, each with a convolutional LSTM unit. The continuity among time steps makes the model easy to train. Additionally, a cascaded architecture has fewer parameters to learn, as both of the sub-networks are iterated multiple times and each instance of a sub-network shares the same weights. Moreover, a convolutional LSTM network has more complete information exchange either within itself or between the two sub-networks, which is more in line with our iterative boost idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective Function</head><p>Residual Reconstruction Loss. For the existing linear models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34]</ref> for generating synthetic images, the general steps are to perform a series of complex operations on a reflection image to produce a reflection layer R, then to generate a synthetic image I by a linear operation: I = clip(? ? T + R). Usually ? ? [0.8, 1] due to the slight attenuation of light as it passes through a glass plane. The weight of the reflection layer R is 1 as the original reflection image has been subtracted adaptively by the synthesis method. The clipping operation forces all values of the synthetic image to be in [0,1].</p><p>We introduce a new loss to the proposed network, called the residual reconstruction loss. We adopt the above synthesis model, but replace R with R, where R is determined from I and T. R offers more effective auxiliary information for transmission prediction, and a more convincing ground truth, as compared to the artificially constructed R. R is obtained by reverting the linear synthesis model, as</p><formula xml:id="formula_1">R = I ? ? ? T.</formula><p>(1)</p><p>With this definition of R, the clipping operation is not needed and we avoid its loss of information. After R is calculated, it can be used as the ground truth of G R to guide the generation of the predicted residual reflectionR. Then, we can simply revert Eq. (1) in the objective function, a?</p><formula xml:id="formula_2">I = ? ?T +R,<label>(2)</label></formula><p>whereT,R and? are the predicted transmission, predicted residual reflection and the reconstructed image, respectively. ? is the same as in the synthesis model. Note that all the above linear operations are done in the linear color space, so the Gamma correction <ref type="bibr" target="#b1">[2]</ref> on each image is removed before inclusion in linear operations.</p><p>It is intuitive that the reconstructed image? should be similar to the original input through a well-trained network. The residual reconstruction loss is defined as:</p><formula xml:id="formula_3">L residual = I?D N t=1 L M SE (I,? t ).</formula><p>(3)</p><p>L M SE indicates the mean squared error. t denotes the time step of the two sub-networks. N represents the final time step whenT converges. The residual reconstruction loss works well experimentally. One potential reason is that the two sub-networks have the same architecture but complementary objectives. With the same architecture, they may be under-trained or overtrained concurrently. The complementary objectives within the residual reconstruction loss can balance the error from the two sub-networks. If both of the two sub-networks are either under-trained or over-trained, the error will be doubled in the residual reconstruction loss. Multi-scale Perceptual Loss. Multi-scale losses are effective in image decomposition tasks such as raindrop removal <ref type="bibr" target="#b17">[18]</ref>. A multi-scale loss extracts the features from different decoder layers and feeds them into a convolutional layer to form outputs at different resolutions. The outputs are then compared to those of real images by their L M SE distance. By adopting such a loss in our task, we can capture more contextual information from various scales. We change the L M SE distance to the perceptual distance between the predicted image and the real image over different scales. This loss thus considers different scales of both lowlevel and high-level information. We define the loss function as:</p><formula xml:id="formula_4">L M P = T,T 3 ,T 5 ?D (L V GG (T,T) + ? 3 L V GG (T 3 ,T 3 ) + ? 5 L V GG (T 5 ,T 5 )),<label>(4)</label></formula><p>whereT,T 3 ,T 5 indicate the outputs of the last, 3 rd last and 5 th last layers at time step N , whose sizes are 1, 1 2 and 1 4 of the original size, respectively. T, T 3 and T 5 indicate the ground truth that has the same scale as that of the outputs, respectively. Layers with smaller size are not considered since their information is relatively insignificant. We set ? 3 = 0.8 and ? 5 = 0.6. All the images are fed into the VGG19 network <ref type="bibr" target="#b20">[21]</ref>. We compare the outputs of the layers 'conv1 2' and 'conv2 2 in the VGG19 network. Pixel Loss. To ensure that the outputs become as close to the ground truth as possible, we utilize the L M SE loss to measure the pixel-wise distance between them. Our pixel loss is defined as follows:</p><formula xml:id="formula_5">L pixel = T ?D N t=1 [L M SE (T,T t ) + L M SE ( R,R t )],<label>(5)</label></formula><p>where R is the residual reflection.T t andR t are the outputs at time step t. Adversarial Loss. To improve the realism of the generated transmission layers, we further add an adversarial loss. We define an opponent discriminator network D. The adversarial loss is defined as (refer to <ref type="bibr" target="#b33">[34]</ref> for details):</p><formula xml:id="formula_6">L adv = T ?D ? log D(T,T).<label>(6)</label></formula><p>Overall Loss. Overall, our objective function of IBCLN is defined as:</p><formula xml:id="formula_7">L = ? 1 L residual + ? 2 L M P + ? 3 L pixel + ? 4 L adv ,<label>(7)</label></formula><p>where we empirically set the weights as ? 1 = 2, ? 2 = 1, ? 3 = 2, ? 4 = 0.01 throughout our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Implementation Details</head><p>We implement the proposed IBCLN in Pytorch on a PC with an Nvidia Geforce GTX 2080 Ti GPU. The overall model is trained for 80 epochs with a batch size of 2, using the Adam optimizer <ref type="bibr" target="#b11">[12]</ref>. The learning rate for the overall network training is set to 0.0002. For the training data, we use 4000 images containing 2800 synthetic images and 1200 image patches of size 256 ? 256 from 290 real-world training images, containing 200 images from our created dataset and 90 images from Zhang et al. <ref type="bibr" target="#b33">[34]</ref>. Similar to current deep learning methods, our method requires a relatively large amount of data with ground truth for training. Our synthesis model is the same as the recently proposed linear method <ref type="bibr" target="#b33">[34]</ref> except for the clipping operation. We utilize their synthetic dataset as well. In our experiments, different methods are evaluated on the publicly available real-world images from the SIR 2 datasets [26], Zhang et al. <ref type="bibr" target="#b33">[34]</ref> and the real-world dataset we create.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset Preparation</head><p>Our created dataset, called Nature, contains 220 realworld image pairs: images with reflection and the corresponding ground-truth transmission layers (see samples in <ref type="figure" target="#fig_2">Figure 4</ref>). We use a Canon EOS 750D for image acquisition. Each ground-truth transmission layer is captured when the portable glass is removed. The dataset is randomly partitioned into a training set and a testing set. We use 200 images for training and 20 images for quantitative evaluation. Inspired by Zhang et al. <ref type="bibr" target="#b33">[34]</ref>, we captured the images with the following considerations to simulate diverse imaging conditions: 1) Environments: indoor and outdoor; 2) Lighting conditions: skylight, sunlight, and incandescent; 3) Thickness of the glass slabs: 3 mm and 8 mm; 4) Distance between the glass and the camera: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison to State-of-the-art Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Quantitative Evaluations</head><p>We compare our IBCLN against state-of-the-art methods including CEILNet <ref type="bibr" target="#b3">[4]</ref>, Zhang et al. <ref type="bibr" target="#b33">[34]</ref>, BDN <ref type="bibr" target="#b32">[33]</ref>, Rm-Net <ref type="bibr" target="#b30">[31]</ref> and ERRNet <ref type="bibr" target="#b29">[30]</ref>. For an apples-to-apples comparison, we finetune each model (if the model provides training code) on our training dataset and report the best result of the original pre-trained model and finetuned version (denoted with a suffix -F). RmNet <ref type="bibr" target="#b30">[31]</ref> has three models for different reflection types, and we report the best result from among the three models. <ref type="table">Table 1</ref> summarizes results of all the competing methods on five real-world datasets, including three sub-datasets from SIR 2 <ref type="bibr" target="#b25">[26]</ref>, Zhang et al. <ref type="bibr" target="#b33">[34]</ref> and our dataset. The number of images in each dataset is shown after the name. The quality metrics include PSNR and SSIM <ref type="bibr" target="#b28">[29]</ref>. Larger values of PSNR and SSIM indicate better performance. IB-CLN achieves the best performance on four of the datasets, but not on 20 images of "Zhang et al.". As ERRNet <ref type="bibr" target="#b29">[30]</ref> is developed based on model Zhang et al. <ref type="bibr" target="#b33">[34]</ref>, EERNet and Zhang et al. both have better performance on the dataset "Zhang et al.". In terms of overall performance over all the test datasets, IBCLN surpasses the other methods. <ref type="figure" target="#fig_4">Figure 5</ref> presents visual results and the ground truth on realworld images from SIR 2 <ref type="bibr" target="#b25">[26]</ref>, Zhang et al. <ref type="bibr" target="#b33">[34]</ref> and our dataset. We select two images from each dataset. It can be seen that Zhang et al. <ref type="bibr" target="#b33">[34]</ref> tends to over-remove the reflection layer, while the other baseline methods tend to underremove. Our model is more accurate and removes most of the undesirable reflections. <ref type="table">Table 1</ref>. Quantitative comparison of different methods on three real-world benchmark datasets. The best results are in bold and orange color, and the second best results are underlined and in blue color. 'Average' is obtained by averaging the metric scores of all images from all the above real-world datasets. <ref type="bibr">Dataset</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Qualitative Evaluations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Controlled Experiments</head><p>For better analyzing our network architecture and the objective function of IBCLN, we separately remove the subnetwork G R , the iteration step, and the three-loss terms one by one. Then we train new models with the modified networks. The results from these ablations on the architecture are given in <ref type="table" target="#tab_3">Table 2</ref>. The result of a cascade network with-out LSTM is not shown in the table because it cannot be effectively trained. The ablation study on the loss terms is shown in <ref type="table" target="#tab_4">Table 3</ref>. And visual comparisons among all the modified networks and IBCLN are displayed in <ref type="figure">Figure  6</ref> and <ref type="figure">Figure 7</ref>. We observe that using two iterative subnetworks, time steps, L adv , L residual and L M P all enhance the performance of IBCLN, and all the blocks and the losses  yield different contributions to the removal performance.</p><p>The complete IBCLN with all structures and objective function terms yields the best results. To explore how many time steps are appropriate for the predicted transmission to converge, we train the model with different total time steps. <ref type="figure">Figure 8</ref> exhibits the results. We see that the output approximately converges when total time steps are equal to 3. We experimented with having the net- work learn the total time steps automatically for different images, but we found that providing this much flexibility causes the performance to decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present an Iterative Boost Convolutional LSTM Network (IBCLN) that can effectively remove the reflection from a single image in a cascaded fashion. To formulate an effective cascade network, we propose to iteratively refine the transmission and reflection layers at each step in a manner that they can boost prediction quality for each other, and to employ LSTM to facilitate training over multiple cascade steps. The intuition is that a better estimate of the complementary residual reflection can boost the prediction of the transmission, and vice versa. Besides, we incorporate a residual reconstruction loss as further training guidance at each cascade step. Moreover, we combine a multi-scale loss with the perceptual loss to form a multiscale perceptual loss. Quantitative and qualitative evaluations on five datasets (including ours) demonstrate that the proposed IBCLN outperforms state-of-the-art methods on the challenging single image reflection removal problem. In future work, we will try our cascaded prediction refinement approach on other image layer decomposition tasks such as raindrop removal, flare removal and dehazing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of IBCLN. The cascaded network consists of a transmission generative sub-network G T and a reflection generative sub-network G R with skip connections, both of which are convolutional LSTM networks. The images generated at each time step by the two sub-networks will be fed back at the next time step. The overall network is trained in an end-to-end manner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1Figure 3 .</head><label>3</label><figDesc>Code and model: https://github.com/JHL-HUST/IBCLN/. Characterizing IBCLN with increasing number of time steps. All blocks labeled as G T indicate one sub-network and all blocks labeled as G R indicate another sub-network. The output at time step t ? 1 serves as the input at time step t.T1,T2, ...,TN are the predicted transmission.R1,R2, ...,RN are the predicted residual reflection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Samples from our real world Nature dataset. Top: images with reflection. Bottom: the corresponding ground-truth transmission layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>3 to 15 cm; 5) Camera viewing angles: front view and oblique view; 6) Camera exposure value: 8.0 -16.0; 7) Camera apertures (affecting the reflection blurriness): f/4.0 f/16.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison among state-of-the-art approaches and our method on images from three real-world image datasets, namely, Nature (Rows 1-2), SIR 2 (Rows 3-4) and. More results can be found in the suppl. material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Visual comparison among IBCLN and versions with a modified loss on real-world images. More results are in the suppl. material.Without GRWithout iteration Input IBCLN Visual comparison among IBCLN and versions with architecture modifications on real-world images. More results can be found in the suppl. material. Results using different total time steps N in IBCLN on SIR 2<ref type="bibr" target="#b25">[26]</ref>. Total time steps N = 3 yields the best performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>a long cascade, however, the training becomes difficult due to the arXiv:1911.06634v2 [cs.CV] 5 Apr 2020</figDesc><table><row><cell>T0 ?</cell><cell>T1 ?</cell><cell>T2 ?</cell><cell>T3 ?</cell><cell>T</cell></row><row><cell>R0 R</cell><cell>R1 R</cell><cell>R2 R</cell><cell>R3 R</cell><cell>R</cell></row><row><cell></cell><cell>Cascading Results</cell><cell></cell><cell></cell><cell>Ground Truth</cell></row></table><note>Figure 1. Visualization of results at different cascade steps of the two sub-networks in the proposed model. The estimates of transmissions and residual reflections become increasingly more accurate as they progress through the cascade. More results are in the suppl. material.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of IBCLN for architecture on three testing sets. w/o G R means training with only one sub-network G T . w/o iteration means the total time steps is 1. Each term contributes to the SIRR performance, and combining all achieves the best results. G R 21.79 0.759 20.65 0.742 22.36 0.868 w/o iteration 21.82 0.764 20.49 0.739 23.09 0.872 Complete 23.57 0.783 21.86 0.762 24.20 0.884</figDesc><table><row><cell>Model</cell><cell>Nature PSNR SSIM PSNR SSIM PSNR SSIM Zhang et al. SIR 2</cell></row><row><cell>w/o</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of IBCLN for loss terms on three testing sets. Each loss contributes to IBCLN's performance, and combining all achieves the best result. PSNR SSIM PSNR SSIM PSNR SSIM L pixel only 21.98 0.739 19.54 0.722 22.91 0.843 w/o L adv 23.24 0.746 21.74 0.755 23.86 0.885 w/o L residual 22.54 0.770 20.98 0.755 23.74 0.881 w/o L M P 23.14 0.744 21.47 0.734 22.96 0.863 Complete 23.57 0.783 21.86 0.762 24.20 0.884</figDesc><table><row><cell>Model</cell><cell>Nature</cell><cell>Zhang et al.</cell><cell>SIR 2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is supported by the Fundamental Research Funds for the Central Universities (2019kfyXKJC021) and Microsoft Research Asia.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Single image reflection suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Arvanitopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radhakrishna</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4498" to="4506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Chapter 4 -digital picture formats and representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>Bull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communicating Pictures</title>
		<editor>David R. Bull</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="99" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Single image reflection removal using deep encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhixiang</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00094</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A generic deep architecture for single image reflection removal and image smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingnan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3238" to="3247" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Blind separation of superimposed moving images using image statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="19" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Robust separation of reflection from multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2187" to="2194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reflection removal using low-rank matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeong-Ju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Young</forename><surname>Sim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5438" to="5446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hidden community detection in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingru</forename><surname>Kun He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucheta</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Soundarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hopcroft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">425</biblScope>
			<biblScope unit="page" from="92" to="106" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Revealing multiple layers of hidden community structure in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sucheta</forename><surname>Kun He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhi</forename><surname>Soundarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">E</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Hopcroft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<idno>abs/1501.05700</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to see through reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meiguang</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>S?sstrunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Generative single image reflection separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhwai</forename><surname>Oh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04102</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to perceive transparency from the statistics of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1271" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Separating reflections from a single image using local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anat</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Assaf</forename><surname>Zomet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting reflection change for automatic reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2432" to="2439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Single image layer separation using relative smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael S Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2752" to="2759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attentive generative adversarial network for raindrop removal from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2482" to="2491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Separating transparent layers through layer information exchange</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Sarel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="328" to="341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reflection separation using guided annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Springer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1192" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic reflection removal using gradient intensity and motion cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taotao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanghui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM international conference on Multimedia</title>
		<meeting>the 24th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="466" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Layer extraction from multiple images containing reflections and transparency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Szeliski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shai Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition. CVPR 2000 (Cat. No. PR00662)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="246" to="253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Ground reflection removal in compressive sensing ground penetrating radars. IEEE Geoscience and remote sensing letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagri</forename><surname>Tuncer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Cafer Gurbuz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Benchmarking single-image reflection removal algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3922" to="3930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Crrn: Multi-scale guided concurrent reflection removal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah-Hwee</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4777" to="4785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth of field guided reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><forename type="middle">Ah</forename><surname>Hwee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Eero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Single image reflection removal exploiting misaligned training data and network enhancements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8178" to="8187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Single image reflection removal beyond linearity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinjie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3771" to="3779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A computational approach for obstruction-free photography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">79</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Seeing deeply and bidirectionally: A deep learning approach for single image reflection removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinfeng</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="654" to="669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Single image reflection separation with perceptual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuaner</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4786" to="4794" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
