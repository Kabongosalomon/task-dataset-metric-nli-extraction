<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FONTNET: ON-DEVICE FONT UNDERSTANDING AND PREDICTION PIPELINE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakshith</forename><forename type="middle">S</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute India</orgName>
								<address>
									<postCode>560037</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Khurana</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute India</orgName>
								<address>
									<postCode>560037</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Agarwal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute India</orgName>
								<address>
									<postCode>560037</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jayesh</forename><forename type="middle">Rajkumar</forename><surname>Vachhani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute India</orgName>
								<address>
									<postCode>560037</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guggilla</forename><surname>Bhanodai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Samsung R&amp;D Institute India</orgName>
								<address>
									<postCode>560037</postCode>
									<settlement>Bangalore</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FONTNET: ON-DEVICE FONT UNDERSTANDING AND PREDICTION PIPELINE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Font Detection</term>
					<term>Font Prediction</term>
					<term>Convolutional neural network</term>
					<term>k-Means Clustering</term>
					<term>k- Nearest Neighbors</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Fonts are one of the most basic and core design concepts. Numerous use cases can benefit from an in depth understanding of Fonts such as Text Customization which can change text in an image while maintaining the Font attributes like style, color, size. Currently, Text recognition solutions can group recognized text based on line breaks or paragraph breaks, if the Font attributes are known multiple text blocks can be combined based on context in a meaningful manner. In this paper, we propose two engines: Font Detection Engine, which identifies the font style, color and size attributes of text in an image and a Font Prediction Engine, which predicts similar fonts for a query font. Major contributions of this paper are three-fold: First, we developed a novel CNN architecture for identifying font style of text in images. Second, we designed a novel algorithm for predicting similar fonts for a given query font. Third, we have optimized and deployed the entire engine On-Device which ensures privacy and improves latency in real time applications such as instant messaging. We achieve a worst case On-Device inference time of 30ms and a model size of 4.5MB for both the engines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>In this modern digital age, approximately 3.2 billion people are accessing social networking platforms through their mobile phones. The use of visual media such as memes, stickers and GIFs is ever increasing and the current options are limited. There is an urgent need for customizable stickers and GIFs which can be personalized. By identifying the font style, color and size, users can reuse any one template for multiple scenarios. The options available can be further increased by providing a list of aesthetically similar fonts. Such a subtle art of selecting visually similar fonts requires a sophisticated skillset. Also the font names themselves are rarely meaningful which makes this task really challenging for most users.</p><p>Artificial intelligence is moving towards an On-Device platform from the cloud platform for better reliability, more privacy, and consistent performance. However, it requires lightweight, fast, and accurate neural network models to run on a resource constraint mobile platform. This is another crucial aspect we would like to focus in this paper.</p><p>Even though there are websites that detect font styles from images (WhatTheFont, Font Matcherator, etc.), there exists no solution which works offline. Similar limitations exist for font color and font size as well. Websites such as "Identifont", suggest similar fonts based on proximity between visual features with respect to a query font, but there is a lack of variety in these fonts.</p><p>In this paper, we propose a Font Detection Engine (FDE), which is capable of identifying all the font attributes (style, color, size) of text in images. By observing the current literature we find that there have been many attempts to solve this task. For example, Wang et al. <ref type="bibr">[1]</ref> have used a CNN based architecture for font classification. Their work attempts to solve the Visual Font Recognition (VFR) task. Even though their network accommodates both real world as well as synthetic data and supports a larger set of fonts, their network is infeasible to be used On-Device because of a large model size and inference time. In <ref type="bibr">[2]</ref> the authors use a similar approach to include both synthetic and real world data, but they have implemented it with VGG <ref type="bibr" target="#b11">[9]</ref> and AlexNet <ref type="bibr" target="#b10">[8]</ref> architectures and have the same shortcomings as <ref type="bibr">[1]</ref>. Chen et al. <ref type="bibr" target="#b4">[3]</ref> use a Nearest Class Mean Classifier based approach for solving the VFR task. Such a statistical approach, makes their model scalable and accommodates new font classes. Since it is based on a local feature embedding of the image, it is not very accurate for the font detection task. Furthermore, it is very difficult to define such local feature vectors which suits all font types. The results for wild images prove that this approach does not work well for noisy images with complex backgrounds. In <ref type="bibr" target="#b5">[4]</ref>, Yifan Chang has also used a CNN for identifying typefaces of Chinese text in images. It exhibits good results only for black and white synthetic images.</p><p>There have been multiple attempts for predicting suitable fonts for design tasks. In <ref type="bibr" target="#b17">[15]</ref>, the authors adopt a deep neural network based model to predict most suitable fonts for a given web design. They use visual features from a CNN along with semantic tags from the webpage for understanding context. In <ref type="bibr" target="#b13">[11]</ref>, the authors follow a complicated method to learn the distance/similarity between two fonts for predicting visually similar fonts. They use crowd sourced data which is costly and cumbersome to collect. Therefore, we use an alternate approach for finding similar fonts as discussed in section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Font Detection Engine</head><p>FDE uses localized text regions generated by a text recognition API (Google ML Kit) <ref type="bibr" target="#b18">[16]</ref>. Our engine captures all the font attributes (style, color, size) of the text. We use deep CNN architecture for the font style identification. For training the model we use a dataset of 1814 fonts (Google Fonts <ref type="bibr" target="#b15">[13]</ref>) with 700 images each. The dataset consists of 3 image resolutions, 16 text sizes, 8 text colors, lower/upper case, and augmentations like scaling, contrast etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Deep CNN</head><p>We start with the architecture from the DeepFont paper <ref type="bibr">[1]</ref> with a fixed input size of 105x105 comprising of Convolutional, Normalization and MaxPool layers, followed by 3 Dense layers with Softmax activation for the final classification. The 3 Dense layers made the model size excessively large and exhibited poor validation accuracy due to overfitting. Therefore, we reduced the number of Dense layers to 1. Though this improved the accuracy, the model size and inference time were still high. The fixed input size, requires preprocessing operations (dividing the input image into individual patches of size 105x105) before inferring the model which is another limitation.</p><p>Due to these shortcomings, we introduced necessary changes as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>. To solve the fixed input problem, we decided to use a Fully Convolutional Network (FCN) architecture with a sliding window approach. To reduce the model size, we decreased the input height from 105 pixels to 50. The final convolutional layer uses a Softmax activation function and outputs the probabilities for each font. There are no Dense layers and the number of output channels is equal to the effective number of 50x50 patches created due to the stride of the sliding window. This stride is fixed by the number of MaxPool layers. As this stride value increases the inference time of the model reduces. From <ref type="figure" target="#fig_1">Figure 2</ref>, it is evident that each RGB image is converted to grayscale. The image is also uniformly rescaled to a height of 50 pixels and a width less than 80 pixels to limit the maximum inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Text Color</head><p>For identifying the text color we used an algorithm inspired from K-Means clustering. We clustered the colors in an image based on their individual RGB pixel values and picked the top 'K' colors. We consider the pixel values of the three channels as points in 3D space. Euclidean distance was used as a metric to assign each point to one of the K clusters. The centroids of all the K clusters are updated accordingly.</p><formula xml:id="formula_0">= ?[( 1 ? 2 ) 2 + ( 1 ? 2 ) 2 + ( 1 ? 2 ) 2 ]<label>(1)</label></formula><p>Here (R 1 , G 1 , B 1 ) is color 1 and (R 2 , G 2 , B 2 ) is color 2 and is the distance between them. Once the centroid locations are constant after successive iterations we choose the top K colors as the K centroids that were obtained and rank them based on the number of points in the cluster (i.e. the area they occupy in the image). Based on our analysis over a wide variety of images, we found that the text color always occupies the second highest area in the bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Text Size</head><p>Using a simple edge detection algorithm, we find the first and last vertical edge in an image. The difference between them is chosen as the maximum height of the text. In the algorithm, we include a threshold parameter . This parameter helps in differentiating between stray edges due to the background noise and the edges of text in the image. Here represents  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Font Prediction Engine</head><p>This engine predicts a list of visually similar fonts for any given query font from our dataset of 1762 fonts. These similar fonts can be used to replace the query font without compromising on the overall aesthetic quality of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">Dataset Creation</head><p>For estimating the similarity of two fonts, we need a mapping between a font and a set of attributes. These attributes are basically adjectives that explain the visual characteristics of the font, example 'legible', 'serif', 'thin' etc. There are 37 such attributes and their range is [0,100]. In <ref type="bibr" target="#b13">[11]</ref>, they use crowdsourced data to obtain such a mapping. This crowdsourced dataset is available only for 200 fonts from the Google Fonts <ref type="bibr" target="#b15">[13]</ref> website.</p><p>In order to extend this dataset, we use a kNN based algorithm <ref type="bibr" target="#b12">[10]</ref>, where we consider the nearest neighbors of a new font to calculate its attribute vector ? . In <ref type="bibr" target="#b12">[10]</ref>, the authors have proven the effectiveness of using such an approach for extending the dataset. For finding the neighbors we use 200-dimensional CNN embeddings ? defined for 1883 Google fonts <ref type="bibr" target="#b16">[14]</ref> from a pre-trained network. Out of the 1883 font dataset and 200 font dataset, there are 156 common fonts. These 156 fonts will be used as seed data for finding the nearest neighbors. Out of the 1883 fonts, only 1762 fonts are supported for Android devices, therefore we can extend our dataset up to 1762 fonts (156 old fonts + 1606 new fonts).</p><formula xml:id="formula_1">= 1 ? 1 ( ? ( ?, ?????) =1 ? ? ( ?, ?????) =1 ) (3) ? = ? w i ? ?? =1<label>(4)</label></formula><p>Once we get the nearest neighbors for each new font, we then calculate its attribute vector using the weighted average of attribute vectors of these nearest neighbors as shown in <ref type="formula">(3)</ref> and <ref type="bibr" target="#b5">(4)</ref>. Here, ( 1 ?????, 2 ?????) is the L2-distance between the two embeddings, this metric is used to find the nearest neighbors. The same procedure is repeated for all the 1606 fonts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Font Prediction Algorithm</head><p>This algorithm predicts visually similar fonts to a given query font using the attribute vector. We first select 11 most faithful attributes, i.e. the attributes whose values and their manifestation in the visual properties of the fonts is in congruence. We define these as priority attributes ' ' as depicted in <ref type="figure">Figure 4</ref>. In addition to this, we define relative weights for each of these priority attributes using weight vector ' '. We choose an Interval vector ' ' that defines ranges for all attributes which behave as search windows around the attributes of the query font. We consider fonts which lie in this range and then rank them based on their weighted distance with respect to the priority attributes.</p><p>In <ref type="figure">Figure 3</ref>, we have shown the top-4 fonts for each of the attributes. In (a), the attributes (italic, thin) describe the visual nature of the fonts and therefore we consider them in our list of priority attributes. But in (b), we notice that the attributes (cursive, delicate) and the fonts have no correlation with each other. Therefore these are less reliable and we don't consider them in the Font Prediction algorithm. <ref type="figure">Figure 4</ref> outlines the high level description of the font prediction algorithm. We start with our query font f q . From our newly created dataset we get its corresponding 37dimensional attribute vector. The interval vector defines the search space for nearby fonts. From f q and we get the search window for each attribute. While searching for the prospective candidates we only consider the priority attributes . Once we have the list of candidate fonts, we use a weighted Euclidean distance measure to rank the candidate fonts. This list of sorted fonts is our final list of predicted fonts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Model Compression and On-Device Deployment</head><p>A very crucial part of this paper is to deploy both the engines on a mobile device and use it in real time. We applied multiple optimizations to our model making it more efficient with respect to time and space constraints. Firstly, we reduced the input height from 105 to 50 while preserving the aspect ratio. This reduced the model size drastically while maintaining its accuracy. We used "Depth-wise Separable Convolutional" layers which reduce the model size and work well in edge devices as shown in <ref type="bibr" target="#b14">[12]</ref>. This is the final model (shown in <ref type="figure" target="#fig_0">Figure 1</ref>) used in the FDE which shows a 70% reduction in model size (15MB to 4.5 MB) and 86% reduction in inference time (300ms to 40 ms). This model was converted to TFLite and used in an Android application. We used native inferencing using C++ to get the optimal results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">OBSERVATIONS AND RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Font Detection Engine</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Model Comparison</head><p>From <ref type="table">Table 1</ref>, it is evident that there is a drastic reduction in the number of parameters in the FCN compared to the model with a Dense layer and also improves the validation accuracy. For the inference time calculation, we use two sets of images, one with a width of 50 pixels and the other with 80 pixels. For the inference of the dense layer model, we created image patches of size 50x50 after a width of every 4 pixels. These are the same patches that are being seen internally by the FCN.</p><p>With FCN we achieved a worst case inference time of 30ms. This is one of the crucial improvements which made it possible to port the model on a mobile device and achieve seamless experience. We achieved a Top-1 validation accuracy of 78% on the VFR synthetic dataset with a model size as small as 5MB. In <ref type="bibr">[1]</ref>, we see that the DeepFont model has a Top-1 accuracy of 80% on the VFR synthetic dataset. But their model has a whopping 26M parameters which is 5 times larger than our model. The VFR model is also compressed using matrix factorization techniques. Therefore in comparison to the state of the art, our model has a commendable accuracy considering the massive difference in model size. <ref type="figure" target="#fig_3">Figure 5</ref> shows detailed steps of the FDE pipeline for replacing the original message ("Happy Birthday") with a new message ("Happy New Year") while preserving the background. The FDE detects style, color and size on the text region selected by the user. Inpainting API from Open CV [17] is used for removing original text. The detected font attributes are applied on the new input message and replaced in the image. The entire process takes less than 30ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Font Detection in Full Sized Digital Images</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Font Prediction Engine</head><p>The results for the FPE can be seen in <ref type="figure">Figure 6</ref>. For every query font, we have listed the top 3 predictions. We notice that the predicted fonts have many common qualities with the query font. For a cursive font such as Sacramento all of its top predictions also have the same nature and similar stroke thickness. For a thick and bold font such as Erica One, the predictions have a consistent visual quality. The serif quality is preserved for a font such as Abril Fatface. The predicted fonts have the same features and preserves the underlying intent as much as possible while also maintaining the requisite amount of diversity. The features and styles to be preserved is controlled by tuning the priority attributes and the weight vector. In <ref type="bibr" target="#b13">[11]</ref>, the authors have used a learned "distance metric" over crowd sourced data to quantify the similarity between two fonts. Compared to this, we have devised a simpler method.  <ref type="table">Table.</ref> 1. Comparison between models. (All results were evaluated on a mobile device with 12 GB RAM and an Octa-core processor).    <ref type="figure">6</ref>. Font Prediction results for three different query fonts <ref type="figure" target="#fig_4">Figure 7</ref> depicts the 37 font attributes of the font Abril Fatface and its top three predictions. The predicted fonts closely resemble the query font in terms of the distance between attributes. This is reflected in their visual characteristics in <ref type="figure">Figure 6</ref>. The attributes showing large deviation in values (eg: attribute 9, 10, 17 etc.) belong to the low priority attributes which are not being used by the FPE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSIONS AND FUTURE WORK</head><p>In this paper, we implemented a Font Detection Engine, which identifies font style, font color and font size of text in an image. We developed a novel algorithm for predicting visually similar fonts for a given query font. Both the engines were then deployed on a mobile device and can work in real time. This Font Detection Engine can be used in applications such as, text customization and recognition. Both engines reduce the design effort required for selecting suitable fonts. As future work, we would want to make our current neural network scalable, to include extra fonts and also fonts from languages other than English. We can also try to link fonts and the corresponding moods they convey, making the prediction process much more robust and accurate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The Fully Convolutional Neural Network architecture for Font style detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>FDE procedure. Here, 'Width' is min (80, rescaled width). A factor of 4 in the denominator is due the 2 MaxPool layers. the , and channels. An edge exists in the ? row of the image, only if this condition is satisfied.( [ , ] ? [ + 1, ]) &gt;(2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Basis for selecting high priority attributes Overview of the Font Prediction algorithm</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>FDE pipeline for a sample image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Attribute Comparison between Query font (Sacramento) andPredicted fonts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig.</head><label></label><figDesc>Fig. 6. Font Prediction results for three different query fonts</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Article Title</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal</title>
		<imprint>
			<biblScope unit="page" from="1" to="10" />
			<publisher>Publisher, Location</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<pubPlace>Book Title, Publisher, Location, Date</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepfont: Identify your font from an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="451" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Font recognition in natural images via transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhui</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on multimedia modeling</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large-scale visual font recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3598" to="3605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Chinese Font Recognition Based on Convolution Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 3rd International Conference on Automation, Mechanical Control and Computational Engineering</title>
		<imprint>
			<publisher>Atlantis Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th</title>
		<meeting>the 13th</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<title level="m">European Conference Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="6" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Navier-stokes, fluid dynamics, and image and video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcelo</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><forename type="middle">L</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</title>
		<editor>I-I. IEEE</editor>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An image inpainting technique based on the fast marching method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Telea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Journal of graphics tools</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="23" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting Semantic Signatures of Fonts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tugba</forename><surname>Kulahcioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><forename type="middle">De</forename><surname>Melo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 12th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="115" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploratory font selection using crowdsourced attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>O&amp;apos;donovan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?nis</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aseem</forename><surname>L?beks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Agarwala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Adam</forename><surname>Mobilenets</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<ptr target="https://fonts.google.com/" />
		<title level="m">Google Fonts</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fontjoy</surname></persName>
		</author>
		<ptr target="https://github.com/Jack000/fontjoy" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling fonts in context: Font prediction on web designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanxuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="385" to="395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Google</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kit</surname></persName>
		</author>
		<ptr target="https://docs.opencv.org/master/df/d3d/tutorial_py_inpainting.html" />
	</analytic>
	<monogr>
		<title level="j">Image Inpainting</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
