<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TIPCB: A Simple but Effective Part-based Convolutional Baseline for Text-based Person Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-05-25">25 May 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Software</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Software</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Institute of Natural and Mathematical Sciences</orgName>
								<orgName type="institution">Massey University</orgName>
								<address>
									<postCode>4442</postCode>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujiang</forename><surname>Lu</surname></persName>
							<email>yujiang_lu@163.comyujianglu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Software</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenxing</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Mathematics and Statistics</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer and Software</orgName>
								<orgName type="institution">Nanjing University of Information Science and Technology</orgName>
								<address>
									<postCode>210044</postCode>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruili</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Institute of Natural and Mathematical Sciences</orgName>
								<orgName type="institution">Massey University</orgName>
								<address>
									<postCode>4442</postCode>
									<settlement>Auckland</settlement>
									<country key="NZ">New Zealand</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">TIPCB: A Simple but Effective Part-based Convolutional Baseline for Text-based Person Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-05-25">25 May 2021</date>
						</imprint>
					</monogr>
					<note type="submission">Preprint submitted to Journal of L A T E X Templates May 26, 2021</note>
					<note>(Zhenxing Wang), zheng_yuhui@nuist.edu.cn (Yuhui Zheng), Ruili.wang@massey.ac.nz (Ruili Wang)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Cross-modality</term>
					<term>Person serach</term>
					<term>Local representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Text-based person search is a sub-task in the field of image retrieval, which aims to retrieve target person images according to a given textual description.</p><p>The significant feature gap between two modalities makes this task very challenging. Many existing methods attempt to utilize local alignment to address this problem in the fine-grained level. However, most relevant methods introduce additional models or complicated training and evaluation strategies, which are hard to use in realistic scenarios. In order to facilitate the practical application, we propose a simple but effective end-to-end learning framework for text-based person search named TIPCB (i.e., Text-Image Part-based Convolutional Baseline). Firstly, a novel dual-path local alignment network structure is proposed to extract visual and textual local representations, in which images are segmented horizontally and texts are aligned adaptively. Then, we propose a multi-stage cross-modal matching strategy, which eliminates the modality gap from three feature levels, including low level, local level and global level. Extensive experiments are conducted on the widely-used benchmark dataset (CUHK-PEDES) and verify that our method outperforms the state- * Corresponding author of-the-art methods by 3.69%, 2.95% and 2.31% in terms of Top-1, Top-5 and Top-10. Our code has been released in https://github.com/OrangeYHChen/ TIPCB.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person search is a key technology in the field of image retrieval, aiming to find target person images from large databases with given retrieval conditions, including person images, relevant attributes or natural language descriptions.</p><p>According to the modality of the query, this technology can be broadly divided into image-based search <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, attribute-based search <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> and text-based search <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. In recent years, person search has gained increasing attention due to its wide applications in public security and video surveillance, e.g., searching for suspects and missing persons.</p><p>In this paper, we research on the task of text-based person search, as is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Specifically, it is required to sort all person images in a large gallery according to their similarity with the textual description of the query, and select the top person images as matching items <ref type="bibr" target="#b6">[7]</ref>. Since textual descriptions are much more natural and accessible as retrieval queries, text-based person search has large potential values in conditions without target person images, e.g., searching for a suspect according to the description of the eyewitness.</p><p>Text-based person search is still a challenging task because it has the difficulties of both person re-identification and cross-modal retrieval. On the one hand, it is hard to extract robust visual representations due to the disturbance from occlusion, background clutter and pose/viewpoint variances. On the other hand, some images or descriptions of different persons have very similar highlevel semantics, while the domains of image and text have significant differences, resulting in inter-modal feature variances much larger than intra-modal feature variances.</p><p>Therefore, a series of relevant methods have been proposed to reduce the gap between image domain and text domain in recent years. We broadly categorize them into global-matching methods and local-matching methods.</p><p>Global-matching methods mainly focus on the global visual and textual representation learning and obtain the unified feature space regardless of modality <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. However, images contain many distinctive local details which are hard to explore by global representation extraction. Besides, there are a few irrelevant regions in images, which bring noises to global information. In order to further mine discriminative and comprehensive information, some local-matching methods are proposed, which match person images and textual descriptions by local alignment <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>However, most of the existing local-matching methods are not practical enough to meet the requirements of realistic scenarios due to their high complexity. Some of these methods introduce additional models or apply multi-task learning strategies, such as human pose estimation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, semantic segmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24]</ref> or attribute recognition <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>, which bring a huge amount of computation and make networks unable to perform end-to-end learning. Be-sides, some of these methods adopt the multi-granularities similarity measure strategy <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref>. In the use phase, these method need to learn multiple local representations for each image or text, and repeatedly calculate the local similarity. Both additional models and complex similarity measure are timeconsuming for practical applications. Thus, it is necessary to design a simple but effective framework for text-based person search problem.</p><p>In this paper, we propose a novel end-to-end learning framework named TIPCB (i.e., Text-Image Part-based Convolutional Baseline) to facilitate the practical application. Firstly, a novel dual-path local alignment network structure is proposed to extract visual and textual local representations. Visual local representations are extracted by general PCB strategy <ref type="bibr" target="#b1">[2]</ref>, in which person images are horizontally segmented into several stripes. In the textual representation learning path, the word embeddings are learnt by a BERT model with pretrained and fixed parameters <ref type="bibr" target="#b25">[26]</ref>, and further processed by a multi-branch residual network. In each branch, the textual representation is learnt to adaptively match a corresponding visual local representation, so as to extract the aligned textual local representation. Besides, a multi-stage cross-modal matching strategy is proposed, which eliminates the modality gap from low-level, local-level and global-level features, and then the feature gap between image domain and text domain can be reduced in a progressive way.</p><p>The main contributions of this paper can be summarized as follows:</p><p>? A novel dual-path local alignment network is proposed to jointly learn visual and textual representations, which can align local features in a simple but effective way.</p><p>? A multi-stage cross-modal matching strategy is designed to reduce the gap between two modalities in a progressive way. The whole framework can be trained in end-to-end manner.</p><p>? Extensive experiments are conducted on CUHK-PEDES dataset <ref type="bibr" target="#b6">[7]</ref>, and the results clearly verify that our proposed TIPCB framework achieves the state-of-the-art performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Person Re-identification</head><p>In the past decade, a variety of person re-identification (image-based person search) methods have sprung up, attempting to extract discriminative and robust representations for person images, and overcome the difficulties of occlusion, background clutter and so on <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref>. Among them, local representation learning is an efficient and widely-used technology, which can explore some detailed and distinctive features. Specifically, PCB <ref type="bibr" target="#b1">[2]</ref> applies horizontal segmentation on the feature map of each person image, and extract local representations for obtained stripes independently. MGN <ref type="bibr" target="#b28">[29]</ref> adopts a multi-granularities representation learning strategy which cuts each feature map into several parts with different scales. Besides, spatial attention mechanism is introduced to align human parts and further improves the robustness of local representation <ref type="bibr" target="#b29">[30]</ref>.</p><p>Furthermore, some methods utilize additional models to assist local segmentation and mine detailed information, such as pose estimation <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and human semantic segmentation <ref type="bibr" target="#b32">[33]</ref>.</p><p>Extensive works have also achieved great progress in video-based re-ID <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36]</ref>, occluded re-ID <ref type="bibr" target="#b36">[37]</ref>, unsupervised re-ID <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>, cross-resolution re-ID <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref>, RGB-IR cross-modality re-ID <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> and so on. Person re-identification has obtained rapid development, but this technology cannot be applied in the scenario without query images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Text-based Person Search</head><p>A series of high-performance methods have been proposed for text-based person search in recent years, which can be broadly categorized into globalmatching methods and local-matching methods. GNA-RNN <ref type="bibr" target="#b6">[7]</ref> is the first designed framework for this task, which combines a recurrent neural network with the proposed gated neural attention mechanism to learn affinities between textual descriptions and person images. Later, an identity-aware two-stage network <ref type="bibr" target="#b12">[13]</ref> is proposed to jointly minimize the intra-identity distance and the cross-modal distance. Besides, in order to embed images and texts to a shared visual-textual space, Zheng et al. <ref type="bibr" target="#b11">[12]</ref> design an end-to-end trainable model with a CNN+CNN dual-path structure, and Zhang et al. <ref type="bibr" target="#b13">[14]</ref> introduce the cross-modal projection learning in objective functions. Furthermore, TIMAM <ref type="bibr" target="#b14">[15]</ref> attempts to learn modality-invariant representations in a shared space by adversarial learning. However, the above methods only focus on global representations, which may miss some distinctive local details or mix a little noise information.</p><p>Therefore, some local-matching methods are explored to overcome this shortcoming. Aggarwal et al. <ref type="bibr" target="#b20">[21]</ref> introduce human attribute recognition to help bridge the modality gap between the image-text inputs. Wang et al. <ref type="bibr" target="#b17">[18]</ref> design a light auxiliary attribute segmentation layer to guide the alignment between visual local representations with parsed textual attributes. Jing et al. <ref type="bibr" target="#b16">[17]</ref> propose a multi-granularities attention network to align visual and textual local representations with the aid of the human pose information. These methods apply additional models or multi-task learning strategies to enhance the local alignment but bring a huge amount of computation. In addition, MIA <ref type="bibr" target="#b18">[19]</ref> aligns the local representations from multiple granularities, including global-global, global-local and local-local levels. NAFS <ref type="bibr" target="#b15">[16]</ref> conducts joint alignments over full-scale representations with a novel staircase CNN and a locality-constrained BERT. Nevertheless, such methods are still time-consuming in the use phase due to their complex similarity measure strategies. By comparison, our proposed method utilizes an end-to-end trainable dual-path network to learn local aligned representations simply and effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we will detailly explain our proposed Text-Image Part-based Convolutional Baseline (TIPCB) for text-based person search problem. We first illustrate the dual-path local alignment network structure, including the visual CNN branch and the textual CNN branch, and then the multi-stage cross-modal matching strategy is introduced to eliminate the modality gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Visual Representation Learning</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, our proposed TIPCB contains two CNN branches which aim to learn discriminative and compatible visual and textual representations from the input person images and descriptions, respectively. In the training phase, we assume the training data as</p><formula xml:id="formula_0">D= {I i , T i } N i=1</formula><p>where N represents the number of image-text pair in each batch, and each pair consists of an image I and a corresponding description T . (The subscript i is omitted in the following for simplicity unless necessary.) In visual CNN branch, ResNet-50 <ref type="bibr" target="#b46">[47]</ref> is adopted as the backbone to extract visual features, which mainly consists of four residual blocks. Different residual blocks can capture semantic information from different level <ref type="bibr" target="#b47">[48]</ref>. For each image I, we define the feature generated by the 3 rd and 4 th residual block as its low-level feature map f I l ? R H?W ?C1 and high-level feature map f I h ? R H?W ?C2 , where H, W and C 1 /C 2 represent the dimension of height, width and channel in the above feature maps. Then we</p><formula xml:id="formula_1">obtain its visual low-level representation v I l ? R C1 by: v I l = GMP(f I l ) (1)</formula><p>where GMP represents a global max-pooling layer as a filter to mine salient information.</p><p>Here, we adopt the PCB <ref type="bibr" target="#b1">[2]</ref> strategy to obtain the local regions. Specially,</p><formula xml:id="formula_2">the high-level feature map f I h is segmented into K horizontal stripes which are denoted as f I p1 , f I p2 , ..., f I pK where f I pi ? R H K ?W ?C2 .</formula><p>For each stripe, similar to Formula (1), we still adopt a global max-pooling layer to extract the visual local representation v I pi ? R C2 . In order to fuse all local representations, we select the maximum value of each element in channel dimension, and get the</p><formula xml:id="formula_3">visual global representation v I g ? R C2 : v I g = Max v I p1 , v I p2 , ..., v I pK<label>(2)</label></formula><p>Therefore, we get the visual feature set</p><formula xml:id="formula_4">V I = v I l , v I p1 , ..., v I pK , v I g containing</formula><p>low-level, local-level and global-level representations. In the testing phase, only the global-level representation is adopted to measure similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Textual Representation Learning</head><p>In textual CNN branch, a high-performance language representation model BERT <ref type="bibr" target="#b25">[26]</ref> is applied to extract discriminative word embeddings, which can learn the contextual relations between the words by the bi-directional training of Transformer <ref type="bibr" target="#b48">[49]</ref>. Specifically, we break each textual description T up into a list of words, and insert [CLS] and [SEP] into the beginning and end of each sentence. Then this list is embedded into tokens by a pretrained tokenizer. To ensure the consistency of text length, we select the first L tokens when the text is longer than L, and apply zero-padding in the end of the text when the text is shorter than L. After that, each tokenized textual description is input into the BERT model, which is pretrained and parameter-fixed, to extract word embeddings t ? R L?D , where D represents the dimension of each word embedding. Here we "freeze" the weights of the BERT model for the following three reasons: 1) the pretrained BERT itself has the strong semantic representation ability, and we only use it as the word embedding layer, 2) the following CNN structure is capable to further process the word embeddings, and 3) only training the CNN structure can significantly reduce the amount of training parameters and accelerate the convergence of the model.</p><p>In order to meet the inputting requirement of convolutional layers, we expand the dimension of word embedding from t ? R L?D to t * ? R 1?L?D , where 1, L and D are regarded as the height, width and channel dimension of convolutional input, respectively. Motivated by the residual network <ref type="bibr" target="#b46">[47]</ref> and the deep textual CNN <ref type="bibr" target="#b11">[12]</ref>, we design the multi-branch textual CNN, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In the textual CNN, in order to map the word embeddings to the same channel dimension as the visual low-level feature map f I l ? R H?W ?C1 , the filter size of the first convolutional layer is set to 1 ? 1 ? D ? C 1 , which can be viewed as a lookup table. Then, we can obtain the textual low-level feature map f T l ?</p><formula xml:id="formula_5">R 1?L?C1 .</formula><p>The multi-branch textual CNN contains K residual branches, which corre- Different from the deep textual CNN <ref type="bibr" target="#b11">[12]</ref>, we only stack a few bottlenecks rather than use a very deep residual network to extract textual representations for the following two reasons: 1) the downsampling between different stages in deep textual CNN brings obvious information loss, and 2) deep network does not bring obvious improvement compared with shallow network, which is contrary to the experience in image area and has been proven in <ref type="bibr" target="#b49">[50]</ref>. In our experiment section, we will further verify the above viewpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Multi-stage Cross-modal Matching</head><p>In order to eliminate the feature gap between image modality and text modality, we adopt the Cross-Modal Projection Matching (CMPM) loss <ref type="bibr" target="#b13">[14]</ref> on low-level, local-level and global-level representations, which can associate The probability that v I i and v T j are a matched pair can be calculated by:</p><formula xml:id="formula_6">p i,j = exp (v I i ) v T j N k=1 exp (v I i ) v T k<label>(3)</label></formula><p>wherev T j is the normalized textual representation and denoted asv</p><formula xml:id="formula_7">T j = v T j v T j .</formula><p>In CMPM, the scalar projection of v I i on v T j is regarded as their similarity, and matching probability p i,j is the proportion of the similarity between v I i and v T j to the sum of similarity between v I i and v T j N j=1 in a batch. Then the CMPM loss can be calculated by:</p><formula xml:id="formula_8">L I2T = 1 N N i=1 N j=1 p i,j log p i,j q i,j + ?<label>(4)</label></formula><p>where ? is a small number to avoid numerical problems, and q i,j is the normalized true matching probability between v I i and v T j since there might be more than one matched text descriptions in a batch, denoted as q i,j = yi,j N k=1 y i,k . The above procedure reduces the distance between each visual representation and its matched textual representations in a single direction, and we reversely conduct the similar procedure to draw each textual representation and its matched visual representations closer. Therefore, the bi-directional CMPM loss is computed by:</p><formula xml:id="formula_9">L CM P M = L I2T + l T 2I<label>(5)</label></formula><p>The objective in our framework contains the cross-modal representation matching from three levels. The CMPM loss in low-level representations is to reduce the modality gap in an early stage. The CMPM loss in local-level representations can realize the local alignment between images and texts. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Dataset. We evaluate our proposed TIPCB framework on the CUHK-PEDES dataset <ref type="bibr" target="#b6">[7]</ref>, which is the only large scale available benchmark for textbased person search problem, as is shown in <ref type="figure" target="#fig_6">Figure 4</ref>. It totally contains 40,206 person images of 13,003 identities by collecting samples from several re-ID datasets. Each person image has two corresponding textual descriptions on average, and each textual description has more than 23 words. These textual descriptions have a vocabulary of 9,408 different words. We adopt the same data split as <ref type="bibr" target="#b6">[7]</ref>. The training set has 34,054 images of 11,003 identities. The validation and testing set have 3,078 images and 3,074 images of 1,000 identities, respectively.</p><p>Evaluation Protocol. We follow the standard evaluation metrics, and report the top-k (k = 1, 5, 10) accuracy to evaluate the performance. Specifically, In the training phase, Adam is selected to optimize our model with weight decay 4 ? 10 ?5 . The model is trained for 80 epochs in total. The base learning rate is set to 3?10 ?3 and decreased by 0.1 after 50 epochs. Besides, we initialize the learning rate by the warm-up trick in first 10 epochs. We adopt the trick of horizontally flipping to augment data, where each image has 50% chance to flip randomly. The hyper-parameters in the objective function are set to ? 1 = 1, ? 2 = 1 and ? 3 = 1. In the testing phase, the cosine distance is used to measure the similarity value of image-text pairs. We perform our experiments with PyTorch on a single Tesla V100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-The-Art Methods</head><p>We compare our method with the existing text-based person search methods, and the comparable results are reported in <ref type="table">Table 1</ref>. We divide these methods into two categories. Global-matching methods (type column is marked as "G") consist of GNA-RNN, IATV, Dual Path, CMPM + CMPC and TIMAM, and local-matching methods (type column is marked as "L") contain PWM + ATH, GLA, MIA, PMA, ViTAA, CMAAM and NAFS. We can find that the methods based on local alignment have become a hot topic and relatively achieved better performance in recent years, which can prove the importance of the local finegrained alignment to strengthen the compatibility and discrimination of imagetext features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>Effects of local representations. In order to verify the effectiveness of local representations, we conduct ablation studies to compare the performance of global representations and local representations of different region scales, and the results are listed in <ref type="figure" target="#fig_7">Figure 5</ref>(a). We can obtain the following two findings. On the one hand, compared with the global representations, the local representations have the much stronger discriminative ability, and the method with 6 local regions gains 3.25% improvement in Top-1 accuracy. One important reason is that global representations are hard to capture some distinctive local details. On the other hand, it can be observed that the Top-1 accuracy increases first and then decreases with the increase of the number of local regions, which can be explained by the local misalignment among person images. Due to the unstable vibration of person detection boxes and variances of viewpoints, the spatial distribution of head, body and limbs exists significant differences. We <ref type="table">Table 1</ref>: Comparison with state-of-the-art methods on CUHK-PEDES dataset. Top-1, Top-5</p><p>and Top-10 accuracies (%) are reported. The 1 st , 2 nd and 3 rd top results are indicated by red, blue and green bold numbers, respectively. In the second column, "G" represents the methods only using global features, and "L" indicates the methods aligning local features.</p><p>All listed methods do not use post-processing including re-ranking, to ensure the fairness of comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Type apply the hard segmentation strategy based on PCB <ref type="bibr" target="#b1">[2]</ref> in the visual CNN branch, which brings the noises in local region set. When the granularity of the local regions is too small, abundant noises in local region set will bring difficulties for the network to extract the common features of this region, as is shown in <ref type="figure" target="#fig_7">Figure 5</ref>(b).</p><p>Effects of shallow network for textual representation learning. Motivated by the conclusion that deep network does not bring obvious improvement for text classification compared with shallow network in <ref type="bibr" target="#b49">[50]</ref>, we design a much shallower network than the deep textual CNN <ref type="bibr" target="#b11">[12]</ref> to extract textual representations from word embeddings. We conduct a series of ablation experiments to prove the superiority of shallow network in textual representation learning.</p><p>As is shown in <ref type="figure" target="#fig_8">Figure 6(a)</ref>, we test the multi-branch textual CNNs with dif-  Therefore, our multi-branch textual CNN performs better in the condition that each branch has only 2?3 bottlenecks.</p><p>In addition, we keep the size of textual feature maps rather than use multistage downsampling in <ref type="bibr" target="#b11">[12]</ref>. We compare the residual branches with different times of downsampling and the results are shown in <ref type="figure" target="#fig_8">Figure 6</ref> Effects of multi-stage cross-modal matching strategy. During the training phase, in order to stimulate the modality gap step by step, we apply a multi-stage cross-modal matching strategy, which applies the CMPM loss on the representations of three stages, including low-level and high-level representations. Note that both local-level and global-level representations belong to high-level representations. We conduct the following ablation experiments to verify the CMPM loss in each stage, and the results are reported in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>(The accuracy of the method with only low-level CMPM loss is not listed here, because we only select the high-level representations during the testing phase, and the result will be not referential if the high-level CMPM loss is ignored.)</p><p>From the experimental results, we can observe the following findings:</p><p>1) According to the results of variants (a) and (b) which only use one stage of CMPM loss, we find that their Top-1 accuracies of more than 57% have already surpasses most of the existing methods except for NAFS. It proves the high efficiency of our dual-path local alignment network.</p><p>2) According to the results of variants (a), (b), (c) and (d), we observe that the addition of low-level CMPM loss brings 0.91% and 1.37% improvements in Top-1 accuracy for variants (a) and (b), respectively. It proves the positive  Effects of fusion strategy. We conduct a series of ablation experiments to compare the performance of different fusion strategies, including avg-pooling, max-pooling and the addition of them, and the results are listed in <ref type="table" target="#tab_2">Table 3</ref>. It can be observed that the accuracy of the strategy with max-pooling is far higher than the strategy with avg-pooling, and sightly higher than the strategy with the addition of them. Here, global max-pooling layer can filter salient information to extract more discriminative representation, while global avg-pooling may mix irrelevant noises in representations since it integrates each element in the feature  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>map.</head><p>Visualizaton of features. We apply the t-SNE <ref type="bibr" target="#b51">[52]</ref> to visualize the features in <ref type="figure" target="#fig_11">Figure 7</ref> and show the changing process of feature distributions in four steps. Before training, there is a significant gap between text modality and image modality, and the distributions inside the modalities are disordered. After several training epochs, it can be observed that samples of the same identity begin to cluster, but the two modalities still have a large gap. Then, the distributions of two modalities begin to converge gradually until their centers are close. Finally, the feature distributions of two modalities coincide well to some extent, and the samples from the same identity can have a good clustering performance. This demonstrates that our TIPCB is capable to learn discriminative visual and textual representations, and eliminate the cross-modal distribution gap.</p><p>Sample analysis. As is shown in <ref type="figure" target="#fig_12">Figure 8</ref> ful case, we can find that the listed top-5 person images have multiple regions that can well match parts of the corresponding textual description. Note that our proposed TIPCB is capable to distinguish some hard samples, which only partly match the textual description. For example, in the case 6, our method successfully finds the perfect matched images which simultaneously meet the characteristics of "black short", "dark backpack", "short dark hair" and "sitting on a bike", and lists the image that only mismatches the condition of "black short" behind.</p><p>In terms of failure cases, we list the following two representative examples.</p><p>In the first case, the textual description is too ambiguous, which only contains useful information about backpack. We can find that the top-3 person images all have the "black backpack", but they are not the matched ones. Besides, the detailed description of "white stripes" is too subtle to extract discriminative features from it. In the second case, the text uses "70s looking" to describe the dress, which is a rare phase and is difficult for network to learn. Thus, the top-3 person images have the dresses with different styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, in order to facilitate the practical application, we propose a simple but effective end-to-end learning framework for text-based person search named TIPCB (i.e., Text-Image Part-based Convolutional Baseline). In contrast to the existing local-matching methods, TIPCB applies an end-to-end trainable structure without additional models and complex evaluation strategies. We design a novel dual-path local alignment network to learn visual and textual local representations, in which images are segmented horizontally and texts are aligned adaptively. Besides, we introduce a multi-stage cross-modal matching strategy to match the visual and textual representations from three levels and eliminate the modality gap step by step. The outstanding experimental results verify the superiority of our proposed TIPCB method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of text-based person search problem. Given a textual description, it is aimed to retrieve the corresponding person images from a large gallery database. Since some key information is hidden in the local details, matching local features is necessary for the enhancement of compatibility between person images and textual descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the proposed TIPCB. This framework consists of a dual-path local alignment structure, where the visual CNN branch applies the PCB after the backbone network, and the textual CNN branch applies a multi-branch residual network after a pretrained BERT model. This framework adopts a multi-stage cross-modal matching strategy, which conducts projection matching on low-level, local-level and global-level representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The details of the multi-branch textual CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>spond to the K stripes of person images. For each branch, it contains P textual residual bottlenecks and is aimed to adaptively learn the textual representations which can match the visual local representations. The textual residual bottleneck has the similar structure as the modules in ResNet, consisting of several convolutional layers and batch normalization layers. The skip connection is applied to transmit information from low layers to high layers, which can effectively restrain the network degradation problem and speed up the model training. Specifically, in order to keep the textual information uncompressed, the strides of all convolutional layers in bottlenecks are set to 1 ? 1. For the first bottleneck of each branch, we modify the channel dimension of the textual feature map to C 2 , which is consistent with the visual high-level feature map f I h ? R H?W ?C2 , and then we keep the channel dimension unchanged in the following bottlenecks. After the multi-branch textual CNN, we obtain the textual local feature maps. Similar to visual CNN branch, we adopt a global max-pooling layer to extract the textual local representations and select the maximum value of each element in channel dimension to fuse these local representations. Then, we get the textual feature set V T = v T l , v T p1 , ..., v T pK , v T g containing low-level, local-level and global-level representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>representations across different modalities by incorporating the cross-modal projection into KL divergence. For each visual representation v I i , we assume that the set of image-text representation pairs is v I i , v T j , y i,j N j=1 , where y i,j = 1 represents that v I i and v T j are from the same person, while y i,j = 0 means that they are not a matched pair. (The subscript used to indicate the representation level is omitted, because it is applicable for any representation pair from V I and V T .)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>? 1 , ? 2 ,</head><label>12</label><figDesc>CMPM loss in global-level representations ensure that the final representations for evaluation have the stronger modal compatibility. Through the multiple stages of CMPM loss, the matching degree of image-text representations can be gradually improved, which will be further verified in ablation study. Finally, according to the visual and textual representation sets V I and V T , the overall objective function is calculated by: L = ? 1 L l CM P M + ? 2 ? 3 are hyper-parameters to control the importance of different CMPM losses, and L l CM P M , indicate the CMPM loss of low-level, local-level and global-level representations, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Samples in the CUHK-PEDES dataset. The person images in each line are of the same identity, and each image has two corresponding textual descriptions.given a query text description, all gallery images are ranked according to their similarity values. A successful search means that a matched person image is existed among the top-k images. Implementation Details. In the visual CNN branch, we adopt ResNet-50 pretrained on ImageNet<ref type="bibr" target="#b50">[51]</ref> as the backbone to extract visual feature maps, and we modify the stride of conv5 1 to 1 instead of 2 for larger feature maps. Inthe textual CNN branch, we extract word embeddings by the language model BERT-Base-Uncase pretrained on a large corpus including Toronto Book Corpus and Wikipedia. All the input images are resized to 384?128, and the text length is unified to L = 64. The number of local regions is set to K=6. In the multibranch textual CNN, the number of bottlenecks in each residual branch is set to P = 3. In the visual and textual features, some parameters of dimensions are set to H = 24, W = 8, D = 768, C 1 = 1024 and C 2 = 2048. Each batch contains N = 64 image-text pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>(a) Comparable results of different number of regions. Top-1 accuracy (%) is reported. (b) The region sets of different scales. The noises inside are marked by red bounding boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Comparable results of different number of bottlenecks (in Figure 6(a)) and different times of downsampling in each branch (in Figure 6(b)). Top-1 accuracy (%) is reported.ferent number of bottlenecks in each branch. The result presents an overall trend of first increasing and then decreasing with the increase of the number of bottlenecks, and the network achieves the best performance when each branch has 3 bottlenecks. When it has only one residual bottleneck, the network has insufficient ability to extract discriminative features. Besides, the network for textual representation learning does not need too deep layers since text data is generally discrete and sparse, which is significantly different from image data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(b). Specifically, each residual branch has 3 residual bottlenecks by default, and the branch without downsampling outputs the feature map of 1 ? 64 ? 2048. The spatial size of feature map will be reduced by half when each bottleneck applies the downsampling strategy. That is to say, the branch with 1, 2 and 3 times of downsampling output the feature maps of 1 ? 32 ? 2048, 1 ? 16 ? 2048 and 1 ? 8 ? 2048, respectively. It can be observed that the Top-1 accuracy decreases significantly with the increase of times of downsampling. One of the important reasons is that the downsampling strategy brings obvious textual information loss. Therefore, we do not adopt downsampling and keep the size of textual feature maps unchanged in residual branches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>3 ) 4 )</head><label>34</label><figDesc>From the variants (a), (b) and (e), it can be found that the combination of local-level and global-level CMPM losses has 5.23% and 3.92% improvements in Top-1 accuracy for single local-level and single global-level CMPM loss. The reason is that the local-level CMPM loss can realize the local alignment between images and texts, and the global-level CMPM loss can ensure that the final representations have the stronger modal compatibility. This group of experiments can simultaneously verify the validity of matching local-level and global-level representations. From the variant (f), we can find that the combination of low-level, locallevel and global-level CMPM losses achieves the best performance, which further shows the high efficiency of our proposed multi-stage cross-modal matching strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of features via t-SNE. We show the changing process of cross-modal feature distributions with training. There are shown 64 images with corresponding 128 textual descriptions. The feature of each image and text is marked as a circle and a rectangle, respectively. Each identity is indicated in a specific color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>, we visualize and analyze several examples of text-based person search by our proposed TIPCB. For each success-Visualization of text-based person search results by our proposed TIPCB. The green and red bounding boxes indicate correct and incorrect matches, respectively. The top 6 groups are successful searches, while the bottom 2 groups are failure searches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparable results of combinations of CMPM loss in different stages. Top-1, Top-5 and Top-10 accuracies (%) are reported.</figDesc><table><row><cell cols="4">Variant Low-level Local-level Global-level Top-1 Top-5 Top-10</cell></row><row><cell>(a)</cell><cell>57.16</cell><cell>78.03</cell><cell>84.94</cell></row><row><cell>(b)</cell><cell>58.47</cell><cell>80.19</cell><cell>86.97</cell></row><row><cell>(c)</cell><cell>58.07</cell><cell>79.12</cell><cell>85.70</cell></row><row><cell>(d)</cell><cell>59.84</cell><cell>81.39</cell><cell>87.95</cell></row><row><cell>(e)</cell><cell>62.39</cell><cell>81.93</cell><cell>88.71</cell></row><row><cell>(f)</cell><cell cols="3">63.63 82.81 89.01</cell></row><row><cell cols="4">effect of matching low-level representations, which can reduce the modality gap</cell></row><row><cell>in an early stage.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparable results of different fusion strategies. Top-1, Top-5 and Top-10 accuracies (%) are reported.</figDesc><table><row><cell>Fusion Strategy</cell><cell cols="3">Top-1 Top-5 Top-10</cell></row><row><cell>Avg-pooling</cell><cell>55.63</cell><cell>76.59</cell><cell>84.14</cell></row><row><cell>Max-pooling</cell><cell cols="3">63.63 82.81 89.01</cell></row><row><cell cols="2">Max-pooling+Avg-pooling 63.22</cell><cell>82.73</cell><cell>88.21</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="480" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Person re-identification by probabilistic relative distance comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="649" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person re-identification by deep learning attribute-complementary information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<publisher>CVPRW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep attributes driven multicamera person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="475" to="491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attribute-based people search in surveillance environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Vaquero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hampapur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Person search with natural language description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fusing two directions in cross-domain adaption for real life person search by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving text-based person search by spatial matching and adaptive threshold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1879" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language person search with mutually connected classification loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2057" to="2061" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03083</idno>
		<title level="m">Visual-textual association with hardest and semi-hard negative pairs mining for person search</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dual-path convolutional image-text embeddings with instance loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-D</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Multimedia Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Communications, and Applications (TOMM)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Identity-aware textual-visual matching with latent co-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1890" to="1899" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep cross-modal projection learning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="686" to="701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adversarial representation learning for text-to-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5814" to="5824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03036</idno>
		<title level="m">Contextual non-local alignment over full-scale representation for text-based person search</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pose-guided multigranularity attention network for text-based person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11189" to="11196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Vitaa: Visual-textual attributes alignment in person search by natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="402" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving description-based person re-identification by multi-granularity image-text alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="5542" to="5556" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving deep visual representation for person re-identification by global and local imagelanguage association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="54" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Text-based person search via attribute-aided matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">B</forename><surname>Radhakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2617" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A cnn-based 3d human pose estimation based on projection of depth and ridge data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107462</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An adversarial human pose estimation network injected with graph structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">107863</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Contextual deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">107152</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>arXiv:1901.07</idno>
		<title level="m">Pedestrian attribute recognition: A survey</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02984</idno>
		<title level="m">Person re-identification: Past, present and future</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Cross-view kernel collaborative representation classification for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>Multimedia Tools and Applications</publisher>
			<biblScope unit="page" from="1" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM international conference on Multimedia (ACMMM)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeply-learned part-aligned representations for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3219" to="3228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Region-based quality estimation network for large-scale person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hetang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spindle net: Person re-identification with human body region guided feature decomposition and fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1077" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human semantic parsing for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Kalayeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Basaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Kamasak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1062" to="1071" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video-based person re-identification by simultaneously learning intra-video and inter-video distance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5683" to="5695" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep asymmetric video-based person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="430" to="441" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11319</idno>
		<title level="m">Reference-aided part-aligned feature disentangling for video person re-identification</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-order information matters: Learning relation and topology for occluded person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6449" to="6458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Robust anchor embedding for unsupervised video person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A bottom-up clustering approach to unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8738" to="8745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive re-identification: Theory and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="page">107173</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep low-resolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inter-task association critic for crossresolution person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2605" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adaptive super-resolution for person re-identification with low-resolution images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page">107682</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with shared-specific feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13379" to="13389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Hi-cmd: hierarchical crossmodality disentanglement for visible-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10257" to="10266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modality adversarial neural network for visible-thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition (PR)</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107533</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rethinking temporal fusion for video-based person re-identification on semantic and time aspect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11133" to="11140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cerisara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Denis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04108</idno>
		<title level="m">Do convolutional networks need to be deep for text classification?</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: a largescale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
