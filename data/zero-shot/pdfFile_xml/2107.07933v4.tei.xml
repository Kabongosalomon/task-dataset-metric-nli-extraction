<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Sainte</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Loic Landrieu</orgName>
								<orgName type="laboratory" key="lab2">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ. Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">ENSG</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mande</settlement>
									<region>IGN</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fare</forename><surname>Garnot</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory" key="lab1">Loic Landrieu</orgName>
								<orgName type="laboratory" key="lab2">LASTIG</orgName>
								<orgName type="institution" key="instit1">Univ. Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">ENSG</orgName>
								<address>
									<postCode>F-94160</postCode>
									<settlement>Saint-Mande</settlement>
									<region>IGN</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Panoptic Segmentation of Satellite Image Time Series with Convolutional Temporal Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unprecedented access to multi-temporal satellite imagery has opened new perspectives for a variety of Earth observation tasks. Among them, pixel-precise panoptic segmentation of agricultural parcels has major economic and environmental implications. While researchers have explored this problem for single images, we argue that the complex temporal patterns of crop phenology are better addressed with temporal sequences of images. In this paper, we present the first end-to-end, single-stage method for panoptic segmentation of Satellite Image Time Series (SITS). This module can be combined with our novel image sequence encoding network which relies on temporal selfattention to extract rich and adaptive multi-scale spatiotemporal features. We also introduce PASTIS, the first openaccess SITS dataset with panoptic annotations. We demonstrate the superiority of our encoder for semantic segmentation against multiple competing architectures, and set up the first state-of-the-art of panoptic segmentation of SITS. Our implementation and PASTIS are publicly available.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The precision and availability of Earth observations have continuously improved thanks to sustained advances in space-based remote sensing, such as the launch of the Planet <ref type="bibr" target="#b4">[5]</ref> and the open-access Sentinel constellations <ref type="bibr" target="#b7">[8]</ref>. In particular, satellites with high revisit frequency contribute to a better understanding of phenomena with complex temporal dynamics. Crop mapping-the driving application of this paper-relies on exploiting such temporal patterns <ref type="bibr" target="#b37">[38]</ref> and entails major financial and environmental stakes. Indeed, remote monitoring of the surface and nature of agricultural parcels is necessary for a fair allocation of agricultural subsidies (50 and 22 billion euros per year in Europe and in the US, respectively) and for ensuring that best crop rotation practices are respected. More generally, the automated analysis of SITS represents a significant interest for a wide <ref type="figure" target="#fig_1">Figure 1</ref>: Overview. We propose an end-to-end, singlestage model for panoptic segmentation of agricultural parcels from time series of satellite images. Note the difficulty of resolving the parcels' borders from a single image, highlighting the need for modeling temporal dynamics. range of applications, such as surveying urban development and deforestation.</p><p>The task of monitoring both the content and extent of agricultural parcels can be framed as the panoptic segmentation of an image sequence. Panoptic segmentation consists of assigning to each pixel a class and a unique instance label, and has become a standard visual perception task in computer vision <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b25">26]</ref>. However, panoptic segmentation is a fundamentally different task for SITS versus sequences of natural images or videos. Indeed, understanding videos requires tracking objects through time and space <ref type="bibr" target="#b43">[44]</ref>. In yearly SITS, the targets are static in a geo-referenced frame, which removes the need for spatial tracking. Additionally, SITS share a common temporal frame of reference, which means that the time of acquisition itself contains information useful for modeling the underlying temporal dynamics. In contrast, the frame number in videos is often arbitrary. Finally, while objects on the Earth surface generally do not occlude one another, as is commonly the case for objects in natural images, varying cloud cover can make the analysis of SITS arduous. For the specific problem addressed in this paper, individualizing agricultural parcels requires learning complex and specific temporal, spatial, and spectral patterns not commonly encountered in video processing, such as differences in plant phenological profiles, subpixel border information, and swift human interventions such as harvests.</p><p>While deep networks have proven efficient for learning such complex patterns for pixel classification <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1]</ref>, there is no dedicated approach for detecting individual objects in SITS. Existing work on instance segmentation has been restricted to analysing a single satellite image <ref type="bibr" target="#b32">[33]</ref>. In summary, specialized remote sensing methods are limited to semantic segmentation or single-image instance segmentation, while computer vision's panoptic-ready networks require significant adaptation to be applied to SITS.</p><p>In this paper, we introduce U-TAE (U-net with Temporal Attention Encoder), a novel spatio-temporal encoder combining multi-scale spatial convolutions <ref type="bibr" target="#b33">[34]</ref> and a temporal self-attention mechanism <ref type="bibr" target="#b37">[38]</ref> which learns to focus on the most salient acquisitions. While convolutional-recurrent methods are limited to extracting temporal features at the highest <ref type="bibr" target="#b34">[35]</ref> or lowest <ref type="bibr" target="#b36">[37]</ref> spatial resolutions, our proposed method can use the predicted temporal masks to extract specialized and adaptive spatio-temporal features at different resolutions simultaneously. We also propose Parcels-as-Points (PaPs), the first end-to-end deep learning method for panoptic segmentation of SITS. Our approach is built upon the efficient CenterMask network <ref type="bibr" target="#b48">[49]</ref>, which we modify to fit our problem. Lastly, we present Panoptic Agricultural Satellite TIme-Series (PASTIS), the first open-access dataset for training and evaluating panoptic segmentation models on SITS, with over 2 billion annotated pixels covering over 4000km 2 . Evaluated on this dataset, our approach outperforms all reimplemented competing methods for semantic segmentation, and defines the first state-of-the-art of SITS panoptic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>To the best of our knowledge, no instance or panoptic segmentation method operating on SITS has been proposed to date. However, there is a large body of work on both the encoding of satellite sequences, and the panoptic segmentation of videos and single satellite images.</p><p>Encoding Satellite Image Sequences. While the first automated tools for SITS analysis relied on traditional machine learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b45">46]</ref>, deep convolutional networks allow for the extraction of richer spatial descriptors <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b15">16]</ref>. The temporal dimension was initially dealt via handcrafted temporal descriptors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b51">52]</ref> or probabilistic models <ref type="bibr" target="#b2">[3]</ref>, which have been advantageously replaced by recurrent <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b27">28]</ref>, convolutional <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b14">15]</ref>, or differential <ref type="bibr" target="#b24">[25]</ref> architectures. Recently, attention-based approaches have been adapted to encode sequences of remote sensing images and have led to significant progress for pixel-wise and parcel-wise classification <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b53">54]</ref>. In parallel, hybrid architectures <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref> relying on U-Net-type architectures <ref type="bibr" target="#b33">[34]</ref> for encoding the spatial dimension and recurrent networks for the temporal dimension have shown to be well suited for the semantic segmentation of SITS. In this paper, we propose to combine this hybrid architecture with the promising temporal attention mechanism.</p><p>Instance Segmentation of Satellite Images. The first step of panoptic segmentation is to delineate all individual instances, i.e. instance segmentation. Most remote sensing instanciation approaches operate on a single acquisition. For example, several methods have been proposed to detect individual instances of trees <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b54">55]</ref>, buildings <ref type="bibr" target="#b46">[47]</ref>, or fields <ref type="bibr" target="#b32">[33]</ref>. Several algorithms start with a delineation step (border detection) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48]</ref>, and require postprocessing to obtain individual instances. Other methods use segmentation as a preprocessing step and compute cluster-based features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, but do not produce explicit cluster-to-object mappings. Petitjean et al. <ref type="bibr" target="#b30">[31]</ref> propose a segmentationaided classification method operating on image time series. However, their approach partitions each image separately and does not attempt to retrieve individual objects consistently across the entire sequence. In this paper, we propose the first end-to-end framework for directly performing joint semantic and instance segmentation on SITS.</p><p>Panoptic Segmentation of Videos. Among the vast literature on instance segmentation, Mask-RCNN <ref type="bibr" target="#b10">[11]</ref> is the leading method for natural images. Recently, Wang et al.</p><p>proposed CenterMask <ref type="bibr" target="#b48">[49]</ref>, a lighter and more efficient single-stage method which we use as a starting point in this paper. Several approaches propose extending instance or panoptic segmentation methods from image to video <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b16">17]</ref>. However, as explained in the introduction, SITS differs from natural video in several key ways which require specific algorithmic and architectural adaptations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We consider an image time sequence X, organized into a four-dimensional tensor of shape T ? C ? H ? W , with T the length of the sequence, C the number of channels, and H ? W the spatial extent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spatio-Temporal Encoding</head><p>Our model, dubbed U-TAE (U-Net with Temporal Attention Encoder), encodes a sequence X in three steps: (a) each image in the sequence is embedded simultaneously and independently by a shared multi-level spatial convolutional encoder, (b) a temporal attention encoder collapses </p><formula xml:id="formula_0">(?2) Upsampling T ? C 1 ? H ? W C 1 ? H ? W D 4 ? H 8 ? W 8 D 3 ? H 4 ? W 4 D 2 ? H 2 ? W 2 D 1 ? H ? W Attention masks d 4 d 3 d 2 d 1 T e 3</formula><p>e 2 e 1 (?8) <ref type="figure">Figure 2</ref>: Spatio-temporal Encoding. A sequence of images is processed in parallel by a shared convolutional encoder. At the lowest resolution, an attention-based temporal encoder produces a set of temporal attention masks for each pixel, which are then spatially interpolated at all resolutions. These masks are used to collapse the temporal dimension of the feature map sequences into a single map per resolution. A convolutional decoder then computes features at all resolution levels. All convolutions operate purely on the spatial and channel dimensions, and we use strided convolutions for both spatial up and down-sampling. The feature maps are projected in RGB space to help visual interpretation.</p><p>the temporal dimension of the resulting sequence of feature maps into a single map for each level, (c) a spatial convolutional decoder produces a single feature map with the same resolution as the input images, see <ref type="figure">Figure 2</ref>.</p><p>a) Spatial Encoding. We consider a convolutional encoder E with L levels 1, ? ? ? , L. Each level is composed of a sequence of convolutions, Rectified Linear Unit (ReLu) activations, and normalizations. Except for the first level, each block starts with a strided convolution, dividing the resolution of the feature maps by a factor 2.</p><p>For each time stamp t simultaneously, the encoder E l at level l takes as input the feature map of the previous level e l?1 t , and outputs a feature map e l t of size C l ? H l ? W l with H l = H/2 l?1 and W l = W/2 l?1 . The resulting feature maps are then temporally stacked into a feature map sequence e l of size T ? C l ? H l ? W l :</p><formula xml:id="formula_1">e l = [E l (e l?1 t )] T t=0 for l ? [1, L] ,<label>(1)</label></formula><p>with e 0 = X and [ ? ] the concatenation operator along the temporal dimension. When constituting batches, we flatten the temporal and batch dimensions. Since each sequence comprises images acquired at different times, the batches' samples are not identically distributed. To address this issue, we use Group Normalization <ref type="bibr" target="#b49">[50]</ref> with 4 groups instead of Batch Normalization <ref type="bibr" target="#b13">[14]</ref> in the encoder.</p><p>b) Temporal Encoding. In order to obtain a single representation per sequence, we need to collapse the temporal dimension of each feature map sequence e l before using them as skip connections. Convolutional-recurrent U-Net networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b28">29]</ref> only process the temporal dimension of the lowest resolution feature map with a temporal encoder. The rest of the skip connections are collapsed with a simple temporal average. This prevents the extraction of spatially adaptive and parcel-specific temporal patterns at higher resolutions. Conversely, processing the highest resolution would result in small spatial receptive fields for the temporal encoder, and an increased memory requirement. Instead, we propose an attention-based scheme which only processes the temporal dimension at the lowest feature map resolution, but is able to utilize the predicted temporal attention masks at all resolutions simultaneously. Based on its performance and computational efficiency, we choose the Lightweight-Temporal Attention Encoder (L-TAE) <ref type="bibr" target="#b9">[10]</ref> to handle the temporal dimension. The L-TAE is a simplified multi-head self-attention network <ref type="bibr" target="#b44">[45]</ref> in which the attention masks are directly applied to the input sequence of vectors instead of predicted values. Additionally, the L-TAE implements a channel grouping strategy similar to Group Normalization <ref type="bibr" target="#b49">[50]</ref>.</p><p>We apply a shared L-TAE with G heads independently at each pixel of e L , the feature map sequence at the lowest level resolution L. This generates G temporal attention masks for each pixel, which can be arranged into G tensors a L,g with values in [0, 1] and of shape T ? H L ? W L : a L,1 , ? ? ? , a L,G = LTAE(e L ) , applied pixelwise. <ref type="bibr" target="#b1">(2)</ref> In order to use these attention masks at all scale levels l of the encoder, we compute spatially-interpolated masks a l,g of shape T ? H l ? W l for all l in [1, L ? 1] and g in <ref type="bibr">[1, G]</ref> with bilinear interpolation:</p><formula xml:id="formula_2">a l,g = resize a L,g to H l ? W l .<label>(3)</label></formula><p>The interpolated masks a l,g at level l of the encoder are then used as if they were generated by a temporal attention module operating at this resolution. We apply the L-TAE channel-grouping strategy at all resolution levels: the channels of each feature map sequence e l are split into G contiguous groups e l,1 , ? ? ? , e l,G of identical shape</p><formula xml:id="formula_3">T ?C l /G ? W l ? H l .</formula><p>For each group g, the feature map sequence e l,g is averaged on the temporal dimension using a l,g as weights. The resulting maps are concatenated along the channel dimension, and processed by a shared 1?1 convolution layer Conv l 1?1 of width C l . We denote by f l the resulting map of size C l ? W l ? H l by :</p><formula xml:id="formula_4">f l = Conv l 1?1 ? ? T t=1 a l,g t ? e l,g t G g=1 ? ? ,<label>(4)</label></formula><p>with [ ? ] the concatenation along the channel dimension and ? the term-wise multiplication with channel broadcasting. c) Spatial Decoding. We combine the feature maps f l learned at the previous step with a convolutional decoder to obtain spatio-temporal features at all resolutions. The decoder is composed of L ? 1 blocks D l for 1 ? l &lt; L, with convolutions, ReLu activations, and BatchNorms <ref type="bibr" target="#b13">[14]</ref>. Each decoder block uses a strided transposed convolution D up l to up-sample the previous feature map. The decoder at level l produces a feature map d l of size D l ? H l ? W l . In a U-Net fashion, the encoder's map at level l is concatenated with the output of the decoder block at level l ? 1:</p><formula xml:id="formula_5">d l = D l ([D up l (d l+1 ), f l ]) for l ? [1, L ? 1] ,<label>(5)</label></formula><p>with d L = f L and [ ? ] is the channelwise concatenation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Panoptic Segmentation</head><p>Our goal is to use the multi-scale feature maps {d l } L l=1 learned by the spatio-temporal encoder to perform panoptic segmentation of a sequence of satellite images over an area of interest. The first stage of panoptic segmentation is to produce instance proposals, which are then combined into a single panoptic instance map. Since an entire sequence of images (often over 50) must be encoded to compute {d l } L l=1 , we favor an efficient design for our panoptic segmentation module. Furthermore, given the relative Our parcel detection module maps the raw sequence of observation (c) to a predicted heatmap (d). The predicted centerpoints (red crosses) are the local maxima of the predicted heatmap (d). The black dots are the true parcels centers. simplicity of parcels' borders, we avoid complex region proposal networks such as Mask-RCNN. Instead, we adapt the single-stage CenterMask instance segmentation network <ref type="bibr" target="#b48">[49]</ref>, and detail our modifications in the following paragraphs. We name our approach Parcels-as-Points (PaPs) to highlight our inspiration from CenterNet/Mask <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b48">49]</ref>. We denote by P the set of ground truth parcels in the image sequence X. Note that the position of these parcels is time-invariant and hence only defined by their spatial extent. Each parcel p is associated with (i) a centerpoint? p ,? p with integer coordinates, (ii) a bounding box of size? p ,? p , (iii) a binary instance mask? p ? {0, 1} H?W , (iv) a clas? k p ? [1, K] with K the total number of classes.</p><p>Centerpoint Detection. Following CenterMask, we perform parcel detection by predicting centerness heatmaps supervized by the ground truth parcels' bounding boxes. In the original approach <ref type="bibr" target="#b55">[56]</ref>, each class has its own heatmap: detection doubles as classification. This is a sensible choice for natural images, since the tasks of detecting an object's nature, location, and shape are intrinsically related. In our setting however, the parcels' shapes and border characteristics are mostly independent of the cultivated crop. For this  <ref type="figure">Figure 4</ref>: Panoptic Segmentation. The local maxima of the predicted centerness heatmap defines M tentative parcels. For each one, the pixel features at all levels are concatenated and used to predict a bounding box size, a semantic class, and an S ? S shape patch. The latter is combined with a global saliency map for predicting pixel-precise masks. The instance predictions are combined into a panoptic segmentation using the centerness as quality.</p><p>reason, we use a single centerness heatmap and postpone class identification to a subsequent specialized module. See <ref type="figure" target="#fig_0">Figure 3</ref> for an illustration of our parcel detection method. We associate each parcel p with a Gaussian kernel of deviations ? ver p and ? hor p taken respectively as 1/20 of the height and width of the parcels' bounding box. Unlike Law and Deng <ref type="bibr" target="#b20">[21]</ref>, we use heteroschedastic kernels to reflect the potential narrowness of parcels. We then define the target centerness heatmapm ? [0, 1] H?W as the maximum value of all parcel kernels at each pixel (i, j) in H ? W :</p><formula xml:id="formula_6">m i,j = max p?P exp ? (i ?? p ) 2 2(? ver p ) 2 + (j ?? p ) 2 2(? hor p ) 2<label>(6)</label></formula><p>A convolutional layer takes the highest-resolution feature map d 1 as input and predicts a centerness heatmap m ? [0, 1] H?W . The predicted heatmap is supervized using the loss defined in Equation 7 with ? = 4:</p><formula xml:id="formula_7">L center = ?1 |P | i=1???H j=1???W log(m i,j ) ifm i,j = 1 (1?m i,j ) ? log(1?m i,j ) else.<label>(7)</label></formula><p>We define the predicted centerpoints as the local maxima of m, i.e. pixels with larger values than their 8 adjacent neighbors. This set can be efficiently computed with a single max-pooling operation. Replacing the max operator by argmax in <ref type="figure" target="#fig_4">Equation 6</ref> defines a mapping H ? W ? P between pixels and parcels. During training, we associate each true parcel p with the predicted centerpoint c(p) with highest predicted centerness m among the set of centerpoints which coordinates are mapped to p. If this set is empty, then c(p) is undefined: the parcel p is not detected. We denote by P ? the subset of detected parcels, i.e. for which c(p) is well defined.</p><p>Size and Class Prediction. We associate to a predicted centerpoint c of coordinate (i c , j c ) the multi-scale feature vectord c of size D 1 + ? ? ? + D L by concatenating channelwise the pixel features at location (i c , j c ) in all maps d l :</p><formula xml:id="formula_8">d c = d l i c /2 l?1 , j c /2 l?1 L l=1 ,<label>(8)</label></formula><p>with [ ? ] the channelwise concatenation. This vectord c is then processed by four different multilayer perceptrons (MLP) to obtain three vectors of sizes 2, K, and S 2 representing respectively: (i) a bounding box size h c , w c , (ii) a vector of class probabilities k c of size K, and (iii) a shape patch s c of fixed size S ? S. The latter is described in the next paragraph. The class prediction k c(p) associated to the true parcel p is supervized with the cross-entropy loss, and the size prediction with a normalized L1 loss. For all p in P ? , we have:</p><formula xml:id="formula_9">L p class = ? log(k c(p) [k p ])<label>(9)</label></formula><formula xml:id="formula_10">L p size = |h c(p) ?? p | h p + |w c(p) ?? p | w p .<label>(10)</label></formula><p>Shape Prediction. The idea of this step is to combine for a predicted centerpoint c a rough shape patch s c with a fullresolution global saliency map z to obtain a pixel-precise instance mask, see <ref type="figure">Figure 4</ref>. For a centerpoint c of coordinates (i c , j c ), the predicted shape patch s c of size S ? S is resized to the predicted size ?h c ? ? ?w c ? with bilinear interpolation. A convolutional layer maps the outermost feature map d 1 to a saliency map z of size H ? W , which is shared by all predicted parcels. This saliency map is then cropped along the predicted bounding box (i c , j c , ?h c ?, ?w c ?). The resized shape and the cropped saliency are added <ref type="bibr" target="#b10">(11)</ref> to obtain a first local shapel c , which is then further refined with a residual convolutional network CNN <ref type="bibr" target="#b11">(12)</ref>. We denote the resulting predicted shape by l c :</p><formula xml:id="formula_11">l c = resize c (s c ) + crop c (z)<label>(11)</label></formula><formula xml:id="formula_12">l c = sigmoid(l c + CNN(l c )) ,<label>(12)</label></formula><p>with resize c and crop c defined by the coordinates (i c , j c ) and predicted bounding box size (?h c ?, ?w c ?). The shape and saliency predictions are supervized for each parcel p in P ? by computing the pixelwise binary cross-entropy (BCE) between the predicted shape l c(p) and the corresponding true binary instance mask? p cropped along the predicted bounding box</p><formula xml:id="formula_13">(i c(p) , j c(p) , ?h c(p) ?, ?w c(p) ?): L p shape = BCE(l c(p) , crop c(p) (? p )) .<label>(13)</label></formula><p>For inference, we associate a binary mask with a predicted centerpoint c by thresholding l c with the value 0.4.</p><p>Loss Function : These four losses are combined into a single loss with no weight and optimized end-to-end:</p><formula xml:id="formula_14">L = L center + 1 |P ? | p?P ? L p class + L p size + L p shape . (14)</formula><p>Differences with CenterMask. Our approach differs from CenterMask in several key ways: (i) We compute a single saliency map and heatmap instead of K different ones. This represents the absence of parcel occlusion and the similarity of their shapes. (ii) Accounting for the lower resolution of satellite images, centerpoints are computed at full resolution to detect potentially small parcels, thus dispensing us from predicting offsets. (iii) The class prediction is handled centerpoint-wise instead of pixel-wise for efficiency. (iv) Only the selected centerpoints predict shape, class, and size vectors, saving computation and memory.</p><p>(v) We use simple feature concatenation to compute multiscale descriptors instead of deep layer aggregation <ref type="bibr" target="#b52">[53]</ref> or stacked Hourglass-Networks <ref type="bibr" target="#b26">[27]</ref>. (vi) A convolutional network learns to combine the saliency and the mask instead of a simple term-wise product.</p><p>Converting to Panoptic Segmentation. Panoptic segmentation consists of associating to each pixel a semantic label and, for non-background pixels (our only stuff class), an instance label <ref type="bibr" target="#b18">[19]</ref>. Our predicted binary instance masks can have overlaps, which we resolve by associating to each predicted parcel a quality measure equal to the predicted centerness m at its associated centerpoint. Masks with higher quality overtake the pixels of overlapping masks with lesser predicted quality. If a mask loses more than 50% of its pixels through this process, it is removed altogether from the predicted instances. Predicted parcels with a quality under a given threshold are dropped. This threshold can be tuned on a validation set to maximize the parcel detection F-score. All pixels not associated with a parcel mask are labelled as background.</p><p>Implementation Details. Our implementation of U-TAE allows for batch training on sequences of variable length thanks to a simple padding strategy. The complete configuration and training details can be found in the Appendix. A Pytorch implementation is available at https: //github.com/VSainteuf/utae-paps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The PASTIS Dataset</head><p>We present PASTIS (Panoptic Agricultural Satellite TIme Series), the first large-scale, publicly available SITS dataset with both semantic and panoptic annotations. This dataset, as well as more information about its composition, are publicly available at https://github.com/ VSainteuf/pastis-benchmark .</p><p>Description. PASTIS is comprised of 2 433 sequences of multi-spectral images of shape 10 ? 128 ? 128. Each sequence contains between 38 and 61 observations taken between September 2018 and November 2019, for a total of over 2 billion pixels. The time between acquisitions is uneven with a median of 5 days. This lack of regularity is due to the automatic filtering of acquisitions with extensive cloud cover by the satellite data provider THEIA. The 10 channels correspond to the non-atmospheric spectral bands of the Sentinel-2 satellite, after atmospheric correction and re-sampling at a spatial resolution of 10 meters per pixel. The dataset spans over 4000 km 2 , with images taken from four different regions of France with diverse climates and crop distributions, covering almost 1% of the French Metropolitan territory. We estimate that close to 28% of images have at least partial cloud cover.</p><p>Annotation. Each pixel of PASTIS is associated with a semantic label taken from a nomenclature of 18 crop types plus a background class. As is common in remote sensing applications, the dataset is highly unbalanced, with a ratio of over 50 between the most and least common classes.  <ref type="figure">Figure 5</ref>: Qualitative results. We consider an image sequence (a) with panoptic annotations (b). We represent the results of our method in terms of panoptic segmentation (c) and semantic segmentation (d). The parcels' and pixels' color corresponds to the crop type, according to a legend given in the appendix. The predominantly correct class predictions highlight the fact that the difficulty of panoptic segmentation lies in the precise delineation of each individual parcel. We observe cases where the temporal structure of the SITS was successfully leveraged to resolve boundary ambiguities that could not be seen from a single image (cyan circle ). Conversely, some visually fragmented parcels are annotated as a single instance (red circle ).</p><p>Each non-background pixel also has a unique instance label corresponding to its parcel index. In total, 124 422 parcels are individualized, each with their bounding box, pixel-precise mask, and crop type. All annotations are taken from the publicly available French Land Parcel Identification System. The French Payment Agency estimates the accuracy of the crop annotations via in situ control over 98% and the relative error in terms of surfaces under 0.3%. To allow for cross-validation, the dataset is split into 5 folds, chosen with a 1km buffer between images to avoid crossfold contamination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Segmentation</head><p>Our U-TAE has L = 4 resolution levels and a LTAE with G = 16 heads, see appendix for an exact configuration. For the semantic segmentation task, the feature map d 1 with highest resolution is set to have K channels, with K the number of classes. We can then interpret d 1 as pixel-wise predictions to be supervized with the cross-entropy loss. In this setting, we do not use the PaPs module.</p><p>Competing Methods. We reimplemented six of the topperforming SITS encoders proposed in the literature:</p><p>? ConvLSTM <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b39">40]</ref> and ConvGRU <ref type="bibr" target="#b3">[4]</ref>. These approaches are recurrent neural networks in which all linear layers are replaced by spatial convolutions.</p><p>? U-ConvLSTM <ref type="bibr" target="#b36">[37]</ref> and U-BiConvLSTM <ref type="bibr" target="#b22">[23]</ref>. To reproduce these UNet-Based architectures, we replaced the L-TAE in our architecture by either a convLSTM <ref type="bibr" target="#b40">[41]</ref> or a bidirectional convLSTM. Skip connections are temporally averaged. In contrast to the original methods, we replaced the batch normalization in the encoders with group normalization which significantly improved the results across-the-board.</p><p>? 3D-Unet <ref type="bibr" target="#b36">[37]</ref>. A U-Net in which the convolutions of the encoding branch are three-dimensional to handle simultaneously the spatial and temporal dimensions.</p><p>? FPN-ConvLSTM <ref type="bibr" target="#b22">[23]</ref>. This model combines a feature pyramid network <ref type="bibr" target="#b21">[22]</ref> to extract spatial features and a bidirectional ConvLSTM for the temporal dimension.</p><p>Analysis. In <ref type="table" target="#tab_1">Table 1</ref>, we detail the performance obtained with 5-fold cross validation of our approach and the six reimplemented baselines. We report the Overall Accuracy (OA) as the ratio between correct and total predictions, and (mIoU) the class-averaged classification IoU. We observe that the convolutional-recurrent methods ConvGRU and ConvLSTM perform worse. Recurrent networks embedded in an U-Net or a FPN share similar performance, with a much longer inference time for FPN. Our approach significantly outperforms all other methods in terms of precision. In <ref type="figure">Figure 5</ref>, we present a qualitative illustration of the semantic segmentation results.</p><p>Ablation Study. We first study the impact of using spatially interpolated attention masks to collapse the temporal dimension of the spatio-temporal feature maps at different levels of the encoder simultaneously. Simply computing the temporal average of skip connections for levels without temporal encoding as proposed by <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b36">37]</ref>, we observe a drop of 4.8 mIoU points (Skip Mean). This puts our method performance on par with its competing approaches. Adding a 1 ? 1 convolutional layer after the temporal average reduces this drop to 4.2 points (Skip Mean + Conv). Lastly, using interpolated masks but foregoing the channel grouping strategy by averaging the masks group-wise into a single attention mask per level results in a drop of 3.1 points (Mean Attention). This implies that our network is able to use the grouping scheme at different resolutions simultaneously. In conclusion, the main advantage of our proposed attention scheme is that the temporal collapse is controlled at all resolutions, in contrast to recurrent methods. Using batch normalization in the encoder leads to a severe degradation of the performance of 27.1 points (Batch-Norm). We conclude that the temporal diversity of the acquisitions requires special considerations. This was observed for all U-Net models alike. We also train our model on a single acquisition date (with a classic U-Net and no temporal encoding) for two different cloudless dates in August and May (Single Date). We observe a drop of 24.8 and 42.5 points respectively, highlighting the crucial importance of the temporal dimension for crop classification. We also observed that images with at least partial cloud cover received on average 58% less attention than their cloud-free counterparts. This suggests that our model is able to use the attention module to automatically filter out corrupted data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Panoptic Segmentation</head><p>We use the same U-TAE configuration for panoptic segmentation, and select a PaPs module with 190k parameters and a shape patch size of 16 ? 16. In <ref type="table" target="#tab_2">Table 2</ref>, we report the class-averaged Segmentation Quality (SQ), Recognition Quality (RQ), and Panoptic Quality (PQ) <ref type="bibr" target="#b18">[19]</ref>. We observe that while the network is able to correctly detect and clas- sify most parcels, the task remains difficult. In particular, the combination of ambiguous borders and hard-to-classify parcel content makes for a challenging panoptic segmentation problem. We illustrate these difficulties in <ref type="figure">Figure 5</ref>, along with qualitative results. Replacing the temporal encoder by a U-BiConvLSTM as described in Section 4.2 (U-BiConvLSTM+PaPs), we observe a noticeable performance drop of 8.2 PQ, which is consistent with the results of <ref type="table" target="#tab_1">Table 1</ref>. As expected, our model's performance is not too sensitive to changes in the size S of the shape patch. Indeed, the shape patches only determine the rough outline of parcels while the pixel-precise instance masks are derived from the saliency map. Performing shape prediction with a simple element-wise multiplication as in <ref type="bibr" target="#b48">[49]</ref> (Multiplicative Saliency) instead of our residual CNN results in a drop of over ?6.9 SQ. Using a single image (August) leads to a low panoptic quality. Indeed, identifying crop types and parcel borders from a single image at the resolution of Sentinel-2 is particularly difficult.</p><p>Inference on 490 sequences takes 129s: 26s to generate U-TAE embeddings, 1s for the heatmap and saliency, 90s for instance proposals, and 12s to merge them into a panoptic segmentation. Note that the training time is also doubled compared to simple semantic segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced U-TAE, a novel spatio-temporal encoder using a combination of spatial convolution and temporal attention. This model can be easily combined with PaPs, the first panoptic segmentation framework operating on SITS. Lastly, we presented PASTIS, the first large-scale panopticready SITS dataset. Evaluated on this dataset, our approach significantly outperformed all other approaches for semantic segmentation, and set up the first state-of-the-art for panoptic segmentation of satellite image sequences.</p><p>We hope that the combination of our open-access dataset and promising results will encourage both remote sensing and computer vision communities to consider the challenging problem of panoptic SITS segmentation, whose economic and environmental stakes can not be understated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metric Correction</head><p>The values reported in this version of the article for the Panoptic Segmentation experiment differ from the version published in the ICCV 2021 proceedings. Indeed, a bug in the computation of the Recognition Quality (RQ) metric was present in the original implementation resulting in the void target instances not being properly ignored. Instead, all predictions matched to void target instances were counted as false positives, thus artificially reducing the RQ score. Since the panoptic metrics are not involved in the training loss, this bug did not impact the overall training procedure. All models of <ref type="table" target="#tab_2">Table 2</ref> were reevaluated with the corrected implementation. Across methods this resulted in a ? 3 PQ increase, driven by a similar increase in RQ. Refer to github.com/VSainteuf/ utae-paps/issues/11 for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this appendix, we provide additional information on the PASTIS dataset and our exact model configuration. We also provide complementary qualitative experimental results.  Overview. The PASTIS dataset is composed of 2433 square 128 ? 128 patches with 10 spectral bands and at 10m resolution, obtained from the open-access Sentinel-2 platform. <ref type="bibr" target="#b0">1</ref> For each patch, we stack all available acquisitions between September 2018 and November 2019, forming our four dimensional multi-spectral SITS: T ?C ?H ?W . The publicly available French Land Parcel Identification System (FLPIS) allows us to retrieve the extent and content of all parcels within the tiles, as reported by the farmers. Each patch pixel is annotated with a semantic label corresponding to either the parcels' crop type or the background class. The pixels of each unique parcel in the patch receive a corresponding instance label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. PASTIS Dataset</head><p>Dataset Extent. The SITS of PASTIS are taken from 4 different Sentinel-2 tiles in different regions of the French metropolitan territory as depicted in <ref type="figure" target="#fig_4">Figure 6a</ref>. These regions cover a wide variety of climates and culture distributions. Sentinel tiles span 100 ? 100km and have a spatial resolution of 10 meter per pixel. Each pixel is characterized by 13 spectral bands. We select all bands except the atmospheric bands B01, B09, and B10. Each of these tiles is subdivided in square patches of size 1.28 ? 1.28km (128 ? 128 pixels at 10m/pixel), for a total of around 24, 000 patches. We then select 2, 433 patches ( 10% of all available patches, see <ref type="figure" target="#fig_4">Figure 6b</ref>), favoring patches with rare crop types in order to decrease the otherwise extreme class imbalance of the dataset.</p><p>Nomenclature The FLPIS uses a 73 class breakdown for crop types. We select classes with at least 400 parcels and with samples in at least 2 of the 4 Sentinel-2 tiles. This leads us to adopt a 18 classes nomenclature, presented in <ref type="figure" target="#fig_5">Figure 7</ref>. Parcels belonging to classes not in our 18-classes nomenclature are annotated with the void label, see below.</p><p>Patch Boundaries. The FLPIS allows us to retrieve the pixel-precise borders of each parcel. We also compute bounding boxes for each parcel. The parcels' extents are cropped along the extent of their 128 ? 128 patch, and the bounding boxes are modified accordingly. Parcels whose surface is more than 50% outside of the patch are annotated with the void label, see <ref type="figure" target="#fig_4">Figure 6c</ref>.</p><p>Void and Background Labels. Pixels which are not within the extent of any declared parcel are annotated with the background "stuff" label, corresponding to all nonagricultural land uses. For the semantic segmentation task, this label becomes the 20-th class to predict. In the panoptic setting, this label is associated with pixels not within the extent of any predicted parcel. We do not compute the panoptic metrics for the background class, since our focus is on retrieving the parcels' extent rather than an extensive landcover prediction. In other words, the reported panoptic metrics are the "things" metrics, which already penalize parcels predicted for background pixels by counting them as false positives.  The void class is reserved for out-of-scope parcels, either because their crop type is not in our nomenclature or because their overlap with the selected square patch is too small. We remove these parcels from all semantic or panoptic metrics and losses. Predicted parcels which overlap with an IoU superior to 0.5 with a void parcel are not counted as false positive or true positive, but are simply ignored by the metric, as recommended in <ref type="bibr" target="#b18">[19]</ref>.  Cross-Validation. The 2, 433 selected patches are randomly subdivided into 5 splits, allowing us to perform cross-validation. The official 5-fold cross-validation scheme used for benchmarking is given in <ref type="table" target="#tab_4">Table 3</ref>. In order to avoid heterogeneous folds, each fold is constituted of patches taken from all four Sentinel tiles. We also chose folds with comparable class distributions, as measured by their pairwise Kullback-Leiber divergence. We show the resulting class distribution for each fold in <ref type="figure" target="#fig_7">Figure 8</ref>. Finally, we prevent adjacent patches from being in different folds to avoid data contamination. Geo-referencing metadata of the patches and parcels is included in PASTIS, allowing for the constitution of geographically consistent folds to evaluate spatial generalization. However, this is out of the scope of this paper. Temporal Sampling. The temporal sampling of the sequences in PASTIS is irregular: depending on their location, patches are observed a different number of times and at different intervals. This is a result of both the orbit schedule of Sentinel-2 and the policy of Sentinel data providers not to process tile observations identified as covered by clouds for more than 90% of the tile's surface. As this corresponds to the real world setting, we decided to leave the SITS as is, and thus to encourage methods that can favourably address this technical challenge. As a result, the proposed SITS are constituted of 33 to 61 acquisitions. In order to assess how our model handles lower sampling frequencies, we limited the number of available acquisitions at inference time 2 , and observed a drop of performance of ?0.7, ?2.0, ?5.5, and ?14.6 points of mIoU with 32, 24, 16, and 8 available dates, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Label and Color Class Name</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of parcels</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fold Train Val Test</head><formula xml:id="formula_15">I 1-2-3 4 5 II 2-3-4 5 1 III 3-4-5 1 2 IV 4-5-1 2 3 V 5-1-2 3 4</formula><p>Clouds Cover. Even after the automatic filtering of predominantly cloudy acquisitions, some patches are still partially or completely obstructed by cloud cover. We opt to not apply further pre-processing or cloud detection, and produce the raw data in PASTIS. Our reasoning is that an adequate algorithm should be able to learn to deal with such acquisitions. Indeed, robustness to cloud-cover has been experimentally demonstrated for deep learning methods by Ru?wurm and K?rner <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Implementation Details</head><p>In this section, we detail the exact configuration of our method as well as the competing algorithms evaluated.</p><p>Training Details. Across our experiments, we use Adam <ref type="bibr" target="#b17">[18]</ref> optimizer with default parameters and a batch size of 4 sequences. The semantic segmentation experiments use a fixed learning rate of 0.001 for 100 epochs. For the panoptic segmentation experiments, we start with a higher learning rate of 0.01 for 50 epochs, and decrease it to 0.001 for the last 50 epochs.</p><p>U-TAE. In <ref type="table" target="#tab_5">Table 4</ref>, we report the width of the feature maps outputted by each level of the U-TAE's encoder and decoder. In both networks, we use the the same convolutional block shown in <ref type="figure">Figure 9</ref> and constituted of one 3 ? 3 convolution from the input to the output's width, and one residual 3 ? 3 convolution. In the encoding branch, we use Group Normalisation with 4 groups and Batch Normalisation in the decoding branch .</p><p>For the temporal encoding, we chose a L-TAE with 16 heads, and a key-query space of dimension d k = 4. We use Group Normalisation with 16 groups at the input and output of the L-TAE, meaning that that the inputs of each head are layer-normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv 3x3</head><p>Norm ReLu</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D in D out D out</head><p>Conv 3x3 Norm ReLu <ref type="figure">Figure 9</ref>: Structure of the convolutional block used in the spatial encoder-decoder network. This block maps a feature map with D in channels to a feature map with D out channels.</p><p>Recurrent Models. We use the same U-Net architecture for our models and U-BiConvLSTM and U-ConvLSTM, but simply replace the L-TAE by a ConvLSTM or BiConvL-STM respectively. The hidden state's size of the biCon-vLSTM is chosen as 32 in both directions, and 64 for the convLSTM. For the recurrent-convolutional methods Con-vLSTM and ConvGRU not using a U-Net, we set hidden sizes of 160 and 188 respectively.</p><p>3D-Unet. For this network, we use the official PyTorch implementation of Rustowicz et al. <ref type="bibr" target="#b36">[37]</ref>. This network is constituted of five successive 3D-convolution blocks with spatial down-sampling after the 2nd and 4th blocks. Each convolutional block doubles the number of channels of the processed feature maps, and the innermost feature maps have a channel dimension set to 128. Leaky ReLu and 3D Batch Normalisation are used across the convolutional blocks of this architecture. The sequence of feature maps is averaged along the temporal dimension to produce the final embedding of the input image sequence. In their implementation, the authors used a linear layer to collapse the temporal dimension, yet this was not a valid option for PASTIS as the sequences have highly variable lengths and the sequence indices do not correspond to the same acquisition date from one sequence to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FPN-ConvLSTM.</head><p>For this architecture, the input sequence of images is first mapped to feature maps of channel dimension 64 with two consecutive 3 ? 3 convolution layers, followed by Group Normalization and ReLu. A 5-level feature pyramid is then constructed for each date of the sequence by applying to the feature maps 4 different 3 ? 3 convolution of respective dilation rates 1, 2, 4 and 8, and computing the spatial average of the feature map. These 5 maps are concatenated along the channel dimension, and processed by a ConvLSTM with a hidden state size of 88.</p><p>We found it beneficial to use a supplementary convolution before the ConvLSTM to reduce the number of channels of the feature pyramid by a factor 2.</p><p>PaPs module. In the PaPs module, the saliency and heatmap predictions are obtained with two separate convolutional blocks operating on the high resolution feature map d 1 with 32 channels. These blocks are composed of two convolutional layers of width 32 and 1 respectively. We use Batch Normalisation and ReLu after the first convolution, and a sigmoid after the second. The 256-dimensional multi-scale feature vector (128 + 64 + 32 + 32) is mapped to the shape, class and size predictions by three different MLPs described in <ref type="table" target="#tab_6">Table 5</ref>. The inner layers use Batch Normalisation and ReLu activation.</p><p>The residual CNN used for shape refinement is composed of three convolutional layers : 1 ? 16 ? 16 ? 1, with ReLu activation and instance normalisation on the first layer only.</p><p>Handling Sequences of Variable Lengths. All models are trained on batches of sequences of variable length. To  facilitate the handling of batches by the GPU, we append all-zeroes images at the end of shorter sequences to match the length of the longer sequence in the batch. We retain a padding mask to prevent the spatial and temporal encoding of padded values, and to exclude these padded values from temporal averages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Additional Results</head><p>In <ref type="figure" target="#fig_1">Figure 10</ref>, we show the class-wise performance of the three best performing semantic segmentation models, displaying an improvement of U-TAE compared to the other methods across all crop types. We also show on <ref type="figure" target="#fig_1">Figure 11</ref> the confusion matrix of U-TAE. Unsurprisingly, confusions seem to occur between semantically close classes such as different cereal types, or Sunflower and Fruits, Vegetable, Flower.</p><p>In <ref type="figure" target="#fig_1">Figure 12</ref>, we present qualitative results illustrating the predicted panoptic and semantic segmentations compared to the ground truth. In particular, we show some failure cases in which thin or visually fragmented parcels are not recovered correctly.</p><p>In <ref type="figure" target="#fig_0">Figure 13</ref>, we illustrate the results of the semantic segmentation for our method and three other competing approaches: 3D-Unet, U-BiConvLSTM, and convGRU. We show how our multi-scale temporal attention masks allow our predictions to be both pixel-precise and consistent for large parcels.</p><p>Finally, we present in <ref type="figure" target="#fig_1">Figure 14</ref> an example of inference using a single image from the sequence. As expected for mono-temporal segmentation, the parcel classification is poor. Furthermore, we show a case of a border that is essentially invisible on a single image, but that our full model is able to detect using the entire sequence of satellite images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Centerpoint Detection. The ground truth instance masks (a) is used to construct a target heatmap (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 +</head><label>1</label><figDesc>D 2 + D 3 + D 4 ) ? H ? W</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Image from the sequence.(b) Panoptic annotation. (c) Panoptic segmentation. (d) Semantic segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Selected patches. (c) Single patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Data Location. Spatial distribution of the four Sentinel tiles used in PASTIS 6a, and of the selected patches of tile T30UXV 6b. We show an example of patch in 6c, and highlight with red circles examples of parcels that are mostly outside of the patch's extent and thus annotated with the void label. The green circle highlight a parcel partially cut off by the patch borders, but with sufficient overlap to be kept as a valid parcel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Color code of our class nomenclature, and the number of parcel per class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>M e a d o w C o r n G r a p e v i n e S o f t w i n t e r w h e a t F r u i t s , v e g e t a b l e s , f l o w e r s O r c h a r d L e g u m i n o u s f o d d e r W i n t e r b a r l e y W i n t e r d u r u m w h e a t W i n t e r r a p e s e e d S o y b e a n s S u n f l o w e r W i n t e r t r i t i c a l e M i x e d c e r e a l S p r i n g b a r l e</head><label>t</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Class distribution for the five folds (in log-scale).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>W i n t e r r a p e s e eFigure 10 :Figure 11 :</head><label>1011</label><figDesc>t w i n t e r w h e a t S o y b e a n s W i n t e r b a r l e y B a c k g r o u n d S u n f l o w e r M e a d o w W i n t e r d u r u m w h e a t S p r i n g b a r l e y P o t a t o e s G r a p e v i n e W i n t e r t r i t i c a l e O r c h a r d L e g u m i n o u s f o d d e r F r u i t s , v e g e t a b l e s , f l o w e r s M i x e d c e r e a l S Per class IoU of the three best performing semantic segmentation models. Our U-TAE outperforms the other two approaches on every classes, and brings noticeable improvement on hard classes such as Mixed cereal and Sorghum. B a c k g r o u n d M e a d o w S o f t w i n t e r w h e a t C o r n W i n t e r b a r l e y W i n t e r r a p e s e e d S p r i n g b a r l e y S u n f l o w e r G r a p e v i n e B e e t W i n t e r t r i t i c a l e W i n t e r d u r u m w h e a t F r u i t s , v e g e t a b l e s , f l o w e r s P o t a t o e s L e g u m i n o u s f o d d e r S o y b e a n s O r c h a r d M i x e d c e r e a l S Confusion matrix of U-TAE for semantic segmentation on PASTIS. The color of each pixel at line i and column j corresponds to the proportion of samples of the class i that were attributed to the class j.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Model</cell><cell># param ?1000</cell><cell cols="3">OA mIoU IT (s)</cell></row><row><cell>U-TAE (ours)</cell><cell cols="2">1 087 83.2</cell><cell>63.1</cell><cell>25.7</cell></row><row><cell>3D-Unet [37]</cell><cell cols="2">1 554 81.3</cell><cell>58.4</cell><cell>29.5</cell></row><row><cell>U-ConvLSTM [37]</cell><cell cols="2">1 508 82.1</cell><cell>57.8</cell><cell>28.3</cell></row><row><cell>FPN-ConvLSTM [23]</cell><cell cols="2">1 261 81.6</cell><cell cols="2">57.1 103.6</cell></row><row><cell>U-BiConvLSTM [23]</cell><cell cols="2">1 434 81.8</cell><cell>55.9</cell><cell>32.7</cell></row><row><cell>ConvGRU [4]</cell><cell cols="2">1 040 79.8</cell><cell>54.2</cell><cell>49.0</cell></row><row><cell>ConvLSTM [35, 40]</cell><cell cols="2">1 010 77.9</cell><cell>49.1</cell><cell>49.1</cell></row><row><cell>Mean Attention</cell><cell cols="2">1 087 82.8</cell><cell>60.1</cell><cell>24.8</cell></row><row><cell>Skip Mean + Conv</cell><cell cols="2">1 087 82.4</cell><cell>58.9</cell><cell>24.5</cell></row><row><cell>Skip Mean</cell><cell cols="2">1 074 82.0</cell><cell>58.3</cell><cell>24.5</cell></row><row><cell>BatchNorm</cell><cell cols="2">1 087 71.9</cell><cell>36.0</cell><cell>22.3</cell></row><row><cell>Single Date (August)</cell><cell cols="2">1 004 65.6</cell><cell>28.3</cell><cell>1.3</cell></row><row><cell>Single Date (May)</cell><cell cols="2">1 004 58.1</cell><cell>20.6</cell><cell>1.3</cell></row></table><note>Semantic Segmentation. We report for our method and six competing methods the model size in train- able parameters, Overall Accuracy (OA), mean Intersection over Union (mIoU), and Inference Time for one fold of ? 490 sequences (IT). The second part of the table report results from our ablation study.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Panoptic Segmentation Experiment. We report class-averaged panoptic metrics: SQ, RQ, PQ (see Metric Correction paragraph before references).</figDesc><table><row><cell>SQ</cell><cell>RQ</cell><cell>PQ</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Official 5-fold cross validation scheme. Each line gives the repartition of the splits into train, validation and test set for each fold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Width of the feature maps outputted at each level of the encoding and decoding branches of the spatial module.</figDesc><table><row><cell>Encoder</cell><cell>Decoder</cell></row><row><cell>e 1 64</cell><cell>d 1 32</cell></row><row><cell>e 2 64</cell><cell>d 2 32</cell></row><row><cell>e 3 64</cell><cell>d 3 64</cell></row><row><cell>e 4 128</cell><cell>d 4 128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Configuration of the four MLPs of PaPs MLP Layers Final Layer Shape 256 ? 128 ? S 2 -Size 256 ? 128 ? 2 Softplus Class 256 ? 128 ? 64 ? K Softmax</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://scihub.copernicus.eu</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This can be interpreted as the test set having an increased cloud cover.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The satellite images used in PASTIS were gathered from THEIA: "Value-added data processed by the CNES for the Theia data cluster using Copernicus data. The treatments use algorithms developed by Theia's Scientific Expertise Centres." The annotations of PASTIS were taken from the French LPIS produced by IGN, the French mapping agency.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work was partly supported by ASP, the French Payment Agency.  <ref type="figure">Figure 12</ref>: Qualitative Panoptic Segmentation Results. We represent a single image from the sequence using the RGB channels (a), and whose ground truth parcel's limit and types are known (b). We then represent the parcels predicted by our panoptic segmentation module (c), and the pixelwise prediction of our semantic segmentation module (d). See <ref type="figure">Figure 7</ref> for the color to crop type correspondence. We highlight with a green circle a large, fragmented parcel declared as one single field. This leads to predictions with low confidence and a low panoptic quality. Conversely, the cyan circle highlights such fragmented parcel which is correctly predicted as a single instance. This suggests that our network is able to use the temporal dynamics to recover ambiguous borders. We highlight a failure case with the red circle , for which many thin parcels are not properly detected, resulting in a low panoptic quality. We observe that the semantic segmentation model struggles as well for such thin parcels. Finally, we highlight with a blue circle an example in which the panoptic prediction is superior to the semantic segmentation, indicating that detecting parcels' boundaries and extent can be informative for their classification.  <ref type="figure">Figure 13</ref>: Qualitative Semantic Segmentation Results. We represent a single image from the sequence using the RGB channels (a), and whose ground truth parcel's limit and crop type are known (b). We then represent the pixelwise prediction from our approach (c), and for three other competing algorithms (d-f). The different predictions shown on this figure illustrate the importance of the resolution at which temporal encoding is performed. ConvGRU applies a recurrent-convolutional network at the highest resolution, which results in predictions with high spatial variability. As a consequence, the prediction over large parcels are inconsistent (blue circles ). Conversely, U-BiConvLSTM applies temporal encoding to feature maps with a larger receptive field, resulting in more spatially consistent predictions. Yet, this architecture often fails to retrieve small or thin parcels. In contrast, our U-TAE produces spatially consistent predictions on large parcels, while being able to retrieve such small parcels (green circles ). 3D-Unet also uses temporal encoding at different resolution levels, yet fails to recover these small parcels.  First, we observe that many parcels are not detected by the mono-temporal model, indicating an overall low predicted quality. Second, we can see that most detected parcels are misclassified by the mono-temporal model. This is in accordance with the low semantic segmentation score of the mono-temporal model: crop types are hard to distinguish from a single observation. Last, adjacent parcels with no clear borders are predicted as a single parcel, when the multi-temporal model is able to differentiate between the two parcels (cyan circle ). This illustrates how using SITS instead of single images can help resolve ambiguous parcels delineation.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Semantic segmentation of earth observation data using multimodal and multi-scale deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Audebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertrand</forename><forename type="middle">Le</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Lef?vre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dense bag-of-temporal-siftwords for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeline</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laetitia</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Guyet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Advanced Analysis and Learning on Temporal Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Loic Landrieu, and Nesrine Chehata. Crop-rotation structured classification using multi-source Sentinel images and LPIS for crop type mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Giordano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IGARSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Results from the planet labs flock constellation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Boshuizen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Mason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Klupar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shannon</forename><surname>Spanhake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA/USU Conference on Small Satellites</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Spatial-temporal GraphCNN for land cover mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">Michele</forename><surname>Censi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawogan</forename><surname>Jean Eudes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruggero</forename><surname>Gbodjo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Gaetano Pensa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Interdonato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaetano</surname></persName>
		</author>
		<imprint>
			<publisher>IEEE Access</publisher>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatially precise contextual features based on superpixel neighborhoods for land cover mapping with high resolution satellite image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawa</forename><surname>Derksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Inglada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IGARSS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sentinel-2: Esa&apos;s optical high-resolution mission for gmes operational services. Remote sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Drusch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umberto</forename><forename type="middle">Del</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Carlier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Colin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veronica</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Gascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bianca</forename><surname>Hoersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudia</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Laberinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Martimort</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A machine learning approach for agricultural parcel delineation through agglomerative segmentation. International journal of remote sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Garcia-Pedrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Consuelo</forename><surname>Gonzalo-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lillo-Saavedra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lightweight temporal self-attention for classifying satellite images time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Sainte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fare</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Advanced Analytics and Learning on Temporal Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Land cover classification via multitemporal spatial data by deep recurrent neural networks. Geoscience and Remote Sensing Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dino</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Gaetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Dupaquier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Maurel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Assessment of an operational system for crop type map production using high temporal and spatial resolution satellite optical imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Inglada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcela</forename><surname>Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tardy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Hagolle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Valero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Dedieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guadalupe</forename><surname>Sepulcre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophie</forename><surname>Bontemps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Defourny</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for crop classification with multi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunping</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep learning in agriculture: A survey. Computers and electronics in agriculture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kamilaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Francesc X Prenafeta-Bold?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joon-Young Lee, and In So Kweon. Video panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep learning classification of land cover and crop types using remote sensing data. Geoscience and Remote Sensing Letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataliia</forename><surname>Kussul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykola</forename><surname>Lavreniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergii</forename><surname>Skakun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrii</forename><surname>Shelestov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Raul Queiroz Feitosa, Ieda Del&apos;Arco Sanches, and Patrick Nigri Happ. Fully convolutional recurrent networks for multidate crop recognition from multitemporal image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge Andres Chamorro</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">Elena</forename><surname>Cu?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">La</forename><surname>Rosa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Delineation of agricultural field boundaries from Sentinel-2 images using a novel super-resolution contour detector based on fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Khairiya Mudrik Masoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Persello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Valentyn A Tolpekin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Remote sensing</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Metzger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Ozgur Turkoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Dirk</forename><surname>Aronco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.02542</idno>
		<title level="m">Crop classification under varying cloud cover with neural ordinary differential equations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficientps: Efficient panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Valada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehmet Ozgur Turkoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D&amp;apos;</forename><surname>Stefano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregor</forename><surname>Aronco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Perich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Liebisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konrad</forename><surname>Streit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Dirk</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wegner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08820</idno>
		<title level="m">Crop mapping from image time series: deep learning with multi-scale label hierarchies</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A deep multi-task learning framework coupling semantic segmentation and fully convolutional LSTM networks for urban change detection. Transactions on Geoscience and Remote Sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Papadomanolaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Vakalopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Karantzalos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Temporal convolutional neural network for the classification of satellite image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlotte</forename><surname>Pelletier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Spatio-temporal reasoning for the classification of satellite image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Petitjean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Kurtz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Passat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Gan?arski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Cl?ment Mallet, and Corina Iovan. Individual tree segmentation over large areas using airborne lidar point cloud and very high resolution optical imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Ferraz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning for instance segmentation of agricultural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Rieke</surname></persName>
		</author>
		<ptr target="https://github.com/chrieke/InstanceSegmentation_Sentinel2" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional LSTMs for cloud-robust segmentation of remote sensing imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Ru?wurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>K?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-attention for raw optical satellite time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Ru?wurm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>K?rner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Semantic segmentation of crop type in africa: A novel dataset and analysis of deep learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Rustowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marshall</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lobell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Time-space tradeoff in deep learning models for crop classification on satellite multispectral image time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Sainte Fare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nesrine</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chehata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IGARSS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Satellite image time series classification with pixel-set encoders and temporal self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivien</forename><surname>Sainte Fare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Garnot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nesrine</forename><surname>Giordano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chehata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04214</idno>
		<title level="m">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Land cover maps production with high resolution satellite image time series and convolutional neural networks: Adaptations and limits for operational systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Stoian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Poulain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Inglada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Poughon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawa</forename><surname>Derksen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Efficient temporal kernels between feature sets for time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Tavenard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laetitia</forename><surname>Chapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adeline</forename><surname>Bailly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heider</forename><surname>Sanchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Bustos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML-KDD</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to segment moving objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need. NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">How much does multi-temporal Sentinel-2 data improve crop type classification? International journal of applied earth observation and geoinformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Vuolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Neuwirth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Immitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Atzberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Tim</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Unet-id, an instance segmentation model for building extraction from satellite images-case study in the joan?polis city</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fabien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliya</forename><surname>Dalagnol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tarabalka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">F</forename><surname>Tassiana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rog?rio</forename><surname>Segantine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayumi</forename><surname>Thom?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hirye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>brazil. Remote Sensing</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Deep learning on edge: extracting field boundaries from satellite images with a convolutional neural network. Remote Sensing of Environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Waldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Foivos I Diakogiannis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Centermask: single shot instance segmentation with point representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lirong</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Time series shapelets: a new primitive for data mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lexiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Evan Shelhamer, and Trevor Darrell. Deep layer aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-supervised pre-training of transformers for satellite image time series classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Tree canopy differentiation using instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiebiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>De La Rosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangquan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ASABE Annual International Meeting</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
