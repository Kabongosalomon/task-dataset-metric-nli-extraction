<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenghao</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Palakorn</forename><surname>Achananuparp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Fellow, IEEE</roleName><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
						</author>
						<title level="a" type="main">Cross-Modal Food Retrieval: Learning a Joint Embedding of Food Images and Recipes with Semantic Consistency and Attention Mechanism</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Cross-Modal Retrieval</term>
					<term>Vision- and-Language</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Food retrieval is an important task to perform analysis of food-related information, where we are interested in retrieving relevant information about the queried food item such as ingredients, cooking instructions, etc. In this paper, we investigate cross-modal retrieval between food images and cooking recipes. The goal is to learn an embedding of images and recipes in a common feature space, such that the corresponding image-recipe embeddings lie close to one another. Two major challenges in addressing this problem are 1) large intra-variance and small inter-variance across cross-modal food data; and 2) difficulties in obtaining discriminative recipe representations. To address these two problems, we propose Semantic-Consistent and Attention-based Networks (SCAN), which regularize the embeddings of the two modalities through aligning output semantic probabilities. Besides, we exploit a self-attention mechanism to improve the embedding of recipes. We evaluate the performance of the proposed method on the large-scale Recipe1M dataset, and show that we can outperform several state-of-the-art crossmodal retrieval strategies for food images and cooking recipes by a significant margin.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Food plays an essential role in human daily life. To discover the relationship between cross-modal food data, i.e. food images and recipes, we aim to address the problem of crossmodal food retrieval based on a large amount of heterogeneous food dataset <ref type="bibr" target="#b0">[1]</ref>. Specifically, we take the cooking recipes (ingredients &amp; cooking instructions) as the query to retrieve the food images, and vice versa. Recently, there have been many works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref> on cross-modal food retrieval, where they mainly learn the joint embeddings for recipes and images with vanilla pair-wise loss to achieve the cross-modal alignment. Despite those efforts, cross-modal food retrieval remains challenging mainly due to the following two reasons: 1) the large intra-class variance across food data pairs, and 2) the difficulties of obtaining discriminative recipe representation.</p><p>In cross-modal food data, given a recipe, we may have many food images that are cooked by different chefs. Besides, the images from different recipes can look very similar because they have similar ingredients. Hence, the data representation from the same food can be different, but different food may have similar data representations. This leads to large intraclass variance but small inter-class variance in food data. Existing studies <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> only address the small interclass variance problem by utilizing triplet loss to measure the similarities between cross-modal data. Specifically, the objective of triplet loss is to make inter-class feature distance larger than intra-class feature distance by a predefined margin <ref type="bibr" target="#b5">[6]</ref>. Therefore, cross-modal instances from the same class may form a loose cluster with a large average intra-class distance. As a consequence, it eventually results in less-than-optimal ranking, i.e., irrelevant images are closer to the queried recipe than relevant images. See <ref type="figure" target="#fig_2">Figure 1</ref> for an example.</p><p>Besides, many recipes share common ingredients in different food. For instance fruit salad has ingredients of apple, orange and sugar etc., where apple and orange are the main ingredients, while sugar is one of the ingredients in many other foods. If the embeddings of ingredients are treated equally during training, the features learned by the model may not be discriminative enough. In addition, the cooking instructions crawled from cooking websites tend to be noisy, some instructions turn out irrelevant to cooking e.g. 'Enjoy!', which convey no information for cooking instruction features but degrade the performance of cross-modal retrieval task. In order to find the attended ingredients, Chen et al. <ref type="bibr" target="#b4">[5]</ref> apply a two-layer deep attention mechanism, which learns joint features by locating the visual food regions that correspond to ingredients. However, this method relies on high-quality food images and essentially increases the computational complexity.</p><p>To resolve those issues, we propose a novel unified framework of Semantic-Consistent and Attention-Based Network (SCAN) to improve the cross-modal food retrieval performance. The pipeline of the framework is shown in <ref type="figure">Figure 2</ref>. To reduce the intra-class variance, we introduce a semantic consistency loss, which imposes Kullback-Leibler (KL) Divergence to minimize the difference between the output semantic probabilities of paired image and recipe, such that the image and recipe representations would follow similar distributions. In order to obtain discriminative recipe representations, we combine the self-attention mechanism <ref type="bibr" target="#b6">[7]</ref> with LSTM to find the key ingredients and cooking instructions for each recipe. Without requiring food images or adding extra layers, we can learn better discriminative recipe embeddings, compared to 1. Add greens, apple slices, pecan halves, dried cherries, and blue cheese chunks into a large salad bowl. 2. In a small jar, mix Dijon, maple syrup, vinegar, olive oil, and salt and pepper. 3. Put the lid on the jar and shake well to mix. 4. Pour a little salad dressing over the top of the salad and toss to combine. 5. Taste salad and add more salad dressing to taste.</p><p>Recipe Query <ref type="bibr">(</ref>  <ref type="figure" target="#fig_2">Fig. 1</ref>. Recipe-to-image retrieval ranked results: Take an apple salad recipe as the query, which contains ingredients and cooking instructions, we show the retrieval results based on Euclidean distance (as the numbers indicated in the figure) for 3 different food images with large intra-class variance and small inter-class variance, i.e. images of apple salad have different looks, while chicken salad image is more similar to apple salad. We rank the retrieved results of using (i) vanilla triplet loss; and (ii) our proposed SCAN model. It shows vanilla triplet loss outputs a wrong ranking order, while SCAN can provide more precise ranking results. that trained with plain LSTM.</p><p>Our work makes two major contributions as follows:</p><p>? We introduce a semantic consistency loss to cross-modal food retrieval task. The result shows that it can align cross-modal matching pairs and reduce the intra-class variance of food data representations. ? We integrate the self-attention mechanism with LSTM, and learn discriminative recipe features without requiring the food images. It is useful to discriminate samples of similar recipes. We perform extensive experimental analysis on Recipe1M, which is the largest cross-modal food dataset and available in the public. We find that our proposed cross-modal food retrieval approach SCAN outperforms state-of-the-art methods. Finally, we show some visualizations of the retrieved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK A. Cross-modal Retrieval</head><p>Our work is closely related to the general cross-modal retrieval task, which aims to retrieve the corresponding instance of different modalities based on the given query. The general idea of cross-modal retrieval is to correlate heterogeneous data, mapping the data from different modalities to the common space. As an early work for multi-media, Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b7">[8]</ref> utilizes global alignment to allow the data mapping of different modalities with similar semantics to be close in the common space, by maximizing the correlation between cross-modal similar pairs. However, CCAbased approaches model the cross-modal data only by linear projections, when it comes to large-scale complex real-world data, it is difficult for CCA to fully model the correlations.</p><p>Many recent works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> utilize deep architectures for cross-modal retrieval, which have the advantage of capturing complex non-linear cross-modal correlations. Specifically, to improve the efficiency in retrieval process, hashing has been introduced to multimedia retrieval <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>. With the advent of generative adversarial networks (GANs) <ref type="bibr" target="#b11">[12]</ref>, which are helpful to model the data distributions, some adversarial training methods <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref> are frequently used for modality fusion. Peng et al. <ref type="bibr" target="#b13">[14]</ref> utilize two kinds of discriminative models to simultaneously conduct intra-modality and inter-modality discrimination, and model the joint distribution over the data of different modalities. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref> incorporate generative processes into the cross-modal feature embedding. Specifically, Gu et al. <ref type="bibr" target="#b12">[13]</ref> try to generate the images from text features and generate corresponding captions from the image features. In this way, they learn not only the global abstract features but also the local grounded features. To address the challenge that unpaired data may exist in the cross-modal dataset, Jing et al. <ref type="bibr" target="#b15">[16]</ref> propose to learn modalityinvariant representations with autoencoders, which are further dual-aligned at the distribution level and the semantic level. To learn fine-grained phrase correspondence, Liu et al. <ref type="bibr" target="#b16">[17]</ref> construct textual and visual graph, they learn the cross-modal correspondence by node-level and structure-level matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Food Computing</head><p>Food computing <ref type="bibr" target="#b17">[18]</ref> utilizes computational methods to analyze the food data including the food images and recipes. Many food-related computational tasks have been widely researched, like food recognition <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, retrieval <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b0">[1]</ref>, recommendation <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> and recipe generation <ref type="bibr" target="#b25">[26]</ref>, etc. In this paper, we mainly investigate the cross-modal food retrieval problem based on Recipe1M dataset <ref type="bibr" target="#b0">[1]</ref>.</p><p>Recipe1M <ref type="bibr" target="#b0">[1]</ref> is currently the largest cross-modal food dataset, which was scraped from over two dozen popular cooking websites, and contains rich cooking instructions, ingredient information, and the corresponding cooked food Our proposed framework for cross-modal retrieval task. We have two branches to encode food images and recipes respectively. One embedding function E I is designed to extract food image representations I, where a CNN is used. The other embedding function E R is composed of two LSTMs with self-attention mechanism, designed for obtaining discriminative recipe representations R. I and R are fed into retrieval loss (triplet loss) L Ret to do cross-modal retrieval learning. We add another FC transformation on I and R with the output dimensionality as the number of food categories, to obtain the semantic probabilities p img and p rec , where we utilize semantic consistency loss L SC to correlate food image and recipe data.</p><p>images. Besides, half amount of the data in Recipe1M has semantic food category labels, which are extracted from the food titles in the cooking websites. Recipe1M is proposed mainly for the cross-modal food retrieval task. <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b22">[23]</ref> are early works in cross-modal food retrieval. In <ref type="bibr" target="#b26">[27]</ref>, a multi-task deep learning architecture is proposed for simultaneous ingredient and food recognition. The learned visual features and semantic attributes of ingredients are then used for recipe retrieval, but they only test their model in a small-scale dataset, and cannot demonstrate the efficacy in real-world large-scale data. Min et al. <ref type="bibr" target="#b22">[23]</ref> utilize a multimodal Deep Boltzmann Machine for recipe-image retrieval. <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b3">[4]</ref> integrate attention mechanism into cross-modal retrieval, Chen et al. <ref type="bibr" target="#b4">[5]</ref> introduce a stacked attention network (SAN) to learn joint space from images and recipes for crossmodal retrieval. However, SAN only considers ingredient lists and ignores the rich information provided by cooking instructions, so they have poor performance in Recipe1M dataset. Consequently, Chen et al. improve the previous work SAN in <ref type="bibr" target="#b3">[4]</ref>, where they make full use of the ingredient, cooking instruction, and title (food category) information of Recipe1M, and concatenate the three types of features above to construct the recipe embeddings. Compared with the selfattention mechanism we adopt in our model, both <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b3">[4]</ref> add extra learnable parameters to compute the attended parts, which increase the computational complexity. In order to have better regularization on the shared representation space learning, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref> both corporate the semantic labels with the joint training. Salvador et al. <ref type="bibr" target="#b0">[1]</ref> develop a hybrid neural network architecture with a cosine embedding loss for retrieval learning and a cross-entropy loss for classification, such that a joint common space for image and recipe embeddings can be learned for cross-modal retrieval. <ref type="bibr" target="#b1">[2]</ref> is an extended version of <ref type="bibr" target="#b0">[1]</ref>, providing a double-triplet strategy to express both the retrieval loss and the classification loss. Different from existing cross-modal food retrieval work, we propose a novel semantic consistency loss with a selfattention mechanism, where we impose regularization on the output semantic probabilities of paired food image and recipe embeddings, to correlate the learned food image and recipe representations. Self-attention helps learn discriminative recipe embeddings without depending on the food images or adding some extra learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODS</head><p>In this section, we introduce our proposed model, where we utilize food image-recipe paired data to learn cross-modal embeddings as shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>We formulate the proposed cross-modal food retrieval with three networks, i.e. one convolutional neural network (CNN) for food image embeddings, and two LSTMs to encode ingredients and cooking instructions respectively. The food image representations I can be obtained from the output of CNN directly, while the recipe representations R come from the concatenation of the ingredient features f ingredient and instruction features f instruction . Specifically, for obtaining discriminative ingredient and instruction embeddings, we integrate the selfattention mechanism <ref type="bibr" target="#b6">[7]</ref> into the LSTM embedding. Triplet loss is used as the main loss function L Ret to map crossmodal data to the common space, and semantic consistency loss L SC is utilized to align cross-modal matching pairs for retrieval task, reducing the intra-class variance of food data. The overall objective function of the proposed SCAN is given as:</p><formula xml:id="formula_0">L = L Ret + ?L SC ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Recipe Embedding</head><p>We use two LSTMs to get ingredient and instruction representations f ingredient , f instruction , concatenate them and pass through a fully-connected layer to give a 1024-dimensional feature vector, as the recipe representation R.</p><p>1) Ingredient Representation Learning: Instead of wordlevel word2vec representations, ingredient-level word2vec representations are used in ingredient embedding. To be specific, ground ginger is regarded as a single word vector, instead of two separate word vectors of ground and ginger.</p><p>We integrate the self-attention mechanism with LSTM output to construct recipe embeddings. The purpose of applying the self-attention model lies in assigning higher weights to main ingredients for different food items, making the attended ingredients contribute more to the ingredient embedding, while reducing the effect of common ingredients. The self-attention structure is shown in <ref type="figure" target="#fig_1">Figure 3</ref>.</p><p>Given an ingredient input {z 1 , z 2 , ..., z n }, we first encode it with pretrained embeddings from word2vec algorithm to obtain the ingredient representation Z t . Then {Z 1 , Z 2 , ..., Z n } will be fed into the one-layer bidirectional LSTM as a sequence step by step. For each step t, the recurrent network takes in the ingredient vector Z t and the output of previous step h t?1 as the input, and produces the current step output h t by a non-linear transformation, as follow:</p><formula xml:id="formula_1">h t = tanh(WZ t + Uh t?1 + b),<label>(2)</label></formula><p>The bidirectional LSTM consists of a forward hidden state ? ? h t which processes ingredients from Z 1 to Z n and a backward hidden state ? ? h t which processes ingredients from Z n to Z 1 . We obtain the representation h t of each ingredient z t by concatenating</p><formula xml:id="formula_2">? ? h t and ? ? h t , i.e. h t = [ ? ? h t , ? ? h t ], so that the representation of the ingredient list of each food item is H = {h 1 , h 2 , ..., h n }.</formula><p>We further measure the importance of ingredients in the recipe with the self-attention mechanism which has been studied in Transformer <ref type="bibr" target="#b6">[7]</ref>, where the input comes from queries Q and keys K of dimension d k , and values V of dimension d v (the definition of Q, K and V can be referred in <ref type="bibr" target="#b6">[7]</ref>), we compute the attention output as:</p><formula xml:id="formula_3">Attention(Q, K, V ) = softmax( QK T ? d k )V,<label>(3)</label></formula><p>Different from the earlier attention-based methods <ref type="bibr" target="#b3">[4]</ref>, we use self-attention mechanism where all of the keys, values and queries come from the same ingredient representation H. Therefore, the computational complexity is reduced since it is not necessary to add extra layers to train attention weights. The ingredient attention output H attn can be formulated as:</p><formula xml:id="formula_4">H attn = Attention(H, H, H) = softmax( HH T ? d h )H,<label>(4)</label></formula><p>where d h is the dimension of H. In order to enable unimpeded information flow for recipe embedding, skip connections are used in the attention model. Layer normalization <ref type="bibr" target="#b27">[28]</ref> is also used since it is effective in stabilizing the hidden state dynamics in recurrent network. The final ingredient representation f ingredient is generated from summation of H and H attn , which can be defined as:</p><formula xml:id="formula_5">f ingredient = LayerNorm(H attn + H),<label>(5)</label></formula><p>2) Instruction Representation Learning: Considering that cooking instructions are composed of a sequence of variableform and lengthy sentences, we compute the instruction embedding with a two-stage LSTM model. For the first stage, we apply the same approach as <ref type="bibr" target="#b0">[1]</ref> to obtain the representations of each instruction sentence, in which it uses skip-instructions <ref type="bibr" target="#b0">[1]</ref> with the technique of skip-thoughts <ref type="bibr" target="#b28">[29]</ref>.</p><p>The next stage is similar to the ingredient representation learning. We feed the pre-computed fixed-length instruction sentence representation into the LSTM model to generate the hidden representation of each cooking instruction sentence. Based on that, we can obtain the self-attention representation. The final instruction feature f instruction is generated from the layer normalization function on the previous two representations, as we formulate in the last section. By doing so, we are able to find the key sentences in cooking instruction. Some visualizations on attended ingredients and cooking instructions can be found in Section IV-G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Embedding</head><p>We use ResNet-50 <ref type="bibr" target="#b29">[30]</ref> pretrained on ImageNet to encode food images. The dimension of the final food image features I is 1024, which is identical to that of recipe features R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Cross-modal Food Retrieval Learning</head><p>Triplet loss is utilized to do retrieval learning, the objective function is:</p><formula xml:id="formula_6">L Ret = I [d(I a , R p ) ? d(I a , R n ) + ?] + + R [d(R a , I p ) ? d(R a , I n ) + ?] + ,<label>(6)</label></formula><p>where d(?) is the Euclidean distance, subscripts a, p and n refer to anchor, positive and negative samples respectively and ? is the margin. The summation symbol means that we construct triplets and do the training for all samples in the mini-batch. To improve the effectiveness of training, we adopt the BatchHard idea proposed in <ref type="bibr" target="#b30">[31]</ref>. Specifically in a minibatch, each sample can be used as an anchor, then for each anchor, we select the closest negative sample and the farthest positive sample to construct the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Semantic Consistency</head><p>Given the pairs of food image and recipe representations I, R, we first transform I and R into I and R for classification with an extra FC layer. The dimension of the output is same as the number of categories N . The probabilities of food category i can be computed by a softmax activation as:</p><formula xml:id="formula_7">p img i = exp(I i ) N i=1 exp(I i ) ,<label>(7)</label></formula><formula xml:id="formula_8">p rec i = exp(R i ) N i=1 exp(R i ) ,<label>(8)</label></formula><p>where N represents the total number of food categories. Given p img i and p rec i , where i ? {1, 2, ..., N }, the predicted label l img and l rec for each food item can be obtained. We formulate the classification (cross-entropy) loss as L cls (p img , p rec , c img , c rec ), where c img , c rec are the groundtruth class label for food image and recipe respectively.</p><p>In the prior work <ref type="bibr" target="#b0">[1]</ref>, L cls (p img , p rec , c img , c rec ) consists of L img cls and L rec cls , which are treated as two independent classifiers, focusing on the regularization on the embeddings from food images and recipes separately. However, food image and recipe embeddings come from heterogeneous modalities, the output probabilities of each category can be significantly different, i.e. for each food item, the distributions of p img i and p rec i remain big variance. As a result, the distance of intra-class features remains large. To improve image-recipe matching and make the probabilities predicted by different classifiers consistent, we minimize Kullback-Leibler (KL) Divergence between p img i and p rec i of paired cross-modal data for each food item, which can be formulated as:</p><formula xml:id="formula_9">L KL (p img p rec ) = N i=1 p img i log p img i p rec i ,<label>(9)</label></formula><formula xml:id="formula_10">L KL (p rec p img ) = N i=1 p rec i log p rec i p img i ,<label>(10)</label></formula><p>By aligning the output probabilities of cross-modal data representations, we minimize the intra-class variance with back-propagation. The overall semantic consistency loss L SC is defined as: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS A. Dataset</head><p>We conduct extensive experiments to evaluate the performance of our proposed methods in Recipe1M dataset <ref type="bibr" target="#b0">[1]</ref>, the largest cooking dataset with recipe and food image pairs available to the public. Recipe1M was scraped from over 24 popular cooking websites and it not only contains the imagerecipe paired labels but also more than half the amount of the food data with semantic category labels extracted from food titles on the websites. The category labels provide semantic information for the cross-modal retrieval task, making it fit in our proposed method well. The paired labels and category labels construct the hierarchical relationships among the food. One food category (e.g. fruit salads) may contain hundreds of different food pairs, since there are many recipes of different fruit salads.</p><p>We perform the cross-modal food retrieval task based on food data pairs, i.e. when we take the recipes as the query to do the retrieval, the ground truth will be the food images in food data pairs, and vice versa. We use the original Recipe1M data   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Evaluation Protocol</head><p>We evaluate our proposed model with the same metrics used in prior works <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b2">[3]</ref>. To be specific, median retrieval rank (MedR) and recall at top K (R@K) are used. MedR measures the median rank position among where true positives are returned. Therefore, higher performance comes with a lower MedR score. Given a food image, R@K calculates the fraction of times that the correct recipe is found within the top-K retrieved candidates, and vice versa. Different from MedR, the performance is directly proportional to the score of R@K. In the test phase, we first sample 10 different subsets of 1,000 pairs (1k setup), and 10 different subsets of 10,000 (10k setup) pairs. It is the same setting as in <ref type="bibr" target="#b0">[1]</ref>. We then consider each item from food image modality in subset as a query, and rank samples from recipe modality according to L2 distance between the embedding of image and that of the recipe, which is served as image-to-recipe retrieval, and vice versa for recipe-to-image retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>We set the trade-off parameter ? in Eq. (1) based on empirical observations, where we tried a range of values and evaluated the performance on the validation set, as shown in <ref type="table" target="#tab_1">Table II</ref>. We set the ? as 0.05. The model was trained using Adam optimizer <ref type="bibr" target="#b34">[35]</ref> with the batch size of 64 in all our experiments. The initial learning rate is set as 0.0001, and the learning rate decreases 0.1 in the 30th epoch. We take a pretrained ResNet-50 and the bidirectional LSTMs as E I and E R respectively, whose output dimension is 1024. Note that we update the two sub-networks, i.e. image encoder E I and recipe encoder E R , alternatively. It only takes 40 epochs to get the best performance with our proposed methods, while <ref type="bibr" target="#b0">[1]</ref> requires 220 epochs to converge. Our training records can be viewed in <ref type="figure" target="#fig_3">Figure 4</ref>. We do our experiments on a single Tesla V100 GPU, which costs about 16 hours to finish the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Baselines</head><p>We compare the performance of our proposed methods with several state-of-the-art baselines, and the results are shown in <ref type="table" target="#tab_1">Table I</ref>.</p><p>CCA <ref type="bibr" target="#b7">[8]</ref>: Canonical Correlation Analysis (CCA) is one of the most widely-used classic models for learning a common embedding from different feature spaces. CCA learns two linear projections for mapping text and image features to a common space that maximizes their feature correlation.</p><p>SAN <ref type="bibr" target="#b4">[5]</ref>: Stacked Attention Network (SAN) considers ingredients only (and ignores recipe instructions), and learns the feature space between ingredient and image features via a two-layer deep attention mechanism.</p><p>JE <ref type="bibr" target="#b0">[1]</ref>: They use pairwise cosine embedding loss to find a joint embedding (JE) between the different modalities. To impose regularization, they add classifiers to the cross-modal embeddings which predict the category of a given food item.</p><p>AM <ref type="bibr" target="#b3">[4]</ref>: Attention mechanism (AM) over the recipe is adopted in <ref type="bibr" target="#b3">[4]</ref>, applied at different parts of a recipe (title, ingredients and instructions). They use an extra transformation matrix and context vector in the attention model.</p><p>AdaMine <ref type="bibr" target="#b1">[2]</ref>: A double triplet loss is used, where triplet loss is applied to both the joint embedding learning and the auxiliary classification task of categorizing the embedding into an appropriate category. They also integrate the adaptive learning schema (AdaMine) into the training phase, which performs adaptive mining for significant triplets. R 2 GAN [32]: After embedding the image and recipe information, R 2 GAN adopt GAN learning and semantic classification for cross-modal retrieval. They also introduce twolevel ranking loss at embedding and image spaces.</p><p>MCEN <ref type="bibr" target="#b32">[33]</ref>: Fu et al. adopt the generative idea, where they convert the embedding computation into a generative process. They first sample the latent variables from Gaussian distributions, based on which they use several layers to generate new feature embeddings. This method inevitably increase the computational cost. ACME <ref type="bibr" target="#b2">[3]</ref>: Adversarial training methods are utilized in ACME for modality alignment, to make the feature distributions from different modalities to be similar. To further preserve the semantic information in the cross-modal food data representation, Wang et al. introduce a translation consistency component.</p><p>tri-pro <ref type="bibr" target="#b33">[34]</ref>: Zan et al. propose to use the improved triplet loss, where they enforce the embeddings of all images for a given recipe to be close to this recipe, while to be distant from other recipes. They also attempt to discover some hard negatives during training.</p><p>It has been validated that using attention can improve feature representations. Both SAN <ref type="bibr" target="#b4">[5]</ref> and AM <ref type="bibr" target="#b3">[4]</ref> adopt the attention mechanism to improve the recipe embeddings, while these methods add extra learnable layers to compute the attention weights and need to rely on the high-quality food images, which may affect the model performance. In contrast, without adding extra layers or requiring food images, our adopted selfattention method can find the attended ingredients and cooking instructions effectively. We show some attention results by our model in <ref type="figure">Figure 6</ref>. To improve the modality alignment, R 2 GAN [32] and ACME <ref type="bibr" target="#b2">[3]</ref> use adversarial learning. Specifically, Wang et al. <ref type="bibr" target="#b2">[3]</ref> preserve the semantic consistency by transforming the feature representations to another modality. While our proposed method can achieve semantic alignment with the KL divergence, which is light-weight and effective. In summary, it can be observed our proposed model SCAN is useful on cross-modal food retrieval and outperforms all of earlier methods, as is shown in <ref type="table" target="#tab_1">Table I</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Studies</head><p>Extensive ablation studies are conducted to evaluate the effectiveness of each component of our proposed model. <ref type="table" target="#tab_1">Table III</ref> illustrates the contributions of self-attention model (SA), semantic consistency loss (SC) and their combination on improving the image to recipe retrieval performance. We test these different components based on different retrieval learning loss functions L Ret , i.e. triplet loss (TL) and cosine embedding loss (CL).</p><p>TL serves as a baseline for SA, which adopts the BatchHard [31] training strategy. We then add SA and SC incrementally, butter, lemon juice, frozen whipped topping, brown sugar, strawberries, all -purpose flour, walnuts, white sugar </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recipe Query</head><p>Remove strawberries</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remove walnuts</head><p>Retrieved Images <ref type="figure">5</ref>. Recipe-to-image retrieval results in Recipe1M dataset. We give an original recipe query from dessert, and then remove different ingredients of strawberries and walnuts separately to construct new recipe queries. We show the retrieved results by SCAN and different components of our proposed model. and significant improvements can be found in both of the two components. To be specific, integrating SA into TL helps improve the performance of the image-to-recipe retrieval more than 4% in R@1, illustrating the effectiveness of the selfattention mechanism to learn discriminative recipe representations. The model trained with triplet loss and classification loss (cls) used in <ref type="bibr" target="#b0">[1]</ref> is another baseline for SC. It shows that our proposed semantic consistency loss improves the performance in R@1 and R@10 by more than 2%, which suggests that reducing intra-class variance can be helpful in the cross-modal retrieval task. When we add SA and SC to CL, similar improvements can also been observed.</p><formula xml:id="formula_12">SCAN SCAN w/o SA SCAN w/o SC SCAN w/o SA &amp; SC Fig.</formula><p>We show the training records in <ref type="figure" target="#fig_3">Figure 4</ref>, in the left figure, we can see that for the first 20 epochs, the performance gap between TL and TL+SA gets larger, while the performance of TL+cls and TL+SC keeps being similar, which is shown in the middle figure. But for the last 20 training epochs, the performance of TL+SC improves significantly, which indicates that for those hard samples whose intra-variance can hardly be reduced by TL+cls, TL+SC contributes further to the alignment of paired cross-modal data. The effect of trade-off parameter ? is shown in <ref type="table" target="#tab_1">Table II</ref>. We illustrate the performance of models trained with four different ?, and we can see that setting ? as 0.05 can obtain the best performance.</p><p>In conclusion, we observe that each of the proposed components improves the cross-modal retrieval model, and the combination of those components yields better performance overall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Recipe-to-Image Retrieval Results</head><p>We show three recipe-to-image retrieval results in <ref type="figure">Figure 5</ref>. In the top row, we select a recipe query dessert from Recipe1M dataset, which has the ground truth for retrieved food images. Images with the green box are the correctly retrieved ones, which come from the retrieved results by SCAN and TL+SA. But we can see that the model trained only with semantic consistency loss (TL+SC) has a reasonable retrieved result as well, which is relevant to the recipe query.</p><p>In the middle and bottom row, we remove some ingredients and the corresponding cooking instruction sentences in the recipe, and then construct the new recipe embeddings for the recipe-to-image retrieval. In the bottom row where we remove the walnuts, we can see that all of the retrieved images have no walnuts. However, only the image retrieved by our proposed SCAN reflects the richest recipe information. For instance, the image from SCAN remains visible ingredients of frozen whipped topping, while images from TL+SC and TL+SA have no frozen whipped toppings.</p><p>The recipe-to-image retrieval results indicate an interesting way to satisfy users' needs to find the corresponding food images for their customized recipes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Image-to-Recipe Retrieval Results &amp; Effect of Self-Attention model</head><p>In this section, we show some of the image-to-recipe retrieval results in <ref type="figure">Figure 6</ref> and then focus on analyzing the effect of our self-attention model. Given images from cheese Ingredients :</p><p>french -fried onions cheddar cheese water whole kernel corn mashed potatoes milk butter Instructions: Preheat oven to 350 degrees Fahrenheit. Spray pan with non stick cooking spray. Heat milk, water and butter to boiling; stir in contents of both pouches of potatoes; let stand one minute. Stir in corn. Spoon half the potato mixture in pan. Sprinkle half each of cheese and onions; top with remaining potatoes. Sprinkle with remaining cheese and onions. Bake 10 to 15 minutes until cheese is melted. Enjoy!</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredients:</head><p>salad greens sticks cherry tomatoes pineapple vinaigrette dressing Instructions: Preheat greased grill to medium-high heat. Grill fruit 3 min, on each side or until lightly browned on both sides. Cut fruit into 2-inch sticks; place in large salad bowl. Add greens, jicama and tomatoes; toss lightly. Drizzle with dressing just before serving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ingredients:</head><p>frozen chopped spinach onion, eggs, salt romano cheese Greek seasoning ground lamb tzatziki Instructions: If making your own sauce, prepare it the day before you are planning to make the meatloaf. In a large bowl, combine everything but the lamb/beef and tzatziki sauce. Shape into a loaf and place in a greased 11x7" baking dish. Bake uncovered at 350 for 55-60 minutes or until no pink remains and a thermostat reads 160. Let stand for 15 minutes before slicing. Drizzle with tzatziki sauce. <ref type="figure">Fig. 6</ref>. Visualizations of image-to-recipe retrieval. We show the retrieved recipes of the given food images, along with the attended ingredients and cooking instruction sentences. cake, meat loaf and salad, we show the retrieved recipe results by SCAN, which are all correct. We visualize the attended ingredients and instructions for the retrieved recipes with the yellow background, where we choose the ingredients and cooking instruction sentences of the top 2 attention weights as the attended ones. We can see that some frequently used ingredients like water, milk, salt, etc. are not attended with high weights, since they are not visible and shared by many kinds of food, which cannot provide enough discriminative information for cross-modal food retrieval. This is an intuitive explanation for the effectiveness of our self-attention model.</p><p>Another advantage of using the self-attention mechanism is that the image quality cannot affect the attended outputs. Obviously, the top two rows of food images cheese cake and meat loaf do not have good image quality, while our self-attention model still outputs reasonable attended results. This suggests that our proposed attention model has good capabilities to capture informative and reasonable parts for recipe embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Effect of Semantic Consistency</head><p>To have a concrete understanding of the ability of our proposed semantic consistency loss on reducing the mean intraclass feature distance (intra-class variance) between paired food image and recipe representations, we show the difference of the intra-class feature distance on cross-modal data trained without and with semantic consistency loss, i.e. TL and SCAN, in <ref type="figure">Figure 7</ref>. In the test set, we select the recipe and food image data from chocolate chip, which in total has 425 pairs. We obtain the food data representations from models trained with two different methods, then we compute the Euclidean distance between paired cross-modal data to obtain the mean intra-class feature distance. We adopt t-SNE <ref type="bibr" target="#b35">[36]</ref> to do dimensionality reduction to visualize the food data.</p><p>It can be observed that cross-modal food data which is trained with semantic consistency loss (SCAN) has smaller intra-class variance than that trained without semantic consis-  <ref type="figure">Fig. 7</ref>. The difference on the intra-class feature distance of cross-modal paired data trained without and with semantic consistency loss. The food data is selected from the same category, chocolate chip. SCAN obtains closer image-recipe feature distance than TL. (Best viewed in color.) tency loss (TL). This means that semantic consistency loss is able to correlate paired cross-modal data representations effectively by reducing the intra-class feature distance, and also our experiment results suggest its efficacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In conclusion, we propose SCAN, an effective training framework, for cross-modal food retrieval. It introduces a novel semantic consistency loss and employs the self-attention mechanism to learn the joint embedding between food images and recipes for the first time. To be specific, we apply semantic consistency loss to cross-modal food data pairs to reduce the intra-class variance, and utilize the self-attention mechanism to find the important parts in the recipes to construct discriminative recipe representations. SCAN is easy to implement and can extend to other general cross-modal datasets. We have conducted extensive experiments and ablation studies. We achieved state-of-the-art results in Recipe1M dataset. Chenghao Liu is currently a senior applied scientist of Salesforce Research Asia. Before, he was a research scientist in the School of Information Systems (SIS), Singapore Management University (SMU), Singapore. He received his Bachelor degree and Ph.D degrees from the Zhejiang University. His research interests include large-scale machine learning (online learning and deep learning) with application to tackle big data analytics challenges across a wide range of real-world applications.</p><p>Ke Shu is a research engineer at the Living Analytics Research Centre (LARC), Singapore Management University. His research interests include machine learning and deep learning.</p><p>Palakorn Achananuparp is a senior research scientist at the Living Analytics Research Centre (LARC), Singapore Management University. He is interested in developing and applying machine learning, natural language processing, and crowdsourcing techniques to solve problems in a variety of domains, including online social networks, politics, and public health.</p><p>Ee-peng Lim is the Lee Kong Chian Professor with the School of Computing and Information Systems at the Singapore Management University. He is also the Director of Living Analytics Research Centre in the School, a research centre focusing developing personalized and participatory analytics capabilities for smart city and smart nation relevant applications. Dr Lim received his PhD degree from University of Minnesota. His research expertise covers social media mining, social/urban data analytics, and information retrieval. He is the recipient of the Distinguished Contribution Award at the 2019 Pacific Asia Conference on Knowledge Discovery and Data Mining (PAKDD), and the Test of Time award at 2020 ACM Conference on Web Search and Data Mining (WSDM).</p><p>Steven C. H. Hoi is currently the Managing Director of Salesforce Research Asia, and a Professor of Information Systems at Singapore Management University, Singapore. Prior to joining SMU, he was an Associate Professor with Nanyang Technological University, Singapore. He received his Bachelor degree from Tsinghua University, P.R. China, in 2002, and his Ph.D degree in computer science and engineering from The Chinese University of Hong Kong, in 2006. His research interests are machine learning and data mining and their applications to multimedia information retrieval, social media and web mining, and computational finance, etc. He has served as the Editor-in-Chief for Neurocomputing Journal, general co-chair for ACM SIGMM Workshops on Social Media, program co-chair for the fourth Asian Conference on Machine Learning, book editor for "Social Media Modeling and Computing", guest editor for ACM Transactions on Intelligent Systems and Technology. He is an IEEE Fellow and ACM Distinguished Member.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Ingredients: honey, water, ground ginger, lime zest, fresh lime juice, sugar, orange zest Instructions: Combine first 6 ingredients in a small saucepan. Bring to a boil over medium heat; cook 5 minutes, whisking constantly. Remove from heat?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The structure of ingredient (instruction) embedding model with self-attention mechanism. Q, K and V denote queries, keys and values respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1</head><label>1</label><figDesc>Pseudocode of SCAN in a PyTorch-like style. # load a minibatch containing ingredients, instructions and images with N samples for (ingr, instr, img) in loader: # perform word embedding on ingredients Z = Embedding.forward(ingr) # compute LSTM features, Eq. (2) H = LSTM.forward(Z) # set the temperate t = sqrt(dimension_H) # compute attention scores attn = bmm(H, H.T) / t # compute attention outputs, Eq. (4) output = bmm(attn, H) # use residual connection to get the final selfattention outputs, Eq. (5) f_ingredient = LayerNorm(output + H) # use self-attention to get the instruction features f_instruction = SelfAttention.forward(instr) # compute the recipe features R = cat([f_ingredient, f_instruction], dim=1) # compute the image features I = CNN.forward(img) # compute triplet loss, Eq. (6) L_Ret = TripletLoss(R, I) # compute the class probabilities for R and I, Eq. (7, 8) p_rec = softmax(R) p_img = softmax(I) # compute cross-entropy loss L_cls_rec = CrossEntropyLoss(p_rec, labels) L_cls_img = CrossEntropyLoss(p_img, labels) L_cls = (L_cls_rec + L_cls_img) / 2 # compute KL divergence between recipes and images, Eq. (9, 10) L_KL = (KL(p_rec||p_img) + KL(p_img||p_rec)) / 2 # compute the semantic consistency loss, Eq. (11) L_SC = L_cls + L_KL # Eq. (1) loss = L_Ret + L_SC # Adam update loss.backward() sqrt: square root; bmm: batch matrix multiplication; cat: concatenation.L SC = {(L img cls + L KL (p rec p img )) +(L rec cls + L KL (p img p rec ))}/2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The training records of our proposed model SCAN and each component of SCAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I</head><label>I</label><figDesc>MAIN RESULTS. EVALUATION OF THE PERFORMANCE OF OUR PROPOSED METHOD COMPARED AGAINST THE BASELINES. THE MODELS ARE EVALUATED ON THE BASIS OF MEDR, WHERE LOWER IS BETTER, AND R@K (%), WHERE HIGHER IS BETTER.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Size of Test Set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">Image-to-Recipe Retrieval</cell><cell></cell><cell></cell><cell cols="6">Recipe-to-Image Retrieval</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Methods</cell><cell></cell><cell cols="4">medR ? R@1 ?</cell><cell>R@5 ?</cell><cell cols="6">R@10 ? medR ? R@1 ?</cell><cell cols="2">R@5 ?</cell><cell cols="3">R@10 ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CCA [8]</cell><cell></cell><cell cols="2">15.7</cell><cell cols="2">14.0</cell><cell>32.0</cell><cell></cell><cell cols="2">43.0</cell><cell>24.8</cell><cell>9.0</cell><cell></cell><cell cols="2">24.0</cell><cell></cell><cell>35.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SAN [5]</cell><cell></cell><cell cols="2">16.1</cell><cell cols="2">12.5</cell><cell>31.1</cell><cell></cell><cell cols="2">42.3</cell><cell>-</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>JE [1]</cell><cell></cell><cell></cell><cell cols="2">5.2</cell><cell cols="2">24.0</cell><cell>51.0</cell><cell></cell><cell cols="2">65.0</cell><cell>5.1</cell><cell>25.0</cell><cell></cell><cell cols="2">52.0</cell><cell></cell><cell>65.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AM [4]</cell><cell></cell><cell cols="2">4.6</cell><cell cols="2">25.6</cell><cell>53.7</cell><cell></cell><cell cols="2">66.9</cell><cell>4.6</cell><cell>25.7</cell><cell></cell><cell cols="2">53.9</cell><cell></cell><cell>67.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1k</cell><cell></cell><cell cols="3">AdaMine [2] R 2 GAN [32]</cell><cell cols="2">2.0 2.0</cell><cell cols="2">39.8 39.1</cell><cell>69.0 71.0</cell><cell></cell><cell cols="2">77.4 81.7</cell><cell>1.0 2.0</cell><cell>40.2 40.6</cell><cell></cell><cell cols="2">68.1 72.6</cell><cell></cell><cell>78.7 83.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MCEN [33]</cell><cell cols="2">2.0</cell><cell cols="2">48.2</cell><cell>75.8</cell><cell></cell><cell cols="2">83.6</cell><cell>2.0</cell><cell>48.4</cell><cell></cell><cell cols="2">76.1</cell><cell></cell><cell>83.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ACME [3]</cell><cell></cell><cell cols="2">1.0</cell><cell cols="2">51.8</cell><cell>80.2</cell><cell></cell><cell cols="2">87.5</cell><cell>1.0</cell><cell>52.8</cell><cell></cell><cell cols="2">80.2</cell><cell></cell><cell>87.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">tri-pro [34]</cell><cell cols="2">1.0</cell><cell cols="2">52.7</cell><cell>81.0</cell><cell></cell><cell cols="2">88.1</cell><cell>1.0</cell><cell>53.8</cell><cell></cell><cell cols="2">81.1</cell><cell></cell><cell>88.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SCAN (Ours)</cell><cell cols="2">1.0</cell><cell cols="2">54.0</cell><cell>81.9</cell><cell></cell><cell cols="2">89.2</cell><cell>1.0</cell><cell>54.9</cell><cell></cell><cell cols="2">81.9</cell><cell></cell><cell>89.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>JE [1]</cell><cell></cell><cell></cell><cell cols="2">41.9</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell>39.2</cell><cell>-</cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">AM [4]</cell><cell></cell><cell cols="2">39.8</cell><cell>7.2</cell><cell></cell><cell>19.2</cell><cell></cell><cell cols="2">27.6</cell><cell>38.1</cell><cell>7.0</cell><cell></cell><cell cols="2">19.4</cell><cell></cell><cell>27.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">AdaMine [2]</cell><cell cols="2">13.2</cell><cell cols="2">14.9</cell><cell>35.3</cell><cell></cell><cell cols="2">45.2</cell><cell>12.2</cell><cell>14.8</cell><cell></cell><cell cols="2">34.6</cell><cell></cell><cell>46.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>10k</cell><cell></cell><cell cols="3">R 2 GAN [32] MCEN [33]</cell><cell cols="2">13.9 7.2</cell><cell cols="2">13.5 20.3</cell><cell>33.5 43.3</cell><cell></cell><cell cols="2">44.9 54.4</cell><cell>11.6 6.6</cell><cell>14.2 21.4</cell><cell></cell><cell cols="2">35.0 44.3</cell><cell></cell><cell>46.8 55.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">tri-pro [34]</cell><cell cols="2">7.0</cell><cell cols="2">22.1</cell><cell>45.9</cell><cell></cell><cell cols="2">56.9</cell><cell>7.0</cell><cell>23.4</cell><cell></cell><cell cols="2">47.3</cell><cell></cell><cell>57.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ACME [3]</cell><cell></cell><cell cols="2">6.7</cell><cell cols="2">22.9</cell><cell>46.8</cell><cell></cell><cell cols="2">57.9</cell><cell>6.0</cell><cell>24.4</cell><cell></cell><cell cols="2">47.9</cell><cell></cell><cell>59.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SCAN (Ours)</cell><cell cols="2">5.9</cell><cell cols="2">23.7</cell><cell>49.3</cell><cell></cell><cell cols="2">60.6</cell><cell>5.1</cell><cell>25.3</cell><cell></cell><cell cols="2">50.6</cell><cell></cell><cell>61.6</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.45</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Recall@1</cell><cell>0.1 0.2 0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TL TL+SA SCAN</cell><cell></cell><cell>Recall@1</cell><cell>0.15 0.20 0.25 0.30 0.35 0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TL+cls TL+SC SCAN</cell><cell>Recall@1</cell><cell>0.4 0.1 0.2 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TL TL+SA TL+cls TL+SC SCAN</cell></row><row><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15 Epochs 20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell></cell><cell>0</cell><cell>5</cell><cell>10</cell><cell>15 Epochs 20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>0</cell><cell>5</cell><cell>10</cell><cell cols="2">15 Epochs 20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II THE</head><label>II</label><figDesc></figDesc><table><row><cell cols="5">PERFORMANCE OF PROPOSED MODEL SCAN TRAINED WITH</cell></row><row><cell></cell><cell cols="3">DIFFERENT TRADE-OFF PARAMETER ?.</cell><cell></cell></row><row><cell>?</cell><cell cols="4">MedR R@1 (%) R@5 (%) R@10 (%)</cell></row><row><cell>0.01</cell><cell>1.0</cell><cell>53.2</cell><cell>81.3</cell><cell>88.2</cell></row><row><cell>0.05</cell><cell>1.0</cell><cell>54.0</cell><cell>81.9</cell><cell>89.2</cell></row><row><cell>0.1</cell><cell>1.0</cell><cell>51.9</cell><cell>80.5</cell><cell>87.6</cell></row><row><cell>0.5</cell><cell>1.7</cell><cell>42.7</cell><cell>71.8</cell><cell>81.1</cell></row><row><cell cols="5">split [1], containing 238,999 image-recipe pairs for training,</cell></row><row><cell cols="5">51,119 and 51,303 pairs for validation and test, respectively.</cell></row><row><cell cols="4">In total, the dataset has 1,047 categories.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III ABLATION</head><label>III</label><figDesc>STUDIES. EVALUATION OF BENEFITS OF DIFFERENT COMPONENTS OF THE SCAN FRAMEWORK. THE MODELS ARE EVALUATED BASED ON MEDR, WHERE LOWER IS BETTER, AND R@K (%), WHERE HIGHER IS BETTER.</figDesc><table><row><cell>L Ret</cell><cell>Component</cell><cell cols="3">medR ? R@1 ? R@5 ?</cell><cell>R@10 ?</cell></row><row><cell></cell><cell>CL</cell><cell>2.0</cell><cell>46.9</cell><cell>76.5</cell><cell>84.9</cell></row><row><cell></cell><cell>CL+SA</cell><cell>1.0</cell><cell>51.0</cell><cell>79.9</cell><cell>86.3</cell></row><row><cell>Cosine Loss</cell><cell>CL+cls</cell><cell>1.9</cell><cell>47.0</cell><cell>75.5</cell><cell>83.4</cell></row><row><cell></cell><cell>CL+SC</cell><cell>1.0</cell><cell>50.7</cell><cell>79.7</cell><cell>86.4</cell></row><row><cell></cell><cell>CL+SC+SA</cell><cell>1.0</cell><cell>52.1</cell><cell>80.4</cell><cell>87.2</cell></row><row><cell></cell><cell>TL</cell><cell>2.0</cell><cell>47.5</cell><cell>76.2</cell><cell>85.1</cell></row><row><cell></cell><cell>TL+SA</cell><cell>1.0</cell><cell>52.5</cell><cell>81.1</cell><cell>88.4</cell></row><row><cell>Triplet Loss</cell><cell>TL+cls</cell><cell>1.7</cell><cell>48.5</cell><cell>78.0</cell><cell>85.5</cell></row><row><cell></cell><cell>TL+SC</cell><cell>1.0</cell><cell>51.9</cell><cell>80.3</cell><cell>88.0</cell></row><row><cell></cell><cell>SCAN</cell><cell>1.0</cell><cell>54.0</cell><cell>81.9</cell><cell>89.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>1. combine flour, nuts, and brown sugar. 2. Add melted butter, tossing to combine the ingredients. 3. Bake the crust at 350 degrees F (175 degrees C) for 15</figDesc><table><row><cell>minutes.</cell></row><row><cell>4. Sprinkle 1/2 cups crust mixture in a 9x13 inch pan.</cell></row><row><cell>5. Reserve the remaining crust mixture.</cell></row><row><cell>6. Filling: In large bowl combine strawberries, sugar and</cell></row><row><cell>lemon juice.</cell></row><row><cell>butter, lemon juice, frozen whipped topping, brown</cell></row><row><cell>sugar, strawberries, all -purpose flour, walnuts,</cell></row><row><cell>white sugar</cell></row><row><cell>1. combine flour, nuts, and brown sugar.</cell></row><row><cell>2. Add melted butter, tossing to combine the ingredients.</cell></row><row><cell>3. Bake the crust at 350 degrees F (175 degrees C) for 15</cell></row><row><cell>minutes.</cell></row><row><cell>4. Sprinkle 1/2 cups crust mixture in a 9x13 inch pan.</cell></row><row><cell>5. Reserve the remaining crust mixture.</cell></row><row><cell>6. Filling: In large bowl combine strawberries, sugar and</cell></row><row><cell>lemon juice.</cell></row><row><cell>butter, lemon juice, frozen whipped topping, brown</cell></row><row><cell>sugar, strawberries, all -purpose flour, walnuts,</cell></row><row><cell>white sugar</cell></row><row><cell>1. combine flour, nuts, and brown sugar.</cell></row><row><cell>2. Add melted butter, tossing to combine the ingredients.</cell></row><row><cell>3. Bake the crust at 350 degrees F (175 degrees C) for 15</cell></row><row><cell>minutes.</cell></row><row><cell>4. Sprinkle 1/2 cups crust mixture in a 9x13 inch pan.</cell></row><row><cell>5. Reserve the remaining crust mixture.</cell></row><row><cell>6. Filling: In large bowl combine strawberries, sugar and</cell></row><row><cell>lemon juice.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Hao Wang is a PhD candidate with the School of Computer Science and Engineering, Nanyang Technological University, Singapore. His research interests include multi-modal analysis and computer vision. Sahoo is a Senior Research Scientist at Salesforce Research Asia. Prior to joining Salesforce, Doyen was a Research Fellow at the Living Analytics Research Center at Singapore Management University (SMU). He was also serving as Adjunct Faculty in SMU. Doyen earned his PhD in Information Systems from SMU in 2018 and B.Eng in Computer Science from Nanyang Technological University in 2012. His research interests include Online Learning, Deep Learning, Computer Vision, and he also works on applied research including AIOps, Computational Finance and Cyber Security applications. He has published over 40 articles in top tier conferences and journals including ICLR, CVPR, ACL, KDD, JMLR, etc.</figDesc><table><row><cell>Doyen</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ACKNOWLEDGEMENT This research is supported by the National Research Foundation, Singapore under its International Research Centres in Singapore Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning cross-modal embeddings for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hynes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ofli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3020" to="3028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-modal retrieval in the cooking context: Learning semantic textimage embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cad?ne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Soulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning crossmodal embeddings with adversarial networks for cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep understanding of cooking procedure for cross-modal recipe retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ACM Multimedia Conference on Multimedia Conference</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1020" to="1028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cross-modal recipe retrieval: How to cook this dish</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia Modeling</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="588" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Person reidentification by multi-channel parts-based cnn with improved triplet loss function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1335" to="1344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Relations between two sets of variates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hotelling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="321" to="377" />
			<date type="published" when="1936" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep collaborative embedding for social image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="2070" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly-supervised semantic guided hashing for social image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="2265" to="2278" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep semantic multimodal hashing network for scalable image-text and video-text retrievals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cm-gans: Cross-modal generative adversarial networks for common representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05106</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised deep generative adversarial hashing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sadoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3664" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incomplete cross-modal retrieval with dual-aligned variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3283" to="3291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph structured network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">930</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A survey on food computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">You are what you eat: Exploring rich recipe information for cross-region food analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-K</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="950" to="964" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Personalized classifier for food image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Horiguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2836" to="2848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-scale multi-view deep feature aggregation for food recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="265" to="276" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Isia food-500: A dataset for large-scale food recognition via stacked globallocal attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="393" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Being a supercook: Joint food attributes and multimodal content modeling for recipe retrieval and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1100" to="1113" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Using tags and latent factors in a food recommender system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ferna?ndez-Tob?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Massimo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Digital Health</title>
		<meeting>the 5th International Conference on Digital Health</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Food recommendation: Framework, existing solutions, and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2659" to="2671" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structure-aware generation network for recipe generation from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="359" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep-based ingredient recognition for cooking recipe retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="32" to="41" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Skip-thought vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3294" to="3302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">R2gan: Cross-modal recipe retrieval with generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mcen: Bridging cross-modal gap between cooking recipes and dish images with latent variable model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">580</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sentence-based and noise-robust cross-modal retrieval on cooking recipes and food images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 International Conference on Multimedia Retrieval</title>
		<meeting>the 2020 International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="117" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
