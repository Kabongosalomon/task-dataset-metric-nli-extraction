<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tree Decomposed Graph Neural Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Virtual Event</publisher>
				<availability status="unknown"><p>Copyright Virtual Event</p>
				</availability>
				<date type="published" when="2021-11-01">2021. November 1-5, 2021. November 1-5, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Acm Reference</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Format</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Derr</surname></persName>
						</author>
						<title level="a" type="main">Tree Decomposed Graph Neural Network</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM &apos;21)</title>
						<meeting>the 30th ACM International Conference on Information and Knowledge Management (CIKM &apos;21) <address><addrLine>QLD, Australia</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Virtual Event</publisher>
							<date type="published" when="2021-11-01">2021. November 1-5, 2021. November 1-5, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3459637.3482487</idno>
					<note>Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https:// ACM ISBN 978-1-4503-8446-9/21/11. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>graph neural networks</term>
					<term>tree decomposition</term>
					<term>multi-hop dependency</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have achieved significant success in learning better representations by performing feature propagation and transformation iteratively to leverage neighborhood information. Nevertheless, iterative propagation restricts the information of higher-layer neighborhoods to be transported through and fused with the lower-layer neighborhoods', which unavoidably results in feature smoothing between neighborhoods in different layers and can thus compromise the performance, especially on heterophily networks. Furthermore, most deep GNNs only recognize the importance of higher-layer neighborhoods while yet to fully explore the importance of multi-hop dependency within the context of different layer neighborhoods in learning better representations. In this work, we first theoretically analyze the feature smoothing between neighborhoods in different layers and empirically demonstrate the variance of the homophily level across neighborhoods at different layers. Motivated by these analyses, we further propose a tree decomposition method to disentangle neighborhoods in different layers to alleviate feature smoothing among these layers. Moreover, we characterize the multi-hop dependency via graph diffusion within our tree decomposition formulation to construct Tree Decomposed Graph Neural Network (TDGNN), which can flexibly incorporate information from large receptive fields and aggregate this information utilizing the multi-hop dependency. Comprehensive experiments demonstrate the superior performance of TDGNN on both homophily and heterophily networks under a variety of node classification settings. Extensive parameter analysis highlights the ability of TDGNN to prevent over-smoothing and incorporate features from shallow layers with deeper multi-hop dependencies, which provides new insights towards deeper graph neural networks. The implementation of TDGNN is available at https://github.com/YuWVandy/TDGNN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph representation learning has recently emerged as a powerful strategy for node classification <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31]</ref>, graph classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39]</ref> and link prediction <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42]</ref> on graph-structured data. As the generalization of deep learning to the graph domain, Graph Neural Networks (GNNs) have become one of the most promising paradigms <ref type="bibr" target="#b25">[26]</ref>, which adopts a neighborhood aggregation scheme to learn node representations by utilizing both the node features and the graph topology <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. A typical GNN architecture for node classification consists of two stages: propagation/aggregation and transformation. First, messages are propagated from neighboring nodes to their corresponding center nodes and then aggregated together. Afterwards, the aggregated messages are transformed by the transformation layer to extract useful node representations. These two stages are packed together and termed as one layer of graph convolution. Deep GNNs iteratively perform multiple graph convolutions to obtain a larger receptive field and thus incorporate information of neighborhoods in higher layers <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Although GNNs have gained significant achievements, a common challenge faced by GNNs is known as the over-smoothing problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>: the performance of GNNs degrades when stacking multiple graph convolution layers. Most popular models, such as GCN <ref type="bibr" target="#b11">[12]</ref> and GAT <ref type="bibr" target="#b30">[31]</ref>, achieve their best performance with 2-layer graph convolutions. Such shallow architectures limit their ability to extract information from higher-layer neighborhoods. However, stacking multiple layers to increase the receptive field tends to fuse representations of nodes from different classes and thus make them indistinguishable due to iterative propagation <ref type="bibr" target="#b16">[17]</ref>. Earlier works have found that the stationary point that node representations converge to is determined by node degrees and their features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27]</ref>. However, these works focus on the theoretical analysis of the steady state in the limit of propagation while yet to provide effective solutions to solve the over-smoothing problem.</p><p>Stepping further, several methods propose deep GNNs to incorporate higher-layer neighborhood information through iterative propagation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. For example, GCNII <ref type="bibr" target="#b2">[3]</ref> applies an initial residual connection and identity mapping to enable GCN to express a th -order polynomial filter with arbitrary coefficients. Nevertheless, more recently, it has been analyzed that the performance degradation due to over-smoothing is because the entanglement of feature transformation and propagation <ref type="bibr" target="#b16">[17]</ref>. Building upon this, DAGNN <ref type="bibr" target="#b16">[17]</ref> proposes to decouple the feature propagation and transformation, and learn node representations by adaptively incorporating information from larger receptive fields. However, the propagation in all of previous deep models is executed iteratively so that the information of higher-layer neighborhoods is restricted to be transported through and fused with the lower-layer neighborhoods' and then propagated to their corresponding center nodes, which unavoidably informs feature smoothing between neighborhoods in different layers. Therefore, although different deep GNNs <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17]</ref> develop their own techniques, their core component is to apply multiple graph convolutions (mixed-order propagation <ref type="bibr" target="#b7">[8]</ref>) to incorporate more neighborhood information from a broader neighboring range. As a result, the importance of depth of GNNs has raised significant concern while the importance of width of GNNs is rarely researched. It is shown in <ref type="bibr" target="#b7">[8]</ref> that the mixed order propagation rule enables their model to incorporate more local information but does not clarify the relationship between the propagation rule and the local information. Additionally, in <ref type="bibr" target="#b31">[32]</ref>, they observe that nodes can not only attend to their immediate neighbors but can also extract useful information from their multi-hop neighboring context. However, this work considers this multi-hop neighboring context in a specific attention framework. Different from previous works, we propose the general concept of multi-hop dependency as a measurement of the width of GNNs, and empirically demonstrate its importance in learning node representations.</p><p>In view of the challenge that features of higher-layer neighborhoods are over-smoothed with the lower-layer neighborhoods and noticing the importance of the multi-hop dependency, we propose an effective framework, termed as Tree Decomposed Graph Neural Network (TDGNN), to learn node representations from larger receptive fields without causing feature over-smoothing between different layers of neighborhoods and allow flexible layer configurations to avoid under-performance on heterophily networks. Our major contributions are listed as follows:</p><p>? Motivated by our theoretical analysis on feature smoothing and empirically demonstration of the variance of the homophily level across neighborhoods in different layers, we propose a tree decomposition method to disentangle features of neighborhoods in different layers, which can help alleviate the problem of feature smoothing and provides more flexible layer configurations for complex networks. ? We capture and maintain the importance of multi-hop dependency in learning better representations within our tree decomposition method by characterizing this multi-hop dependency by graph diffusion, which ultimately leads to the construction of proposed Tree Decomposed Graph Neural Network (TDGNN). ? We conduct experiments in both semi-supervised and fullsupervised settings and on both homophily and heterophily network datasets to comprehensively demonstrate the superiority of our proposed TDGNN framework over existing methods. Additionally, we perform a parameter analysis to better understand and contrast TDGNN to prior GNNs.</p><p>The rest of the paper is organized as follows. In Section 2, we define necessary notations and briefly introduce the supervised node classification problem and GNNs. We present our proposed TDGNN framework in Section 3, which consists of the tree decomposition procedure to disentangle feature information of neighborhoods in different layers, the graph diffusion procedure to model multi-hop dependencies, and layer aggregation procedure to enable adaptive combination of aggregated representations of different layers. In Section 4, experiments are performed to evaluate the effectiveness of our framework. Related work is then presented in Section 5. Finally, we conclude and discuss future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES</head><p>In this section, we first introduce the notations and definitions that are used throughout this paper, and then provide a brief background on the supervised node classification problem and GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notations</head><p>Let G = (V, E, X) denote an unweighted and undirected network, where V = { 1 , 2 , ..., } is the set of nodes (i.e., = |V |), E ? V ? V is the set of edges (i.e., = |E |) between nodes in V with denoting the edge between the node and node , and X ? R ? denotes the node feature matrix, where each row x ? R represents the feature vector of node and is the dimension of node features. The topological information of the whole network G is described by the adjacency matrix A ? R ? , where A = 1 if an edge exists between node and node (i.e., if ? E), and A = 0 otherwise. The diagonal matrix of node degrees are notated as D ? R ? , where the degree of the node is calculated by D =</p><p>A . Additionally, we let A = A + I represent the adjacency matrix with added self-loops and similarly let D represent the diagonal degree matrix with the diagonal element D = D + I. N is the neighborhood node set of the center node , which is given by N = { | ? E}. We then extend this definition by using N to denote the -th layer neighborhood nodes of the center node , which includes all nodes that can be reached from the center node in exactly hops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Supervised Node Classification Task</head><p>In this work, we focus on the node classification task and leave the application of our framework on other tasks, such as link prediction, as one future work. More specifically, we consider the transductive node classification problem where we are provided with the labeled node set V ? V associated with the node label matrix Y ? R |V |? with number of classes, the goal is to learn a mapping : R ? ? R ? ? R ? , which takes as input the feature matrix X and adjacency matrix A, and outputs the predicted -dimensional node representation Z ? R ? .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Neural Networks</head><p>Typically, most graph neural networks (GNNs) can be decomposed into two operational procedures: (1) neighborhood propagation and aggregation, and (2) feature transformation. The neighborhood propagation and aggregation can be formalized as follows:</p><formula xml:id="formula_0">h = AGGREGATION (h ?1 , {h ?1 | ? N }),<label>(1)</label></formula><p>where representations of neighborhoods at the previous layer {h ?1 | ? N } are propagated to the center node and further fused with its own representation h ?1 from the previous layer via AGGREGATION function at layer to get the partial representation h . Note that h 0 of node is initialized as the original node feature x . Then, after the aggregation procedure, the TRANSFORMATION function at layer is applied on h to get the representation h of node at layer and defined as follows: where we can denote the node representations at th layer for all the nodes in the network as H ? R ? , where the th row corresponds to the representation of the node at layer (i.e., h ? R ). Most graph convolutions, such as GCN <ref type="bibr" target="#b11">[12]</ref>, GraphSAGE <ref type="bibr" target="#b9">[10]</ref>, GAT <ref type="bibr" target="#b30">[31]</ref>, GIN <ref type="bibr" target="#b34">[35]</ref>, and SGC <ref type="bibr" target="#b33">[34]</ref>, can be obtained under this framework by adopting and configuring different functions in AGGRE-GATION and TRANSFORMATION. For instance, the vanilla GCN model suggests using H = AH ?1 in AGGREGATION followed by</p><formula xml:id="formula_1">h = TRANSFORMATION ( h ),<label>(2)</label></formula><formula xml:id="formula_2">H = ( H W ) in TRANSFORMATION, where A = D ? 1 2 A D ? 1 2</formula><p>is the renormalized adjacency matrix to prevent gradient explosion, W ? R ?1 ? represents the weight matrix at layer transforming features of dimension ?1 to , and is ReLU. Although GNNs may differ in their unique AGGREGATION and TRANSFORMATION designs, most graph convolutions employ an iterative propagation to increase their receptive field and incorporate information of neighborhoods in higher layers. However, such iterative propagation inevitably causes feature smoothing between neighborhoods in different layers. Furthermore, the enhanced performance caused by leveraging iterative feature propagation guides us to pay more attention to the importance of higher-layer neighborhoods while the effect of multi-hop dependency still remains unclear. Next, having defined the basic notations, background, and discussed some of the challenges for supervised node classification and basic GNNs, we present our proposed framework with the purpose of alleviating the feature smoothing between neighborhoods in different layers and incorporating the multi-hop dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED FRAMEWORK</head><p>In this section, we design a Tree Decomposed Graph Neural Network (TDGNN) by mainly solving the two challenges mentioned in Section 1, which are feature smoothing between neighborhoods in different layers and lack of considering the multi-hop dependency in GNNs. For the first challenge, we theoretically show the feature smoothing between different layers when applying iterative propagation and further propose a tree decomposition method to disentangle neighborhood information in different layers. For the second challenge, we formalize the definition of the multi-hop dependency and characterize it through a graph diffusion process.</p><p>Combining the tree decomposition method to disentangle the neighborhood information on different layers and the graph diffusion to model the multi-hop dependency, we propose TDGNN. We also introduce two mechanisms to aggregate node representations of each layer, TDGNN-s that directly sums the representations of all layers together, and TDGNN-w that assigns learnable weights and adaptively combines the node representations of each layer. The whole framework is shown in <ref type="figure">Figure 3</ref>, which has three main components: tree decomposition to handle feature smoothing between different neighborhood layers, graph diffusion to model multi-hop dependency, and aggregation to combine representations of different layers. Next, we describe each of these components in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Tree Decomposition</head><p>The basic assumption in GNNs is that the neighborhood information of the center node leveraged by applying feature propagation and aggregation can enhance the prediction performance of the center node itself <ref type="bibr" target="#b35">[36]</ref>. Such an assumption is justified by the core network property, homophily, where linked nodes tend to share similar features and typically belong to the same class <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44]</ref>. However, the level of homophily might be completely different among different networks or even vary among different subgraphs within the same network. One extreme situation would be the heterophily network where linked nodes are likely from different classes or have dissimilar features <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>In <ref type="figure" target="#fig_0">Figure 1</ref>, we show the level of homophily across different neighborhood layers in the Cora and Texas datasets. More specifically, the level of homophily is measured by distributions of the ratio of neighborhoods N in different neighborhood layers that share the same class as (in <ref type="figure" target="#fig_0">Figure 1</ref>  <ref type="figure" target="#fig_0">Figure 1(d)</ref>) their corresponding center nodes, which are obtained from feeding node features only through the transformation layers in a pre-trained 2-layer GCN <ref type="bibr" target="#b11">[12]</ref> without introducing any bias from feature propagation. In the Cora dataset, it can be observed that the majority of neighbors among their 1 st -layer neighborhoods have the same class as their corresponding center nodes, but the number of center nodes that have most of their neighborhoods sharing the same class as themselves decreases as the layer increases. Furthermore, the embeddings of these neighborhoods on the 1 st -layer, on average, have high similarity to their corresponding center nodes. This demonstrates the high homophily of the Cora dataset on low layers and propagating features of nodes in these low layers fuses embeddings of nodes in the same class and thus makes embeddings of different classes more separable. However, even for this extreme homophily layer in this strong homophily Cora dataset <ref type="bibr" target="#b36">[37]</ref>, not all of the nodes in the 1 st -layer have all of their neighborhoods sharing the same label with themselves and this strong homophily becomes progressively weaker as we reach further out to higher layers. For example, only around 10% of the nodes in the 3 rd -layer have all their neighborhoods sharing the same class and over half of the nodes in the 10 th -layer have nearly all neighborhoods different from themselves. Even worse, in Texas dataset, even for low layers, most of the neighborhoods have different classes from their center nodes, especially for nodes on the 1 st -layer, almost all nodes belong to different classes from their center nodes, which demonstrates the strong heterophily of the Texas dataset. Propagating features of nodes in such low layers to their center nodes fuses embeddings of nodes in different classes and makes those nodes indistinguishable, which results in learning worse node representations. Such feature smoothing among different layers is unavoidable as long as the procedure of iterative propagation is taken, that is: during iterative feature propagation, the information of neighborhoods in higher layers has to be transported through and fused with the information of neighborhoods in lower layers and then propagated to their corresponding center nodes.</p><p>If we take the most popular GNN-variant, a 2-layer GCN, as an example (for simplicitly), then after 2-layer graph convolutions, the representation of the node is:</p><formula xml:id="formula_3">h 2 = (h 0 W 0 )W 1 ( 1 ( + 1) 2 + ?? ?N 1 ( + 1)( + 1) ) 0 th -layer features (self) + ?? ?N (h 0 W 0 )W 1 ( + + 2 ( + 1) 1.5 ( + 1) 1.5 ) 1 st -layer neighborhood features + ?? ?N ?? ? {N ?N } (h 0 W 0 )W 1 1 ? + 1 ?? + 1( + 1) 1 st -layer neighborhood features + ?? ?N ?? ? {N ?N 2 } (h 0 W 0 )W 1 1 ? + 1 ?? + 1( + 1) 2 nd -layer neighborhood features ,<label>(3)</label></formula><p>which consists of three components corresponding to the feature information of neighborhoods in the 0 th , 1 st , and 2 nd layers. Eq. 3 intuitively shows that the representation of node after two iterative graph convolutions contains the information of both 1 st and 2 nd -layer neighborhood information, which will compromise the performance on Texas dataset since the feature and class information of the 1 st -layer is of great difference from their corresponding center nodes according to <ref type="figure" target="#fig_0">Figure 1</ref>  the Cora dataset, where the feature and class information of the 1 st -layer neighborhoods is similar to their corresponding center nodes, still some center nodes have neighborhoods in the 1 st -layer different from themselves and for these nodes, incorporating their neighborhood information might compromise their predictions. Thus, based on our analysis, to advance the frontier of GNNs to be able to selectively leverage neighborhood information in different layers, we propose a tree decomposition method. More specifically, our proposed method disentangles neighborhoods in different layers and connects them directly with their corresponding center nodes. These direct connections allow the propagation of higher-layer neighborhoods' features to their corresponding center nodes without any interference of lower-layer neighborhoods along the way. Furthermore, this tree decomposition procedure enables more flexible layer configurations of neighborhoods. For example in <ref type="figure" target="#fig_3">Figure 2</ref>, we decompose the computational tree in GNNs of the center node <ref type="bibr" target="#b0">1</ref> . Then in the training process, we selectively propagate features of nodes in different layers: propagating along 1 st -layer subgraph, along 2 nd -layer subgraph, and along both of these two subgraphs. The choice depends on the network homophily of different layers and is determined by hyperparameter-tuning. The adjacency matrix of the th -layer subgraph T obtained from tree decomposition can be computed by the difference between corresponding powers of the normalized adjacency matrices with added self-loops and formalized as follows:</p><formula xml:id="formula_4">T = sign( A ) ? sign( A ?1 ) + I,<label>(4)</label></formula><formula xml:id="formula_5">sign( A ) = 1, if A &gt; 0 0, if A = 0,<label>(5)</label></formula><p>where A 0 = I is the identity matrix and A = D ? 1 2 A D ? 1 2 is the renormalized adjacency matrix as previously defined. The equivalence between T and the th -layer subgraph including the self-loop can be easily proven, so we omit the details for brevity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-hop dependency</head><p>Although the tree decomposition could avoid the issue of feature smoothing between different layers, we also lose the multi-hop dependency captured by the original iterative propagation which might cause over-smoothing <ref type="bibr" target="#b7">[8]</ref>. Two nodes have multi-hop dependency if they are connected by a path in the network and specifically -hop dependency is defined as two nodes that are connected by at least one simple path with length . For example in <ref type="figure" target="#fig_3">Figure 2</ref>, features of node 2 cannot only be propagated along the edge 2 ? 1 <ref type="figure">Figure 3</ref>: An illustration of the proposed Tree Decomposed Graph Neural Network (TDGNN). For brevity, the pipeline to generate the prediction for only one node is presented.</p><p>to 1 but can also along the longer path 2 ? 5 ? 6 ? 3 ? 1 to 1 . However after tree decomposition, the edge 2 ? 1 is the only way for propagating features of 2 to 1 . Instead of inserting multiple edges between the higher-layer neighborhood nodes and their corresponding center nodes, we model this multi-hop dependency via graph diffusion <ref type="bibr" target="#b13">[14]</ref>. Specifically, A represents the th -hop dependency and its entry A measures the strength of paths of length in propagating features from node to . Assuming the maximum hop of the dependency we consider is , since th -layer ( ? ) neighborhood nodes can only propagate their features along paths of length from to , thus the total multi-hop dependencies from node to along these paths is calculated as = A . Such multi-hop dependency across the spectrum from to between a single pair of nodes can be further generalized to all pairs of nodes in the graph via diffusion and defined as:</p><formula xml:id="formula_6">E , = ?? = A ,<label>(6)</label></formula><p>where E , considers dependencies from paths of length to , which could be used as the edge weights for propagating node features in the th -layer subgraph obtained from tree decomposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tree Decomposed Graph Neural Network</head><p>Now, having motivated and introduced the two major components of our proposed framework, namely the tree decomposition and multi-hop dependency formulations, we collect them together and present our Tree Decomposed Graph Neural Network (TDGNN). As previously noted, an illustration of our proposed TDGNN is shown in <ref type="figure">Figure 3</ref> and its corresponding mathematical formulation is defined as:</p><formula xml:id="formula_7">H 0 = MLP(X),<label>(7)</label></formula><formula xml:id="formula_8">H = (T ? E , )H 0 , = 1, 2, ..., ,<label>(8)</label></formula><formula xml:id="formula_9">Z = =0 H , TDGNN-s =0 H , TDGNN-w.<label>(9)</label></formula><p>We first apply a Multilayer Perceptron (MLP) network to the original feature matrix X to get the initial representations of nodes H 0 <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b16">17]</ref>. Then, we decompose the whole network by calculating the adjacency matrix T of th -layer tree based on Eq. 4 and Eq. 5. This th -layer subgraph contains only edges between center nodes and their corresponding th -layer neighborhood nodes including a self-loop. Since here we consider the neighborhood nodes up to th -layer, the is from 1 to . Next we utilize graph diffusion to calculate the multi-hop dependency E , based on Eq. 6 and is the predefined maximum hop of dependency we consider. Afterwards, we propagate the initial node representations H 0 along edges in each subgraph following each corresponding adjacency matrix T with the corresponding edge weight from multi-hop dependency E , to get representations H for each layer based on Eq. 8. We collect representations from each layer and aggregate them together using two aggregation mechanisms to get the final representations Z based on Eq. (9). The first aggregation mechanism is to directly sum up the representations of all layers together. The second aggregation mechanism is to assign learnable weights and adaptively combine the representations of each layer. The corresponding two versions of our model are termed as TDGNN-s and TDGNN-w, respectively. Ultimately, Z is employed to compute the cross-entropy loss for all labeled nodes as:</p><formula xml:id="formula_10">L = ? ?? ?V ?? =1 Y log Z ,<label>(10)</label></formula><p>where Z is the probability distribution of each node belonging to each class and is obtained by applying softmax on the final representation Z. Note that V ? V is the set of training nodes with known label information as previously defined and is the total number of classes to be predicted.</p><p>In summary, our model decouples representation transformation from propagation <ref type="bibr" target="#b16">[17]</ref>, which enlarges the receptive fields without introducing more trainable parameters. Obtaining low dimensional representations before propagation follows is the idea of predicting than propagating <ref type="bibr" target="#b12">[13]</ref>, which makes the training process of TDGNN computationally efficient. Additionally, the tree decomposition preprocessing allows more flexible choice of utilizing and combining different layers to propagate features. The multi-hop dependency enables feature propagation along paths of various lengths, which conforms to other recent work <ref type="bibr" target="#b31">[32]</ref>. Furthermore, applying learnable weight coefficients equips TDGNN with the ability to flexibly select an effective receptive field based on a specific network and generate adaptive representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Complexity Analysis</head><p>In comparison to vanilla GCN, the additional computational load mostly comes from the tree decomposition and the graph diffusion. Since tracking down the corresponding -layer subgraphs is equivalent to calculating the difference between corresponding powers of the normalized adjacency matrices with added self-loops by Eq. 4, which is exactly given by graph diffusion, the time for the tree decomposition could be saved. The time complexity for performing graph diffusion process is ( 3 ) = ( 3 ) due to times matrix multiplication and can be reduced approximately to ( 2.81 ) if using the Strassen algorithm <ref type="bibr" target="#b28">[29]</ref> or even further to ( 2.38 ) <ref type="bibr" target="#b4">[5]</ref>. Moreover, in practice real-world graphs are extremely sparse and thus sparse matrix multiplication methods <ref type="bibr" target="#b39">[40]</ref> could also be used to further improve computational efficiency. Notably, both of the tree decomposition and the graph diffusion are preprocessing outside of the training, which significantly reduces the computational load of the whole framework.</p><p>For the space complexity, the bottleneck would be saving diffusion matrices A , = 1, 2, ..., , and the adjacency matrices of each subgraph T , = 1, 2, ..., , which leads to ( 2 ) and constitutes a severe threat for networks of large scale. However, as we highlighted before and demonstrate in <ref type="figure" target="#fig_4">Figure 4</ref>, since higherlayer neighborhoods may have completely different features from their corresponding center nodes and incorporating their information gain little benefits in learning better representation, we could only consider lower-layer neighborhoods <ref type="bibr" target="#b40">[41]</ref> and thus keep only the first few adjacency matrices. On the other hand, most of the real-world networks have the small-world property that most nodes can be reached from every other node by a small number of hops <ref type="bibr" target="#b17">[18]</ref>, which confirms and helps justify why we can remove the higher-layer adjacency matrices. Moreover, we could apply the same strategy as GraphSAGE <ref type="bibr" target="#b9">[10]</ref> where we sample nodes from center node's local neighborhood via random walk and propagate features among these sampled nodes <ref type="bibr" target="#b40">[41]</ref>. Since this work mainly focuses on disentangling neighborhoods to avoid feature smoothing between different layers and characterizing the multi-hop dependency, the aforementioned is left as one future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct extensive node classification experiments to evaluate the superiority of our proposed TDGNN model. We begin by introducing the datasets and experimental setup we employed. Then, we compare TDGNN with prior baselines and some state-of-the-art (SOTA) deep GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets.</head><p>We evaluate the performance of our TDGNN model and baseline models with node classification on multiple real-world datasets. More specifically, we use the three standard citation network datasets Cora, Citeseer, and Pubmed <ref type="bibr" target="#b27">[28]</ref> for semi-supervised node classification <ref type="bibr" target="#b37">[38]</ref>, where nodes correspond to documents associated with the bag-of-words as the features and edges correspond to citations. For full-supervised node classification, in addition to the three citation networks we include three extra web network datasets, Cornell, Texas, and Wisconsion <ref type="bibr" target="#b23">[24]</ref>, where nodes and edges represent web pages and hyperlinks, and one actor cooccurrence network dataset, Actor <ref type="bibr" target="#b23">[24]</ref>, where nodes and edges represent actors and their co-occurrence in the same movie. <ref type="table" target="#tab_0">Table 1</ref> contains the basic network statistics for each of these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Baselines.</head><p>To evaluate the effectiveness of TDGNN, we choose the following representative supervised node classification baselines including SOTA GNN models.</p><p>? MLP [19]: 2-layer multilayer perceptron with dropout and ReLU non-linearity, which is empirically shown in other works to perform well on non-homophily network datasets <ref type="bibr" target="#b44">[45]</ref>. ? GCN <ref type="bibr" target="#b11">[12]</ref>: GCN is one of the most popular graph convolutional models and our proposed model is modified based on it. ? GAT <ref type="bibr" target="#b30">[31]</ref>: Graph attention network employs attention mechanism to pay different levels of attention to nodes within the neighorhood set, and is widely used as a GNN baseline. ? SGC <ref type="bibr" target="#b33">[34]</ref>: Simple graph convolution network removes nonlinearities and collapsing weight matrices between consecutive layers, which obtains the comparable accuracy and yields orders of magnitude speedup over GCN. We note that SGC collapses the traditional GNN aggregation tree such that the center node receives the features directly from the flattened neighborhood while being weighted according to the higherorder neighborhood information. ? APPNP <ref type="bibr" target="#b12">[13]</ref>: APPNP links GCN and PageRank to derive an improved propagation scheme based on personalized PageRank, which incorporates higher-order neighborhood information and meanwhile keeps the local information. ? Geom-GCN <ref type="bibr" target="#b23">[24]</ref>: Geom-GCN explores to capture long-range dependencies in non-homophily networks. It uses the geometric relationships defined in the latent space to build structural neighorhoods for aggregation. Since Geom-GCN is mainly designed for non-homophily networks, we only report its performance in full-supervised node classification where three non-homophily networks are included.  ? DAGNN <ref type="bibr" target="#b16">[17]</ref>: Deep adaptive graph neural network first decouples the representation transformation from propagation so that large receptive fields can be applied without suffering from performance degradation. Then, it utilizes an adaptive adjustment mechanism, which adaptively balances the information from local and global neighborhoods for each node. ? GCNII <ref type="bibr" target="#b2">[3]</ref>: GCNII employs residual connection to retain part of the information from the previous layer and adds an identity mapping to ensure the non-decreasing performance as the GNN model goes deeper (i.e., successfully adds more layers).</p><p>For baselines that have multiple variants (Geom-GCN, GCNII), we only choose the best for each dataset and denote it as model*.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Parameter Settings.</head><p>We implement our proposed TDGNN and some necessary baselines using Pytorch <ref type="bibr" target="#b22">[23]</ref> and Pytorch Geometric <ref type="bibr" target="#b21">[22]</ref>, a library for deep learning on graph-structured data built upon Pytorch. For DAGNN 1 , and GCNII 2 , we use the original code from the authors' github repository. We aim to provide a rigorous and fair comparison between different models on each dataset by tuning hyperparameters for all models individually. The number of hidden unit is searched from {16, 32, 64, 128}, the dropout rate is searched from {0, 0.5, 0.8}, the weight decay is searched from [1 ?4 , 2 ?2 ], the training epochs is searched from {300, 500, 1000, 1500, 3000, 4000} and the learning rate is set to be 0.01. We find that some baselines even achieve better results than their original reports. Note that in this work, we do not treat the random seed as a hyperparamter and therefore, the random seed fixed in previous models for reproducing results, if any, is reset to be totally random to remove any potential bias and thus allow for more generalized comparison. For reproducibility, codes of all of our models and corresponding hyperparameter configurations for results in <ref type="table" target="#tab_1">Table 2-3 are publicly available 3 .</ref> 1 https://github.com/vthost/DAGNN 2 https://github.com/chennnM/GCNII 3 https://github.com/YuWVandy/TDGNN</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Semi-supervised Node Classification</head><p>For the semi-supervised node classification task, we apply the fixed split following <ref type="bibr" target="#b37">[38]</ref> and random training/validation/testing split on Cora, Citeseer, and Pubmed, with 20 nodes per class for training, 500 nodes for validation and 1000 nodes for testing. For each model, we conduct 100 runs and report the mean classification accuracy with the standard deviation in both the fixed and random splitting cases. <ref type="table" target="#tab_1">Table 2</ref> reports the best mean accuracy with the standard deviation over different data splits where the best model per benchmark is highlighted in bold and the number in parentheses corresponds to layers of neighborhoods utilized at which the best performance is achieved. For example, (0-4) means the corresponding performance is achieved when we use neighborhood of layers up to 4 and 0-layer neighborhoods correspond to using center nodes themselves.</p><p>We observe that TDGNN-s performs the best in terms of the average rank through all datasets and across both random and fixed splits, which suggests the comprehensive superiority of TDGNN-s to other baselines. Specifically, our TDGNN-s model outperforms the representative baselines including GCN, GAT, SGC, and APPNP across all datasets by significant margins. Compared with two recent deep GNN models, DAGNN and GCNII*, TDGNN-s can still achieve the comparable or even better performance. Especially when the data split is random, TDGNN-s outperforms all other models, which demonstrates the strong robustness of TDGNN-s (in terms of dataset splits). It is also worthwhile to note that our TDGNN model achieves the SOTA performance with relatively shallow layers compared with DAGNN and GCNII*. On Cora dataset, the best performance is achieved when layers are used up to 4 and 6 for our TDGNN-s model, respectively, in fixed and random data splitting, while DAGNN and GCNII require up to 10 and 64 layers to achieve the best, which demands heavy computation and thus are time inefficient. On Citeseer dataset, our model also utilizes up to the most shallow layers compared with DAGNN and GC-NII* to achieve the SOTA performance. Surprisingly, the weighted version of our model, TDGNN-w, has poorer performance than TDGNN-s while still outperforms most of the baselines. This is </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Full-supervised Node Classification</head><p>For the full-supervised node classification task, we evaluate our TDGNN model and existing GNNs using 7 datasets: Cora, Citeseer, Pubmed, Cornell, Texas, Wisconsin, and Actor. For each dataset, we use 10 random splits (48%/32%/20% of nodes per class for training/validation/testing) from <ref type="bibr" target="#b23">[24]</ref>  <ref type="bibr" target="#b3">4</ref> . We conduct 100 runs with each split evaluated 10 times and report the mean accuracy with the standard deviation in <ref type="table" target="#tab_2">Table 3</ref>. We note that here the numbers in the parentheses again correspond to layers of neighborhoods utilized (e.g., (0,3-5) means the corresponding performance is achieved when we use neighborhoods of layers 3 to 5 and 0-layer neighborhoods corresponding to the center nodes themselves.) First, we observe from <ref type="table" target="#tab_2">Table 3</ref> that TDGNN-w has the best average rank across the two types of networks (i.e., homophily and heterophily) with TDGNN-s ranks second. Next, we observe that TDGNN-w significantly outperforms the baselines across the heterophily networks. However, both variants of TDGNN are slightly outperformed on the homophily networks in this full-supervised setting (whereas in most homophily networks under the semisupervised setting TDGNN-s performs the best). Thus, to better understand the inner workings of TDGNN, we next perform a detailed parameter analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Parameter Analysis</head><p>Here we compare the performance of TDGNN with other baselines when utilizing neighborhoods in different layers. Furthermore, we perform a parameter analysis of TDGNN by varying the neighborhood layers ( ) and the multi-hop dependencies ( ).</p><p>First, to demonstrate the strength of the TDGNN-s model in shallow layers, we visualize the performance of each model using layers from up to 1 to up to 10 in <ref type="figure" target="#fig_4">Figure 4</ref>. For the Cora and Citeseer datasets, our model achieves around 84% and 73% using only the first 2-layer neighborhoods and the first three-layer neighborhoods, respectively. Compared to two SOTA deep GNNs where DAGNN achieves the same level performance using 5 layers and 7 layers, and GCNII* achieves using 8 layers and at least 32 layers, our model can leverage less neighborhood information to achieve comparable performance, which clearly validates the importance of considering multi-hop dependency. This further to some extent raises the concern over whether we need deep GNNs to incorporate higher-layer neighborhood information in homophily networks, or if shallow feature information aggregated according to higher-order multi-hop dependencies provides sufficient information. Besides, the continued high-level performance as model depth increases demonstrates the higher resilience of TDGNN-s against over-smoothing. Second, we vary the maximum layer of neighborhoods and the multi-hop dependency to study their effect on the performance of the proposed two models: TDGNN-s on two representative homophily networks and TDGNN-w on two representative heterophily networks. Both of the maximum layer of the neighborhoods and the length of the multi-hop dependency are selected from {1, 2, 3, 5, 10} due to the small-world theory that two nodes will be connected through few series of intermediaries <ref type="bibr" target="#b17">[18]</ref>. <ref type="figure" target="#fig_5">Figure 5</ref> visualizes the averaged accuracy across 10 runs for various layers and dependency configurations. For two homophily networks, including extra neighborhood layers significantly increase the model performance for lower-layers and such boosting effect becomes progressively weaker as more and more higher-layer neighborhood layers are included, e.g., the performance increases from 79.85 to 84.00 and from 71.50 to 72.85 for Cora and Citeseer when including the 2 nd -layer neighborhood while only from 84.00 to 85.06 and from 72.85 to 73.28 when including the 3 rd -layer. This weaker boost as the layer number increases is also in line with the decreasing homophily level as observed in <ref type="figure" target="#fig_0">Figure 1</ref>. In comparison, for heterophily networks, in <ref type="figure" target="#fig_5">Figures 5c and 5d</ref> we can observe a more significant need for the decoupling of neighborhood layers since increasing the receptive field (i.e., increasing the maximum layer of neighborhoods) is not always advantageous. Similarly including deeper multi-hop dependencies is not always a clear advantage as seen in the homophily networks because lower-layer neighborhoods that have different labels or representations from their corresponding center nodes may contribute more to their center nodes' prediction through longer dependency. We note that these findings also align with our empirical analysis in <ref type="figure" target="#fig_0">Figure 1</ref>. Therefore, we believe that the increased performance obtained by TDGNN over prior work is partially credited to its ability to separate the concept of graph convolutions in deeper GNNs with higher-layer neighborhoods into both multi-hop dependencies and decoupled neighborhood layers, which can allow any deep GNN model to be more flexibly customized via hyperparameter tuning on a wider variety of complex networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Our work is related to previous work attempting to tackle the oversmoothing problem in GNNs (especially in deeper GNNs), and also related to prior work attempting to alleviate the challenges faced when applying GNNs to non-homophily graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Over-smoothing Problem and Deep GNNs</head><p>Over-smoothing derives from stacking multiple propagation layers resulting in feature information of nodes among different classes becoming indistinguishable. Previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b42">43]</ref> have proven that over-smoothing is a common phenomenon in many GNNs and smoothness among nodes from the same class is helpful for node classification. In <ref type="bibr" target="#b5">[6]</ref>, they discovered that noisy topology information results in feature over-smoothing and may lead to node misclassification. The smoothing in GNNs was later classified into two kinds by the information-to-noise ratio <ref type="bibr" target="#b1">[2]</ref>. In this work, we study the feature smoothing between different layers, show that such type of smoothing inevitably happens when employing an iterative propagation framework and that it can be solved by tree decomposition.</p><p>Deep GNNs are related to over-smoothing and have the primary goal to incorporate higher-layer neighborhood information through iterative propagation. For example, SGC <ref type="bibr" target="#b33">[34]</ref> and S 2 GC <ref type="bibr" target="#b24">[25]</ref> attempt to capture higher-layer neighborhood information by applying th power of the graph convolution in a single neural network layer. APPNP <ref type="bibr" target="#b12">[13]</ref> replaces the power of the graph convolution with the Personalized PageRank <ref type="bibr" target="#b19">[20]</ref> and GDC <ref type="bibr" target="#b13">[14]</ref> further extends APPNP by generalizing Personalized PageRank to an arbitrary graph diffusion process. There are also more recent methods, such as GCNII <ref type="bibr" target="#b2">[3]</ref> and DAGNN <ref type="bibr" target="#b16">[17]</ref>, which we have used as baselines for TDGNN since they have outperformed previously mentioned deep GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">GNNs on Heterophily Networks</head><p>Heterophily has recently been raised as an important issue since it breaks the traditional network homophily assumption that is widely adopted in many GNNs. More specifically, in a heterophily (i.e., nonhomophily) network, the concept that linked nodes are likely from different classes or have dissimilar features is initially recognized within the context of GNNs in <ref type="bibr" target="#b23">[24]</ref>. Zhu et al. <ref type="bibr" target="#b44">[45]</ref> proposes a set of effective designs that allow GNNs to generalize to challenging heterophily settings, and Chen et al. <ref type="bibr" target="#b2">[3]</ref> leverages initial residual connection and identity mapping to enable GCN to express a th order polynomial filter with arbitrary coefficients, which achieves great progress in both homophily and heterophily networks. In comparison, our work demonstrates that the poor performance of GNNs on heterophily networks is caused by feature smoothing between neighborhoods in different layers. By decomposing the computational tree of center nodes and increasing the depth of the GNNs, we can selectively devise suitable layer configurations to boost the model performance on heterophily network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we theoretically analyze the feature smoothing of neighborhoods in different layers and propose a tree decomposition method that disentangles neighborhoods of different layers and thus allows more flexible layer configuration. Moreover, our work provides the first theoretical and empirical analysis that unveils the importance of multi-hop dependency in learning better node representations and discloses its connection with graph diffusion. Based on these insights, we design our Tree Decomposed Graph Neural Network (TDGNN) model with two variants, TDGNNs and TDGNN-w, which simultaneously address the problem of feature smoothing between different layers and incorporate the multi-hop dependency. Extensive experiments demonstrate that TDGNN outperforms representative baselines on a wide range of real-world datasets across network types (including homophily and heterophily) and various node classification task settings.</p><p>For future work, we plan to devise a node-adaptive layer aggregation mechanism which can optimize the configurations of representations from different layers in a node specific way. Such optimization could be realized by applying policy-based reinforcement learning. Furthermore, self-supervised learning (SSL) could be utilized to pre-train the MLP in our framework, which could enhance the capability of our model to embed more useful feature and topology information from diverse datasets, since SSL has recently been shown effective on GNNs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualizing the variance of homophily across neighborhoods at different levels according to the distribution of the ratio of different layer neighborhoods in the same class as their corresponding center nodes (i.e., (a) and (c)) and the cosine similarity of their embeddings (obtained from feeding node features through only the transformation layers of a pre-trained 2-layer GCN) to their center nodes (i.e., (b) and (d)) for the Texas and Cora datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) and Figure 1(c)) and have similar embeddings to (in Figure 1(b) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) andFigure 1(b). Even for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Tree decomposition of the center node 1 in the given graph to two layers compared to the computational graph in the original GNNs (e.g., GCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Results of models with different layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>(a) Cora (TDGNN-s) (b) Citeseer (TDGNN-s) (c) Cornell (TDGNN-w) (d) Texas (TDGNN-w) Visualizing the effect of varying the maximum layer neighborhoods and the length of mutli-hop dependency on the performance of TDGNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of datasets.</figDesc><table><row><cell>Networks</cell><cell></cell><cell cols="5">Nodes Edges Features Classes Train/Val/Test</cell><cell>Type</cell></row><row><cell></cell><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>1433</cell><cell>7</cell><cell>140/500/1000</cell><cell>Citation network</cell></row><row><cell>Homophily</cell><cell>Citeseer</cell><cell>3327</cell><cell>4732</cell><cell>3703</cell><cell>6</cell><cell>120/500/1000</cell><cell>Citation network</cell></row><row><cell></cell><cell>Pubmed</cell><cell cols="2">19717 44338</cell><cell>500</cell><cell>3</cell><cell>60/500/1000</cell><cell>Citation network</cell></row><row><cell></cell><cell>Cornell</cell><cell>183</cell><cell>295</cell><cell>1703</cell><cell>5</cell><cell>48%/32%/20%</cell><cell>Webpage network</cell></row><row><cell>Non-</cell><cell>Texas</cell><cell>183</cell><cell>309</cell><cell>1703</cell><cell>5</cell><cell>48%/32%/20%</cell><cell>Webpage network</cell></row><row><cell>homophily</cell><cell>Wisconsin</cell><cell>251</cell><cell>499</cell><cell>1703</cell><cell>5</cell><cell>48%/32%/20%</cell><cell>Webpage network</cell></row><row><cell></cell><cell>Actor</cell><cell cols="2">7600 33544</cell><cell>931</cell><cell>5</cell><cell cols="2">48%/32%/20% Actor co-occurrence network</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of semi-supervised classification accuracy (%) ? stdev over Cora, Citeseer, and Pubmed datasets.</figDesc><table><row><cell>Method</cell><cell>Fixed</cell><cell cols="2">Cora</cell><cell>Random</cell><cell>Fixed</cell><cell cols="2">Citeseer</cell><cell>Random</cell><cell>Fixed</cell><cell cols="2">Pubmed</cell><cell>Random</cell><cell>Avg. Rank</cell></row><row><cell>GCN</cell><cell cols="2">81.50?0.79 (0-2)</cell><cell></cell><cell>79.91?1.64 (0-2)</cell><cell cols="2">71.42?0.48 (0-2)</cell><cell cols="2">68.78?2.01 (0-2)</cell><cell cols="2">79.12?0.46 (0-2)</cell><cell>77.84?2.36 (0-2)</cell><cell>7.17</cell></row><row><cell>GAT</cell><cell cols="2">83.10?0.40 (0-2)</cell><cell></cell><cell>80.80?1.60 (0-2)</cell><cell cols="2">70.80?0.50 (0-2)</cell><cell cols="2">68.90?1.70 (0-2)</cell><cell cols="2">79.10?0.40 (0-2)</cell><cell>77.80?2.10 (0-2)</cell><cell>7.00</cell></row><row><cell>SGC</cell><cell cols="2">82.63?0.01 (0-2)</cell><cell></cell><cell>80.18?1.57 (0-2)</cell><cell cols="2">72.10?0.14 (0-2)</cell><cell cols="2">69.33?1.90 (0-2)</cell><cell cols="2">79.12?0.10 (0-2)</cell><cell>76.74?2.84 (0-2)</cell><cell>6.83</cell></row><row><cell>APPNP</cell><cell cols="2">83.34?0.56 (0-10)</cell><cell cols="2">82.26?1.39 (0-10)</cell><cell cols="2">72.22?0.50 (0-10)</cell><cell cols="2">70.53?1.57 (0-10)</cell><cell cols="2">80.14?0.24 (0-10)</cell><cell>79.54?2.23 (0-10)</cell><cell>3.83</cell></row><row><cell>DAGNN</cell><cell cols="2">84.88?0.49 (0-10)</cell><cell cols="2">83.47?1.18 (0-10)</cell><cell cols="2">73.39?0.57 (0-9)</cell><cell cols="2">70.87?1.44 (0-10)</cell><cell cols="3">80.51?0.42 (0-20) 79.52?2.19 (0-20)</cell><cell>2.33</cell></row><row><cell>GCNII*</cell><cell cols="4">85.57?0.45 (0-64) 82.58?1.68 (0-64)</cell><cell cols="2">73.24?0.61 (0-32)</cell><cell cols="2">70.04?1.72 (0-10)</cell><cell cols="2">80.00?0.48 (0-16)</cell><cell>79.03?1.68 (0-16)</cell><cell>3.83</cell></row><row><cell>TDGNN-s</cell><cell cols="2">85.35?0.49 (0-4)</cell><cell cols="6">83.84?1.45 (0-6) 73.78?0.60 (0-8) 71.27?1.71 (0-8)</cell><cell cols="2">80.20?0.33 (0-5)</cell><cell>80.01?1.96 (0-5)</cell><cell>1.33</cell></row><row><cell>TDGNN-w</cell><cell cols="2">84.42?0.59 (0-4)</cell><cell></cell><cell>83.43?1.35 (0-6)</cell><cell cols="2">72.14?0.49 (0-6)</cell><cell cols="2">70.32?1.57 (0-6)</cell><cell cols="2">80.12?0.44 (0-5)</cell><cell>79.77?2.04 (0-5)</cell><cell>3.67</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Summary of full-supervised classification accuracy (%) ? stdev over 8 datasets. We reuse the results reported in<ref type="bibr" target="#b36">[37]</ref> for Geom-GCN. 'N/A' indicate the corresponding layers are not reported in the paper. because the weight coefficients { 0 , 1 , ..., } are only decided by training nodes and the suitable weights for combining aggregated features {H 0 , H 1 , ..., H } and getting good predictions on training and validation nodes might not be suitable for testing nodes, which inspires future work for a layer aggregation mechanism that enables node-adaptive layer combination.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Cite.</cell><cell>Pub.</cell><cell>Corn.</cell><cell>Tex.</cell><cell>Wisc.</cell><cell>Act.</cell><cell>Avg. Rank</cell></row><row><cell>MLP</cell><cell>75.78?1.84 (0)</cell><cell>73.81? 1.74 (0)</cell><cell>86.90?0.37 (0)</cell><cell>80.97?6.33 (0)</cell><cell>81.32? 4.19 (0)</cell><cell>85.38?3.95 (0)</cell><cell>36.60?1.25 (0)</cell><cell>5.57</cell></row><row><cell>GCN</cell><cell>86.97?1.32 (0-2)</cell><cell>76.37?1.47 (0-2)</cell><cell>88.19?0.48 (0-2)</cell><cell>58.57?3.57 (0-2)</cell><cell>58.68?4.64 (0-2)</cell><cell>53.14?6.25 (0-2)</cell><cell>28.65?1.38 (0-2)</cell><cell>8.14</cell></row><row><cell>GAT</cell><cell>87.30?1.01 (0-2)</cell><cell>75.55?1.32 (0-2)</cell><cell>85.33?0.48 (0-2)</cell><cell>61.89?5.05 (0-2)</cell><cell>58.38?6.63 (0-2)</cell><cell>55.29?4.09 (0-2)</cell><cell>28.45?0.89 (0-2)</cell><cell>8.00</cell></row><row><cell>SGC</cell><cell>87.07?1.20 (0-2)</cell><cell>76.01?1.78 (0-2)</cell><cell>85.11?0.52 (0-2)</cell><cell>58.68?3.75 (0-2)</cell><cell>60.43?5.11 (0-2)</cell><cell>53.49?5.13 (0-2)</cell><cell>27.46?1.46 (0-2)</cell><cell>8.57</cell></row><row><cell cols="4">Geom-GCN* 85.35?1.57 (0-2) 78.02?1.15 (0-2) 89.95?0.47 (N/A)</cell><cell>60.54?3.67 (0-2)</cell><cell>66.76?2.72 (N/A)</cell><cell>64.51?3.66 (N/A)</cell><cell>31.63?1.15 (N/A)</cell><cell>5.86</cell></row><row><cell>APPNP</cell><cell cols="3">86.76?1.74 (0-10) 77.08?1.56 (0-10) 88.45?0.42 (0-10)</cell><cell>74.59?5.11 (0-10)</cell><cell>74.30?4.74 (0-10)</cell><cell>81.10?2.93 (0-10)</cell><cell>34.36?1.09 (0-10)</cell><cell>5.43</cell></row><row><cell>DAGNN</cell><cell cols="3">87.26?1.42 (0-10) 76.47?1.54 (0-10) 87.49?0.63 (0-20)</cell><cell>80.97?6.33 (0)</cell><cell>81.32?4.19 (0)</cell><cell>85.38?3.95 (0)</cell><cell>36.60?1.25 (0)</cell><cell>4.71</cell></row><row><cell>GCNII*</cell><cell cols="4">88.27?1.31 (0-64) 77.06?1.67 (0-64) 90.26?0.41 (0-64) 76.70?5.40 (0-16)</cell><cell>77.08?5.84 (0-32)</cell><cell>80.94?4.94 (0-16)</cell><cell>35.18?1.30 (0-64)</cell><cell>3.71</cell></row><row><cell>TDGNN-s</cell><cell>88.26?1.32 (0-4)</cell><cell>76.64?1.54 (0-8)</cell><cell>89.13?0.39 (0-1)</cell><cell>80.97?6.33 (0)</cell><cell cols="3">82.95?4.59 (0, 4-5) 85.47?3.88 (0, 4-5) 36.70?1.28 (0, 3-4)</cell><cell>2.86</cell></row><row><cell>TDGNN-w</cell><cell>88.01?1.32 (0-5)</cell><cell>76.58?1.40 (0-2)</cell><cell cols="5">89.22?0.41 (0-1) 82.92?6.61 (0, 2-6) 83.00?4.50 (0, 2) 85.57?3.78 (0, 3-5) 37.11?0.96 (0, 3-4)</cell><cell>2.14</cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Note that although<ref type="bibr" target="#b23">[24]</ref> reports that the ratios are 60%/20%/20%, but this is different from the actual data splits shared on their GitHub<ref type="bibr" target="#b36">[37]</ref>.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Multi-Scale Approach for Graph Link Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring and Relieving the Over-Smoothing Problem for Graph Neural Networks from the Topological View</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-02-07" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Adaptive Universal Generalized PageRank Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olgica</forename><surname>Milenkovic</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Matrix multiplication via arithmetic progressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Don</forename><surname>Coppersmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shmuel</forename><surname>Winograd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the nineteenth annual ACM symposium on Theory of computing</title>
		<meeting>the nineteenth annual ACM symposium on Theory of computing</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Batch virtual adversarial training for graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.09192</idno>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Epidemic graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="160" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph Random Neural Networks for Semi-Supervised Learning on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-12-06" />
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artifical Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1" to="159" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haochen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10141</idno>
		<title level="m">Self-supervised learning on graphs: Deep insights and new direction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diffusion Improves Graph Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wei?enberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deeper Insights Into Graph Convolutional Networks for Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-Second AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Ming</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards Deeper Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD &apos;20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The small world problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Milgram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychology today</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="60" to="67" />
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multilayer perceptrons for classification and regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fionn</forename><surname>Murtagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="183" to="197" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The PageRank citation ranking: Bringing order to the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Page</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Brin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Motwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Winograd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Stanford InfoLab</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horng</forename><surname>Duen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.01703</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga ; Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems (NeurIPS) 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
	<note>Alban Desmaison</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Geom-GCN: Geometric Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingzhe</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Chen-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A Simple Spectral Failure Mode for Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carey</forename><forename type="middle">E</forename><surname>Priebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cencheng</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13152</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep graph learning: Foundations, advances and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingfei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3555" to="3556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On Proximity and Structural Role-based Embeddings in Networks: Misconceptions, Techniques, and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungchul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nesreen</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John Boaz</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">37</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gaussian elimination is not optimal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Volker Strassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische mathematik</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="354" to="356" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep Graph Infomax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Graph Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Direct Multi-hop Attention based Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14332</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Graph Neural Networks: Self-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Neural Networks: Foundations, Frontiers, and Applications, Lingfei Wu</title>
		<editor>Peng Cui, Jian Pei, and Liang Zhao</editor>
		<meeting><address><addrLine>Singapore, Chapter</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="391" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">How Powerful are Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><forename type="middle">Jegelka</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Representation Learning on Graphs with Jumping Knowledge Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomohiro</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning<address><addrLine>Stockholmsm?ssan, Stockholm, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06462</idno>
		<title level="m">Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Revisiting semisupervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hierarchical Graph Representation Learning with Differentiable Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fast sparse matrix multiplication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Yuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Zwick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions On Algorithms (TALG)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2" to="13" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajitesh</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajgopal</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viktor</forename><surname>Prasanna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.01380</idno>
		<title level="m">Deep Graph Neural Networks with Shallow Subgraph Samplers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Link Prediction Based on Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nedim</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nesreen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.13566</idno>
		<title level="m">Graph Neural Networks with Heterophily</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Beyond Homophily in Graph Neural Networks: Current Limitations and Effective Designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leman</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
