<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fake it till you make it: face analysis in the wild using synthetic data alone</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erroll</forename><surname>Wood</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charlie</forename><surname>Hewitt</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Dziadzio</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Estellers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">J</forename><surname>Cashman</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Shotton</forename><surname>Microsoft</surname></persName>
						</author>
						<title level="a" type="main">Fake it till you make it: face analysis in the wild using synthetic data alone</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We demonstrate that it is possible to perform face-related computer vision in the wild using synthetic data alone. The community has long enjoyed the benefits of synthesizing training data with graphics, but the domain gap between real and synthetic data has remained a problem, especially for human faces. Researchers have tried to bridge this gap with data mixing, domain adaptation, and domain-adversarial training, but we show that it is possible to synthesize data with minimal domain gap, so that models trained on synthetic data generalize to real in-the-wild datasets. We describe how to combine a procedurally-generated parametric 3D face model with a comprehensive library of hand-crafted assets to render training images with unprecedented realism and diversity. We train machine learning systems for face-related tasks such as landmark localization and face parsing, showing that synthetic data can both match real data in accuracy as well as open up new approaches where manual labeling would be impossible. * Denotes equal contribution. https://microsoft.github.io/FaceSynthetics</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>When faced with a machine learning problem, the hardest challenge often isn't choosing the right machine learning model, it's finding the right data. This is especially difficult in the realm of human-related computer vision, where concerns about the fairness of models and the ethics of deployment are paramount <ref type="bibr" target="#b30">[31]</ref>. Instead of collecting and labelling real data, which is slow, expensive, and subject to bias, it can be preferable to synthesize training data using computer graphics <ref type="bibr" target="#b66">[68]</ref>. With synthetic data, you can guarantee perfect labels without annotation noise, generate rich labels that are otherwise impossible to label by hand, and have full control over variation and diversity in a dataset.</p><p>Rendering convincing humans is one of the hardest problems in computer graphics. Movies and video games have shown that realistic digital humans are possible, but with <ref type="bibr">Figure 1</ref>. We render training images of faces with unprecedented realism and diversity. The first example above is shown along with 3D geometry and accompanying labels for machine learning. significant artist effort per individual <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b25">26]</ref>. While it's possible to generate endless novel face images with recent self-supervised approaches <ref type="bibr" target="#b26">[27]</ref>, corresponding labels for supervised learning are not available. As a result, previous work has resorted to synthesizing facial training data with simplifications, with results that are far from realistic. We have seen progress in efforts that attempt to cross the domain gap using domain adaptation <ref type="bibr" target="#b58">[60]</ref> by refining synthetic images to look more real, and domain-adversarial training <ref type="bibr" target="#b12">[13]</ref> where machine learning models are encouraged to ignore differences between the synthetic and real domains, but less work has attempted to improve the quality of synthetic data itself. Synthesizing realistic face data has been considered so hard that we encounter the assumption that synthetic data cannot fully replace real data for problems in the wild <ref type="bibr" target="#b58">[60]</ref>.</p><p>In this paper we demonstrate that the opportunities for synthetic data are much wider than previously realised, and are achievable today. We present a new method of acquiring training data for faces -rendering 3D face models with an unprecedented level of realism and diversity (see <ref type="figure">Figure 1</ref>). With a sufficiently good synthetic framework, it is possible arXiv:2109.15102v2 [cs.CV] 5 Oct 2021</p><p>Template face + identity + expression + texture + hair + clothes + environment <ref type="figure">Figure 2</ref>. We procedurally construct synthetic faces that are realistic and expressive. Starting with our template face, we randomize the identity, choose a random expression, apply a random texture, attach random hair and clothing, and render the face in a random environment.</p><p>to create training data that can be used to solve real world problems in the wild, without using any real data at all.</p><p>It requires considerable expertise and investment to develop a synthetics framework with minimal domain gap. However, once implemented, it becomes possible to generate a wide variety of training data with minimal incremental effort. Let's consider some examples; say you have spent time labelling face images with landmarks. However, you suddenly require additional landmarks in each image. Relabelling and verifying will take a long time, but with synthetics, you can regenerate clean and consistent labels at a moment's notice. Or, say you are developing computer vision algorithms for a new camera, e.g. an infrared facerecognition camera in a mobile phone. Few, if any, hardware prototypes may exist, making it hard to collect a dataset. Synthetics lets you render faces from a simulated device to develop algorithms and even guide hardware design itself.</p><p>We synthesize face images by procedurally combining a parametric face model with a large library of high-quality artist-created assets, including textures, hair, and clothing (see <ref type="figure">Figure 2</ref>). With this data we train models for common face-related tasks: face parsing and landmark localization. Our experiments show that models trained with a single generic synthetic dataset can be just as accurate as those trained with task-specific real datasets, achieving results in line with the state of the art. This opens the door to other face-related tasks that can be confidently addressed with synthetic data instead of real.</p><p>Our contributions are as follows. First, we describe how to synthesize realistic and diverse training data for face analysis in the wild, achieving results in line with the state of the art. Second, we present ablation studies that validate the steps taken to achieve photorealism. Third is the synthetic dataset itself, which is available from our project webpage: https://microsoft.github.io/FaceSynthetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Diverse face datasets are very difficult to collect and annotate. Collection techniques such as web crawling pose significant privacy and copyright concerns. Manual annota-tion is error-prone and can often result in inconsistent labels. Hence, the research community is increasingly looking at augmenting or replacing real data with synthetic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Synthetic face data</head><p>The computer vision community has used synthetic data for many tasks, including object recognition <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b71">73]</ref>, scene understanding <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref>, eye tracking <ref type="bibr" target="#b61">[63,</ref><ref type="bibr" target="#b66">68]</ref>, hand tracking <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b59">61]</ref>, and full-body analysis <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b63">65]</ref>. However, relatively little previous work has attempted to generate full-face synthetics using computer graphics, due to the complexity of modeling the human head.</p><p>A common approach is to use a 3D Morphable Model (3DMM) <ref type="bibr" target="#b4">[5]</ref>, since these can provide consistent labels for different faces. Previous work has focused on parts of the face such as the eye region <ref type="bibr" target="#b60">[62]</ref> or the hockey mask <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b74">76]</ref>. Zeng et al. <ref type="bibr" target="#b74">[76]</ref>, Richardson et al. <ref type="bibr" target="#b45">[46]</ref>, and Sela et al. <ref type="bibr" target="#b56">[58]</ref> used 3DMMs to render training data for reconstructing detailed facial geometry. Similarly, Wood et al. <ref type="bibr" target="#b67">[69]</ref> rendered an eye region 3DMM for gaze estimation. However, since these approaches only render part of the face, the resulting data has limited use for tasks that consider the whole face.</p><p>Building parametric models is challenging, so an alternative is to render 3D scans directly <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b66">68]</ref>. Jeni et al. <ref type="bibr" target="#b23">[24]</ref> rendered the BU-4DFE dataset <ref type="bibr" target="#b72">[74]</ref> for dense 3D face alignment, and Kuhnke and Ostermann <ref type="bibr" target="#b29">[30]</ref> rendered commercially-available 3D head scans for head pose estimation. While often realistic, these approaches are limited by the diversity expressed in the scans themselves, and cannot provide rich semantic labels for machine learning.</p><p>Manipulating 2D images can be an alternative to using a 3D graphics pipeline. Zhu et al. <ref type="bibr" target="#b77">[79]</ref> fit a 3DMM to face images, and warped them to augment the head pose. Nojavanasghari et al. <ref type="bibr" target="#b41">[42]</ref> composited hand images onto faces to improve face detection. These approaches can only make minor adjustments to existing images, limiting their use.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Training with synthetic data</head><p>Although it is common to rely on synthetic data alone for full-body tasks <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b57">59]</ref>, synthetic data is rarely used on its own for face-related machine learning. Instead it is either first adapted to make it look more like some target domain, or used alongside real data for pre-training <ref type="bibr" target="#b74">[76]</ref> or regularizing models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>. The reason for this is the domain gap -a difference in distributions between real and synthetic data which makes generalization difficult <ref type="bibr" target="#b24">[25]</ref>.</p><p>Learned domain adaptation modifies synthetic images to better match the appearance of real images. Shrivastava et al. <ref type="bibr" target="#b58">[60]</ref> use an adversarial refiner network to adapt synthetic eye images with regularization to preserve annotations. Similarly, Bak et al. <ref type="bibr" target="#b2">[3]</ref> adapt synthetic data using a Cycle-GAN <ref type="bibr" target="#b75">[77]</ref> with a regularization term for preserving identities. A limitation of learned domain adaptation is the tendency for image semantics to change during adaptation <ref type="bibr" target="#b14">[15]</ref>, hence the need for regularization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b58">60]</ref>. These techniques are therefore unsuitable for fine-grained annotations, such as per-pixel labels or precise landmark coordinates.</p><p>Instead of adapting data, it is possible to learn features that are resistant to the differences between domains <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b55">57]</ref>. Wu et al. <ref type="bibr" target="#b69">[71]</ref> mix real and synthetic data through a domain classifier to learn domain-invariant features for text detection, and Saleh et al. <ref type="bibr" target="#b54">[56]</ref> exploit the observation that shape is less affected by the domain gap than appearance for scene semantic segmentation.</p><p>In our work, we do not perform any of these techniques and instead minimize the domain gap at the source, by generating highly realistic synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Synthesizing face images</head><p>The Visual Effects (VFX) industry has developed many techniques for convincing audiences that 3D faces are real, and we build upon these in our approach. However, a key difference is scale: while VFX might be used for a handful of actors, we require diverse training data of thousands of synthetic individuals. To address this, we use procedural generation to randomly create and render novel 3D faces without any manual intervention.</p><p>We start by sampling a generative 3D face model that captures the diversity of the human population. We then randomly 'dress up' each face with samples from large collections of hair, clothing, and accessory assets. All collections are sampled independently to create synthetic individuals who are as diverse as possible from one another. This section describes the technical components we built in order to enable asset collections that can be mixed-and-matched atop 3D faces in a random, yet plausible manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D face model</head><p>Our generative 3D face model captures how face shape varies across the human population, and changes during facial expressions. It is a blendshape-based face rig similar to previous work <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b33">34]</ref>, and comprises a mesh of N = 7, 667 $JH\HDUV )HPDOH 0DOH where L(X, ?, J; W) is a standard linear blend skinning (LBS) function <ref type="bibr" target="#b32">[33]</ref> that rotates vertex positions X ? R N ?3 about joint locations J ? R K?3 by local joint rotations ?, with per-vertex weights W ? R K?N determining how rotations are interpolated across the mesh. T ( ?, ?) : R | ?|?| ?| ? R N ?3 constructs a face mesh in the bind pose by adding displacements to the template mesh T ? R N ?3 , which represents the average face with neutral expression:</p><formula xml:id="formula_0">: K LW H $ V LD Q % OD F N + LV S D Q LF ,Q G LD Q $ U D E )HPDOH 0DOH</formula><formula xml:id="formula_1">T ( ?, ?) j k = T j k + ? i S ij k + ? i E ij k</formula><p>given linear identity basis S ? R | ?|?N ?3 and expression basis E ? R | ?|?N ?3 . Note the use of Einstein summation notation in this definition and below. Finally, J ( ?) : R | ?| ? R K?3 moves the template joint locations J ? R K?3 to account for changes in identity:</p><formula xml:id="formula_2">J ( ?) j k = J j k + W j l ? i S il k .</formula><p>We learn the identity basis S from high quality 3D scans of M = 511 individuals with neutral expression. Each scan Raw Clean <ref type="figure">Figure 5</ref>. We manually "clean" raw high-resolution 3D head scans to remove noise and hair. We use the resulting clean scans to build our generative geometry model and texture library. was cleaned (see <ref type="figure">Figure 5</ref>), and registered to the topology of T using commercial software [52], resulting in training dataset V ? R M ?3N . We then jointly fit identity basis S and parameters [ ? 1 , . . . , ? M ] to V. In order to generate novel face shapes, we fit a multivariate normal distribution to the fitted identity parameters, and sample from it (see <ref type="figure" target="#fig_0">Figure 3</ref>). As is common in computer animation, both expression basis E and skinning weights W were authored by an artist, and are kept fixed while learning S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Expression</head><p>We apply random expressions to each face so that our downstream machine learning models are robust to facial motion. We use two sources of facial expression. Our primary source is a library of 27,000 expression parameters { ? i } built by fitting a 3D face model to a corpus of 2D images with annotated face landmarks. However, since the annotated landmarks are sparse, it is not possible to recover all types of expression from these landmarks alone, e.g. cheek puffs. Therefore, we additionally sample expressions from a manually animated sequence that was designed to fill the gaps in our expression library by exercising the face in realistic, but extreme ways. <ref type="figure" target="#fig_2">Figure 6</ref> shows samples from our expression collection. In addition to facial expression, we layer random eye gaze directions on top of sampled expressions, and use procedural logic to pose the eyelids accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Texture</head><p>Synthetic faces should look realistic even when viewed at extremely close range, for example by an eye-tracking camera in a head-mounted device. To achieve this, we collected 200 sets of high resolution (8192?8192 px) textures   from our cleaned face scans. For each scan, we extract one albedo texture for skin color, and two displacement maps (see <ref type="figure" target="#fig_4">Figure 7</ref>). The coarse displacement map encodes scan geometry that is not captured by the sparse nature of our vertex-level identity model. The meso-displacement map approximates skin-pore level detail and is built by high-pass filtering the albedo texture, assuming that dark pixels correspond to slightly recessed parts of the skin. Unlike previous work <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b74">76]</ref>, we do not build a generative model of texture, as such models struggle to faithfully produce high-frequency details like wrinkles and pores. Instead, we simply pick a corresponding set of albedo and displacement textures from each scan. The textures are combined in a physically-based skin material featuring subsurface scattering <ref type="bibr" target="#b8">[9]</ref>. Finally, we optionally apply makeup effects to simulate eyeshadow, eyeliner and mascara.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Hair</head><p>In contrast to other work which approximates hair with textures or coarse geometry <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b53">55]</ref>, we represent hair as individual 3D strands, with a full head of hair comprising over 100,000 strands. Modelling hair at the strand level allows us to capture realistic multi-path illumination effects. Shown in <ref type="figure" target="#fig_5">Figure 8</ref>, our hair library includes 512 scalp hair styles, 162 eyebrows, 142 beards, and 42 sets of eyelashes. Each asset was authored by a groom artist who specializes in creating digital hair. At render time, we randomly combine scalp, eyebrow, beard, and eyelash grooms.</p><p>We use a physically-based procedural hair shader to ac- <ref type="figure">Figure 9</ref>. Each face is dressed in a random outfit assembled from our digital wardrobe -a collection of diverse 3D clothing and accessory assets that can be fit around our 3D head model. <ref type="figure">Figure 10</ref>. We use HDRIs to illuminate the face. The same face can look very different under different illumination.</p><p>curately model the complex material properties of hair <ref type="bibr" target="#b7">[8]</ref>. This shader allows us to control the color of the hair with parameters for melanin <ref type="bibr" target="#b37">[38]</ref> and grayness, and even lets us dye or bleach the hair for less common hair styles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Clothing</head><p>Images of faces often include what someone is wearing, so we dress our faces in 3D clothing. Our digital wardrobe contains 30 upper-body outfits which were manually created using clothing design and simulation software <ref type="bibr" target="#b9">[10]</ref>. As shown in <ref type="figure">Figure 9</ref>, these outfits include formal, casual, and athletic clothing. In addition to upper-body garments, we dress our faces in headwear (36 items), facewear (7 items) and eyewear (11 items) including helmets, head scarves, face masks, and eyeglasses. All clothing items were authored on an unclothed body mesh with either the average male or female body proportions <ref type="bibr" target="#b36">[37]</ref> in a relaxed stance.</p><p>We deform garments with a non-rigid cage-based deformation technique [2] so they fit snugly around different shaped faces. Eyeglasses are rigged with a skeleton, and posed using inverse kinematics so the temples and nosebridge rest on the corresponding parts of the face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Rendering</head><p>We render face images with Cycles, a photorealistic raytracing renderer <ref type="bibr" target="#b5">[6]</ref>. We randomly position a camera around the head, and point it towards the face. The focal length and depth of field are varied to simulate different cameras and lenses. We employ image-based lighting <ref type="bibr" target="#b10">[11]</ref> with high <ref type="figure">Figure 11</ref>. Examples of synthetic faces that we randomly generated and rendered for use as training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Albedo</head><p>Normals Depth Mask UVs Vertices <ref type="figure">Figure 12</ref>. We also synthesize labels for machine learning. Above are additional label types beyond those shown in <ref type="figure">Figure 1</ref>.</p><p>dynamic range images (HDRI) to illuminate the face and provide a background (see <ref type="figure">Figure 10</ref>). For each image, we randomly pick from a collection of 448 HDRIs that include a range of different environments <ref type="bibr" target="#b73">[75]</ref>. See <ref type="figure">Figure 11</ref> for examples of faces rendered with our framework. In addition to rendering color images, we generate ground truth labels (see <ref type="figure">Figure 12</ref>). While our experiments in section 4 focus on landmark and segmentation annotations, synthetics lets us easily create a variety of rich and accurate labels that enable new face-related tasks (see subsection 4.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Face analysis</head><p>We evaluate our synthetic data on two common face analysis tasks: face parsing and landmark localization. We show that models trained on our synthetic data demonstrate competitive performance to the state of the art. Note that all evaluations using our models are cross-dataset -we train purely on synthetic data and test on real data, while the state of the art evaluates within-dataset, allowing the models to learn potential biases in the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training methodology</head><p>We render a single training dataset for both landmark localization and face parsing, comprising 100,000 images at 512?512 resolution. It took 48 hours to render using 150 NVIDIA M60 GPUs.</p><p>During training, we perform data augmentation including rotations, perspective warps, blurs, modulations to brightness and contrast, addition of noise, and conversion to grayscale. Such augmentations are especially important for synthetic images which are otherwise free of imperfection (see subsection 4.4). While some of these could be done at render time, we perform them at training time in order to randomly </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Face parsing Label adaptation</head><p>GT label GT label <ref type="figure" target="#fig_0">Figure 13</ref>. We train a face parsing network (using synthetic data only) followed by a label adaptation network to address systematic differences between synthetic and human-annotated labels.</p><p>apply different augmentations to the same training image. We implemented neural networks with PyTorch <ref type="bibr" target="#b42">[43]</ref>, and trained them with the Adam optimizer <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Face parsing</head><p>Face parsing assigns a class label to each pixel in an image, e.g. skin, eyes, mouth, or nose. We evaluate our synthetic training data on two face parsing datasets: Helen <ref type="bibr" target="#b31">[32]</ref> is the best-known benchmark in the literature. It contains 2,000 training images, 230 validation images, and 100 testing images, each with 11 classes. Due to labelling errors in the original dataset, we use Helen* <ref type="bibr" target="#b34">[35]</ref>, a popular rectified version of the dataset which features corrected training labels, but leaves testing labels unmodified for a fair comparison. LaPa <ref type="bibr" target="#b35">[36]</ref> is a recently-released dataset which uses the same labels as Helen, but has more images, and exhibits more challenging expressions, poses, and occlusions. It contains 18,176 training images, 2,000 validation images and 2,000 testing images.</p><p>As is common <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, we use the provided 2D landmarks to align faces before processing. We scale and crop each image so the landmarks are centered in a 512?512px region of interest. Following prediction, we undo this transform to compute results against the original label annotation, without any resizing or cropping.</p><p>Method We treat face parsing as image-to-image translation. Given an input color image x containing C classes, we wish to predict a C-channel label image? of the same spatial dimensions that matches the ground truth label image y. Pixels in y are one-hot encoded with the index of the true class. For this, we use a UNet <ref type="bibr" target="#b48">[49]</ref> with ResNet-18 encoder <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b70">72]</ref>. We train this network with synthetic data only, minimizing a binary cross-entropy (BCE) loss between predicted and ground truth label images. Note that there is nothing novel about our choice of architecture or loss function, this is a well-understood approach for this task.</p><p>Label adaptation. There are bound to be minor systematic differences between synthetic labels and humanannotated labels. For example, where exactly is the boundary between the nose and the rest of the face <ref type="table">? To evaluate   Input  Trained with  + label  Trained with  Ground  (LaPa)</ref> synth. data adaptation real data truth <ref type="figure" target="#fig_1">Figure 14</ref>. Face parsing results by networks trained with synthetic data (with and without label adaptation) and real data. Label adaptation addresses systematic differences between synthetic and real labels, e.g. the shape of the nose class, or granularity of hair.</p><p>our synthetic data without needing to carefully tweak our synthetic label generation process for a specific real dataset, we use label adaptation. Label adaptation transforms labels predicted by our face parsing network (trained with synthetic data alone) into labels that are closer to the distribution in the real dataset (see <ref type="figure" target="#fig_0">Figure 13</ref>). We treat label adaptation as another image-to-image translation task, and use a UNet with ResNet18 encoder <ref type="bibr" target="#b70">[72]</ref>. To ensure this stage is not able to 'cheat', it is trained only on pairs of predicted labels? and ground truth labels y. It is trained entirely separately from the face parsing network, and never sees any real images.</p><p>Results See <ref type="table">Tables 1 and 2</ref> for comparisons against the state of the art, and <ref type="figure" target="#fig_1">Figure 14</ref> for some example predictions. Although networks trained with our generic synthetic data do not outperform the state of the art, it is notable that they achieve similar results to previous work trained withindataset on task-specific data.</p><p>Comparison to real data. We also trained a network on the training portion of each real dataset to separate our training methodology from our synthetic data, presented as "Ours (real)" in <ref type="table">Tables 1 and 2</ref>. It can be seen that training with synthetic data alone produces comparable results to training with real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Landmark localization</head><p>Landmark localization finds the position of facial points of interest in 2D. We evaluate our approach on the 300W [53] dataset, which is split into common (554 images), challenging (135 images) and private (600 images) subsets.</p><p>Method We train a ResNet34 <ref type="bibr" target="#b20">[21]</ref> with mean squared error loss to directly predict 68 2D landmark coordinates per-image. We use the provided bounding boxes to extract a 256?256 pixel region-of-interest from each image. The <ref type="table">Table 1</ref>. A comparison with the state of the art on the Helen dataset, using F1 score. As is common, scores for hair and other fine-grained categories are omitted to aid comparison to previous work. The overall score is computed by merging the nose, brows, eyes, and mouth categories. Training with our synthetic data achieves results in line with the state of the art, trained with real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Skin   private set has no bounding boxes, so we use a tight crop around landmarks. Label adaptation is performed using a two-layer perceptron to address systematic differences between synthetic and real landmark labels ( <ref type="figure" target="#fig_6">Figure 15</ref>). This network is never exposed to any real images during training.</p><p>Results As evaluation metrics we use: Normalized Mean Error (NME) <ref type="bibr" target="#b51">[53]</ref> -normalized by inter-ocular outer eye distance; and Failure Rate below a 10% error threshold (FR 10% ). See <ref type="table">Table 3</ref> for comparisons against state of the art on 300W dataset. It is clear that the network trained with our synthetic data can detect landmarks with accuracy comparable <ref type="table">Table 3</ref>. Landmark localization results on the common, challenging, and private subsets of 300W. Lower is better in all cases. Note that 0.5 FR rate translates to 3 images, while 0.17 corresponds to 1. to recent methods trained with real data. Comparison to real data We apply our training methodology (including data augmentations and label adaptation) to the the training and validation portions of the 300W dataset, to more directly compare real and synthetic data. <ref type="table">Table 3</ref> clearly shows that training with synthetic data leads to better results, even when comparing to a model trained on real data and evaluated within-dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We investigate the effect of synthetic dataset size on landmark accuracy. <ref type="figure" target="#fig_4">Figure 17</ref> shows that landmark localization improves as we increase the number of training images, 1RUPDOL]HG0HDQ(UURU :&amp;RPPRQ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1XPEHURIWUDLQLQJLPDJHVORJVFDOH</head><p>:&amp;KDOOHQJLQJ :3ULYDWH <ref type="figure" target="#fig_4">Figure 17</ref>. Landmark localization accuracy improves as we use more and more synthetic training data. <ref type="figure" target="#fig_5">Figure 18</ref>. It is easy to generate synthetic training data for eye tracking (left) which generalizes well to real-world images (right). before starting to plateau at 100,000 images.</p><p>We study the importance of data augmentation when training models on synthetic data. We train models with: 1) no augmentation; 2) appearance augmentation only (e.g. colour shifts, brightness and contrast); 3) full augmentation, varying both appearance and geometry (e.g. rotation and warping). <ref type="table">Table 3</ref> shows the importance of augmentation, without which synthetic data does not outperform real. <ref type="table">Table 3</ref> also shows the importance of label adaptation when evaluating models trained on synthetic data -using label adaptation to improve label consistency reduces error. Adding label adaptation to a model trained on real data results in little change in performance, showing that it does not benefit already-consistent within-dataset labels.</p><p>If we remove clothing and hair, landmark accuracy suffers <ref type="table">(Table 3</ref>). This verifies the importance of our hair library and digital wardrobe, which improve the realism of our data.</p><p>Additional ablation studies analyzing the impact of render quality, and variation in pose, expression, and identity can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Other examples</head><p>In addition to the quantitative results above, this section qualitatively demonstrates how we can solve additional problems using our synthetic face framework.</p><p>Eye tracking can be a key feature for virtual or augmented reality devices, but real training data can be difficult to acquire <ref type="bibr" target="#b13">[14]</ref>. Since our faces look realistic close-up, it is easy for us to set up a synthetic eye tracking camera and render diverse training images, along with ground truth. <ref type="figure" target="#fig_5">Figure 18</ref> shows example synthetic training data for such a camera, along with results for semantic segmentation.</p><p>Dense landmarks. In subsection 4.3, we presented results for localizing 68 facial landmarks. What if we wanted to predict ten times as many landmarks? It would be impossi- <ref type="figure">Figure 19</ref>. With synthetic data, we can easily train models that accurately predict ten times as many landmarks as usual. Here are some example dense landmark predictions on the 300W dataset. ble for a human to annotate this many landmarks consistently and correctly. However, our approach lets us easily generate accurate dense landmark labels. <ref type="figure">Figure 19</ref> shows the results of modifying our landmark network to regress 679 coordinates instead of 68, and training it with synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>We have shown that it is possible to achieve results comparable with the state of the art for two well-trodden tasks: face parsing and landmark localization, without using a single real image during training. This is important since it opens the door to many other face-related tasks that can be addressed using synthetic data in the place of real data.</p><p>Limitations remain. As our parametric face model includes the head and neck only, we cannot simulate clothing with low necklines. We do not include expression-dependent wrinkling effects, so realism suffers during certain expressions. Since we sample parts of our model independently, we sometimes get unusual (but not impossible) combinations, such as feminine faces that have a beard. We plan to address these limitations with future work.</p><p>Photorealistic rendering is computationally expensive, so we must consider the environmental cost. In order to generate the dataset used in this paper, our GPU cluster used approximately 3,000kWh of electricity, equivalent to roughly 1.37 metric tonnes of CO 2 , 100% of which was offset by our cloud computing provider. This impact is mitigated by the ongoing progress of cloud computing providers to become carbon negative and use renewable energy sources <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39]</ref>. There is also the financial cost to consider. Assuming $1 per hour for an M60 GPU (average price across cloud providers), it would cost $7,200 to render 100,000 images. Though this seems expensive, real data collection costs can run much higher, especially if we take annotation into consideration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>3D faces sampled from our generative model, demonstrating how our model captures the diversity of the human population.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Histograms of self-reported age, gender, and ethnicity in our scan collection, which was used to build our face model and texture library. Our collection covers a range of age and ethnicity. vertices and 7, 414 polygons, and a minimal skeleton of K = 4 joints: the head, neck, and two eyes. The face mesh vertex positions are defined by mesh generating function M( ?, ?, ?) : R | ?|?| ?|?| ?| ? R N ?3 which takes parameters ? ? R | ?| for identity, ? ? R | ?| for expression, and ? ? R K?3 for skeletal pose. The pose parameters ? are per-joint local rotations represented as Euler angles. M is defined as M( ?, ?, ?) = L(T ( ?, ?), ?, J ( ?); W)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Examples from our data-driven expression library and manually animated sequence, visualized on our template face.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>We apply coarse and meso-displacement to our 3D face model to ensure faces look realistic even when viewed close-up.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Our hair library contains a diverse range of scalp hair, eyebrows, and beards. When assembling a 3D face, we choose hair style and appearance at random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 15 .</head><label>15</label><figDesc>Predictions before (top) and after (bottom) label adaptation. The main difference is changing the jawline from a 3D-to-2D projection to instead follow the facial outline in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 16 .</head><label>16</label><figDesc>Predictions by networks trained with real (top) and synthetic data (bottom). Note how the synthetic data network generalizes better across expression, illumination, pose, and occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Nose Upper lip Inner mouth Lower lip Brows Eyes Mouth Overall Guo et al. [19] AAAI'18Table 2. A comparison with the state of the art on LaPa, using F1 score. For eyes and brows, L and R are left and right. For lips, U, I, and L are upper, inner, and lower. Training with our synthetic data achieves results in line with the state of the art, trained with real data.</figDesc><table><row><cell></cell><cell></cell><cell>93.8</cell><cell>94.1</cell><cell></cell><cell>75.8</cell><cell>83.7</cell><cell></cell><cell>83.1</cell><cell>80.4</cell><cell>87.1</cell><cell>92.4</cell><cell>90.5</cell></row><row><cell cols="2">Wei et al. [67] TIP'19</cell><cell>95.6</cell><cell>95.2</cell><cell></cell><cell>80.0</cell><cell>86.7</cell><cell></cell><cell>86.4</cell><cell>82.6</cell><cell>89.0</cell><cell>93.6</cell><cell>91.6</cell></row><row><cell cols="3">Lin et al. [35] CVPR'19 94.5</cell><cell>95.6</cell><cell></cell><cell>79.6</cell><cell>86.7</cell><cell></cell><cell>89.8</cell><cell>83.1</cell><cell>89.6</cell><cell>95.0</cell><cell>92.4</cell></row><row><cell cols="2">Liu et al. [36] AAAI'20</cell><cell>94.9</cell><cell>95.8</cell><cell></cell><cell>83.7</cell><cell>89.1</cell><cell></cell><cell>91.4</cell><cell>83.5</cell><cell>89.8</cell><cell>96.1</cell><cell>93.1</cell></row><row><cell>Te et al. [64]</cell><cell>ECCV'20</cell><cell>94.6</cell><cell>96.1</cell><cell></cell><cell>83.6</cell><cell>89.8</cell><cell></cell><cell>91.0</cell><cell>90.2</cell><cell>84.9</cell><cell>95.5</cell><cell>93.2</cell></row><row><cell>Ours (real)</cell><cell></cell><cell>95.1</cell><cell>94.7</cell><cell></cell><cell>81.6</cell><cell>87.0</cell><cell></cell><cell>88.9</cell><cell>81.5</cell><cell>87.6</cell><cell>94.8</cell><cell>91.6</cell></row><row><cell cols="2">Ours (synthetic)</cell><cell>95.1</cell><cell>94.5</cell><cell></cell><cell>82.3</cell><cell>89.1</cell><cell></cell><cell>89.9</cell><cell>83.5</cell><cell>87.3</cell><cell>95.1</cell><cell>92.0</cell></row><row><cell>Method</cell><cell></cell><cell cols="11">Skin Hair L-eye R-eye U-lip I-mouth L-lip Nose L-Brow R-Brow Mean</cell></row><row><cell cols="2">Liu et al. [36] AAAI'20</cell><cell cols="2">97.2 96.3</cell><cell>88.1</cell><cell>88.0</cell><cell>84.4</cell><cell>87.6</cell><cell>85.7</cell><cell>95.5</cell><cell>87.7</cell><cell>87.6</cell><cell>89.8</cell></row><row><cell>Te et al. [64]</cell><cell>ECCV'20</cell><cell cols="2">97.3 96.2</cell><cell>89.5</cell><cell>90.0</cell><cell>88.1</cell><cell>90.0</cell><cell>89.0</cell><cell>97.1</cell><cell>86.5</cell><cell>87.0</cell><cell>91.1</cell></row><row><cell>Ours (real)</cell><cell></cell><cell cols="2">97.5 86.9</cell><cell>91.4</cell><cell>91.5</cell><cell>87.3</cell><cell>89.8</cell><cell>89.4</cell><cell>96.9</cell><cell>89.3</cell><cell>89.3</cell><cell>90.9</cell></row><row><cell>Ours (synthetic)</cell><cell></cell><cell cols="2">97.1 85.7</cell><cell>90.6</cell><cell>90.1</cell><cell>85.9</cell><cell>88.8</cell><cell>88.4</cell><cell>96.7</cell><cell>88.6</cell><cell>88.5</cell><cell>90.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements We thank Pedro Urbina, Jon Hanzelka, Rodney Brunet, and Panagiotis Giannakopoulos for their artistic contributions. This work was published in ICCV 2021. The author list in the IEEE digital library is missing V.E. and M.J. due to a mistake on our side during the publishing process.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Amazon climate pledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amazon</surname></persName>
		</author>
		<ptr target="https://www.aboutamazon.com/planet/climate-pledge" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Parametric Deformation of Discrete Geometry for Aerodynamic Shape Design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Aftosmis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nemec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Aircraft</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain Adaptation through Synthesis for Unsupervised Person Re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3D Constrained Local Model for Rigid and Non-Rigid Facial Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A morphable model for the synthesis of 3d faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Blanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 26th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="187" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<ptr target="https://www.cycles-renderer.org/,2021.5" />
		<title level="m">Blender Foundation. Cycles renderer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">3FabRec: Fast Few-shot Face alignment by Reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Browatzki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wallraven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A practical and controllable hair and fur model for production path tracing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bitterli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tappan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Burley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An approximate reflectance profile for efficient subsurface scattering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Talks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title/>
		<ptr target="https://www.marvelousdesigner.com/,2021.5" />
	</analytic>
	<monogr>
		<title level="j">CLO Virtual Fashion Inc. Marvelous designer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-based lighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Debevec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Courses</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">VirtualWorlds as Proxy for Multi-object Tracking Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cabon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Schuetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cavin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Talathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.03702</idno>
		<title level="m">OpenEDS: Open eye dataset</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High resolution zero-shot domain adaptation of synthetically rendered face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semisupervised Adversarial Learning to Generate Photorealistic Face Images of New Identities from 3D Morphable Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gecer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Morphable face models -an open framework. Automatic Face and Gesture Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>L?thi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sch?nborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Google cloud sustainability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Google</surname></persName>
		</author>
		<ptr target="https://cloud.google.com/sustainability/,2021.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Residual encoder decoder network and adaptive prior for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">DenseReg: Fully Convolutional Dense Shape Regression In-the-Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>G?ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Avengers: Capturing Thanos&apos;s Complex Face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Battulwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Corral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cloudsdale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roble</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH Talks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Photorealistic image synthesis for object instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hoda?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hanzelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Urbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Dense 3D face alignment from 2D videos in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Meta-Sim: Learning to Generate Synthetic Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cameracci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusiniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Digital humans: Crossing the uncanny valley in ue4</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Karis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Antoniades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caulkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mastilovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Game Developers Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of Style-GAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Config: Controllable neural face image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Estellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep head pose estimation using synthetic images and partial adversarial domain adaption for continuous label spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kuhnke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ostermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fairface: Face attribute dataset for balanced race, gender, and age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>K?rkk?inen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="page" from="2021" to="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose Space Deformation: A Unified Approach to Shape Interpolation and Skeleton-Driven Deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning a model of facial shape and expression from 4D scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face Parsing with RoI Tanh-Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A new dataset and boundary-attention semantic segmentation for face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">SMPL: A skinned multi-person linear model. SIG-GRAPH Asia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The diversity of the human hair colour assessed by visual scales and instrumental measurements. a worldwide survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Loussouarn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of cosmetic science</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="101" to="107" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Microsoft will be carbon negative by 2030</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Microsoft</surname></persName>
		</author>
		<ptr target="https://blogs.microsoft.com/blog/2020/01/16/microsoft-will-be-carbon-negative-by-2030/,2021.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">GANerated Hands for Real-Time 3D Hand Tracking from Monocular RGB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Discriminative learning of visual words for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hand2face: Automatic synthesis and recognition of hand over face occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nojavanasghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACII</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<title level="m">An Imperative Style, High-Performance Deep Learning Library. NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unrealcv: Virtual worlds for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia Open Source Software Competition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d face reconstruction by learning from synthetic data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning detailed face reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Or-El</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Laplace Landmark Localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A Large Collection of Synthetic Images for Semantic Segmentation of Urban Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">On rendering synthetic images for training an object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozantsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<ptr target="https://www.russian3dscanner.com/,2021.4" />
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>2 [52] Russian3DScanner. Wrap3.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<title level="m">300 faces In-the-wild challenge: Database and results. Image and Vision Computing (IMAVIS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">PIFu: Pixel-Aligned Implicit Function for High-Resolution Clothed Human Digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">PIFuHD: Multi-Level Pixel-Aligned Implicit Function for High-Resolution 3D Human Digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Effective Use of Synthetic Data for Urban Scene Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadegh Aliakbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unrestricted facial geometry reconstruction using image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kimmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Hand keypoint detection in single images using multiview bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1145" to="1153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning-by-Synthesis for Appearance-Based 3D Gaze Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Rendering synthetic ground truth images for eye tracker evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>?wirski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Dodgson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ETRA</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Edge-aware Graph Representation Learning and Reasoning for Face Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Te</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adaptive Wing Loss for Robust Face Alignment via Heatmap Regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fuxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Accurate facial image parsing at real-time speed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4659" to="4670" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Rendering of eyes for eye-shape registration and gaze estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning an appearance-based gaze estimator from one million synthesised images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ETRA</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Synthetic-to-real unsupervised domain adaptation for scene text detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Segmentation models pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yakubovskiy</surname></persName>
		</author>
		<ptr target="https://github.com/qubvel/segmentation_models.pytorch" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Simulating Content Consistent Vehicle Datasets with Attribute Descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Naphade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gedeon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A highresolution 3d dynamic facial expression database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Worm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reale</surname></persName>
		</author>
		<idno type="DOI">10.1109/AFGR.2008.4813324.2</idno>
	</analytic>
	<monogr>
		<title level="m">8th IEEE International Conference on Automatic Face Gesture Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zaal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majboroda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mischok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hdri Haven</surname></persName>
		</author>
		<ptr target="https://hdrihaven.com,2020.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Df2net: A dense-fine-finer network for detailed 3d face reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Unpaired Imageto-Image Translation using Cycle-Consistent Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Robust Facial Landmark Detection via Occlusion-Adaptive Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sadiq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Face alignment in full pose range: A 3d total solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
