<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Robust and Reproducible Active Learning using Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Munjal</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasir</forename><surname>Hayat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamshid</forename><surname>Sourati</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Chicago</orgName>
								<address>
									<settlement>Chicago</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shadab</forename><surname>Khan</surname></persName>
						</author>
						<title level="a" type="main">Towards Robust and Reproducible Active Learning using Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Active learning (AL) is a promising ML paradigm that has the potential to parse through large unlabeled data and help reduce annotation cost in domains where labeling data can be prohibitive. Recently proposed neural network based AL methods use different heuristics to accomplish this goal. In this study, we demonstrate that under identical experimental settings, different types of AL algorithms (uncertainty based, diversity based, and committee based) produce an inconsistent gain over random sampling baseline. Through a variety of experiments, controlling for sources of stochasticity, we show that variance in performance metrics achieved by AL algorithms can lead to results that are not consistent with the previously reported results. We also found that under strong regularization, AL methods show marginal or no advantage over the random sampling baseline under a variety of experimental conditions. Finally, we conclude with a set of recommendations on how to assess the results using a new AL algorithm to ensure results are reproducible and robust under changes in experimental conditions. We share our codes to facilitate AL evaluations. We believe our findings and recommendations will help advance reproducible research in AL using neural networks. We open source our code at https://github.com/PrateekMunjal/TorchAL</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Active learning (AL) is a machine learning paradigm that promises to help reduce the burden of data annotation by intelligently selecting a subset of informative samples from a large pool of unlabeled data that. In AL, a model trained with a small amount of labeled seed data is used to parse through the unlabeled data to select the subset that should be sent to an oracle (annotator). To select such a subset, AL methods rely on exploiting the learned latentspace, model uncertainty, or other heuristics. The promise of reducing annotation cost has brought a surge of interest in AL research <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b32">32]</ref> and with it, a few outstanding issues. First, The results reported for Random sampling baseline, RSB vary significantly between studies. For example, using 20% labeled data of CIFAR10, the difference between RSB performance reported by <ref type="bibr" target="#b32">[32]</ref> and <ref type="bibr" target="#b28">[28]</ref> is ? 13% under identical settings. Second, The results reported for the same AL method can vary across studies: using VGG16 <ref type="bibr" target="#b26">[26]</ref> on CIFAR100 <ref type="bibr" target="#b17">[17]</ref> with 40% labeled data, Coreset <ref type="bibr" target="#b25">[25]</ref> reported ? 55% classification accuracy whereas VAAL <ref type="bibr" target="#b27">[27]</ref> reported 47.01% using the method reported in <ref type="bibr" target="#b25">[25]</ref>. Third, Recent AL studies have been inconsistent with each other. For example, <ref type="bibr" target="#b25">[25]</ref> and <ref type="bibr" target="#b5">[6]</ref> state that diversity-based AL methods consistently outperform uncertainty-based methods, which were found to be worse than the RSB. In contrast, recent developments in uncertainty based studies <ref type="bibr" target="#b32">[32]</ref> suggest otherwise. In addition to these issues, results using a new AL method are often reported on simplistic experimental conditions -(i) regularization is not sufficiently explored beyond the usual methods (e.g. weight decay), (ii) with increasing AL iterations, the training data distribution changes, however, the training hyper-parameters are fixed in advance. Such issues in the AL results has spurred a recent interest in benchmarking of AL methods and recent NLP and computer vision studies have raised a number of interesting questions <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b24">24]</ref>. With the goal of improving the reproducibility and robustness of AL methods, in this study we evaluate the performance of these methods for image classification task. Contributions: Through a comprehensive set of experiments performed using consistent settings under a common code base (PyTorch-based 1 ) we compare different AL methods including state-of-the-art diversity-based, uncertainty-based, and committee-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27]</ref> and a well-tuned RSB. We demonstrate that: 1) With strong regularization and hyper-parameters tuned using Au-toML, RSB performs comparably to AL methods in contrast to the previously reported results in the literature. 2) No AL method consistently outperforms other approaches, and conclusions can change with different experimental settings (e.g. using a different architecture for the classifier or with different number of AL iterations). 3) The difference in performance between the AL methods and the RSB is much smaller than reported in the literature. 4) With a stronglyregularized model, the variance in accuracy achieved using AL methods is substantially lower across consistent repeated training runs, suggesting that such a training regime is unlikely to effect misleading results in AL experiments. 5) Finally, we provide a set of guidelines on experimental evaluation of a new AL method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Pool Based Active Learning Methods</head><p>Contemporary pool-based AL methods can be broadly classified into: (i) uncertainty based <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b27">27]</ref>, (ii) diversity based <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">25]</ref>, and (iii) committee based <ref type="bibr" target="#b1">[2]</ref>. AL methods also differ in other aspects, for example, some AL methods use the task model (e.g. model trained for image classification) within their sampling function <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">25]</ref>, where as others use different models for task and sampling functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">27]</ref>. These methods are discussed in detail next.</p><p>Notations: Starting with an initial set of labeled data L 0 0 ={(x i , y i )} N L i=1 and a large pool of unlabeled data U 0 0 ={x i } N U i=1 , pool-based AL methods train a model ? 0 . A sampling function ?(L 0 0 , U 0 0 , ? 0 ) then evaluates x i ? U 0 , and selects k (budget size) samples to be labeled by an oracle. The selected samples with oracle-annotated labels are then added to L 0 0 , resulting in an extended L 1 0 labeled set, which is then used to retrain ?. This cycle of sampleannotate-train is repeated until the sampling budget is exhausted or a satisficing metric is achieved. AL sampling functions evaluated in this study are outlined next. <ref type="bibr" target="#b19">[19]</ref> ranks the unlabeled data, x i ? U in a descending order based on their scores given by max j ?(x i ); j ? {1 . . . C} where C is the number of classes, and chose the top k samples. Typically this approach focuses on the samples in U for which the softmax classifier is least confident. Recently, Huang et al. <ref type="bibr" target="#b11">[11]</ref> proposed to measure the uncertainty by measuring the output discrepancies from the model trained at different AL cycles. 1 https://github.com/PrateekMunjal/TorchAL</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Model Uncertainty on Output (UC)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Bayesian Active Learning (DBAL)</head><p>Gal et al. <ref type="bibr" target="#b6">[7]</ref> train the model ? with dropout layers and use monte carlo dropout to approximate the sampling from posterior. For our experiments, we used the two most reported acquisitions i.e. max entropy and Bayesian Active Learning by Disagreement (BALD). The max entropy method selects the top k data points having maximum entropy as arg max i H[P (y|x i )]; ?x i ? U 0 where the posterior is given by,</p><formula xml:id="formula_0">P (y|x i ) = T j=1 1 T P (y|x i , ? j ).</formula><p>Here T denotes number of forward passes through the model, ?. BALD selects the top k samples that increase the information gain over the model parameters i.e. arg max i I[P (y, ?|x i , L 0 )]; ?x i ? U 0 We implement DBAL as described in <ref type="bibr" target="#b6">[7]</ref> where probability terms in information gain is evaluated using previous equation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Coreset</head><p>Sener et al. <ref type="bibr" target="#b25">[25]</ref> exploit the geometry of data points and choose samples that provide a cover to all data points. Essentially, their algorithm tries to find a set of points (coverpoints), such that distance of any data point from its nearest cover-point is minimized. They proposed two suboptimal but efficient solutions to this NP-Hard problem: coreset-greedy and coreset-MIP (Mixed Integer programming), coreset-greedy is used to initialize coreset-MIP. For our experiments, following <ref type="bibr" target="#b32">[32]</ref>, we implement coresetgreedy since it achieves comparable performance while being significantly compute efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Variational Adversarial Active Learning</head><p>Sinha et al. <ref type="bibr" target="#b27">[27]</ref> combined a VAE <ref type="bibr" target="#b15">[15]</ref> and a discriminator <ref type="bibr" target="#b8">[9]</ref> to learn a metric for AL sampling. VAE encoder is trained on both L and U , and the discriminator is trained on the latent space representations of L and U to distinguish between seen (L) and unseen (U ) images. Sampling function selects samples from U with lowest discriminator confidence (to be seen) as measured by output of discriminator's softmax. Effectively, samples that are most likely to be unseen based on the discriminator's output are chosen. Since VAAL does not account for the end task, recent methods such as SRAAL <ref type="bibr" target="#b35">[35]</ref>, TAVAAL <ref type="bibr" target="#b14">[14]</ref> have incorporated the task awareness too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Ensemble Variance Ratio Learning</head><p>Proposed by <ref type="bibr" target="#b1">[2]</ref>, this is a query-by-committee (QBC) method that uses a variance ratio computed as v = 1 ? f m /N . This variance ratio select the sample set with the largest dispersion (v), where N is the number of committee members (CNNs), and f m is the number of predictions in the modal class category. Variance ratio lies in 0-1 range and can be treated as an uncertainty measure. We note that it is possible to formulate several AL strategies using the ensemble e.g. BALD, max-entropy, etc. Variance ratio was chosen for this study because it was shown by authors to lead to superior results. For training the CNN ensembles, we train 5 models with VGG16 architecture but a different random initialization. Further, following <ref type="bibr" target="#b1">[2]</ref>, the ensembles are used only for sample set selection, a separate task classifier is trained in fully-supervised manner to do image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Regularization and Active Learning</head><p>In a ML training pipeline comprising data-model-metric and training tricks, regularization can be introduced in several forms. In neural networks, regularization is commonly applied using parameter norm penalty (metric), dropout (model), or using standard data augmentation techniques such as horizontal flips and random crops (data). However, parameter norm penalty coefficients are not easy to tune and dropout effectively reduces model capacity to reduce the extent of over-fitting on the training data, and requires the drop probability to be tuned. On the other hand, several recent studies have shown promising new ways of regularizing neural networks to achieve impressive gains. While it isn't surprising that these regularization techniques help reduce generalization error, most AL studies have overlooked them. We believe this is because of a reasonable assumption that if an AL method works better than random sampling, then its relative advantage should be maintained when newer regularization techniques and training tricks are used. Since regularization is critical for low-data training regime of AL where the massively-overparameterized model can easily overfit to the limited training data, we investigate the validity of such assumptions by applying regularization techniques to the entire data-model-metric chain of neural network training.</p><p>Specifically, we employ parameter norm penalty, random augmentation (RA) <ref type="bibr" target="#b4">[5]</ref>, stochastic weighted averaging (SWA) <ref type="bibr" target="#b13">[13]</ref>, and shake-shake (SS) <ref type="bibr" target="#b7">[8]</ref>. In RA, a sequence of n randomly chosen image transforms are sequentially applied to the training data, with a randomly chosen distortion magnitude (m) which picks a value between two extremes. For details of extreme values used for each augmentation choice, we refer the reader to work of <ref type="bibr" target="#b3">[4]</ref>. SWA is applied on the model by first saving e snapshots of model during the time-course of optimization, and then averaging the snapshots as a post-processing step. The mode of action of the regularization techniques used affect different components of the neural network training pipeline: RA is applied to data, SWA and SS is applied to model, parameter norm penalty affects the metric. In our experiments, the models which are trained using such additional regularization are referred to as strongly-regularized models (SR-models). The hyper-parameters associated with these regularization techniques as well as experiments and their results when applied to neural network training with AL-selected sample sets are discussed in Sec. 6.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Tuning Hyper-parameters</head><p>The performance of deep neural networks is sensitive to the choice of hyper-parameters (e.g. learning rate, optimizer, weight decay, etc.) and there is no deterministic approach to find a combination that yields best results. Most AL methods perform grid search to find a set of hyperparameters over the initial labeled set, and these hyperparameters are fixed for the AL iterations <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b27">27]</ref>. Fixing the hyper-parameters in AL iterations is questionable -with an increase in AL iterations, the size of training data increases, and the distribution changes since AL heuristics are used to draw a new set to be labeled by the oracle. Therefore, the hyper-parameters found to work well in one AL iteration may not work well at further AL iterations. To address this concern, we use AutoML at each AL iteration in our implementation, which does 50 trials of random search over the hyper-parameters. To illustrate this point further, in 4 AL iterations for any given AL method, where initial data of 10% is increased to 40% with a budget size of 10%, we train a total of 200 models and choose the best 4 (1 for each AL iteration) to report the performance. This process is repeated for each labeled set partition: L 0 0 ,L 0 1 ,L 0 2 ,L 0 3 ,L 0 4 . To report the variance in accuracy at an AL iteration a labeled partition, say L 0 , we re-use the best hyper-parameters founded using L 0 0 and run on L i 0 , where i ? {1, 2, 3, 4}. Further details regarding the list of hyper-parameters and their range of choices is shared in the supplementary section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>We perform experiments on most commonly used datasets in active learning: CIFAR10, CIFAR100, with limited additional results reported on ImageNet. For details on our training schedule we refer readers to the supplementary. Given a dataset D, we split it into train (T r ), validation (V ), and test (T s ) sets. The train set is further divided into the initial labeled (L 0 ) and unlabeled (U 0 ) sets. A base classifier B is first trained, followed by iterations of sampleannotate-train process using various AL methods. Model selection is done by choosing the best performing model on the validation set. For a fair comparison, a consistent set of experimental settings is used across all methods. Hyperparameters like learning rate (lr) and weight decay (wd) were tuned using AutoML with random search over 50 trials. We make use of the Optuna library <ref type="bibr" target="#b0">[1]</ref> to facilitate these experiments. For ImageNet experiments, we relied on previously published hyper-parameters found using Au-toML <ref type="bibr" target="#b34">[34]</ref>. Models were trained from random initialization in each AL iteration for experiments that used CIFAR 10 and CIFAR 100 datasets. In case of ImageNet, the initial model trained using random drawn seed batch was trained </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% of Labeled Data</head><p>Random Coreset DBAL BALD VAAL QBC UC <ref type="figure">Figure 1</ref>. Mean accuracies achieved by AL methods compared on CIFAR10 dataset for different initial labeled sets L0, L1, ? ? ? , L4. The mean accuracy for the base model (at 10% labeled data) is noted inside each subplot. The model is trained 5 times for different random initialization seeds where for the first seed we use AutoML to tune hyper-parameters and re-use these hyper-parameters for the other 4 seeds. The mean of 25 runs (bottom right) suggest that no AL method performs consistently better than others.</p><p>using random initialization, and subsequent models in AL iterations were initialized using final weights from previous iteration. ImageNet Training Details: We use Resnext-50 <ref type="bibr" target="#b29">[29]</ref> as our classifier and follow the settings of <ref type="bibr" target="#b34">[34]</ref> i.e. Opti-mizer=SGD, wd = 3 ? 10 ?4 . We train the base classifier on L 0 for 200 epochs (300 epochs when we include SWA and RA to the training pipeline) where lr = 0.1 with a linear warm-up schedule (for first 5 epochs) followed by decaying the lr by a factor of 10 on epoch number: {140, 160, 180}. For AL iterations we fine-tune the best model (picked by validation set accuracy) from previous iteration for 100 epochs where lr = 0.01 which gets decayed by a factor of 10 on epoch number: {35, 55, 80}. Further, we choose the best model based on a realistically small validation set (i.e. 12811 images) following <ref type="bibr" target="#b34">[34]</ref>. The input is pre-processed using random crops resized to 224 x 224 followed by horizontal flip (probability=0.5) and normalized to zero mean and one standard deviation using statistics of initial randomly drawn labeled set partition.</p><p>Architecture &amp; Hyper-parameters: All experiments are performed using the VGG16 architecture <ref type="bibr" target="#b26">[26]</ref> with batchnorm <ref type="bibr" target="#b12">[12]</ref>, unless otherwise stated. For transferability experiment (refer Sec. 6.5), we use two target architectures i.e. 18-layer ResNet <ref type="bibr" target="#b10">[10]</ref>, and 28-layer 2-head Wide-ResNet (WRN-28-2) <ref type="bibr" target="#b33">[33]</ref> in our experiments. Both target architectures are taken from publicly available github repository <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b21">[21]</ref>. For CIFAR10/100 models we set the number of neurons in penultimate fully-connected layer of VGG16 to 512 as in <ref type="bibr" target="#b18">[18]</ref>. RA parameteres: N=the number of transformations and M=index of the magnitude, are tuned using AutoML. We empirically select the SWA hyperparameters as: CIFAR 10/100: SWA LR: 5 ? 10 ?4 and frequency: 50. Imagenet: SWA LR: 1 ? 10 ?5 and frequency: 50. Implementation of AL methods: We developed a PyTorch-based toolkit to evaluate the AL methods in a unified implementation. AL methods can be cast into two categories based on whether or not AL sampling relies on the task model (classifier network). For example, coreset uses the latent space representations learnt by task model to select the sample set, whereas VAAL relies on a separate VAE-discriminator network to select the samples, independent of the task model. In our implementation, we abstract these two approaches in a sampling function that may use the task model if required by the AL method. Each AL method was implemented using a separate sampling function, by referencing author-provided code if it was available. Using command line arguments, the toolkit allows the user to configure various aspects of training such as architecture used for task model, AL method, size of initial labeled set, size of acquisition batch, number of AL iterations, hyper-parameters for task model training and AL sampling and number of repetitions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>% of Labeled Data</head><p>Random Coreset DBAL BALD VAAL QBC UC <ref type="figure">Figure 2</ref>. Mean accuracies achieved by AL methods compared on CIFAR100 dataset for different initial labeled sets L0, L1, ? ? ? , L4. The mean accuracy for the base model (at 10% labeled data) is noted inside each subplot. The model is trained 5 times for different random initialization seeds where for the first seed we use AutoML to tune hyper-parameters and re-use these hyper-parameters for the other 4 seeds. The mean of 25 runs (bottom right) suggest that no AL method performs consistently better than others.</p><p>were written in Python using PyTorch and other libraries in addition to third-party code-bases and are made available as part of the supplementary material. Models were trained over a period of many months, AutoML considerably increased time for completing each experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Variance in Evaluation Metrics</head><p>Training a neural network involves many stochastic components including parameter initialization, data augmentation, mini-batch selection, and batchnorm whose parameters change with mini-batch statistics. These elements can lead to a different optima thus resulting in varying performances across different runs of the same experiment. To evaluate the variance in classification accuracy caused by different initial labeled data, we draw five random initial labeled sets (L 0 . . . L 4 ) with replacement. Each of these five sets were used to train the base model, initialized with random weights, 5 times; a total of 25 models were trained for each AL method to characterize variance within-samplesets and between-sample-sets. From the results summarized in <ref type="figure">Fig. 1 and Fig. 2</ref>, we make the following observations: (i) A standard deviation of 1-2% in accuracy among different AL methods, indicating that out of chance, it is possible to achieve seemingly better results, (ii) In contrast to previous studies, our extensive experiments indicate that no AL method performs consistently better, and random sampling baseline performs competitively. As stated previously, we believe that automatic hyper-parameter tuning repeated for each AL method at every AL iteration, while adding computation burden significantly, shows that there is marginal or no improvement achieved using AL methods compared to the random sampling baseline, (iii) Our results averaged over 25 runs in <ref type="figure">Fig. 1</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Statistical Analysis of Variance</head><p>In order to statistically compare the results achieved by the AL methods, we assessed the normality and variance to validate assumptions for parametric tests first. The normality assumption was tested using Kolmogorov-Smirnov test, and the assumption of homoscedasticity was tested using Levene's test. We found that normality could not be assumed for any of the six experiments in Tab. 1. Using Levene's test, we found that the null hypothesis of equality of variance was rejected at ? = 0.05 in 4 out of 6 experiments in Tab. 1. Therefore, for consistency, we used Kruskal-Wallis one-way ANOVA for all 6 experiments, and found that at least one method stochastically dominated one other method in all 6 experiments. Next, we used Games-Howell test for post-hoc multiple pairwise comparison to assess which pairs of methods differed significantly in their results. At a threshold of ? = 0.05, we observed that no AL method consistently outperforms random baseline. The null hypothesis of equal mean accuracy could not be rejected for most pairwise comparisons. We did note that among the methods evaluated, QBC was superior to RSB in 4 out of 6 experiments, with other methods performing as follows: Coreset 2/6, DBAL 2/6, BALD 1/6. Our analysis strongly suggests that variance in performance metric needs to be assessed to fairly compare an AL method against RSB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Differing Experimental Conditions</head><p>Next, we compare AL methods and RSB by modifying different experimental conditions for annotation batch size, size of validation set, and class imbalance. Annotation Batch Size (b): Following previous studies, we experiment with annotation batch size (b) equal to 5%, and 10% of the overall sample count (L + U ). Results in <ref type="figure" target="#fig_2">Fig. 3</ref> (corresponding Tab 6 in suppl.) show that our observation still holds i.e no AL method is consistently better. For example, on CIFAR10 at 40% labeled data and b = 10% (refer <ref type="figure">Fig. 1</ref> and Tab 7. in suppl.), UC outperforms all other meth-ods but when compared to 40% labeled data and b = 5% (refer <ref type="figure" target="#fig_2">Fig. 3</ref> and Tab 6. in suppl.), BALD performs the best. Similarly on CIFAR100 at 30% labeled data and b = 10%, RSB outperforms coreset but when compared to 30% labeled data and b = 10%, coreset performs better. We therefore conclude that no AL method offers consistent advantage over others under different budget size settings.</p><p>Validation Set Size: During training, we select the best performing model on the validation set (V ) to report the test set (T s ) results. To evaluate if size of V can affect the conclusions drawn from comparative AL experiments, we perform experiments on CIFAR100 with three different V sizes: 2%, 5%, and 10% of the total samples (L + U ). From results in Tab. 2, we did not observe discernible trends in accuracy with respect to the size of V . For instance, RSB achieves a mean accuracy of 47.5%, 43.4%, and 46.7%, respectively, for the best model selected using 2%, 5% and 10% of the training data as V . However, when the labeled data increases the range of accuracy values is not as large, indicating that training tends to suffer from less variance when higher volume of labeled data was available. For example, at 40% labeled data the RSB achieves a mean accuracy of 54.58%, 57.24%, and 55.06%. From these experiments, it appears that the initial AL iterations were more sensitive to size of V compared to the later iterations. Furthermore, in line with previous experiment, no AL method was consistently better across AL iterations as size of V changes. Class Imbalance: Here, we evaluate the robustness of different AL methods on imbalanced data. For this, we construct L 0 on CIFAR100 dataset, to simulate long tailed distribution of classes by following a power law, where the number of samples of 100 classes are given by samples <ref type="bibr">[</ref>  The resulting image count per class is normalized to construct a sampling distribution. Models were trained using previously described settings, with the exception of loss function which was set to weighted cross entropy.</p><p>The results in <ref type="figure" target="#fig_3">Fig. 4</ref> show that the difference between RSB and the best AL method reduces with more and more labeled data. More importantly, we notice that AL methods demonstrate different degree of change in the imbalanced class setting, without revealing a clear trend in the plot. From clear trend, we mean that a robust AL method is expected to perform consistently best across all fractions of the data, which is not the case in <ref type="figure" target="#fig_3">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Regularization</head><p>With the motivation stated in Sec. 3, we evaluate the effectiveness of advanced regularization techniques (RA and SWA) in the context of AL using CIFAR10 and CIFAR100 datasets. We refer the models trained using such advanced regularization techniques as strongly-regularized models (SR models) in further experiments. We empirically observed that unlike 2 ?regularization, which requires careful tuning, results using RA &amp; SWA were fairly robust to changes in their hyper-parameters. <ref type="figure" target="#fig_4">Fig. 5</ref> compares different AL methods with RSB on CI-FAR10/100 datasets. We observe that strongly-regularized models consistently achieve significant performance gains across all AL iterations and exhibit appreciably-smaller variance across multiple runs of the experiments. Our strongly-regularized random-sampling baselines on 40% labeled data achieves mean accuracy of 90.87% and 59.36% respectively on CIFAR10 and CIFAR100. We note that for CIFAR10, the RSB-SR model with 20% of training data achieves 3% higher accuracy compared to RSB model trained using 40% of the training data. Similarly for CI-FAR100, the RSB-SR 30%-model performs comparably to the 40%-RSB model. Therefore, we consider techniques under strong regularization to be a valuable addition to the low-data training regime of AL, especially given that it significantly reduces the variance in evaluation metric which can help avoid misleading conclusions. An ablative study to show individual contribution of each regularization technique towards overall performance gain is given in Tab. <ref type="bibr" target="#b2">3</ref>. Results indicate that both RA &amp; SWA show a significant combined gain of ? 12% and ? 5% on CIFAR10 and CIFAR100 respectively. We also experimented with Shake-Shake (SS) <ref type="bibr" target="#b7">[8]</ref> in parallel to RA &amp; SWA, and observed that it significantly increases the runtime, and is not robust to model architectures. We therefore  chose RA &amp; SWA over SS for strongly-regularized models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Active Learning on ImageNet</head><p>Compared to CIFAR10/100, ImageNet is more challenging with larger sample count, 1000 classes and higher resolution images. In order to work with available compute resources, we compared coreset, VAAL and RSB on Ima-geNet. The details for training hyper-params are in supplementary material. Results with and without strong regularization are shown in <ref type="figure" target="#fig_5">Fig. 6</ref> (refer Tab 4 in suppl.) where mean accuracies are reported over 3 runs. Using ResNext-50 architecture <ref type="bibr" target="#b29">[29]</ref> and following the settings of <ref type="bibr" target="#b34">[34]</ref>, we achieve improved baseline performances compared to the previously reported results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">27]</ref>. From <ref type="figure" target="#fig_5">Fig. 6</ref>, we observe that VAAL outperforms all other methods at 15% data but both Coreset and Coreset-SR leads to the highest mean accuracy across other settings. We also note that similar to the observations on CIFAR datasets; (i) strong regularization helps improve performance across the methods and fraction of data, and (ii) it also reduces the performance gap between  RSB and other AL methods. For example, RSB-SR model with 20% data exceeds the performance achieved by both RSB and VAAL using 25% data, for a saving of ? 64000 images with labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Transferability Settings</head><p>In this experiment, we evaluated how does the accuracy compare when an active learning method chosen for one task architecture (e.g. VGG16), is used to train another task model (e.g. ResNet18). We conduct an experiment by storing the indices of sample set drawn in an AL iteration on the source network (VGG16), and use them to train the target network (ResNet18 and WRN-28-2). From Tab. 4, we observe the inconsistency in performance of AL methods. For example, on CIFAR10 with Resnet18, Coreset achieves consistently best performance. However, this observation does not hold true for strongly-regularized models. We also note that for ResNet18 target architecture, the RSB-SR model outperforms the best AL approach (coreset with ResNet18) in all the AL iterations, though DBAL-SR appears at the top with an accuracy of 93.34% at 40% labeled data. We therefore conclude that the AL methods are sensitive to the model architecture being used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Additional Experiments</head><p>Noisy Oracle: In this experiment, we sought to evaluate the stability of strongly regularized network to labels from a noisy oracle. We experimented with two levels of oracle noise by randomly permuting labels of 10% and 20% of samples in the set drawn by random sampling baseline at each iteration. From results in Tab. 5, we found that the drop in accuracy for the strongly-regularized model regularized was nearly half (3%) compared to its complement model trained (6%) on both 30% and 40% data splits. Our findings suggest that the noisy pseudolabels generated for the unlabelled set U by model ?, when applied in conjunction with appropriate regularization, should help improve model's performance. Additional results using AL methods in this setting are shared in the supplementary material.</p><p>Active Learning Sample Set Overlap: For interested readers, we discuss the extent of overlap among the sample sets drawn by AL methods in supplementary.</p><p>Optimizer Choices: Different AL studies have reported different optimizer choices in their experiments. In this light, we analyze the optimizer chosen by AutoML and we analyze it on CIFAR10. The results are present in supplementary section. Contrary to the previous studies where the optimizer is fixed in advance, we found that both adam and sgd optimizer can sometimes work better than the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>Under-Reported Baselines: We note that several recent AL studies show baseline results that are lower than the ones reproduced in this study. Tab 1 in the supplementary summarizes our RSB results with comparisons to RSB reported by some of the recently published AL methods, under similar training settings. Based on this observation, we emphasize that comparison of AL methods must be done under a consistent set of experimental settings. Our observations confirm and provide a stronger evidence for a similar conclusion drawn in <ref type="bibr" target="#b22">[22]</ref>, and to a less related extent, <ref type="bibr" target="#b23">[23]</ref>. Different from <ref type="bibr" target="#b22">[22]</ref> though, we demonstrate that: (i) Relative gains using AL method are found under a narrow combination of experimental conditions. (ii) Fixing the hyper-parameters at the start of AL can result into sub-optimal results. (iii) More distinctly, we show that performance gains (when they exist) are significantly lower for strongly-regularized models.</p><p>The Role of Regularization: Regularization helps reduce generalization error and is particularly useful in training overparameterized neural networks with low data. We show that both RA &amp; SWA can achieve appreciable gain in performance at the expense of a small computational overhead. We observed that along with learning rate (in case of SGD), regularization was one of the key factors in reducing the error while being fairly robust to its hyperparameters (in case of RA and SWA). We also found that the margin of the gain observed with an AL method over RSB on CIFAR10/100 significantly minimize when the model is well-regularized. Strongly-regularized models also exhibited smaller variance in evaluation metric. With these observations, we recommend that AL methods be also tested using well-regularized model to ensure their robustness. Lastly, we note that there are multiple ways to regularize the data-model-metric pipeline, we focus on data and model side regularization using techniques such as RA and SWA, though it is likely that other combination of newer regularization techniques will lead to similar results. We do believe that with their simplicity and applicability to a wide variety of model (compared to shake-shake method), RA &amp; SWA can be effectively used in AL studies to reduce the variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AL Methods Compared To Strong RSB:</head><p>In contrast to previous findings, well-regularized random baseline in our study was either at par with or marginally inferior to the state-of-the-art AL methods. We believe that previous studies that ran a comparison against the random baseline might have insufficiently regularized the models and/or did not tune the hyperparameters. We also observed (Tab. 4) that a change in model architecture can change the conclusions being drawn in comparing an AL method to a random baseline. This observations suggests that either transferability experiments should be conducted to assess if the active sets are indeed useful across architectures, or comparisons are repeated with additional architectures to study the robustness of an AL method to changes in architecture. Similarly we observed that the strong regularization achieves two goals: (i) it helps improve the performance, (ii) more importantly, it helps reduce the variance in results and reduces the performance gap between random baseline and other AL methods. The highly-sensitive nature of AL results using neural networks therefore necessitates a comprehensive suite of experimental tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion and Proposed Guidelines</head><p>Our extensive experiments suggest a strong need for a common evaluation platform that facilitates robust and reproducible development of AL methods. To this end, we recommend the following to ensure results are robust:</p><p>(i) Experiments should be repeated under varying training settings such as model architecture and budget size, among others. (ii) Regularization techniques such as RA &amp; SWA should be incorporated into the training to ensure AL methods are able to demonstrate gains over a strong-regularized random baseline.(iii) Transferability experiments should be performed to test how useful AL-drawn sample sets are. Alternatively, experiments should be repeated using multiple architectures.(iv) To increase the reproducibility of AL results, experiments should ideally be performed using a common evaluation platform under con-sistent settings to minimize the sources of variation in the evaluation metric. (v) Snapshot of experimental settings should be shared, e.g. using a configuration file (.cfg, .json etc). (vi) Index sets for a public dataset used for partitioning the data into training, validation, test, and AL-drawn sets should be shared, along with the training scripts. In order to facilitate the use of these guidelines in AL experiments, we provide a python-based AL toolkit. We provide the index sets for the datasets used in this study that was used to partition the data into training, validation, and test sets. Lastly, all experiment configuration files are also shared as part of the toolkit.</p><p>Societal impact and Limitations: For some of our experiments, we use ImageNet data, which lacks gender and ethnic diversity <ref type="bibr" target="#b30">[30]</ref>. The models learned with this data could therefore have biased representations. Further, ImageNet has privacy concerns due to unblurred faces <ref type="bibr" target="#b31">[31]</ref>. A limitation of our work is that all our experiments are conducted for image classification. We leave other tasks such as detection and segmentation for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Supplementary Section</head><p>In this section we mention interesting observations and the training details which were used to report the experiments in the main paper. In addition to this, we also discuss the training schedule and additional experiments (for example transferability experiment on CIFAR100) which we could not fit in the main paper due to space constraints. In this section we analyze our random baseline (RSB) results with the random baselines reported by published methods in AL literature. From Tab. 1, it is evident that our strongly-regularized settings along with hyper-parameters tuned using AutoML yields strong baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Training Algorithm</head><formula xml:id="formula_1">sample {x j } k j=1 ? U i 0 using ?(L i 0 , U i 0 , ?) 8: {x j , y j } k j=1 ? {x j , A(x j )} k j=1 9: L i 0 ? L i 0 ? {x j , y j } k j=1</formula><p>10:</p><formula xml:id="formula_2">U i 0 ? U i 0 \ {x j , y j } k j=1 11:</formula><p>? ?Initialize randomly <ref type="bibr">12:</ref> while convergence do <ref type="bibr">13:</ref> Train ? using only L i 0 14:</p><p>end while 15: end while For all reported experiments in the main paper we followed the algorithm described in Algorithm 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Auto-ML Hyper-parameters</head><p>Here we enlist our hyper-parameters tuned using AutoML. To implement AutoML we used optuna framework extensively in our codebase. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Transferability Experiment</head><p>We mainly used three different architectures for classifier model i.e. VGG16, ResNet18 (R18) and Wide ResNet-28-2 (WRN) 2 . The VGG network was used as a source model whereas other two networks are used for target models. The results for CIFAR100 are reported in <ref type="table">Table 2</ref> which are achieved when we replace all the relu activations with leaky relu (negative slope set to 0.2) following <ref type="bibr" target="#b23">(Oliver et al., 2018)</ref>. We found CIFAR100 results to be significantly better with leaky relu activation, however, the same change does not affect the performance of CIFAR10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source Model</head><p>Target </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.5.">Optimizer settings</head><p>Different AL studies have reported different optimizer choices in their experiments. In this light, we analyze the optimizer chosen by AutoML and we analyze it on CIFAR10. The results are present in <ref type="table">Table 3</ref> of supplementary section. Contrary to the previous works where the optimizer is fixed in advance, we found that both Adam and SGD can sometimes work better than the other.</p><p>Optimizers 20% 30% 40%  <ref type="table">Table 3</ref>. Analyzing best optimizer chosen by AutoML during random search over 50 trials for all the AL methods (VGG16 classifier) on CIFAR 10. As we implement 7 AL methods in both standard and strongly-regularized settings; so at each AL iteration we have a total of 14 best optimizers chosen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.6.">Noisy Oracle Experiments</head><p>In conjunction to RSB baselines (presented in main paper), we report performance of AL methods under noisy labels in active sets. The results are reported in Tab. 4 where we make the following observations: (i) it is quite evident that stronglyregularized model improves performance even in label corruptions scenarios. (ii) No AL method consistently outperforms the simple RSB baseline. (iii) Strong-regularization help reduce the performance difference between RSB and best AL method at a particular data split.   <ref type="table">Table 4</ref>. Mean accuracy on noisy oracle experiments on CIFAR10 with (n=3) repeated trials where the best hyper-parameters were found using the random search over 50 trials. We note that the noise is added in active sets drawn by AL methods. The strong-regularization experiments involve SWA and RA techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.7.">Overlap in the active set</head><p>For the interested readers we plot the overlap in CIFAR10 active set sampled in the first AL iteration. As we do five runs for a labeled set partition, we therefore report the average overlap in <ref type="figure">Figure 7</ref>.  <ref type="figure">Figure 7</ref>. Overlap in CIFAR10 active set which is sampled during the first AL iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.8.">Annotation Batch Size</head><p>Here we present the results for CIFAR10 and CIFAR100 in <ref type="table">Table 6</ref> for the experiment where annotation batch size is 5% relative to training data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.9.">Unexplained performance degradation</head><p>In this section we discuss an counter-intuitive observation seen during AL iterations i.e. even with the increase in the labeled data, we sometimes observed the model performance (classification accuracy) degrading. More importantly, this observation was seen across different AL methods and datasets. For example on CIFAR10 from 20% to 30% AL cycle, the uncertainty method degrades its performance by 0.54% (refer Tab. 8). Similarly on CIFAR100 and CIFAR10 from 30% to 35% AL cycle, the coreset and vaal method degrades its performance by 0.01% and 0.09% respectively (refer Tab. 6). Infact during our initial experiments without AutoML and strong-regularization, we observed such behaviour more frequent along-with high variance in accuracy and inconsistent ordinal ranking (by accuracy) across fractions of the data. These observations led us to employ AutoML and strong regularization, which helped reduce variance. We hypothesize that the performance drop could occur through a suboptimal active set selection by the AL method, as we do not interfere with active sets or settings used for AutoML (best of 50 experiments).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Additional Results</head><p>In the last we present the exact accuracies which were used to plot the <ref type="figure">Figure 1</ref> and <ref type="figure">Figure 2</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Compute: All experiments were performed using 2 available nVidia DGX-1 servers, with each experiment utilizing 1-4 GPUs out of available 8 GPUs on each server. All codes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Results using 5% of training data is annotated at each iteration of AL on (a) CIFAR10 and (b) CIFAR100. Mean accuracy for the base model (at 10% labeled data) is noted at the bottom of each plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results are average of 5 runs on imbalanced CIFAR100. The mean accuracy for the base model (at 10% labeled data) is noted at the bottom of plot. a + b * exp(?x) where i ? {1 . . . 100}; a = 100, x = i + 0.5, ? = ?0.046 and b = 400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Effect of strong regularization (RA, SWA) on the test accuracy of CIFAR10(a) and CIFAR100(b). The mean accuracy for the base model (at 10% labeled data) is noted at the bottom of each plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Effect of strong regularization (RA, SWA) (shown in dashed lines) on Imagenet where annotation budget is 5% of training data. Reported results are averaged over 3 runs. For exact accuracies we refer readers to the Tab 5. in suppl.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Algorithm 1 : 4 :</head><label>14</label><figDesc>AL Training Schedule 1: Input AL iter , Budget size k and Oracle, A 2: Split D ? {T r , T s , V } 3: Split T r ? {L 0 0 , U 0 0 } Train a base classifier, B using only L 0 0 5: ? = B 6: while i ? {0 . . . AL iter } do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>?</head><label></label><figDesc>Learning rate: log-scale in range [10 ?5 , 10 ?2 ) ? Weight Decay : log-scale in range [10 ?8 , 10 ?3 ) ? Batch Size: Categorical values from [8,16,32....1024] ? Optimizer: Categorical values from [SGD, ADAM] ? Number of Transformation in randaug (RA N) : Categorical values from [1,2,3,....15] ? Magnitude of Transformation in randaug (RA M) : Categorical values from [1,2,3,....8]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>without</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>and Fig. 2 (bottom right) indicate that no AL method performs consistently best.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Kruskal-Wallis</cell></row><row><cell>Experiments</cell><cell>&gt;RSB</cell><cell>&lt;RSB</cell><cell>Undetermined</cell><cell>P-Value</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Coreset, DBAL, BALD,</cell><cell></cell></row><row><cell>C10 (20%)</cell><cell>-</cell><cell>-</cell><cell>VAAL, QBC, UC</cell><cell>9.35 ? 10 ?4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Coreset, DBAL, BALD,</cell><cell></cell></row><row><cell>C10 (30%)</cell><cell>-</cell><cell>VAAL</cell><cell>QBC, UC</cell><cell>2.67 ? 10 ?9</cell></row><row><cell></cell><cell>BALD, DBAL,</cell><cell></cell><cell></cell><cell></cell></row><row><cell>C10 (40%)</cell><cell>QBC</cell><cell>UC</cell><cell>VAAL, Coreset</cell><cell>8.54 ? 10 ?19</cell></row><row><cell></cell><cell></cell><cell></cell><cell>DBAL, BALD,</cell><cell></cell></row><row><cell>C100 (20%)</cell><cell>Coreset, QBC</cell><cell>-</cell><cell>UC</cell><cell>1.13 ? 10 ?8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Coreset, DBAL,</cell><cell></cell></row><row><cell>C100 (30%)</cell><cell>QBC</cell><cell>VAAL</cell><cell>BALD, UC</cell><cell>1.84 ? 10 ?12</cell></row><row><cell></cell><cell>Coreset,</cell><cell></cell><cell>BALD, VAAL,</cell><cell></cell></row><row><cell>C100 (40%)</cell><cell>DBAL, QBC</cell><cell>-</cell><cell>UC</cell><cell>5.63 ? 10 ?16</cell></row><row><cell cols="5">Table 1. Statistical Analysis of variance; C10/100 refers to CIFAR</cell></row><row><cell>10/100</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>33.03 35 40 45 50 55 % of Labeled Data Mean Accuracy (%)</head><label></label><figDesc>? 0.58 52.83 ? 0.86 54.58 ? 0.56 43.43 ? 0.61 52.05 ? 0.6 57.24 ? 0.28 46.67 ? 0.3 51.43 ? 0.81 55.06 ? 0.35 Coreset 43.69 ? 0.53 53.37 ? 0.87 57.41 ? 0.91 44.31 ? 0.26 54.01 ? 0.87 56.98 ? 0.86 47.33 ? 0.64 49.73 ? 0.92 57.05 ? 0.4 DBAL 40.32 ? 0.45 50.63 ? 0.55 57.12 ? 0.45 43.48 ? 1.00 51.45 ? 0.6 57.36 ? 0.73 45.53 ? 2.33 51.04 ? 0.49 58.06 ? 0.51 BALD 46.61 ? 0.37 51.14 ? 0.60 56.11 ? 0.35 44.47 ? 0.74 51.35 ? 0.61 58.18 ? 0.14 47.1 ? 1.24 50.4 ? 0.88 55.65 ? 0.34 VAAL 42.57 ? 0.89 49.94 ? 1.24 54.28 ? 0.77 46.12 ? 0.96 51.27 ? 0.81 54.41 ? 0.59 39.73 ? 0.43 50.95 ? 0.88 55.23 ? 0.63 QBC 44.86 ? 0.57 51.81 ? 0.53 56.9 ? 0.639 44.64 ? 0.77 52.16 ? 0.38 57.02 ? 0.21 46.04 ? 0.57 53.2 ? 0.38 57.63 ? 0.49 UC 44.65 ? 1.14 51.79 ? 0.29 55.81 ? 0.49 43.76 ? 0.19 52.52 ? 0.93 57.66 ? 0.24 41.37 ? 1.29 52.97 ? 0.83 55.45 ? 0.62Table 2. Test set performance for model selected with different validation set sizes on CIFAR100. Results are average of 5 runs.</figDesc><table><row><cell></cell><cell></cell><cell>2%</cell><cell></cell><cell></cell><cell></cell><cell>5%</cell><cell></cell><cell></cell><cell>10%</cell></row><row><cell>Methods</cell><cell>20%</cell><cell>30%</cell><cell></cell><cell>40%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>RSB</cell><cell cols="2">47.52 20 L Random Coreset DBAL BALD</cell><cell>30</cell><cell>VAAL QBC</cell><cell>40 UC</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>i] =</cell></row></table><note>O :</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>80.91 82.05 81.3 81.98 85.68 75.73 79.69 80.69 88.69 90.44 91.8 Coreset 76.18 82.24 85.11 82.32 84.29 86.72 79.27 82.13 85.09 88.48 91.72 92.94 DBAL 78.08 82.42 85.17 80.79 84.54 87.87 75.05 80.69 82.95 89.41 92.23 93.34 VAAL 77.13 79.97 80.55 78.42 83.79 86.58 74.87 79.96 82.24 87.11 90.43 92.19 QBC 78.63 82.66 85.24 81.85 85.06 87.94 76.61 81.77 84.41 88.34 91.37 92.62 RSB 69.16 72.08 76.62 80.88 RSB-SR 82.16 84.96 86.06 89.13</figDesc><table><row><cell cols="3">Source Architecture</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Target Architectures</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods 20%</cell><cell>VGG16 30%</cell><cell>40%</cell><cell>20%</cell><cell>WRN-28-2 30%</cell><cell>40%</cell><cell>20%</cell><cell>R18 30%</cell><cell>40%</cell><cell>20%</cell><cell>R18-SR 30%</cell><cell>40%</cell><cell cols="2">Methods ? 10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell cols="12">RSB 77.34 Table 4. Transferability experiment on CIFAR10 dataset where source model is VGG16</cell><cell>RSB RSB-SR</cell><cell cols="4">Noise: 10% Noise: 20% 69.16 69.42 75.89 79.61 82.16 77.39 85.9 85.12</cell></row><row><cell cols="12">and target model is Resnet18 (R18) and Wide Resnet-28-2 (WRN-28-2). Test accuracies</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">are reported corresponding to the best model trained on CIFAR10/100 dataset. For best</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="12">model hyper-parameters we perform random search over 50 trials. Results with strong</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">regularization is shown in the last column.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>RSB accuracy with and without strong regularization on CIFAR10 with noisy oracle.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 .</head><label>1</label><figDesc>Reported Random Baseline vs our RSB results. We denote RSB results with strong regularization by RSB-SR.</figDesc><table><row><cell>1.1. Underreported Baselines</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>Model</cell></row><row><cell></cell><cell></cell><cell cols="2">CIFAR10</cell><cell></cell><cell></cell></row><row><cell>QBC</cell><cell>74</cell><cell>82.5</cell><cell>-</cell><cell>-</cell><cell>DenseNet121</cell></row><row><cell>VAAL</cell><cell cols="2">61.35 68.17</cell><cell cols="2">72.26 75.99</cell><cell>VGG16</cell></row><row><cell>Coreset</cell><cell>60</cell><cell>68</cell><cell>71</cell><cell>74</cell><cell>VGG16</cell></row><row><cell>RSB(ours)</cell><cell cols="2">69.16 77.34</cell><cell cols="2">80.91 82.05</cell><cell>VGG16</cell></row><row><cell cols="3">RSB-SR(ours) 82.16 85.09</cell><cell cols="2">89.43 91.16</cell><cell>VGG16</cell></row><row><cell>LLAL</cell><cell>81</cell><cell>87</cell><cell>-</cell><cell>-</cell><cell>ResNet18</cell></row><row><cell>CoreGCN</cell><cell>80</cell><cell>85.5</cell><cell>-</cell><cell>-</cell><cell>ResNet18</cell></row><row><cell>TA-VAAL</cell><cell>81</cell><cell>87.5</cell><cell>-</cell><cell>-</cell><cell>ResNet18</cell></row><row><cell cols="5">RSB-SR(ours) 84.69 88.45 89.98 92.29</cell><cell>ResNet18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>56.99 51.25 58.39 60.56 58.76 65.40 69.12 VAAL 39.32 52.17 55.73 49.13 57.72 55.71 59.76 61.36 67.15 QBC 46.53 53.16 55.54 49.02 53.51 57.05 61.06 66.92 69.83 Table 2. Transferability experiment on CIFAR100 dataset where source model is VGG16. The reported numbers are test accuracies corresponding to the best trained on CIFAR100 dataset. For best model hyper-parameters we perform random search over 50 trials(so for 4 AL iterations; we train 200 models in total). For this experiment we replace all relu activations with leaky relu (negative slope set to 0.2).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Model</cell><cell></cell></row><row><cell></cell><cell></cell><cell>VGG16</cell><cell></cell><cell></cell><cell>WRN-28-2</cell><cell></cell><cell></cell><cell>R18-SR</cell></row><row><cell cols="2">Methods ? 20%</cell><cell>30%</cell><cell>40%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell>Random</cell><cell cols="9">46.72 50.63 55.27 47.87 56.53 57.84 60.17 64.8 69.33</cell></row><row><cell>Coreset</cell><cell>48.2</cell><cell>49.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>82.16 85.04 88.04 90.44 BALD 69.16 75.29 80.24 84.04 82.16 82.15 88.24 89.45 VAAL 69.16 73.85 77.35 79.82 82.16 85.32 86.57 89.53 QBC 69.16 75.64 77.87 80.53 82.16 85.25 87.39 88.68 UC 69.16 75.94 80.42 81.92 82.16 82.61 85.19 88.62 Noise: 20% RSB 69.16 69.42 75.89 79.61 82.16 77.39 85.9 85.12 Coreset 69.16 71.13 76.44 80.07 82.16 80.05 88.05 88.32 DBAL 69.16 71.26 76.24 82.2 82.16 81.31 83.67 91.14 BALD 69.16 70.34 77.18 79.86 82.16 85.26 88.52 91.21 VAAL 69.16 70.13 74.94 76.42 82.16 82.39 82.66 88.31 QBC 69.16 71.18 76.52 77.78 82.16 83.17 84.68 85.62 UC 69.16 71.53 75.48 78.48 82.16 84.57 83.00 88.42</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">with</cell></row><row><cell></cell><cell cols="4">strong-regularization</cell><cell cols="4">strong-regularization</cell></row><row><cell cols="2">Methods ? 10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Noise: 10%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RSB</cell><cell cols="8">69.16 72.08 76.62 80.88 82.16 84.96 86.06 89.13</cell></row><row><cell>Coreset</cell><cell cols="8">69.16 75.97 80.07 82.78 82.16 82.99 88.14 90.31</cell></row><row><cell>DBAL</cell><cell cols="3">69.16 76.98 80.5</cell><cell>84.4</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>? 0.13 61.55 ? 0.32 64.51 ? 0.13 66.52 ? 0.2 Coreset 57.89 ? 0.13 62.36 ? 0.19 65.42 ? 0.19 67.8 ? 0.23 VAAL 57.89 ? 0.13 62.87 ? 1.18 65.11 ? 0.74 67.08 ? 0.42 with RA + SWA RSB 60.1 ? 0.09 64.59 ? 0.62 67.14 ? 0.18 69.2 ? 0.11 Coreset 60.1 ? 0.09 64.98 ? 0.50 67.97 ? 0.25 70.12 ? 0.13 VAAL 60.1 ? 0.09 64.88 ? 0.53 67.45 ? 0.26 69.37 ? 0.08 Table 5. Effect of RA and SWA on ImageNet where annotation budget is 5% of training data. Reported results are averaged over 3 runs.</figDesc><table><row><cell>Methods ?</cell><cell>10%</cell><cell>15%</cell><cell>20%</cell><cell>25%</cell></row><row><cell></cell><cell></cell><cell>without RA + SWA</cell><cell></cell><cell></cell></row><row><cell>RSB</cell><cell>57.89</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>? 0.88 78.27 ? 0.47 79.79 ? 0.64 81.86 ? 0.60 81.50 ? 0.45 83.21 ? 1.14 Coreset 74.56 ? 0.70 75.11 ? 0.92 81.23 ? 0.27 82.58 ? 0.57 83.9 ? 0.70 84.30 ? 0.56 DBAL 73.58 ? 0.81 79.33 ? 0.61 80.27 ? 1.10 81.78 ? 1.47 83.30 ? 0.75 83.86 ? 0.47 BALD 75.43 ? 0.63 79.19 ? 0.51 78.29 ? 0.63 81.69 ? 0.38 83.42 ? 1.54 85.23 ? 0.41 VAAL 74.07 ? 2.11 78.28 ? 1.00 78.88 ? 0.97 81.07 ? 0.61 80.98 ? 0.79 81.72 ? 2.33 QBC 72.63 ? 2.14 75.07 ? 2.07 76.95 ? 1.52 80.72 ? 0.34 81.76 ? 1.03 83.53 ? 0.59 UC 76.90 ? 1.12 78.14 ? 0.79 80.75 ? 0.62 81.47 ? 0.53 84.60 ? 0.71 83.13 ? 0.64 CIFAR100 RSB 35.15 ? 0.55 43.10 ? 0.46 49.33 ? 0.73 52.24 ? 0.56 51.76 ? 1.29 55.49 ? 0.64 Coreset 43.19 ? 0.65 42.58 ? 0.32 46.85 ? 0.83 52.47 ? 0.58 52.48 ? 0.93 57.45 ? 0.54 DBAL 35.83 ? 0.83 32.54 ? 1.92 42.93 ? 6.69 52.27 ? 3.59 54.58 ? 1.18 57.68 ? 0.46 BALD 37.55 ? 0.70 43.86 ? 0.48 49.79 ? 0.29 51.96 ? 0.81 54.75 ? 0.63 57.20 ? 0.90 VAAL 36.75 ? 1.36 37.05 ? 1.78 47.62 ? 1.07 47.20 ? 0.25 53.61 ? 0.44 52.87 ? 0.63 QBC 38.91 ? 0.70 43.57 ? 0.62 47.76 ? 0.61 51.16 ? 0.49 54.06 ? 0.33 56.51 ? 0.42 UC 36.52 ? 0.55 41.23 ? 0.89 50.59 ? 0.50 51.42 ? 0.42 55.14 ? 0.97 53.15 ? 0.36 Table 6. Mean Accuracy and Standard Deviation on CIFAR10/100 test set with annotation size as 5% of training set. Results reported are averaged over 5 runs where hyper-parameters are tuned in the first run using AutoML random search over 50 trials.</figDesc><table><row><cell>?</cell><cell>15%</cell><cell>20%</cell><cell>25%</cell><cell>30%</cell><cell>35%</cell><cell>40%</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CIFAR10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RSB</cell><cell>74.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>in main paper. Tab. 7 to Tab. 11 reports the test accuracies for CIFAR10 dataset and Tab. 12 to Tab. 16 reports the test accuracies for CIFAR100 dataset. RSB 77.65 ? 0.82 81.39 ? 0.59 82.19 ? 1.55 Coreset 77.19 ? 1.93 82.58 ? 0.67 83.86 ? 1.08 DBAL 78.81 ? 1.28 80.99 ? 2.25 83.96 ? 2.01 BALD 78.35 ? 1.98 79.95 ? 1.43 84.29 ? 0.25 VAAL 75.89 ? 2.41 80.37 ? 0.34 81.75 ? 0.87 QBC 78.10 ? 0.73 80.31 ? 1.83 84.35 ? 0.64 UC 73.35 ? 4.84 81.98 ? 0.93 84.49 ? 1.18 Table 7. CIFAR10 Test Accuracy on L 0 0 . The base model accuracy is 69.16. RSB 77.85 ? 0.65 81.68 ? 0.39 82.71 ? 0.42 Coreset 77.70 ? 1.31 82.78 ? 0.90 83.79 ? 0.74 DBAL 79.28 ? 0.78 81.16 ? 0.83 85.58 ? 0.19 BALD 78.67 ? 0.39 82.95 ? 0.43 84.11 ? 0.30 VAAL 76.50 ? 0.70 79.12 ? 0.62 82.86 ? 0.69 QBC 78.22 ? 1.84 82.68 ? 0.54 85.34 ? 1.26 UC 80.03 ? 0.27 79.49 ? 0.37 85.45 ? 0.69 Table 8. CIFAR10 Test Accuracy on L 0 1 .The base model accuracy is 68.02. ? 0.71 80.50 ? 0.30 83.82 ? 0.37 Coreset 74.67 ? 0.82 81.14 ? 0.92 81.58 ? 1.19 DBAL 75.9 ? 0.25 80.58 ? 3.16 83.75 ? 0.88 BALD 76.19 ? 0.86 83.26 ? 0.36 85.39 ? 0.97 VAAL 76.88 ? 0.96 81.30 ? 0.29 82.63 ? 0.55 QBC 78.38 ? 0.79 81.39 ? 3.3 85.16 ? 0.77 UC 78.16 ? 0.85 81.80 ? 0.45 84.91 ? 0.69 Table 9. CIFAR10 Test Accuracy on L 0 2 . The base model accuracy is 70.34. ? 1.91 80.93 ? 1.20 83.17 ? 0.52 Coreset 79.42 ? 0.47 81.62 ? 0.86 83.82 ? 0.18 DBAL 79.48 ? 0.35 82.27 ? 1.23 84.74 ? 0.14 BALD 77.58 ? 0.88 82.11 ? 0.65 84.58 ? 0.42 VAAL 77.45 ? 1.21 79.38 ? 1.08 82.90 ? 0.94 QBC 78.60 ? 0.43 82.76 ? 0.92 85.54 ? 0.69 UC 76.97 ? 0.79 81.35 ? 0.82 84.65 ? 0.30 Table 10. CIFAR10 Test Accuracy on L 0 3 .The base model accuracy is 68.19. ? 0.91 81.81 ? 0.71 83.46 ? 0.18 Coreset 77.17 ? 1.82 81.37 ? 0.41 83.13 ? 1.54 DBAL 75.87 ? 0.61 83.00 ? 0.79 85.13 ? 1.25 BALD 78.49 ? 0.46 83.21 ? 0.66 85.06 ? 0.60 VAAL 73.67 ? 1.47 79.49 ? 1.27 82.98 ? 0.78 QBC 78.61 ? 1.65 83.81 ? 0.49 85.35 ? 0.82 UC 77.38 ? 1.17 81.82 ? 1.86 85.62 ? 0.30 Table 11. CIFAR10 Test Accuracy on L 0 4 . The base model accuracy is 67.19. ? 0.30 51.43 ? 0.81 55.06 ? 0.35 Coreset 47.33 ? 0.64 49.73 ? 0.92 57.05 ? 0.40 DBAL 45.53 ? 2.33 51.04 ? 0.49 58.06 ? 0.51 BALD 47.10 ? 1.24 50.40 ? 0.88 55.65 ? 0.34 VAAL 39.73 ? 0.43 50.95 ? 0.88 55.23 ? 0.63 QBC 46.04 ? 0.57 53.20 ? 0.38 57.63 ? 0.49 UC 41.37 ? 1.29 52.97 ? 0.83 55.45 ? 0.62 Table 12. CIFAR100 Test Accuracy on L 0 0 . The base model accuracy is 34.73. ? 0.19 53.45 ? 0.28 56.98 ? 0.31 Coreset 46.05 ? 0.46 52.04 ? 0.23 58.11 ? 0.12 DBAL 41.32 ? 0.23 52.16 ? 0.81 58.00 ? 0.68 BALD 43.57 ? 0.80 53.27 ? 0.12 56.87 ? 0.73 VAAL 42.70 ? 0.75 48.86 ? 1.61 54.81 ? 1.23 QBC 45.61 ? 0.74 53.31 ? 0.91 58.21 ? 0.22 UC 37.48 ? 0.45 53.01 ? 0.16 57.80 ? 0.09 Table 13. CIFAR100 Test Accuracy on L 0 1 .The base model accuracy is 32.73. ? 0.64 50.01 ? 0.36 56.27 ? 0.84 Coreset 46.00 ? 0.79 53.48 ? 0.61 57.22 ? 0.69 DBAL 44.06 ? 0.39 49.29 ? 1.00 57.40 ? 0.34 BALD 46.78 ? 0.52 52.34 ? 0.90 54.97 ? 0.98 VAAL 44.75 ? 0.57 49.72 ? 0.40 55.77 ? 0.62 QBC 46.20 ? 0.72 53.15 ? 0.90 57.96 ? 0.65 UC 43.94 ? 0.60 53.75 ? 0.50 55.10 ? 0.95 Table 14. CIFAR100 Test Accuracy on L 0 2 . The base model accuracy is 34.66. ? 0.44 52.66 ? 0.66 54.15 ? 0.43 Coreset 45.98 ? 0.83 54.34 ? 0.53 56.96 ? 0.95 DBAL 45.49 ? 0.51 48.84 ? 0.42 57.65 ? 0.46 BALD 47.21 ? 1.26 52.53 ? 0.42 55.39 ? 0.72 VAAL 44.93 ? 1.61 46.27 ? 0.72 56.65 ? 0.60 QBC 46.50 ? 0.56 53.49 ? 0.53 57.68 ? 0.51 UC 46.96 ? 0.41 53.07 ? 0.57 56.35 ? 0.79 Table 15. CIFAR100 Test Accuracy on L 0 3 .The base model accuracy is 30.44. RSB 41.15 ? 0.89 50.61 ? 0.40 56.77 ? 0.55 Coreset 45.72 ? 0.77 52.22 ? 0.54 56.28 ? 0.45 DBAL 44.71 ? 0.57 52.33 ? 0.49 56.52 ? 0.51 BALD 40.35 ? 0.75 51.87 ? 0.60 57.40 ? 0.40 VAAL 44.86 ? 1.69 51.32 ? 1.54 53.82 ? 1.08 QBC 45.93 ? 0.46 53.12 ? 0.55 57.78 ? 0.49 UC 43.07 ? 0.74 49.89 ? 0.79 56.15 ? 0.52 Table 16. CIFAR100 Test Accuracy on L 0 4 .The base model accuracy is 34.85.</figDesc><table><row><cell>Methods</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>Methods</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell></row><row><cell cols="6">Methods RSB 75.84 Methods 20% 30% 40% 20% 77.02 Methods RSB 20% 30% 40% RSB 78.59 Methods 20% RSB 46.67 Methods 20% 30% 40% RSB 45.58 Methods 20% RSB 44.71 Methods 20% 30% 40% RSB 42.46 Methods 20%</cell><cell>30% 30% 30% 30%</cell><cell>40% 40% 40% 40%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All Model definitions in AL Toolkit has been provided as a supplementary material</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optuna: A next-generation hyperparameter optimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The power of ensembles for active learning in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Beluch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">M</forename><surname>N?rnberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>K?hler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="9368" to="9377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential graph convolutional network for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Caramalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binod</forename><surname>Bhattarai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Kyun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9583" to="9592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.09501</idno>
		<title level="m">Learning augmentation policies from data</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randaugment</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13719</idno>
		<title level="m">Practical data augmentation with no separate search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial active learning for deep networks: a margin based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ducoffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fr?d?ric</forename><surname>Precioso</surname></persName>
		</author>
		<idno>abs/1802.09841</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep bayesian active learning with image data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riashat</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1183" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<imprint>
			<publisher>Aaron Courville,</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semi-supervised active learning with temporal output discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyi</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3447" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Averaging weights leads to wider optima and better generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Izmailov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitrii</forename><surname>Podoprikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timur</forename><surname>Garipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Gordon</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05407</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Task-aware variational adversarial active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwon</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang</forename><forename type="middle">In</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Se Young</forename><surname>Chun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="8166" to="8175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Batchbald: Efficient and diverse batch acquisition for deep bayesian active learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kirsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Joost Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuangliu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resnet18</surname></persName>
		</author>
		<ptr target="https://github.com/kuangliu/pytorch-cifar" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A sequential algorithm for training text classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William A</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR&apos;94</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">How transferable are the datasets collected by active learners? CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<idno>abs/1807.04801</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meliketoy</surname></persName>
		</author>
		<ptr target="https://github.com/meliketoy/wide-resnet.pytorch" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">wide-resnet</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Parting with illusions about deep active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudhanshu</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>?zg?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Realistic evaluation of deep semi-supervised learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekin</forename><surname>Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3235" to="3246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Prabhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Dognin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09389</idno>
		<title level="m">Sampling bias in deep active classification: An empirical study</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Active learning for convolutional neural networks: A core-set approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00370</idno>
		<title level="m">Variational adversarial active learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bayesian generative active deep learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klint</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 conference on fairness, accountability, and transparency</title>
		<meeting>the 2020 conference on fairness, accountability, and transparency</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="547" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A study of face obfuscation in imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacqueline</forename><surname>Yau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06191</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning loss for active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><surname>So Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">S 4 l: Self-supervised semi-supervised learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">State-relabeling adversarial active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8756" to="8765" />
		</imprint>
	</monogr>
	<note>Zheng-Jun Zha, and Qingming Huang</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
