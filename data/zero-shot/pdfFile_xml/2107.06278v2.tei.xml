<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Per-Pixel Classification is Not All You Need for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign (UIUC)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Per-Pixel Classification is Not All You Need for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern approaches typically formulate semantic segmentation as a per-pixel classification task, while instance-level segmentation is handled with an alternative mask classification. Our key insight: mask classification is sufficiently general to solve both semantic-and instance-level segmentation tasks in a unified manner using the exact same model, loss, and training procedure. Following this observation, we propose MaskFormer, a simple mask classification model which predicts a set of binary masks, each associated with a single global class label prediction. Overall, the proposed mask classification-based method simplifies the landscape of effective approaches to semantic and panoptic segmentation tasks and shows excellent empirical results. In particular, we observe that MaskFormer outperforms per-pixel classification baselines when the number of classes is large. Our mask classification-based method outperforms both current state-of-the-art semantic (55.6 mIoU on ADE20K) and panoptic segmentation (52.7 PQ on COCO) models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of semantic segmentation is to partition an image into regions with different semantic categories. Starting from Fully Convolutional Networks (FCNs) work of Long et al. <ref type="bibr" target="#b29">[30]</ref>, most deep learning-based semantic segmentation approaches formulate semantic segmentation as per-pixel classification <ref type="figure">(Figure 1 left)</ref>, applying a classification loss to each output pixel <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b51">52]</ref>. Per-pixel predictions in this formulation naturally partition an image into regions of different classes.</p><p>Mask classification is an alternative paradigm that disentangles the image partitioning and classification aspects of segmentation. Instead of classifying each pixel, mask classification-based methods predict a set of binary masks, each associated with a single class prediction <ref type="figure">(Figure 1 right)</ref>. The more flexible mask classification dominates the field of instance-level segmentation. Both Mask R-CNN <ref type="bibr" target="#b20">[21]</ref> and DETR <ref type="bibr" target="#b3">[4]</ref> yield a single class prediction per segment for instance and panoptic segmentation. In contrast, per-pixel classification assumes a static number of outputs and cannot return a variable number of predicted regions/segments, which is required for instance-level tasks.</p><p>Our key observation: mask classification is sufficiently general to solve both semantic-and instancelevel segmentation tasks. In fact, before FCN <ref type="bibr" target="#b29">[30]</ref>, the best performing semantic segmentation methods like O2P <ref type="bibr" target="#b4">[5]</ref> and SDS <ref type="bibr" target="#b19">[20]</ref> used a mask classification formulation. Given this perspective, a natural question emerges: can a single mask classification model simplify the landscape of effective approaches to semantic-and instance-level segmentation tasks? And can such a mask classification model outperform existing per-pixel classification methods for semantic segmentation?</p><p>To address both questions we propose a simple MaskFormer approach that seamlessly converts any existing per-pixel classification model into a mask classification. Using the set prediction mechanism proposed in DETR <ref type="bibr" target="#b3">[4]</ref>, MaskFormer employs a Transformer decoder <ref type="bibr" target="#b40">[41]</ref> to compute a set of pairs,  <ref type="figure">Figure 1</ref>: Per-pixel classification vs. mask classification. (left) Semantic segmentation with perpixel classification applies the same classification loss to each location. (right) Mask classification predicts a set of binary masks and assigns a single class to each mask. Each prediction is supervised with a per-pixel binary mask loss and a classification loss. Matching between the set of predictions and ground truth segments can be done either via bipartite matching similarly to DETR <ref type="bibr" target="#b3">[4]</ref> or by fixed matching via direct indexing if the number of predictions and classes match, i.e., if N = K.</p><p>each consisting of a class prediction and a mask embedding vector. The mask embedding vector is used to get the binary mask prediction via a dot product with the per-pixel embedding obtained from an underlying fully-convolutional network. The new model solves both semantic-and instance-level segmentation tasks in a unified manner: no changes to the model, losses, and training procedure are required. Specifically, for semantic and panoptic segmentation tasks alike, MaskFormer is supervised with the same per-pixel binary mask loss and a single classification loss per mask. Finally, we design a simple inference strategy to blend MaskFormer outputs into a task-dependent prediction format.</p><p>We evaluate MaskFormer on five semantic segmentation datasets with various numbers of categories: Cityscapes <ref type="bibr" target="#b14">[15]</ref> (19 classes), Mapillary Vistas <ref type="bibr" target="#b33">[34]</ref> (65 classes), ADE20K <ref type="bibr" target="#b54">[55]</ref> (150 classes), COCO-Stuff-10K <ref type="bibr" target="#b2">[3]</ref> (171 classes), and ADE20K-Full <ref type="bibr" target="#b54">[55]</ref> (847 classes). While MaskFormer performs on par with per-pixel classification models for Cityscapes, which has a few diverse classes, the new model demonstrates superior performance for datasets with larger vocabulary. We hypothesize that a single class prediction per mask models fine-grained recognition better than per-pixel class predictions. MaskFormer achieves the new state-of-the-art on ADE20K (55.6 mIoU) with Swin-Transformer <ref type="bibr" target="#b28">[29]</ref> backbone, outperforming a per-pixel classification model <ref type="bibr" target="#b28">[29]</ref> with the same backbone by 2.1 mIoU, while being more efficient (10% reduction in parameters and 40% reduction in FLOPs).</p><p>Finally, we study MaskFormer's ability to solve instance-level tasks using two panoptic segmentation datasets: COCO <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b23">24]</ref> and ADE20K <ref type="bibr" target="#b54">[55]</ref>. MaskFormer outperforms a more complex DETR model <ref type="bibr" target="#b3">[4]</ref> with the same backbone and the same post-processing. Moreover, MaskFormer achieves the new state-of-the-art on COCO (52.7 PQ), outperforming prior state-of-the-art <ref type="bibr" target="#b41">[42]</ref> by 1.6 PQ. Our experiments highlight MaskFormer's ability to unify instance-and semantic-level segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Both per-pixel classification and mask classification have been extensively studied for semantic segmentation. In early work, Konishi and Yuille <ref type="bibr" target="#b24">[25]</ref> apply per-pixel Bayesian classifiers based on local image statistics. Then, inspired by early works on non-semantic groupings <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b35">36]</ref>, mask classification-based methods became popular demonstrating the best performance in PASCAL VOC challenges <ref type="bibr" target="#b17">[18]</ref>. Methods like O2P <ref type="bibr" target="#b4">[5]</ref> and CFM <ref type="bibr" target="#b15">[16]</ref> have achieved state-of-the-art results by classifying mask proposals <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b1">2]</ref>. In 2015, FCN <ref type="bibr" target="#b29">[30]</ref> extended the idea of per-pixel classification to deep nets, significantly outperforming all prior methods on mIoU (a per-pixel evaluation metric which particularly suits the per-pixel classification formulation of segmentation).</p><p>Per-pixel classification became the dominant way for deep-net-based semantic segmentation since the seminal work of Fully Convolutional Networks (FCNs) <ref type="bibr" target="#b29">[30]</ref>. Modern semantic segmentation models focus on aggregating long-range context in the final feature map: ASPP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> uses atrous convolutions with different atrous rates; PPM <ref type="bibr" target="#b51">[52]</ref> uses pooling operators with different kernel sizes; DANet <ref type="bibr" target="#b18">[19]</ref>, OCNet <ref type="bibr" target="#b50">[51]</ref>, and CCNet <ref type="bibr" target="#b22">[23]</ref> use different variants of non-local blocks <ref type="bibr" target="#b42">[43]</ref>. Recently, SETR <ref type="bibr" target="#b52">[53]</ref> and Segmenter <ref type="bibr" target="#b36">[37]</ref> replace traditional convolutional backbones with Vision Transformers (ViT) <ref type="bibr" target="#b16">[17]</ref> that capture long-range context starting from the very first layer. However, these concurrent Transformer-based <ref type="bibr" target="#b40">[41]</ref> semantic segmentation approaches still use a per-pixel classification formulation. Note, that our MaskFormer module can convert any per-pixel classification model to the mask classification setting, allowing seamless adoption of advances in per-pixel classification.</p><p>Mask classification is commonly used for instance-level segmentation tasks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. These tasks require a dynamic number of predictions, making application of per-pixel classification challenging as it assumes a static number of outputs. Omnipresent Mask R-CNN <ref type="bibr" target="#b20">[21]</ref> uses a global classifier to classify mask proposals for instance segmentation. DETR <ref type="bibr" target="#b3">[4]</ref> further incorporates a Transformer <ref type="bibr" target="#b40">[41]</ref> design to handle thing and stuff segmentation simultaneously for panoptic segmentation <ref type="bibr" target="#b23">[24]</ref>. However, these mask classification methods require predictions of bounding boxes, which may limit their usage in semantic segmentation. The recently proposed Max-DeepLab <ref type="bibr" target="#b41">[42]</ref> removes the dependence on box predictions for panoptic segmentation with conditional convolutions <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b43">44]</ref>. However, in addition to the main mask classification losses it requires multiple auxiliary losses (i.e., instance discrimination loss, mask-ID cross entropy loss, and the standard per-pixel classification loss).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">From Per-Pixel to Mask Classification</head><p>In this section, we first describe how semantic segmentation can be formulated as either a per-pixel classification or a mask classification problem. Then, we introduce our instantiation of the mask classification model with the help of a Transformer decoder <ref type="bibr" target="#b40">[41]</ref>. Finally, we describe simple inference strategies to transform mask classification outputs into task-dependent prediction formats.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Per-pixel classification formulation</head><p>For per-pixel classification, a segmentation model aims to predict the probability distribution over all possible K categories for every pixel of an H?W image:</p><formula xml:id="formula_0">y = {p i |p i ? ? K } H?W i=1 .</formula><p>Here ? K is the Kdimensional probability simplex. Training a per-pixel classification model is straight-forward: given ground truth category labels y gt = {y gt i |y gt i ? {1, . . . , K}} H?W i=1 for every pixel, a per-pixel crossentropy (negative log-likelihood) loss is usually applied, i.e., L pixel-cls (y, y gt ) = H?W i=1 ? log p i (y gt i ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mask classification formulation</head><p>Mask classification splits the segmentation task into 1) partitioning/grouping the image into N regions (N does not need to equal K), represented with binary masks {m i |m i ? [0, 1] H?W } N i=1 ; and 2) associating each region as a whole with some distribution over K categories. To jointly group and classify a segment, i.e., to perform mask classification, we define the desired output z as a set of N probability-mask pairs, i.e., z = {(p i , m i )} N i=1 . In contrast to per-pixel class probability prediction, for mask classification the probability distribution p i ? ? K+1 contains an auxiliary "no object" label (?) in addition to the K category labels. The ? label is predicted for masks that do not correspond to any of the K categories. Note, mask classification allows multiple mask predictions with the same associated class, making it applicable to both semantic-and instance-level segmentation tasks.</p><p>To train a mask classification model, a matching ? between the set of predictions z and the set of N gt ground truth segments</p><formula xml:id="formula_1">z gt = {(c gt i , m gt i )|c gt i ? {1, . . . , K}, m gt i ? {0, 1} H?W } N gt i=1</formula><p>is required. <ref type="bibr" target="#b1">2</ref> Here c gt i is the ground truth class of the i th ground truth segment. Since the size of prediction set |z| = N and ground truth set |z gt | = N gt generally differ, we assume N ? N gt and pad the set of ground truth labels with "no object" tokens ? to allow one-to-one matching.</p><p>For semantic segmentation, a trivial fixed matching is possible if the number of predictions N matches the number of category labels K. In this case, the i th prediction is matched to a ground truth region with class label i and to ? if a region with class label i is not present in the ground truth. In our experiments, we found that a bipartite matching-based assignment demonstrates better results than the fixed matching. Unlike DETR <ref type="bibr" target="#b3">[4]</ref> that uses bounding boxes to compute the assignment costs between prediction z i and ground truth z gt j for the matching problem, we directly use class and mask predictions, i.e., ?p i (c gt j ) + L mask (m i , m gt j ), where L mask is a binary mask loss. To train model parameters, given a matching, the main mask classification loss L mask-cls is composed of a cross-entropy classification loss and a binary mask loss L mask for each predicted segment:  <ref type="figure">Figure 2</ref>: MaskFormer overview. We use a backbone to extract image features F. A pixel decoder gradually upsamples image features to extract per-pixel embeddings E pixel . A transformer decoder attends to image features and produces N per-segment embeddings Q. The embeddings independently generate N class predictions with N corresponding mask embeddings E mask . Then, the model predicts N possibly overlapping binary mask predictions via a dot product between pixel embeddings E pixel and mask embeddings E mask followed by a sigmoid activation. For semantic segmentation task we can get the final prediction by combining N binary masks with their class predictions using a simple matrix multiplication (see Section 3.4). Note, the dimensions for multiplication are shown in gray.</p><formula xml:id="formula_2">L mask-cls (z, z gt ) = N j=1 ? log p ?(j) (c gt j ) + 1 c gt j =? L mask (m ?(j) , m gt j ) .<label>(1)</label></formula><p>Note, that most existing mask classification models use auxiliary losses (e.g., a bounding box loss <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4]</ref> or an instance discrimination loss <ref type="bibr" target="#b41">[42]</ref>) in addition to L mask-cls . In the next section we present a simple mask classification model that allows end-to-end training with L mask-cls alone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">MaskFormer</head><p>We now introduce MaskFormer, the new mask classification model, which computes N probability-</p><formula xml:id="formula_3">mask pairs z = {(p i , m i )} N i=1 .</formula><p>The model contains three modules (see <ref type="figure">Fig. 2</ref>): 1) a pixel-level module that extracts per-pixel embeddings used to generate binary mask predictions; 2) a transformer module, where a stack of Transformer decoder layers <ref type="bibr" target="#b40">[41]</ref> computes N per-segment embeddings; and 3) a segmentation module, which generates predictions {(p i , m i )} N i=1 from these embeddings. During inference, discussed in Sec. 3.4, p i and m i are assembled into the final prediction.</p><p>Pixel-level module takes an image of size H ? W as input. A backbone generates a (typically) low-resolution image feature map F ? R C F ? H S ? W S , where C F is the number of channels and S is the stride of the feature map (C F depends on the specific backbone and we use S = 32 in this work). Then, a pixel decoder gradually upsamples the features to generate per-pixel embeddings E pixel ? R C E ?H?W , where C E is the embedding dimension. Note, that any per-pixel classificationbased segmentation model fits the pixel-level module design including recent Transformer-based models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b28">29]</ref>. MaskFormer seamlessly converts such a model to mask classification.</p><p>Transformer module uses the standard Transformer decoder <ref type="bibr" target="#b40">[41]</ref> to compute from image features F and N learnable positional embeddings (i.e., queries) its output, i.e., N per-segment embeddings Q ? R C Q ?N of dimension C Q that encode global information about each segment MaskFormer predicts. Similarly to <ref type="bibr" target="#b3">[4]</ref>, the decoder yields all predictions in parallel.</p><p>Segmentation module applies a linear classifier, followed by a softmax activation, on top of the per-segment embeddings Q to yield class probability predictions {p i ? ? K+1 } N i=1 for each segment. Note, that the classifier predicts an additional "no object" category (?) in case the embedding does not correspond to any region. For mask prediction, a Multi-Layer Perceptron (MLP) with 2 hidden layers converts the per-segment embeddings Q to N mask embeddings E mask ? R C E ?N of dimension C E . Finally, we obtain each binary mask prediction m i ? [0, 1] H?W via a dot product between the i th mask embedding and per-pixel embeddings E pixel computed by the pixel-level module. The dot product is followed by a sigmoid activation, i.e.,</p><formula xml:id="formula_4">m i [h, w] = sigmoid(E mask [:, i] T ? E pixel [:, h, w]).</formula><p>Note, we empirically find it is beneficial to not enforce mask predictions to be mutually exclusive to each other by using a softmax activation. During training, the L mask-cls loss combines a cross entropy classification loss and a binary mask loss L mask for each predicted segment. For simplicity we use the same L mask as DETR <ref type="bibr" target="#b3">[4]</ref>, i.e., a linear combination of a focal loss <ref type="bibr" target="#b26">[27]</ref> and a dice loss <ref type="bibr" target="#b32">[33]</ref> multiplied by hyper-parameters ? focal and ? dice respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Mask-classification inference</head><p>First, we present a simple general inference procedure that converts mask classification outputs</p><formula xml:id="formula_5">{(p i , m i )} N i=1</formula><p>to either panoptic or semantic segmentation output formats. Then, we describe a semantic inference procedure specifically designed for semantic segmentation. We note, that the specific choice of inference strategy largely depends on the evaluation metric rather than the task.</p><p>General inference partitions an image into segments by assigning each pixel [h, w] to one of the N predicted probability-mask pairs via arg max i:</p><formula xml:id="formula_6">ci =? p i (c i ) ? m i [h, w].</formula><p>Here c i is the most likely class label c i = arg max c?{1,...,K,?} p i (c) for each probability-mask pair i. Intuitively, this procedure assigns a pixel at location [h, w] to probability-mask pair i only if both the most likely class probability p i (c i ) and the mask prediction probability m i [h, w] are high. Pixels assigned to the same probabilitymask pair i form a segment where each pixel is labelled with c i . For semantic segmentation, segments sharing the same category label are merged; whereas for instance-level segmentation tasks, the index i of the probability-mask pair helps to distinguish different instances of the same class. Finally, to reduce false positive rates in panoptic segmentation we follow previous inference strategies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>. Specifically, we filter out low-confidence predictions prior to inference and remove predicted segments that have large parts of their binary masks (m i &gt; 0.5) occluded by other predictions.</p><p>Semantic inference is designed specifically for semantic segmentation and is done via a simple matrix multiplication. We empirically find that marginalization over probability-mask pairs, i.e., arg max c?{1,...,K}</p><formula xml:id="formula_7">N i=1 p i (c) ? m i [h, w]</formula><p>, yields better results than the hard assignment of each pixel to a probability-mask pair i used in the general inference strategy. The argmax does not include the "no object" category (?) as standard semantic segmentation requires each output pixel to take a label. Note, this strategy returns a per-pixel class probability</p><formula xml:id="formula_8">N i=1 p i (c) ? m i [h, w].</formula><p>However, we observe that directly maximizing per-pixel class likelihood leads to poor performance. We hypothesize, that gradients are evenly distributed to every query, which complicates training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate that MaskFormer seamlessly unifies semantic-and instance-level segmentation tasks by showing state-of-the-art results on both semantic segmentation and panoptic segmentation datasets. Then, we ablate the MaskFormer design confirming that observed improvements in semantic segmentation indeed stem from the shift from per-pixel classification to mask classification.</p><p>Datasets. We study MaskFormer using four widely used semantic segmentation datasets: ADE20K <ref type="bibr" target="#b54">[55]</ref> (150 classes) from the SceneParse150 challenge <ref type="bibr" target="#b53">[54]</ref>, COCO-Stuff-10K <ref type="bibr" target="#b2">[3]</ref> (171 classes), Cityscapes <ref type="bibr" target="#b14">[15]</ref> (19 classes), and Mapillary Vistas <ref type="bibr" target="#b33">[34]</ref> (65 classes). In addition, we use the ADE20K-Full <ref type="bibr" target="#b54">[55]</ref> dataset annotated in an open vocabulary setting (we keep 874 classes that are present in both train and validation sets). For panotic segmenation evaluation we use COCO <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b23">24]</ref> (80 "things" and 53 "stuff" categories) and ADE20K-Panoptic <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b23">24]</ref> (100 "things" and 50 "stuff" categories). Please see the appendix for detailed descriptions of all used datasets.</p><p>Evaluation metrics. For semantic segmentation the standard metric is mIoU (mean Intersection-over-Union) <ref type="bibr" target="#b17">[18]</ref>, a per-pixel metric that directly corresponds to the per-pixel classification formulation. To better illustrate the difference between segmentation approaches, in our ablations we supplement mIoU with PQ St (PQ stuff) <ref type="bibr" target="#b23">[24]</ref>, a per-region metric that treats all classes as "stuff" and evaluates each segment equally, irrespective of its size. We report the median of 3 runs for all datasets, except for Cityscapes where we report the median of 5 runs. For panoptic segmentation, we use the standard PQ (panoptic quality) metric <ref type="bibr" target="#b23">[24]</ref> and report single run results due to prohibitive training costs. Baseline models. On the right we sketch the used per-pixel classification baselines. The PerPixelBaseline uses the pixel-level module of MaskFormer and directly outputs per-pixel class scores. For a fair comparison, we design PerPixelBaseline+ which adds the transformer module and mask embedding MLP to the PerPixelBaseline. Thus, PerPixelBaseline+ and MaskFormer differ only in the formulation: per-pixel vs. mask classification. Note that these baselines are for ablation and we compare MaskFormer with state-of-the-art per-pixel classification models as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation details</head><p>Backbone. MaskFormer is compatible with any backbone architecture. In our work we use the standard convolution-based ResNet <ref type="bibr" target="#b21">[22]</ref> backbones (R50 and R101 with 50 and 101 layers respectively) and recently proposed Transformer-based Swin-Transformer <ref type="bibr" target="#b28">[29]</ref> backbones. In addition, we use the R101c model <ref type="bibr" target="#b6">[7]</ref> which replaces the first 7 ? 7 convolution layer of R101 with 3 consecutive 3 ? 3 convolutions and which is popular in the semantic segmentation community <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>Pixel decoder. The pixel decoder in <ref type="figure">Figure 2</ref> can be implemented using any semantic segmentation decoder (e.g., <ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>). Many per-pixel classification methods use modules like ASPP <ref type="bibr" target="#b6">[7]</ref> or PSP <ref type="bibr" target="#b51">[52]</ref> to collect and distribute context across locations. The Transformer module attends to all image features, collecting global information to generate class predictions. This setup reduces the need of the per-pixel module for heavy context aggregation. Therefore, for MaskFormer, we design a light-weight pixel decoder based on the popular FPN <ref type="bibr" target="#b25">[26]</ref> architecture.</p><p>Following FPN, we 2? upsample the low-resolution feature map in the decoder and sum it with the projected feature map of corresponding resolution from the backbone; Projection is done to match channel dimensions of the feature maps with a 1 ? 1 convolution layer followed by GroupNorm (GN) <ref type="bibr" target="#b44">[45]</ref>. Next, we fuse the summed features with an additional 3 ? 3 convolution layer followed by GN and ReLU activation. We repeat this process starting with the stride 32 feature map until we obtain a final feature map of stride 4. Finally, we apply a single 1 ? 1 convolution layer to get the per-pixel embeddings. All feature maps in the pixel decoder have a dimension of 256 channels.</p><p>Transformer decoder. We use the same Transformer decoder design as DETR <ref type="bibr" target="#b3">[4]</ref>. The N query embeddings are initialized as zero vectors, and we associate each query with a learnable positional encoding. We use 6 Transformer decoder layers with 100 queries by default, and, following DETR, we apply the same loss after each decoder. In our experiments we observe that MaskFormer is competitive for semantic segmentation with a single decoder layer too, whereas for instance-level segmentation multiple layers are necessary to remove duplicates from the final predictions.</p><p>Segmentation module. The multi-layer perceptron (MLP) in <ref type="figure">Figure 2</ref> has 2 hidden layers of 256 channels to predict the mask embeddings E mask , analogously to the box head in DETR. Both per-pixel E pixel and mask E mask embeddings have 256 channels.</p><p>Loss weights. We use focal loss <ref type="bibr" target="#b26">[27]</ref> and dice loss <ref type="bibr" target="#b32">[33]</ref> for our mask loss: L mask (m, m gt ) = ? focal L focal (m, m gt ) + ? dice L dice (m, m gt ), and set the hyper-parameters to ? focal = 20.0 and ? dice = 1.0. Following DETR <ref type="bibr" target="#b3">[4]</ref>, the weight for the "no object" (?) in the classification loss is set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training settings</head><p>Semantic segmentation. We use Detectron2 <ref type="bibr" target="#b45">[46]</ref> and follow the commonly used training settings for each dataset. More specifically, we use AdamW <ref type="bibr" target="#b30">[31]</ref> and the poly <ref type="bibr" target="#b6">[7]</ref> learning rate schedule with an initial learning rate of 10 ?4 and a weight decay of 10 ?4 for ResNet <ref type="bibr" target="#b21">[22]</ref> backbones, and an initial learning rate of 6 ? 10 ?5 and a weight decay of 10 ?2 for Swin-Transformer <ref type="bibr" target="#b28">[29]</ref> backbones. Backbones are pre-trained on ImageNet-1K <ref type="bibr" target="#b34">[35]</ref> if not stated otherwise. A learning rate multiplier of 0.1 is applied to CNN backbones and 1.0 is applied to Transformer backbones. The standard random scale jittering between 0.5 and 2.0, random horizontal flipping, random cropping as well as random color jittering are used as data augmentation <ref type="bibr" target="#b13">[14]</ref>. For the ADE20K dataset, if not stated otherwise, we use a crop size of 512 ? 512, a batch size of 16 and train all models for 160k iterations. For the ADE20K-Full dataset, we use the same setting as ADE20K except that we train all models for 200k iterations. For the COCO-Stuff-10k dataset, we use a crop size of 640 ? 640, a batch size of 32 and train all models for 60k iterations. All models are trained with 8 V100 GPUs. We report both performance of single scale (s.s.) inference and multi-scale (m.s.) inference with horizontal flip and scales of 0.5, 0.75, 1.0, 1.25, 1.5, 1.75. See appendix for Cityscapes and Mapillary Vistas settings.</p><p>Panoptic segmentation. We follow exactly the same architecture, loss, and training procedure as we use for semantic segmentation. The only difference is supervision: i.e., category region masks in semantic segmentation vs. object instance masks in panoptic segmentation. We strictly follow the DETR <ref type="bibr" target="#b3">[4]</ref> setting to train our model on the COCO panoptic segmentation dataset <ref type="bibr" target="#b23">[24]</ref> for a fair comparison. On the ADE20K panoptic segmentation dataset, we follow the semantic segmentation setting but train for longer (720k iterations) and use a larger crop size (640 ? 640). COCO models are trained using 64 V100 GPUs and ADE20K experiments are trained with 8 V100 GPUs. We use  the general inference (Section 3.4) with the following parameters: we filter out masks with class confidence below 0.8 and set masks whose contribution to the final panoptic segmentation is less than 80% of its mask area to VOID. We report performance of single scale inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main results</head><p>Semantic segmentation. In <ref type="table" target="#tab_0">Table 1</ref>, we compare MaskFormer with state-of-the-art per-pixel classification models for semantic segmentation on the ADE20K val set. With the same standard CNN backbones (e.g., ResNet <ref type="bibr" target="#b21">[22]</ref>), MaskFormer outperforms DeepLabV3+ [9] by 1.7 mIoU. MaskFormer is also compatible with recent Vision Transformer <ref type="bibr" target="#b16">[17]</ref> backbones (e.g., the Swin Transformer <ref type="bibr" target="#b28">[29]</ref>), achieving a new state-of-the-art of 55.6 mIoU, which is 2.1 mIoU better than the prior state-of-theart <ref type="bibr" target="#b28">[29]</ref>. Observe that MaskFormer outperforms the best per-pixel classification-based models while having fewer parameters and faster inference time. This result suggests that the mask classification formulation has significant potential for semantic segmentation. See appendix for results on test set.</p><p>Beyond ADE20K, we further compare MaskFormer with our baselines on COCO-Stuff-10K, ADE20K-Full as well as Cityscapes in <ref type="table" target="#tab_1">Table 2</ref> and we refer to the appendix for comparison with state-of-the-art methods on these datasets. The improvement of MaskFormer over PerPixelBase-line+ is larger when the number of classes is larger: For Cityscapes, which has only 19 categories, MaskFormer performs similarly well as PerPixelBaseline+; While for ADE20K-Full, which has 847 classes, MaskFormer outperforms PerPixelBaseline+ by 3.5 mIoU.</p><p>Although MaskFormer shows no improvement in mIoU for Cityscapes, the PQ St metric increases by 2.9 PQ St . We find MaskFormer performs better in terms of recognition quality (RQ St ) while lagging in per-pixel segmentation quality (SQ St ) (we refer to the appendix for detailed numbers). This observation suggests that on datasets where class recognition is relatively easy to solve, the main challenge for mask classification-based approaches is pixel-level accuracy (i.e., mask quality). Panoptic segmentation. In <ref type="table" target="#tab_2">Table 3</ref>, we compare the same exact MaskFormer model with DETR <ref type="bibr" target="#b3">[4]</ref> on the COCO panoptic val set. To match the standard DETR design, we add 6 additional Transformer encoder layers after the CNN backbone. Unlike DETR, our model does not predict bounding boxes but instead predicts masks directly. MaskFormer achieves better results while being simpler than DETR. To disentangle the improvements from the model itself and our post-processing inference strategy we run our model following DETR post-processing (MaskFormer (DETR)) and observe that this setup outperforms DETR by 2.2 PQ. Overall, we observe a larger improvement in PQ St compared to PQ Th . This suggests that detecting "stuff" with bounding boxes is suboptimal, and therefore, boxbased segmentation models (e.g., Mask R-CNN <ref type="bibr" target="#b20">[21]</ref>) do not suit semantic segmentation. MaskFormer also outperforms recently proposed Max-DeepLab <ref type="bibr" target="#b41">[42]</ref> without the need of special network design as well as sophisticated auxiliary losses (i.e., instance discrimination loss, mask-ID cross entropy loss, and per-pixel classification loss in <ref type="bibr" target="#b41">[42]</ref>). MaskFormer, for the first time, unifies semantic-and instance-level segmentation with the exact same model, loss, and training pipeline.</p><p>We further evaluate our model on the panoptic segmentation version of the ADE20K dataset. Our model also achieves state-of-the-art performance. We refer to the appendix for detailed results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation studies</head><p>We perform a series of ablation studies of MaskFormer using a single ResNet-50 backbone <ref type="bibr" target="#b21">[22]</ref>.</p><p>Per-pixel vs. mask classification. In <ref type="table" target="#tab_3">Table 4</ref>, we verify that the gains demonstrated by MaskFromer come from shifting the paradigm to mask classification. We start by comparing PerPixelBaseline+ and MaskFormer. The models are very similar and there are only 3 differences: 1) per-pixel vs. mask classification used by the models, 2) MaskFormer uses bipartite matching, and 3) the new model uses a combination of focal and dice losses as a mask loss, whereas PerPixelBaseline+ utilizes per-pixel cross entropy loss. First, we rule out the influence of loss differences by training PerPixelBaseline+ with exactly the same losses and observing no improvement. Next, in <ref type="table" target="#tab_3">Table 4a</ref>, we compare PerPixelBaseline+ with MaskFormer trained using a fixed matching (MaskFormer-fixed), i.e., N = K and assignment done based on category label indices identically to the per-pixel classification setup. We observe that MaskFormer-fixed is 1.8 mIoU better than the baseline, suggesting that shifting from per-pixel classification to mask classification is indeed the main reason for the gains of MaskFormer. In <ref type="table" target="#tab_3">Table 4b</ref>, we further compare MaskFormer-fixed with MaskFormer trained with bipartite matching (MaskFormer-bipartite) and find bipartite matching is not only more flexible (allowing to predict less masks than the total number of categories) but also produces better results. We further calculate the number of classes which are on average present in a training set image. We find these statistics to be similar across datasets despite the fact that the datasets have different number of total categories: 8.2 classes per image for ADE20K (150 classes), 6.6 classes per image for COCO-Stuff-10K (171 classes) and 9.1 classes per image for ADE20K-Full (847 classes). We hypothesize that each query is able to capture masks from multiple categories. The figure to the right shows the number of unique categories predicted by each query (sorted in descending order) of our MaskFormer model on the validation sets of the corresponding datasets. Interestingly, the number of unique categories per query does not follow a uniform distribution: some queries capture more classes than others. We try to analyze how Mask-Former queries group categories, but we do not observe any obvious pattern: there are queries capturing categories with similar semantics or shapes (e.g., "house" and "building"), but there are also queries capturing completely different categories (e.g., "water" and "sofa").</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of unique classes predicted by each query on validation set</head><p>Number of Transformer decoder layers. Interestingly, MaskFormer with even a single Transformer decoder layer already performs well for semantic segmentation and achieves better performance than our 6-layer-decoder PerPixelBaseline+. For panoptic segmentation, however, multiple decoder layers are required to achieve competitive performance. Please see the appendix for a detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Our main goal is to show that mask classification is a general segmentation paradigm that could be a competitive alternative to per-pixel classification for semantic segmentation. To better understand its potential for segmentation tasks, we focus on exploring mask classification independently of other factors like architecture, loss design, or augmentation strategy. We pick the DETR <ref type="bibr" target="#b3">[4]</ref> architecture as our baseline for its simplicity and deliberately make as few architectural changes as possible. Therefore, MaskFormer can be viewed as a "box-free" version of DETR. <ref type="table">Table 5</ref>: Matching with masks vs. boxes. We compare DETR <ref type="bibr" target="#b3">[4]</ref> which uses box-based matching with two MaskFormer models trained with box-and mask-based matching respectively. To use box-based matching in MaskFormer we add to the model an additional box prediction head as in DETR. Note, that with box-based matching MaskFormer performs on par with DETR, whereas with mask-based matching it shows better results. The evaluation is done on COCO panoptic val set. In this section, we discuss in detail the differences between MaskFormer and DETR and show how these changes are required to ensure that mask classification performs well. First, to achieve a pure mask classification setting we remove the box prediction head and perform matching between prediction and ground truth segments with masks instead of boxes. Secondly, we replace the computeheavy per-query mask head used in DETR with a more efficient per-image FPN-based head to make end-to-end training without box supervision feasible.</p><p>Matching with masks is superior to matching with boxes. We compare MaskFormer models trained using matching with boxes or masks in <ref type="table">Table 5</ref>. To do box-based matching, we add to MaskFormer an additional box prediction head as in DETR <ref type="bibr" target="#b3">[4]</ref>. Observe that MaskFormer, which directly matches with mask predictions, has a clear advantage. We hypothesize that matching with boxes is more ambiguous than matching with masks, especially for stuff categories where completely different masks can have similar boxes as stuff regions often spread over a large area in an image.</p><p>MaskFormer mask head reduces computation. Results in <ref type="table">Table 5</ref> also show that MaskFormer performs on par with DETR when the same matching strategy is used. This suggests that the difference in mask head designs between the models does not significantly influence the prediction quality. The new head, however, has significantly lower computational and memory costs in comparison with the original mask head used in DETR. In MaskFormer, we first upsample image features to get highresolution per-pixel embeddings and directly generate binary mask predictions at a high-resolution. Note, that the per-pixel embeddings from the upsampling module (i.e., pixel decoder) are shared among all queries. In contrast, DETR first generates low-resolution attention maps and applies an independent upsampling module to each query. Thus, the mask head in DETR is N times more computationally expensive than the mask head in MaskFormer (where N is the number of queries).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The paradigm discrepancy between semantic-and instance-level segmentation results in entirely different models for each task, hindering development of image segmentation as a whole. We show that a simple mask classification model can outperform state-of-the-art per-pixel classification models, especially in the presence of large number of categories. Our model also remains competitive for panoptic segmentation, without a need to change model architecture, losses, or training procedure. We hope this unification spurs a joint effort across semantic-and instance-level segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets description</head><p>We study MaskFormer using five semantic segmentation datasets and two panoptic segmentation datasets. Here, we provide more detailed information about these datasets.</p><p>A.1 Semantic segmentation datasets ADE20K <ref type="bibr" target="#b54">[55]</ref> contains 20k images for training and 2k images for validation. The data comes from the ADE20K-Full dataset where 150 semantic categories are selected to be included in evaluation from the SceneParse150 challenge <ref type="bibr" target="#b53">[54]</ref>. The images are resized such that the shortest side is no greater than 512 pixels. During inference, we resize the shorter side of the image to the corresponding crop size.</p><p>COCO-Stuff-10K <ref type="bibr" target="#b2">[3]</ref> has 171 semantic-level categories. There are 9k images for training and 1k images for testing. Images in the COCO-Stuff-10K datasets are a subset of the COCO dataset <ref type="bibr" target="#b27">[28]</ref>. During inference, we resize the shorter side of the image to the corresponding crop size.</p><p>ADE20K-Full <ref type="bibr" target="#b54">[55]</ref> contains 25k images for training and 2k images for validation. The ADE20K-Full dataset is annotated in an open-vocabulary setting with more than 3000 semantic categories. We filter these categories by selecting those that are present in both training and validation sets, resulting in a total of 847 categories. We follow the same process as ADE20K-SceneParse150 to resize images such that the shortest side is no greater than 512 pixels. During inference, we resize the shorter side of the image to the corresponding crop size.</p><p>Cityscapes <ref type="bibr" target="#b14">[15]</ref> is an urban egocentric street-view dataset with high-resolution images (1024 ? 2048 pixels  <ref type="bibr" target="#b23">[24]</ref> is one of the most commonly used datasets for panoptic segmentation. It has 133 categories (80 "thing" categories with instance-level annotation and 53 "stuff" categories) in 118k images for training and 5k images for validation. All images are from the COCO dataset <ref type="bibr" target="#b27">[28]</ref>.</p><p>ADE20K panoptic <ref type="bibr" target="#b54">[55]</ref> combines the ADE20K semantic segmentation annotation for semantic segmentation from the SceneParse150 challenge <ref type="bibr" target="#b53">[54]</ref> and ADE20K instance annotation from the COCO+Places challenge <ref type="bibr" target="#b0">[1]</ref>. Among the 150 categories, there are 100 "thing" categories with instance-level annotation. We find filtering masks with a lower threshold (we use 0.7 for ADE20K) than COCO (which uses 0.8) gives slightly better performance.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE20K-Full.</head><p>We further demonstrate the benefits in large-vocabulary semantic segmentation in <ref type="table" target="#tab_6">Table IIb</ref>. Since we are the first to report performance on this dataset, we only compare MaskFormer with our per-pixel classification baselines. MaskFormer not only achieves better performance, but is also more memory efficient on the ADE20K-Full dataset with 847 categories, thanks to decoupling the number of masks from the number of classes. These results show that our MaskFormer has the potential to deal with real-world segmentation problems with thousands of categories.</p><p>Cityscapes. In <ref type="table" target="#tab_6">Table IIIa</ref>, we report MaskFormer performance on Cityscapes, the standard testbed for modern semantic segmentation methods. The dataset has only 19 categories and therefore, the recognition aspect of the dataset is less challenging than in other considered datasets. We  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional ablation studies</head><p>We perform additional ablation studies of MaskFormer for semantic segmentation using the same setting as that in the main paper: a single ResNet-50 backbone <ref type="bibr" target="#b21">[22]</ref>, and we report both the mIoU and the PQ St . The default setting of our MaskFormer is: 100 queries and 6 Transformer decoder layers.</p><p>Inference strategies. In <ref type="table" target="#tab_6">Table VII</ref>, we ablate inference strategies for mask classification-based models performing semantic segmentation (discussed in Section 3.4). We compare our default semantic inference strategy and the general inference strategy which first filters out low-confidence masks (a threshold of 0.3 is used) and assigns the class labels to the remaining masks. We observe 1) general inference is only slightly better than the PerPixelBaseline+ in terms of the mIoU metric, and 2) on multiple datasets the general inference strategy performs worse in terms of the mIoU metric than the default semantic inference. However, the general inference has higher PQ St , due to better recognition quality (RQ St ). We hypothesize that the filtering step removes false positives which increases the RQ St . In contrast, the semantic inference aggregates mask predictions from multiple queries thus it has better mask quality (SQ St ). This observation suggests that semantic and instance-level segmentation can be unified with a single inference strategy (i.e., our general inference) and the choice of inference strategy largely depends on the evaluation metric instead of the task.  MaskFormer trained for semantic segmentation MaskFormer trained for panoptic segmentation ground truth prediction ground truth prediction semantic query prediction panoptic query prediction Figure I: Visualization of "semantic" queries and "panoptic" queries. Unlike the behavior in a MaskFormer model trained for panoptic segmentation (right), a single query is used to capture multiple instances in a MaskFormer model trained for semantic segmentation (left). Our model has the capacity to adapt to different types of tasks given different ground truth annotations.</p><p>Number of Transformer decoder layers. In <ref type="table" target="#tab_6">Table VIII</ref>, we ablate the effect of the number of Transformer decoder layers on ADE20K <ref type="bibr" target="#b54">[55]</ref> for both semantic and panoptic segmentation. Surprisingly, we find a MaskFormer with even a single Transformer decoder layer already performs reasonably well for semantic segmentation and achieves better performance than our 6-layer-decoder per-pixel classification baseline PerPixelBaseline+. Whereas, for panoptic segmentation, the number of decoder layers is more important. We hypothesize that stacking more decoder layers is helpful to de-duplicate predictions which is required by the panoptic segmentation task.</p><p>To verify this hypothesis, we train MaskFormer models without self-attention in all 6 Transformer decoder layers. On semantic segmentation, we observe MaskFormer without self-attention performs similarly well in terms of the mIoU metric, however, the per-mask metric PQ St is slightly worse. On panoptic segmentation, MaskFormer models without self-attention performs worse across all metrics.</p><p>"Semantic" queries vs. "panoptic" queries. In <ref type="figure">Figure I</ref> we visualize predictions for the "car" category from MaskFormer trained with semantic-level and instance-level ground truth data. In the case of semantic-level data, the matching cost and loss used for mask prediction force a single query to predict one mask that combines all cars together. In contrast, with instance-level ground truth, MaskFormer uses different queries to make mask predictions for each car. This observation ground truth prediction ground truth prediction <ref type="figure">Figure</ref>  suggests that our model has the capacity to adapt to different types of tasks given different ground truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Visualization</head><p>We visualize sample semantic segmentation predictions of the MaskFormer model with Swin-L <ref type="bibr" target="#b28">[29]</ref> backbone (55.6 mIoU) on the ADE20K validation set in <ref type="figure">Figure II.</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>II: Visualization of MaskFormer semantic segmentation predictions on the ADE20K dataset. We visualize the MaskFormer with Swin-L backbone which achieves 55.6 mIoU (multi-scale) on the validation set. First and third columns: ground truth. Second and fourth columns: prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Semantic segmentation on ADE20K val with 150 categories. Mask classification-based MaskFormer outperforms the best per-pixel classification approaches while using fewer parameters and less computation. We report both single-scale (s.s.) and multi-scale (m.s.) inference results with ?std. FLOPs are computed for the given crop size. Frames-per-second (fps) is measured on a V100 GPU with a batch size of 1.<ref type="bibr" target="#b2">3</ref> Backbones pre-trained on ImageNet-22K are marked with ? .</figDesc><table><row><cell></cell><cell>method</cell><cell>backbone</cell><cell>crop size</cell><cell>mIoU (s.s.)</cell><cell>mIoU (m.s.)</cell><cell>#params.</cell><cell>FLOPs</cell><cell>fps</cell></row><row><cell>CNN backbones</cell><cell>OCRNet [50] DeepLabV3+ [9] MaskFormer (ours)</cell><cell>R101c R50c R101c R50 R101 R101c</cell><cell>520 ? 520 512 ? 512 512 ? 512 512 ? 512 512 ? 512 512 ? 512</cell><cell>-44.0 45.5 44.5 ?0.5 45.5 ?0.5 46.0 ?0.1</cell><cell>45.3 44.9 46.4 46.7 ?0.6 47.2 ?0.2 48.1 ?0.2</cell><cell>-44M 63M 41M 60M 60M</cell><cell>-177G 255G 53G 73G 80G</cell><cell>-21.0 14.2 24.5 19.5 19.0</cell></row><row><cell></cell><cell>SETR [53]</cell><cell>ViT-L  ?</cell><cell>512 ? 512</cell><cell>-</cell><cell>50.3</cell><cell>308M</cell><cell>-</cell><cell>-</cell></row><row><cell>Transformer backbones</cell><cell>Swin-UperNet [29, 49] MaskFormer (ours)</cell><cell>Swin-T Swin-S Swin-B  ? Swin-L  ? Swin-T Swin-S Swin-B Swin-B  ?</cell><cell>512 ? 512 512 ? 512 640 ? 640 640 ? 640 512 ? 512 512 ? 512 640 ? 640 640 ? 640</cell><cell>----46.7 ?0.7 49.8 ?0.4 51.1 ?0.2 52.7 ?0.4</cell><cell>46.1 49.3 51.6 53.5 48.8 ?0.6 51.0 ?0.4 52.3 ?0.4 53.9 ?0.2</cell><cell>60M 81M 121M 234M 42M 63M 102M 102M</cell><cell>236G 259G 471G 647G 55G 79G 195G 195G</cell><cell>18.5 15.2 8.7 6.2 22.1 19.6 12.6 12.6</cell></row><row><cell></cell><cell></cell><cell>Swin-L  ?</cell><cell>640 ? 640</cell><cell>54.1 ?0.2</cell><cell>55.6 ?0.1</cell><cell>212M</cell><cell>375G</cell><cell>7.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Cityscapes (19 classes)</cell><cell cols="2">ADE20K (150 classes)</cell><cell cols="4">COCO-Stuff (171 classes) ADE20K-Full (847 classes)</cell></row><row><cell></cell><cell>mIoU</cell><cell>PQ St</cell><cell>mIoU</cell><cell>PQ St</cell><cell>mIoU</cell><cell>PQ St</cell><cell>mIoU</cell><cell>PQ St</cell></row><row><cell>PerPixelBaseline</cell><cell>77.4</cell><cell>58.9</cell><cell>39.2</cell><cell>21.6</cell><cell>32.4</cell><cell>15.5</cell><cell>12.4</cell><cell>5.8</cell></row><row><cell>PerPixelBaseline+</cell><cell>78.5</cell><cell>60.2</cell><cell>41.9</cell><cell>28.3</cell><cell>34.2</cell><cell>24.6</cell><cell>13.9</cell><cell>9.0</cell></row><row><cell cols="2">MaskFormer (ours) 78.5 (+0.0)</cell><cell>63.1 (+2.9)</cell><cell>44.5 (+2.6)</cell><cell>33.4 (+5.1)</cell><cell>37.1 (+2.9)</cell><cell>28.9 (+4.3)</cell><cell>17.4 (+3.5)</cell><cell>11.9 (+2.9)</cell></row></table><note>MaskFormer vs. per-pixel classification baselines on 4 semantic segmentation datasets. MaskFormer improvement is larger when the number of classes is larger. We use a ResNet-50 backbone and report single scale mIoU and PQ St for ADE20K, COCO-Stuff and ADE20K-Full, whereas for higher-resolution Cityscapes we use a deeper ResNet-101 backbone following [8, 9].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Panoptic segmentation on COCO panoptic val with 133 categories. MaskFormer seamlessly unifies semantic-and instance-level segmentation without modifying the model architecture or loss. Our model, which achieves better results, can be regarded as a box-free simplification of DETR<ref type="bibr" target="#b3">[4]</ref>. The major improvement comes from "stuff" classes (PQ St ) which are ambiguous to represent with bounding boxes. For MaskFormer (DETR) we use the exact same post-processing as DETR. Note, that in this setting MaskFormer performance is still better than DETR (+2.2 PQ). Our model also outperforms recently proposed Max-DeepLab<ref type="bibr" target="#b41">[42]</ref> without the need of sophisticated auxiliary losses, while being more efficient. FLOPs are computed as the average FLOPs over 100 validation images (COCO images have varying sizes). Frames-per-second (fps) is measured on a V100 GPU with a batch size of 1 by taking the average runtime on the entire val set including post-processing time. Backbones pre-trained on ImageNet-22K are marked with ? .</figDesc><table><row><cell></cell><cell>method</cell><cell>backbone</cell><cell>PQ</cell><cell>PQ Th</cell><cell>PQ St</cell><cell>SQ</cell><cell>RQ</cell><cell cols="2">#params. FLOPs</cell><cell>fps</cell></row><row><cell>CNN backbones</cell><cell>DETR [4] MaskFormer (DETR) MaskFormer (ours) DETR [4] MaskFormer (ours)</cell><cell>R50 + 6 Enc R50 + 6 Enc R50 + 6 Enc R101 + 6 Enc R101 + 6 Enc</cell><cell>43.4 45.6 46.5 45.1 47.6</cell><cell>48.2 50.0 (+1.8) 51.0 (+2.8) 50.5 52.5 (+2.0)</cell><cell>36.3 39.0 (+2.7) 39.8 (+3.5) 37.0 40.3 (+3.3)</cell><cell>79.3 80.2 80.4 79.9 80.7</cell><cell>53.8 55.8 56.8 55.5 58.0</cell><cell>--45M -64M</cell><cell>--181G -248G</cell><cell>--17.6 -14.0</cell></row><row><cell>Transformer backbones</cell><cell>Max-DeepLab [42] MaskFormer (ours)</cell><cell>Max-S Max-L Swin-T Swin-S Swin-B Swin-B  ? Swin-L  ?</cell><cell>48.4 51.1 47.7 49.7 51.1 51.8 52.7</cell><cell>53.0 57.0 51.7 54.4 56.3 56.9 58.5</cell><cell>41.5 42.2 41.7 42.6 43.2 44.1 44.0</cell><cell>--80.4 80.9 81.4 81.4 81.8</cell><cell>--58.3 60.4 61.8 62.6 63.5</cell><cell>62M 451M 42M 63M 102M 102M 212M</cell><cell>324G 3692G 179G 259G 411G 411G 792G</cell><cell>7.6 -17.0 12.4 8.4 8.4 5.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Per-pixel vs. mask classification for semantic segmentation. All models use 150 queries for a fair comparison. We evaluate the models on ADE20K val with 150 categories. 4a: PerPixel-Baseline+ and MaskFormer-fixed use similar fixed matching (i.e., matching by category index), this result confirms that the shift from per-pixel to mask classification is the key. 4b: bipartite matching is not only more flexible (can make less prediction than total class count) but also gives better results.</figDesc><table><row><cell cols="2">(a) Per-pixel vs. mask classification.</cell><cell cols="4">(b) Fixed vs. bipartite matching assignment.</cell></row><row><cell>mIoU</cell><cell>PQ St</cell><cell></cell><cell cols="2">mIoU</cell><cell>PQ St</cell></row><row><cell>PerPixelBaseline+ 41.9</cell><cell>28.3</cell><cell>MaskFormer-fixed</cell><cell>43.7</cell><cell></cell><cell>30.3</cell></row><row><cell>MaskFormer-fixed 43.7 (+1.8)</cell><cell>30.3 (+2.0)</cell><cell cols="4">MaskFormer-bipartite (ours) 44.2 (+0.5) 33.4 (+3.1)</cell></row><row><cell cols="2">Number of queries. The table to the right</cell><cell></cell><cell>ADE20K</cell><cell cols="2">COCO-Stuff ADE20K-Full</cell></row><row><cell cols="2">shows results of MaskFormer trained with a</cell><cell># of queries</cell><cell cols="3">mIoU PQ St mIoU PQ St mIoU PQ St</cell></row><row><cell cols="2">varying number of queries on datasets with dif-</cell><cell cols="4">PerPixelBaseline+ 41.9 28.3 34.2 24.6 13.9</cell><cell>9.0</cell></row><row><cell cols="2">ferent number of categories. The model with</cell><cell>20</cell><cell cols="3">42.9 32.6 35.0 27.6 14.1 10.8</cell></row><row><cell cols="2">100 queries consistently performs the best across the studied datasets. This suggest we may not need to adjust the number of queries w.r.t. the number of categories or datasets much. Interest-</cell><cell>50 100 150 300 1000</cell><cell cols="3">43.9 32.7 35.5 27.9 15.4 11.1 44.5 33.4 37.1 28.9 16.0 11.9 44.2 33.4 37.0 28.9 15.5 11.5 43.5 32.3 36.1 29.1 14.2 10.3 35.4 26.7 34.4 27.6 8.0 5.8</cell></row><row><cell cols="2">ingly, even with 20 queries MaskFormer outper-</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">forms our per-pixel classification baseline.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). It contains 2975 images for training, 500 images for validation, and 1525 images for testing with a total of 19 classes. During training, we use a crop size of 512 ? 1024, a batch size of 16 and train all models for 90k iterations. During inference, we operate on the whole image (1024 ? 2048). Vistas<ref type="bibr" target="#b33">[34]</ref> is a large-scale urban street-view dataset with 65 categories. It contains 18k, 2k, and 5k images for training, validation and testing with a variety of image resolutions, ranging from 1024 ? 768 to 4000 ? 6000. During training, we resize the short side of images to 2048 before applying scale augmentation. We use a crop size of 1280 ? 1280, a batch size of 16 and train all models for 300k iterations. During inference, we resize the longer side of the image to 2048 and only use three scales (0.5, 1.0 and 1.5) for multi-scale testing due to GPU memory constraints.</figDesc><table><row><cell>Mapillary A.2 Panoptic segmentation datasets</cell></row><row><cell>COCO panoptic</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table I :</head><label>I</label><figDesc>Semantic segmentation on ADE20K test with 150 categories. MaskFormer outperforms previous state-of-the-art methods on all three metrics: pixel accuracy (P.A.), mIoU, as well as the final test score (average of P.A. and mIoU). We train our model on the union of ADE20K train and val set with ImageNet-22K pre-trained checkpoint following<ref type="bibr" target="#b28">[29]</ref> and use multi-scale inference.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>P.A.</cell><cell>mIoU</cell><cell>score</cell></row><row><cell>SETR [53]</cell><cell>ViT-L</cell><cell>78.35</cell><cell>45.03</cell><cell>61.69</cell></row><row><cell>Swin-UperNet [29, 49]</cell><cell>Swin-L</cell><cell>78.42</cell><cell>47.07</cell><cell>62.75</cell></row><row><cell>MaskFormer (ours)</cell><cell>Swin-L</cell><cell>79.36</cell><cell>49.67</cell><cell>64.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table II :</head><label>II</label><figDesc>Semantic segmentation on COCO-Stuff-10K test with 171 categories and ADE20K-Full val with 847 categories. Table IIa: MaskFormer is competitive on COCO-Stuff-10K, showing the generality of mask-classification. Table IIb: MaskFormer results on the harder large-vocabulary semantic segmentation. MaskFormer performs better than per-pixel classification and requires less memory during training, thanks to decoupling the number of masks from the number of classes. mIoU (s.s.) and mIoU (m.s.) are the mIoU of single-scale and multi-scale inference with ?std.</figDesc><table><row><cell cols="3">(a) COCO-Stuff-10K.</cell><cell></cell><cell cols="2">(b) ADE20K-Full.</cell></row><row><cell>method</cell><cell>backbone</cell><cell>mIoU (s.s.)</cell><cell>mIoU (m.s.)</cell><cell>mIoU (s.s.)</cell><cell>training memory</cell></row><row><cell>OCRNet [50]</cell><cell>R101c</cell><cell>-</cell><cell>39.5</cell><cell>-</cell><cell>-</cell></row><row><cell>PerPixelBaseline</cell><cell>R50</cell><cell>32.4 ?0.2</cell><cell>34.4 ?0.4</cell><cell>12.4 ?0.2</cell><cell>8030M</cell></row><row><cell>PerPixelBaseline+</cell><cell>R50</cell><cell>34.2 ?0.2</cell><cell>35.8 ?0.4</cell><cell>13.9 ?0.1</cell><cell>26698M</cell></row><row><cell></cell><cell>R50</cell><cell>37.1 ?0.4</cell><cell>38.9 ?0.2</cell><cell>16.0 ?0.3</cell><cell>6529M</cell></row><row><cell>MaskFormer (ours)</cell><cell>R101</cell><cell>38.1 ?0.3</cell><cell>39.8 ?0.6</cell><cell>16.8 ?0.2</cell><cell>6894M</cell></row><row><cell></cell><cell>R101c</cell><cell>38.0 ?0.3</cell><cell>39.3 ?0.4</cell><cell>17.4 ?0.4</cell><cell>6904M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table III :</head><label>III</label><figDesc>Semantic segmentation on Cityscapes val with 19 categories.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>IIIa: MaskFormer is</cell></row><row><cell cols="6">on-par with state-of-the-art methods on Cityscapes which has fewer categories than other considered</cell></row><row><cell cols="6">datasets. We report multi-scale (m.s.) inference results with ?std for a fair comparison across</cell></row><row><cell cols="6">methods. IIIb: We analyze MaskFormer with a complimentary PQ St metric, by treating all categories</cell></row><row><cell cols="6">as "stuff." The breakdown of PQ St suggests mask classification-based MaskFormer is better at</cell></row><row><cell cols="6">recognizing regions (RQ St ) while slightly lagging in generation of high-quality masks (SQ St ).</cell></row><row><cell cols="3">(a) Cityscapes standard mIoU metric.</cell><cell cols="3">(b) Cityscapes analysis with PQ St metric suit.</cell></row><row><cell>method</cell><cell>backbone</cell><cell>mIoU (m.s.)</cell><cell>PQ St (m.s.)</cell><cell>SQ St (m.s.)</cell><cell>RQ St (m.s.)</cell></row><row><cell cols="2">Panoptic-DeepLab [11] X71 [12]</cell><cell>81.5</cell><cell>66.6</cell><cell>82.9</cell><cell>79.4</cell></row><row><cell>OCRNet [50]</cell><cell>R101c</cell><cell>82.0</cell><cell>66.1</cell><cell>82.6</cell><cell>79.1</cell></row><row><cell>MaskFormer (ours)</cell><cell>R101 R101c</cell><cell>80.3 ?0.1 81.4 ?0.2</cell><cell>65.9 66.9</cell><cell>81.5 82.0</cell><cell>79.7 80.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table IV :</head><label>IV</label><figDesc>Semantic segmentation on Mapillary Vistas val with 65 categories. MaskFormer outperforms per-pixel classification methods on high-resolution images without the need of multiscale inference, thanks to global context captured by the Transformer decoder. mIoU (s.s.) and mIoU (m.s.) are the mIoU of single-scale and multi-scale inference.Table Icompares MaskFormer with previous state-of-the-art methods on the ADE20K test set. Following<ref type="bibr" target="#b28">[29]</ref>, we train MaskFormer on the union of ADE20K train and val set with ImageNet-22K pre-trained checkpoint and use multi-scale inference. MaskFormer outperforms previous state-of-the-art methods on all three metrics with a large margin.COCO-Stuff-10K.Table IIacompares MaskFormer with our baselines as well as the state-of-the-art OCRNet model<ref type="bibr" target="#b49">[50]</ref> on the COCO-Stuff-10K<ref type="bibr" target="#b2">[3]</ref> dataset. MaskFormer outperforms our per-pixel classification baselines by a large margin and achieves competitive performances compared to OCRNet. These results demonstrate the generality of the MaskFormer model.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>mIoU (s.s.)</cell><cell>mIoU (m.s.)</cell></row><row><cell>DeepLabV3+ [9]</cell><cell>R50</cell><cell>47.7</cell><cell>49.4</cell></row><row><cell>HMSANet [38]</cell><cell>R50</cell><cell>-</cell><cell>52.2</cell></row><row><cell>MaskFormer (ours)</cell><cell>R50</cell><cell>53.1</cell><cell>55.4</cell></row><row><cell cols="2">B Semantic segmentation results</cell><cell></cell><cell></cell></row><row><cell>ADE20K test.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table V :</head><label>V</label><figDesc>Panoptic segmentation on COCO panoptic test-dev with 133 categories. MaskFormer outperforms previous state-of-the-art Max-DeepLab [42] on the test-dev set as well. We only train our model on the COCO train2017 set with ImageNet-22K pre-trained checkpoint.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>PQ</cell><cell>PQ Th</cell><cell>PQ St</cell><cell>SQ</cell><cell>RQ</cell></row><row><cell>Max-DeepLab [42]</cell><cell>Max-L</cell><cell>51.3</cell><cell>57.2</cell><cell>42.4</cell><cell>82.5</cell><cell>61.3</cell></row><row><cell>MaskFormer (ours)</cell><cell>Swin-L</cell><cell>53.3</cell><cell>59.1</cell><cell>44.5</cell><cell>82.0</cell><cell>64.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table VI :</head><label>VI</label><figDesc>Panoptic segmentation on ADE20K panoptic val with 150 categories. Following DETR<ref type="bibr" target="#b3">[4]</ref>, we add 6 additional Transformer encoders when using ResNet<ref type="bibr" target="#b21">[22]</ref> (R50 + 6 Enc and R101 + 6 Enc) backbones. MaskFormer achieves competitive results on ADE20K panotic, showing the generality of our model for panoptic segmentation.observe that MaskFormer performs on par with the best per-pixel classification methods. To better analyze MaskFormer, inTable IIIb, we further report PQ St . We find MaskFormer performs better in terms of recognition quality (RQ St ) while lagging in per-pixel segmentation quality (SQ St ). This suggests that on datasets, where recognition is relatively easy to solve, the main challenge for mask classification-based approaches is pixel-level accuracy.Mapillary Vistas.Table IVcompares MaskFormer with state-of-the-art per-pixel classification models on the high-resolution Mapillary Vistas dataset which contains images up to 4000 ? 6000 resolution. We observe: (1) MaskFormer is able to handle high-resolution images, and (2) Mask-Former outperforms mulit-scale per-pixel classification models even without the need of mult-scale inference. We believe the Transformer decoder in MaskFormer is able to capture global context even for high-resolution images.C Panoptic segmentation resultsCOCO panoptic test-dev.Table Vcompares MaskFormer with previous state-of-the-art methods on the COCO panoptic test-dev set. We only train our model on the COCO train2017 set with ImageNet-22K pre-trained checkpoint and outperforms previos state-of-the-art by 2 PQ.ADE20K panoptic. We demonstrate the generality of our model for panoptic segmentation on the ADE20K panoptic dataset inTable VI, where MaskFormer is competitive with the state-of-the-art methods.</figDesc><table><row><cell>method</cell><cell>backbone</cell><cell>PQ</cell><cell>PQ Th</cell><cell>PQ St</cell><cell>SQ</cell><cell>RQ</cell></row><row><cell>BGRNet [47]</cell><cell>R50</cell><cell>31.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Auto-Panoptic [48]</cell><cell>ShuffleNetV2 [32]</cell><cell>32.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MaskFormer (ours)</cell><cell>R50 + 6 Enc R101 + 6 Enc</cell><cell>34.7 35.7</cell><cell>32.2 34.5</cell><cell>39.7 38.0</cell><cell>76.7 77.4</cell><cell>42.8 43.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table VII :</head><label>VII</label><figDesc>Inference strategies for semantic segmentation. general: general inference (Section 3.4) which first filters low-confidence masks (using a threshold of 0.3) and assigns labels to the remaining ones. semantic: the default semantic inference (Section 3.4) for semantic segmentation.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ADE20K (150 classes)</cell><cell></cell><cell cols="4">COCO-Stuff (171 classes)</cell><cell cols="4">ADE20K-Full (847 classes)</cell></row><row><cell>inference</cell><cell>mIoU</cell><cell>PQ St</cell><cell>SQ St</cell><cell>RQ St</cell><cell>mIoU</cell><cell>PQ St</cell><cell>SQ St</cell><cell>RQ St</cell><cell>mIoU</cell><cell>PQ St</cell><cell>SQ St</cell><cell>RQ St</cell></row><row><cell>PerPixelBaseline+</cell><cell>41.9</cell><cell>28.3</cell><cell>71.9</cell><cell>36.2</cell><cell>34.2</cell><cell>24.6</cell><cell>62.6</cell><cell>31.2</cell><cell>13.9</cell><cell>9.0</cell><cell>24.5</cell><cell>12.0</cell></row><row><cell>general</cell><cell>42.4</cell><cell>34.2</cell><cell>74.4</cell><cell>43.5</cell><cell>35.5</cell><cell>29.7</cell><cell>66.3</cell><cell>37.0</cell><cell>15.1</cell><cell>11.6</cell><cell>28.3</cell><cell>15.3</cell></row><row><cell>semantic</cell><cell>44.5</cell><cell>33.4</cell><cell>75.4</cell><cell>42.4</cell><cell>37.1</cell><cell>28.9</cell><cell>66.3</cell><cell>35.9</cell><cell>16.0</cell><cell>11.9</cell><cell>28.6</cell><cell>15.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table VIII :</head><label>VIII</label><figDesc>Ablation on number of Transformer decoder layers in MaskFormer. We find that MaskFormer with only one Transformer decoder layer is already able to achieve reasonable semantic segmentation performance. Stacking more decoder layers mainly improves the recognition quality.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ADE20K-Semantic</cell><cell></cell><cell></cell><cell cols="3">ADE20K-Panoptic</cell><cell></cell></row><row><cell># of decoder layers</cell><cell>mIoU</cell><cell>PQ St</cell><cell>SQ St</cell><cell>RQ St</cell><cell>PQ</cell><cell>PQ Th</cell><cell>PQ St</cell><cell>SQ</cell><cell>RQ</cell></row><row><cell>6 (PerPixelBaseline+)</cell><cell>41.9</cell><cell>28.3</cell><cell>71.9</cell><cell>36.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>1</cell><cell>43.0</cell><cell>31.1</cell><cell>74.3</cell><cell>39.7</cell><cell>31.9</cell><cell>29.6</cell><cell>36.6</cell><cell>76.6</cell><cell>39.6</cell></row><row><cell>6</cell><cell>44.5</cell><cell>33.4</cell><cell>75.4</cell><cell>42.4</cell><cell>34.7</cell><cell>32.2</cell><cell>39.7</cell><cell>76.7</cell><cell>42.8</cell></row><row><cell>6 (no self-attention)</cell><cell>44.6</cell><cell>32.8</cell><cell>74.5</cell><cell>41.5</cell><cell>32.6</cell><cell>29.9</cell><cell>38.2</cell><cell>75.6</cell><cell>40.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Different mask classification methods utilize various matching rules. For instance, Mask R-CNN<ref type="bibr" target="#b20">[21]</ref> uses a heuristic procedure based on anchor boxes and DETR<ref type="bibr" target="#b3">[4]</ref> optimizes a bipartite matching between z and z gt .</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">It isn't recommended to compare fps from different papers: speed is measured in different environments. DeepLabV3+ fps are from MMSegmentation<ref type="bibr" target="#b13">[14]</ref>, and Swin-UperNet fps are from the original paper<ref type="bibr" target="#b28">[29]</ref>.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Ross Girshick for insightful comments and suggestions. Work of UIUC authors Bowen Cheng and Alexander G. Schwing was supported in part by NSF under Grant #1718221, 2008387, 2045586, 2106825, MRI #1725729, NIFA award 2020-67021-32799 and Cisco Systems Inc. (Gift Award CG 1377144 -thanks for access to Arcetri).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>We first provide more information regarding the datasets used in our experimental evaluation of MaskFormer (Appendix A). Then, we provide detailed results of our model on more semantic (Appendix B) and panoptic (Appendix C) segmentation datasets. Finally, we provide additional ablation studies (Appendix D) and visualization (Appendix E).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco + Places</surname></persName>
		</author>
		<ptr target="https://places-coco2017.github.io/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CPMC: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SPGNet: Semantic prediction guidance for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust Analysis of Feature Spaces: Color Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorin</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">MMSegmentation: OpenMMLab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation,2020" />
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR, 2021</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The PASCAL visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijie</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">CCNet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Statistical Cues for Domain Specific Image Segmentation with Performance Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Konishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ShuffleNet V2: Practical guidelines for efficient cnn architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningning</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">V-Net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bul?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Segmenter: Transformer for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Strudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.05633</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10821</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">MaX-DeepLab: End-to-end panoptic segmentation with mask transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Abhinav Gupta, and Kaiming He. Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">SOLOv2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bidirectional graph reasoning network for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiajun</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Auto-panoptic: Cooperative multi-component architecture search for panoptic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gengwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Object-contextual representations for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV, 2020. 6</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">OCNet: Object context for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Scene parsing challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://sceneparsing.csail.mit.edu/index_challenge.html" />
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
