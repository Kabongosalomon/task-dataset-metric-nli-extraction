<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SpaceNet: A Remote Sensing Dataset and Challenge Series</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">Van</forename><surname>Etten</surname></persName>
							<email>avanettten@iqt.org</email>
							<affiliation key="aff0">
								<orgName type="institution">In-Q-Tel CosmiQ Works Arlington</orgName>
								<address>
									<postCode>22201</postCode>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Lindenbaum</surname></persName>
							<email>dlindenbaum@iqt.org</email>
							<affiliation key="aff1">
								<orgName type="department">In-Q-Tel CosmiQ Works Arlington</orgName>
								<address>
									<postCode>22201</postCode>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Bacastow</surname></persName>
							<email>todd.bacastow@radiantsolutions.org</email>
							<affiliation key="aff2">
								<orgName type="department">Radiant Solutions Herndon</orgName>
								<address>
									<postCode>20171</postCode>
									<region>VA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SpaceNet: A Remote Sensing Dataset and Challenge Series</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Foundational mapping remains a challenge in many parts of the world, particularly in dynamic scenarios such as natural disasters when timely updates are critical. Updating maps is currently a highly manual process requiring a large number of human labelers to either create features or rigorously validate automated outputs. We propose that the frequent revisits of earth imaging satellite constellations may accelerate existing efforts to quickly update foundational maps when combined with advanced machine learning techniques. Accordingly, the SpaceNet partners (CosmiQ Works, Radiant Solutions, and NVIDIA), released a large corpus of labeled satellite imagery on Amazon Web Services (AWS) called SpaceNet. The SpaceNet partners also launched a series of public prize competitions to encourage improvement of remote sensing machine learning algorithms. The first two of these competitions focused on automated building footprint extraction, and the most recent challenge focused on road network extraction. In this paper we discuss the SpaceNet imagery, labels, evaluation metrics, prize challenge results to date, and future plans for the SpaceNet challenge series.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Background</head><p>The commercialization of the geospatial industry has led to an explosive amount of data being collected to characterize our changing planet. One area for innovation is the application of computer vision and deep learning to extract information from satellite imagery at scale. To this end, CosmiQ Works, Radiant Solutions, and NVIDIA partnered to release SpaceNet data as a Public Dataset on AWS <ref type="bibr" target="#b0">[1]</ref>.</p><p>Today, map features such as roads, building footprints, and points of interest are primarily created through manual mapping techniques. We believe that advancing automated feature extraction techniques will serve important downstream uses of map data, such as humanitarian and disaster response. Furthermore, we believe that solving this challenge is an important stepping stone to unleashing the power of advanced computer vision algorithms applied to a variety of remote sensing data applications in both the public and private sectors.</p><p>The first two SpaceNet challenges focused on building footprint extraction from satellite imagery. The third challenge addressed another foundational geospatial intelligence problem, road network extraction. The ability to create a road network is an important map feature (particularly if one is able to use this road network for routing purposes), while building footprint extraction serves as a useful proxy for population density <ref type="bibr">[2]</ref>, <ref type="bibr">[3]</ref>, <ref type="bibr">[4]</ref>. The potential application of automated building footprint and road network extraction ranges from the determination of optimal aid station locations during an epidemic in under mapped locations, to the establishment of an effective logistical schema in a disaster stricken region. There is a demonstrated need for both automated building footprint extraction as well as automated road network extraction, as the Humanitarian OpenStreetMap Team's Tasking Manager <ref type="bibr">[5]</ref> currently has scores of open tasks for both roads and buildings.</p><p>To address these challenges (and inspired by the ImageNet <ref type="bibr">[6]</ref> model), the SpaceNet team released a large corpus of labeled satellite imagery in conjunction with public challenges aimed to increase the utility of satellite imagery. All SpaceNet data is distributed under a Creative Commons Attribution-ShareAlike 4.0 International License and is hosted as a public dataset on AWS and can be downloaded for free. The future vision for SpaceNet is the continued expansion of automated capabilities available to foundational mapping practitioners and users. This will be accomplished by releasing high quality labeled datasets, and running targeted public challenges to encourage the development of algorithms designed to solve increasingly complex geospatial problems. Two future challenges are already in the execution stage, and will be discussed further in Section 6.</p><p>2 Source Imagery and Labels</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Existing Datasets</head><p>Existing publicly available labeled overhead or satellite imagery datasets tend to be relatively small, or labeled with lower fidelity than desired for foundational mapping. For example, the ISPRS semantic labeling benchmark <ref type="bibr" target="#b6">[7]</ref> dataset contains high quality 2D semantic labels over two cities in Germany and covers a compact area of 4.8 km 2 ; imagery is obtained via an aerial platform and is 3 or 4 channel and 5-10cm in resolution. The TorontoCity Dataset <ref type="bibr" target="#b7">[8]</ref> contains high resolution 5-10cm aerial 4-channel imagery, and ? 700 km 2 of coverage; building and roads are labeled at high fidelity (among other items), but the data has yet to be publicly released. The Massachusetts Roads Dataset <ref type="bibr" target="#b8">[9]</ref> contains 3-channel imagery at 1 meter resolution, and 2600 km 2 of coverage; the imagery and labels are publicly available, though labels are scraped from OpenStreetMap and not independently collected or validated. Another useful overhead dataset (though not so relevant to foundational mapping) is the COWC dataset <ref type="bibr" target="#b9">[10]</ref> of cars, with 15cm aerial imagery collected over six different geographic regions; labels consist of a point at the centroid of each car. A recently released satellite imagery dataset is xVIEW <ref type="bibr" target="#b10">[11]</ref>, which contains 1400 km 2 of 30 cm satellite imagery; labels, however, consist of bounding boxes that are not ideal for foundational mapping of buildings or roads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Other Satellite Imagery Competitions</head><p>A number of public challenges using satellite imagery have been run in the last couple of years. While SpaceNet pre-dates and helped inspire some of these challenges, the results of these challenges are of great benefit to the community and very informative to future SpaceNet competitions. The Dstl Satellite Imagery Feature Detection <ref type="bibr" target="#b11">[12]</ref> ran on Kaggle shortly after the completion of SpaceNet 1, and sought to segment multiple classes in high resolution satellite imagery. The IARPA Functional Map of the World <ref type="bibr" target="#b12">[13]</ref> challenge and dataset aimed to classify region proposals based on building type. The DIUx xView Detection Challenge <ref type="bibr" target="#b13">[14]</ref> subsequently released a large dataset of 60 bounding box object classes and approximately 1 million instances. In a similar vein to SpaceNet, the DeepGlobe Satellite Challenge at CVPR 2018 <ref type="bibr" target="#b14">[15]</ref> consisted of three distinct challenges: road segmentation, building detection, and land cover classification. The SpaceNet team assisted with this challenge, and much of the DeepGlobe data stemmed from the SpaceNet repository. Finally, the ISPRS Semantic Labeling Contest <ref type="bibr" target="#b15">[16]</ref> pre-dates all of these and provided an important benchmark for computer vision performance on remote sensing data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Challenge 1 -Rio De Janeiro Building Footprints</head><p>The first SpaceNet challenge in 2016 aimed to extract building footprints from the DigitalGlobe WorldView 2 satellite imagery at 50cm resolution. Imagery consists of 8-band multispectral imagery at 1m resolution, as well as pan-sharpened red-green-blue (RGB) imagery at 50cm resolution. Released imagery was created from a mosaic of multiple images, and covers 2544 square kilometers.  The challenge imagery was split into 200 meter tiles using the SpaceNet utilities python package <ref type="bibr" target="#b16">[17]</ref>, with 60% of the data released for training, 20% for testing, and 20% reserved for validation.</p><p>For Challenge 1 in the Rio de Janeiro Area of Interest (AOI) over 300,000 building footprints were labeled. A GIS team at DigitalGlobe (now Radiant Solutions) provided a polygon footprint for each building, which were initially extracted through semi-automated means and subsequently improved by hand. Any partially visible rooftops were approximated to represent the shape of the building. Adjoining buildings were marked individually as unique structures (i.e. each street address was marked as a unique building), see <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Challenge 2 -Las Vegas, Paris, Shanghai, Khartoum Building Footprints</head><p>The second SpaceNet challenge aimed to extract building footprints from the DigitalGlobe WorldView 3 satellite in a continuous image strip. The source imagery is distributed as a Level 2A standard product that has been radiometrically and sensor corrected, and normalized to topographic relief using a coarse digital elevation model (DEM). It contains the original panchromatic band, the 1.24m resolution 8-band multi-spectral 11-bit geotiff, and a 30cm resolution Pan-Sharpened 3-band and 8-band 16-bit geotiff. . The labels went through a rigorous QA/QC process, with one expert labeling a specific area and then a second expert performing validation on 100% of the areas. For buildings the final product was then inspected against topology errors to ensure that building footprints were closed and polygons did not overlap. With human-based annotation some small errors are inevitable, especially for rural areas, and we leave the analysis of annotator disagreement for future work.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Challenge 3 -Las Vegas, Paris, Shanghai, Khartoum Road Extraction</head><p>Challenge 3 used the imagery from Challenge 2, only tiled into 400m chips. As with the previous challenges the chipped dataset was randomly split 60%/20%/20% for train/test/validation.</p><p>CosmiQ Works worked with Radiant Solutions to label the road networks for the entire area of the existing SpaceNet Round 2 Building Dataset (see <ref type="figure" target="#fig_1">Figure 2</ref>). Because the competition was designed to enable the creation of routable road networks, a labeling schema based on OpenStreetMap (OSM) guidelines was established to ensure ground truth that would be usable by open source routing tools. In addition to the digitizing of road-centerlines, four other attributes were recorded: 1) road type, 2) surface type, 3) bridge and 4) lane number. A GIS team at DigitalGlobe (now Radiant Solutions) fully annotated each road centerline within 7 pixels (see Appendix B). The roads labels went through the same QA/QC process as SpaceNet Challenge 2 Finally each completed area of interest was inspected and validated, with all reported dangling roads or missed connections corrected. The complete label guidelines can be found in Appendix B. <ref type="table" target="#tab_2">Table 2</ref> provides a summary of road labels by road type and area of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.6">Additional Data</head><p>In addition to the data detailed above, SpaceNet also hosts data from previous competitions such as the IARPA Functional Map of the World Competition <ref type="bibr" target="#b12">[13]</ref> and the Urban 3D Challenge Dataset <ref type="bibr" target="#b17">[18]</ref>. These datasets diversify the SpaceNet data corpus, with the goal encouraging new avenues of machine learning research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The SpaceNet Buildings Metric</head><p>The aim of the first two SpaceNet challenges was to extract building footprints from high-resolution satellite imagery, which is an object detection problem. Accordingly, we adopt a similar approach to ImageNet <ref type="bibr">[6]</ref> and adopt a metric based upon the scale invariant intersection over union (IoU) metric, also known as the Jaccard index, which provides a measure of the overlap between two objects by dividing the area of the intersection by the area of the union.</p><formula xml:id="formula_0">IoU (A, B) = area(A ? B)/area(A ? B)<label>(1)</label></formula><p>As with Equation 5 of ImageNet, we select an IoU threshold of ? 0.5 to denote a true positive detection. One additional feature that SpaceNet adopts from the ImageNet competition is the notion that each labeled region can have at most one true positive associated with that labeled region. This feature is implemented by a sequential search for a true positive sorted by decreasing IoU values. If a true positive is found, then the pair (the label and the proposed region) are removed from the sequence and the search continues.</p><p>With an object detection algorithm, the performance of an algorithm should depend on how many objects the algorithm detects (true positives) how many objects it fails to detect (false negatives), and how many non-objects it detects (false positives). Accordingly, for each scene in the SpaceNet data corpus, we compute the total number of true positives, false positives (proposals with IoU &lt; 0.5), and false negatives (ground truth buildings without a valid proposal). In a given city, we sum the true and false positives and negatives to compute total precision and recall scores; the city-wide F1 score is given by the harmonic mean of precision and recall:</p><formula xml:id="formula_1">F 1 = 2 ? (P recision ? Recall)/(P recision + Recall)<label>(2)</label></formula><p>The total SpaceNet Buildings Metric is tabulated via the arithmetic mean of the F1 scores of each city.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">APLS: The SpaceNet Roads Metric</head><p>The third SpaceNet competition aimed to extract road networks from satellite imagery. Historically, pixel-based metrics have often been used to assess the quality of road proposals, though such metrics are suboptimal for a number of reasons that we discuss below. Two graph-theoretic metrics have also been used (albeit with less frequency than pixel-based metrics), though these metrics still don't capture quite what was intended with SpaceNet 3. Accordingly, we developed a novel metric (Average Path Length Similarity: APLS) <ref type="bibr" target="#b18">[19]</ref> to measure similarity between ground truth and proposal road graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pixel-Based Metrics</head><p>Road localization from overhead imagery has often been treated as an image segmentation problem (i.e. identifying which pixels in an image belong to which class). In segmentation problems, each prediction pixel is either a true positive, false positive, or false negative. Three commonly used evaluation metrics are discussed below.</p><p>Pixel-Based IoU For object detection problems a threshold IoU value is often defined, above which the prediction is assumed to be a true positive. For pixel-based approaches, the number of true positive pixels forms the intersecting area, and the sum of true positive, false positive, and false negative pixels forms the union. IoU is subsequently calculated from Equation 1. Pixel-Based F1 Score Each prediction pixel is either a true positive, false positive, true negative, or false negative. These can be combined into an F1 score via Equation 2. Relaxed F1 This is computed by first calculating the relaxed precision and relaxed recall. Relaxed precision is the fraction of predicted road pixels that are within a predetermined number of road pixels (Q) of a true road pixel <ref type="bibr" target="#b19">[20]</ref>. Relaxed recall is the fraction of true road pixels within Q pixels of a predicted road pixel. The F1 score is subsequently tabulated via Equation 2. , and IoU = 0.91). Therefore, legacy metrics prefer the right prediction to the middle prediction, even though the rightmost prediction would not be useful for routing purposes.</p><p>The three metrics listed above all fail to adequately incentivize the creation of connected road networks. For example, for the IoU and F1 metrics a slight error in the road width is heavily penalized, though a brief break in an inferred road (from a shadow or overhanging tree) is lightly penalized, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">APLS Metric</head><p>Our focus on road networks naturally leads us to consider graph theory as a means to evaluate proposals. Most similarity measures in graph theory focus solely on the logical topology (connections between nodes) of the graph, whereas we care about both logical topology as well as the physical topology of the roads. Nevertheless, metrics have been proposed specifically for road graph similarity matching. <ref type="bibr" target="#b20">[21]</ref> proposed a metric for computing road graph similarity via comparing the nodes that can be reached within a small local vicinity of a number of seed nodes, categorizing proposal nodes as true positives, false positives, or false negatives depending on whether they fall within a buffer region (referred t to as the "hole size"). By design, this metric evaluates local subgraphs in a small subregion (? 300 meters in extent), and relies upon physical geometry. This metric is very sensitive to the locations of seed nodes, and connections between greatly disparate points (&gt; 300 meters apart) are not measured. <ref type="bibr" target="#b21">[22]</ref> proposed computing the percentage of paths where the difference in distance between the ground truth and proposal path lengths was less than 5%; this metric gives some indication of length similarity, though provides little insight if differences are slightly greater than 5%, and does not penalize spurious unconnected proposals.</p><p>Since we are primarily interested in routing, (and to overcome some of the limitations of the metrics proposed in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>) we propose a graph theoretic metric based upon Dijkstra's shortest path algorithm <ref type="bibr" target="#b22">[23]</ref>. In essence, we symmetrically sum the differences in optimal paths between ground truth and proposal graphs.</p><p>To measure the difference between ground truth and proposal graphs, the APLS metric sums the differences in optimal path lengths between nodes in the ground truth graph G and the proposal graph G'. In effect, this metric repeats the path difference calculation shown in <ref type="figure" target="#fig_5">Figure 6</ref> for all paths in the graph. Missing paths in the graph are assigned the maximum proportional difference of 1.0. The APLS metric scales from 0 (poor) to 1 (perfect),</p><formula xml:id="formula_2">C = 1 ? 1 N min 1, | L(a, b) ? L(a , b ) | L(a, b)<label>(3)</label></formula><p>where N = number of unique paths, while L(a, b) = length of path(a, b). The sum is taken over all possible source (a) and target (b) nodes in the ground truth graph. The node a denotes the node in Left: Shortest path between source (green) and target (red) node in the ground truth graph is shown in yellow, with a path length of ? 948 meters. Right: Shortest path between source and target node in the proposal graph with 30 edges removed, with a path length of ? 1027 meters; this difference in length forms the basis for our graph similarity metric. Plotting is accomplished via the osmnx python package <ref type="bibr" target="#b23">[24]</ref>.</p><p>the proposal graph closest to the location of ground truth node a. If path(a , b ) does not exist, the maximum contribution of 1.0 is used, thereby ensuring that missing routes are highly penalized.</p><p>Inherent to this metric is the notion of betweenness centrality, or the number of times a node appears along the shortest paths between other nodes. Missing nodes of high centrality will be penalized much more heavily by the APLS metric than missing nodes of low centrality. This feature is intentional, as heavily trafficked intersections are much more important than cul-de-sacs for routing purposes. Any proposed graph G' with missing edges (e.g. if an overhanging tree is inferred to sever a road) will be heavily penalized by the APLS metric, so ensuring that roads are properly connected is crucial for a high score.</p><p>For small graphs the greedy approach of computing all possible paths in the network is entirely feasible (computing all possible paths for the 400m image chips of SpaceNet Challenge 3 takes less than 1 millisecond). For larger graphs, one must decide which nodes and paths are of paramount interest, lest the computational load become burdensome.</p><p>While we developed the APLS metric as a means to score computer vision competitions, unlike pixel-based metrics the APLS metric is agnostic to data type. The APLS metric applies equally well to road network proposals derived from optical imagery, radar, GPS tracks, LIDAR, and/or hand-crafted labels. Unfortunately, this metric is not differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Graph Augmentation</head><p>We select control nodes in the graph to denote points and routes of interest. In practice, control nodes are comprised of intersections, endpoints, and midpoints along edges. In order to capture the physical topology of roads, we inject midpoint nodes every 50 meters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Node Snapping</head><p>Equation 3 is easy to apply if the proposal graph G' has nodes coincident with the ground truth graph G. In practice, however, proposals will not align perfectly with ground truth graphs. Subsequently, we "snap" the control points of the ground truth graph onto the proposal graph. Proposal nodes must be within a given buffer distance of ground truth nodes. This buffer is 4 meters by default, which is equivalent to the labeling accuracy requirement for the road centerlines. Proposal nodes outside the buffer will not be snapped, and therefore highly penalized. This process creates a new proposal graph with nodes as coincident as possible to the nodes of the ground truth graph. Once the node snapping process is complete, routes between nodes in the ground truth graph (path(a, b)) can be  <ref type="figure">(path(a , b )</ref>) using Equation <ref type="bibr">3</ref>. See Appendix C for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Symmetric Comparisons</head><p>Once the ground truth to proposal node snapping procedure of Section 4.2.2 is completed, optimal path lengths can be compared between proposal and ground truth nodes. In order to penalize spurious road proposals, one must also perform the inverse operation by snapping proposal control nodes onto the ground truth graph. This is illustrated in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4">Total APLS Metric</head><p>APLS part 1 is computed by snapping ground truth control nodes onto the proposal graph and calculating Equation 3. APLS part 2 is computed via the reverse: snapping proposal nodes onto the ground truth graph and calculating Equation 3. The total ALS metric is the harmonic mean of APLS part 1 and APLS part 2, and scales from 0 (poor) to 1 (perfect).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">SpaceNet Competition Results</head><p>In this section we cover highlights of the winning submissions, the full results can be found on the SpaceNet website <ref type="bibr" target="#b24">[25]</ref>. All competitions were hosted on TopCoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">SpaceNet Challenge 1 Results -Rio de Janeiro Building Footprints</head><p>For the first challenge, we baselined performance with a modified version of YOLO <ref type="bibr" target="#b25">[26]</ref> we call YOLT <ref type="bibr" target="#b26">[27]</ref>; this method achieved an F1 score of 0.21. This baseline provides a very different approach (and useful comparison) to the winning 3-class random forest classification + 'polygonization' algorithm of wleite <ref type="bibr" target="#b27">[28]</ref> (F1 = 0.26). One highlight of the insights provided by this initial challenge is the degree to which all algorithms struggled to distinguish individual row houses in the densely populated central regions of Rio de Janeiro.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">SpaceNet Challenge 2 Results -Las Vegas, Paris, Shanghai, Khartoum Building Footprints</head><p>The second buildings challenge yielded far better results than the first. This is likely due to a combination of three factors: increased resolution (30 cm vs 50 cm for SpaceNet Round 1), improved building labels (due to greater experience by the labeling team, higher resolution images, and lower density in the SpaceNet 2 cities), and/or improved competitor algorithms. We baselined performance with both YOLT (F1 = 0.60) and a modified version of MNC <ref type="bibr" target="#b28">[29]</ref> (F1 = 0.57). The winning algorithm submitted by XD_XD <ref type="bibr" target="#b29">[30]</ref> used an ensemble of three deep learning segmentation masks with a U-Net <ref type="bibr" target="#b30">[31]</ref> architecture. This ensemble, combined with thresholding and filtering yielded building polygon predictions and an F1 score of 0.69. Las Vegas proved the easiest city to classify (partly due to the many well separated residential buildings with low variance in size), with Khartoum proving the most challenging (partly due to the high variance in building size and low contract between building and background). See <ref type="table" target="#tab_3">Table 3</ref> for detailed results of the top three competitors, and Appendix D for images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">SpaceNet Challenge 3 Results -Las Vegas, Paris, Shanghai, Khartoum Road Networks</head><p>Given the novelty of the metric used and the demand for geospatial vector submissions, entries to the first roads challenge were surprisingly good. The top performers all took an approach similar to the ), but with greater attention paid to network architecture and post-processing. The winning algorithm was submitted by competitor albu <ref type="bibr">[33]</ref>, which used an ensemble of deep learning segmentation encoders/decoders to train a global road model. Results from the winning algorithm are shown in <ref type="figure" target="#fig_4">Figure 5</ref>, and detailed results of the top five competitors is shown in <ref type="table" target="#tab_4">Table 4</ref> (we show the top five competitors and three significant figures due to the closeness of scores).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>Foundational mapping has myriad applications, from commercial (e.g. autonomous vehicles), to humanitarian (e.g. disaster response), to civil (e.g. traffic congestion mitigation). Satellite imagery may be able to aid foundational mapping, particularly in cases of natural disasters or other dynamic events where the revisit rate of satellites may be able to provide updates far faster than terrestrial or airborne methods, and cover a large geographic area.</p><p>The SpaceNet dataset provides a large corpus of high resolution multi-band imagery, with attendant validated building footprint and road network labels. The SpaceNet building footprint extraction challenges have yielded algorithms with F1 scores of 0.69 on multiple cities, and vast improvement in performance from Challenge 1 to Challenge 2.</p><p>In this paper, we demonstrated that the standard technique for scoring road detection algorithms (pixel-based F1 score) is suboptimal for routing purposes. Accordingly, we developed the graphtheoretic APLS metric based on shortest paths between nodes in ground truth and proposal networks. This metric rewards correct road centerline and intersection determination to a far greater extent than pixel-based metrics. The SpaceNet competition yielded multiple high quality submissions in graph structure format, and a winning score of 0.66. These outputs can be used directly for routing, and provide a step towards addressing the many problems that improved automated routing impacts.</p><p>We will continue adding imagery and labels to the SpaceNet data corpus, as well as hosting challenges to encourage the development of algorithms tailored to the unique aspects of satellite imagery. The next (fourth) challenge will analyze the ability of algorithms to correctly localize building footprints in off-nadir (i.e. oblique look angle) imagery; this capability is sorely needed in many disaster response scenarios, where the first available image of an area may be at a very high off-nadir angle. The fifth SpaceNet competition will revisit road network extraction, this time utilizing metadata to explore road network travel times. Subsequent challenges will use metadata collected in the ongoing labeling campaigns to methodically increase problem complexity and difficulty. Our aspiration is that the large amount of labeled SpaceNet data will stir the imagination of an increasing cadre of computer vision experts, and lead to improvements in remote sensing analytics.    <ref type="figure" target="#fig_5">Figure 6</ref>. Right: Path length difference histogram. Most routes are the same between the proposal and ground truth graphs, resulting in the many blue dots on the left plot and the large spike at path length 0.0 on the right plot. There are a few missing routes due to the disconnected node in the center of the proposal graph, and this results in the red dots on the left plot and the small spike at path length 1.0 on the right plot. Applying Equation 5 to this data yields APLS = 0.81.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APLS Graph Augmentation</head><p>A couple of complications arise when comparing road networks, both relating to the selection of which routes to compare. Accordingly, we select "control nodes" in the graph between which routes are of interest. In practice, control nodes are comprised of intersections, endpoints, and midpoints along edges. In order to capture the physical topology of roads, we inject nodes every 50 meters. Obviously, for very large graphs one would be far more parsimonious with the selection of control nodes, but for the 400m SpaceNet tiles we can afford to select a high density of control nodes. This process is shown in <ref type="figure" target="#fig_7">Figure 8</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Node Snapping</head><p>Equation 3 is easy to apply if the proposal graph G' has nodes coincident with the ground truth graph G. In practice, however, proposals will not align perfectly with ground truth graphs. Subsequently, we "snap" the control points of the ground truth graph onto the proposal graph. Proposal nodes must be within a given buffer distance of ground truth nodes. This buffer is 4 meters by default, which is equivalent to the labeling accuracy requirement. Proposal nodes outside the buffer will not be snapped, and therefore highly penalized. This process creates a new proposal graph with nodes as coincident as possible to the nodes of the ground truth graph. Once the node snapping process is complete, routes between nodes in the ground truth graph (path(a, b)) can be directly compared to routes in the proposal graph (path(a , b ) using Equation <ref type="bibr">3</ref>. See Appendix A for more details.</p><p>Once the process of <ref type="figure" target="#fig_8">Figure 9</ref> is complete, routes between nodes of the ground truth graph with control points ( <ref type="figure" target="#fig_8">Figure 9A</ref>) and the augmented proposal graph ( <ref type="figure" target="#fig_8">Figure 9F</ref>) can be directly compared.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetric Comparisons</head><p>Once the ground truth to proposal node snapping procedure of <ref type="figure" target="#fig_8">Figure 9</ref> is completed, optimal path lengths can be compared between proposal and ground truth nodes. In order to penalize spurious road proposals, one must also perform the inverse operation by snapping proposal control nodes onto the ground truth graph. This is illustrated in <ref type="figure" target="#fig_0">Figure 10</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>200m SpaceNet chip over Rio de Janeiro and attendant building labels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>400m SpaceNet chip of Las Vegas. Left: SpaceNet GeoJSON road label. Right: RGB image overlaid with road centerlines (orange).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Legacy metrics often incentivize poor predictions. Left: Ground truth road network in green. Middle: proposal road mask in orange; the road widths are poorly reproduced, though there are no breaks or extraneous connections in the network, yielding scores of F1 = 0.82, relaxed F1 (rF1) = 0.94 for radius = 3), and IoU = 0.69. Right: second proposal road mask in blue; road widths are correct, though a gap exists in the roads (often due to overhanging trees). The right plot yields higher scores (F1 = 0.95, rF1 = 0.96 (radius = 3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Demonstration of path length difference between sample ground truth and proposal graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Results from the winning implementation for the roads challenge. Top Left: A simple road network in a 400m ? 400m chip from the test set in Las Vegas; the blue line is the ground truth, the yellow line the proposal network, and the APLS score is 0.99. Top Right: A complex road network in Las Vegas; in the center of the graph network there is a disconnect where the divider is located. Bottom Left: A complex network in Shanghai; there are several missed streets in the center of the graph. Bottom Right: A low scoring road network in Khartoum; the proposal network misses several dirt roads, but performs well on the more established paved road network.Appendix C. APLS MetricPath length differences are summed for each possible route in the graphs, which we illustrate inFigures 6,<ref type="bibr" target="#b6">7.</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Demonstration of path length difference between sample ground truth and proposal graphs. Left: Shortest path between source (green) and target (red) node in the ground truth graph is shown in yellow, with a path length of 948 meters. Right: Shortest path between source and target node in the proposal graph with 30 edges removed, with a path length of 1027 meters; this difference in length forms the basis for our graph similarity metric. Plotting is accomplished via the osmnx python package.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Left: Path length differences for all 1720 possible routes possible in the ground truth graph of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Left: Ground truth graph. Right: Ground truth graph with control nodes (purple) injected along routes. The graph at right is the one used to compute the APLS metric.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Node snapping procedure. A: Ground truth graph with control nodes. B: Proposal graph. C: Ground truth graph (orange) and ground truth control points (red) overlaid on proposal graph (grey). D: Same as graph C, but showing the buffer (yellow) around the proposal graph. E: Ground truth control nodes are injected into the proposal graph at the nearest edge, except for nodes outside the buffer (grey). F: Final proposal graph with nodes injected at the appropriate location to compare to graph A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Illustration of the need to apply Equation 3 symmetrically (i.e.: ground truth to proposal, and proposal to ground truth). A: Ground truth graph. B: Proposal graph with many short, spurious connections well outside the buffer. C: Proposal nodes snapped onto the ground truth graph; snapping proposal control points onto the ground truth graph and then computing the metric penalizes the many extraneous predictions (grey nodes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 14 :</head><label>14</label><figDesc>Sample output of baseline algorithm applied to SpaceNet test data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="3">SpaceNet imagery and building label details</cell></row><row><cell>AOI</cell><cell>Area</cell><cell cols="3"># Buildings GSD Sensor</cell><cell>Date</cell></row><row><cell></cell><cell cols="3">(km 2 ) (Polygons) (cm)</cell></row><row><cell>Rio</cell><cell cols="2">2,544 382,534</cell><cell>50</cell><cell>WorldView-2 2011-2014</cell></row><row><cell cols="2">Las Vegas 216</cell><cell>151,367</cell><cell>30</cell><cell>WorldView-3 2015-10-22</cell></row><row><cell>Paris</cell><cell cols="2">1,030 23,816</cell><cell>30</cell><cell>WorldView-3 2016-02-29</cell></row><row><cell>Shanghai</cell><cell cols="2">1,000 92,015</cell><cell>30</cell><cell>WorldView-3 2015-06-06</cell></row><row><cell cols="2">Khartoum 765</cell><cell>35,503</cell><cell>30</cell><cell>WorldView-3 2015-04-13</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>provides collection details for each SpaceNet Area of Interest (AOI).</figDesc><table><row><cell>The dataset includes four areas: Las Vegas, Paris, Shanghai, and Khartoum. The labeled dataset</cell></row><row><cell>consists of 24,586 scenes of size 200 m ? 200 m (650 px ? 650 px) containing 302,701 building foot-</cell></row><row><cell>prints across all areas, and are both urban and suburban in nature. The dataset was split 60%/20%/20%</cell></row><row><cell>for train/test/validation. Each area is covered by a single image strip, which ensures that sun, satellite,</cell></row><row><cell>and atmospheric conditions are consistent across the entire scene. A GIS team at DigitalGlobe (now</cell></row><row><cell>Radiant Solutions) fully annotated each scene to within 5 pixels for building polygon corners (see</cell></row><row><cell>Appendix A)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>SpaceNet road labels</figDesc><table><row><cell>Road Type</cell><cell cols="5">AOI 2 Vegas AOI 3 Paris AOI 4 Shanghai AOI 5 Khartoum Total</cell></row><row><cell>Motorway</cell><cell>115.0 km</cell><cell>9.0 km</cell><cell>102.1 km</cell><cell>13.4 km</cell><cell>239.5 km</cell></row><row><cell>Primary</cell><cell>364.7 km</cell><cell>14.3 km</cell><cell>191.5 km</cell><cell>98.4 km</cell><cell>668.9 km</cell></row><row><cell>Secondary</cell><cell>416.9 km</cell><cell>58.1 km</cell><cell>500.7 km</cell><cell>65.8 km</cell><cell>1041.5 km</cell></row><row><cell>Tertiary</cell><cell>2.9 km</cell><cell>10.5 km</cell><cell>33.6 km</cell><cell>67.9 km</cell><cell>115.0 km</cell></row><row><cell>Residential</cell><cell>1645.6 km</cell><cell>232.4 km</cell><cell>938.3 km</cell><cell>484.5 km</cell><cell>3301.3 km</cell></row><row><cell cols="2">Unclassified 1138. km</cell><cell>95.4 km</cell><cell>1750.8 km</cell><cell>164.5 km</cell><cell>3148.7 km</cell></row><row><cell>Total</cell><cell>3685.0 km</cell><cell>425.3 km</cell><cell>3536.9 km</cell><cell>1029.5 km</cell><cell>8676.6 km</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>SpaceNet Challenge 2 Results (F1 Metric)</figDesc><table><row><cell cols="7">Rank Competitor Las Vegas Paris Shanghai Khartoum Total Score</cell></row><row><cell>1</cell><cell>XD_XD</cell><cell>0.89</cell><cell>0.75</cell><cell>0.60</cell><cell>0.54</cell><cell>0.69</cell></row><row><cell>2</cell><cell>wleite</cell><cell>0.83</cell><cell>0.68</cell><cell>0.58</cell><cell>0.48</cell><cell>0.64</cell></row><row><cell>3</cell><cell>nofto</cell><cell>0.79</cell><cell>0.58</cell><cell>0.52</cell><cell>0.42</cell><cell>0.58</cell></row><row><cell cols="4">directly compared to routes in the proposal graph</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>SpaceNet Challenge 3 Results (APLS Metric)</figDesc><table><row><cell cols="7">Rank Competitor Las Vegas Paris Shanghai Khartoum Total Score</cell></row><row><cell>1</cell><cell>albu</cell><cell>0.798</cell><cell>0.604</cell><cell>0.654</cell><cell>0.609</cell><cell>0.6663</cell></row><row><cell>2</cell><cell>cannab</cell><cell>0.780</cell><cell>0.645</cell><cell>0.640</cell><cell>0.600</cell><cell>0.6661</cell></row><row><cell>3</cell><cell>pfr</cell><cell>0.801</cell><cell>0.601</cell><cell>0.665</cell><cell>0.598</cell><cell>0.666</cell></row><row><cell>4</cell><cell>selim_sef</cell><cell>0.788</cell><cell>0.599</cell><cell>0.647</cell><cell>0.592</cell><cell>0.657</cell></row><row><cell>5</cell><cell>fabastani</cell><cell>0.771</cell><cell>0.547</cell><cell>0.633</cell><cell>0.563</cell><cell>0.628</cell></row><row><cell cols="7">baseline algorithm (APLS = 0.49) outlined in Appendix E (i.e. U-Net + skeletonization + sknw [32]</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Appendix A. SpaceNet Building Footprint Labeling Guide 1. Buildings will primarily be within urban/suburban areas. 2. Rooflines will first be extracted to best represent the shape of the footprint and for any buildings where the base of the building is partially visible due to off-nadir angle the polygon will be shifted to best fit the base corners. 3. Buildings will have squared corners (assuming they are true to the shape of the building). 4. Extraction of building corners will be within 5 pixels of the location on the imagery. 5. Building footprints will be closed polygons and will not overlap. <ref type="bibr">6</ref>. No attributes will be populated. 7. Adjoining buildings with different heights shall be captured at one visible roof for the whole block and shifted to that visible roofs baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. The SpaceNet Roads Dataset Labeling Guidelines</head><p>The SpaceNet Roads Dataset labeling guidelines 1. Road vectors must be drawn as a center line within 2m (7 pixels) of observed road (a) The centerline of a road is defined as the centerline of the roadway. If a road has an even number of lanes, the centerline shall be drawn on the line separating lanes. If the road has an odd number of lanes then the centerline should be drawn down the center of the middle lane. (b) Divided highways should have two centerlines, a centerline for each direction of traffic.</p><p>See below for the definition of a divided highway. 2. Road vectors must be represented as a connected network to support routing. Roads that intersect each other should share points as an intersection like instructed through OSM. Roads that cross each other that are not connected such as while using an overpass should not share a point of connection. 3. Roads must not bisect building footprints. 4. Sections of a road that are a bridge or overpass must be labeled as a bridge via a Boolean flag. 5. Divided highways must be represented as two lines with traffic direction indicated when possible. 6. Surface type must be classified as: paved, unpaved, or unknown. 7. Road will be identified by type: (Motorway, Primary, Secondary, Tertiary, Residential, Unclassified, Cart Track) 8. Number of lanes will be listed as number of lanes for each centerline as defined in rule 1. If road has two lanes in each direction, the number of lanes shall equal 4. If a road has 3 lanes in one direction and 2 directions in another the total number of lanes shall be 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition of Divided Highway</head><p>A divided highway is a road that has a median or barrier that physically prevents turns across traffic. A median can be:</p><p>? Concrete ? Asphalt ? Green Space ? Dirt/unpaved road A median is not:</p><p>? Yellow hatched lines on pavement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Road Type Guidelines</head><p>All road types were defined using the Open Street Maps taxonomy for key=highway. The below descriptions were taken from the Open Street Maps tagging guidelines for highway and the East Africa Tagging Guidelines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Road Label GeoJSON Schema</head><p>Attributes:</p><p>1. "geometry": Linestring 2. "road_id": int; Identifier Index 3. "road_type": int </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D. Building Detection Results</head><p>Here we show a few snapshots of the winning algorithm for SpaceNet Challenge 2. <ref type="figure">Figure 11</ref>: Results from the winning implementation of XD_XD for Buildings Round 2: From top left, Clockwise (Vegas, Vegas, Khartoum, Paris). The blue outline represents the ground truth, the green outlines are true positives, the red are false positives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Road Baseline Algorithm</head><p>The APLS metric is designed to reward correct node placement and connectivity, and so should prove a better metric than pixel-based F1 for automated route inference evaluation. In this section, we discuss the CosmiQ baseline procedures for exploring the application of APLS to satellite imagery <ref type="bibr" target="#b31">[34]</ref>.</p><p>While any number of approaches are valid (and encouraged) to tackle the SpaceNet challenge, the most obvious workflow begins with inferring a segmentation mask using convolutional neural networks (CNNs). We assume a road centerline halfwidth of 2m and create training masks using the raw imagery and SpaceNet geojson road labels (see <ref type="figure">Figure 12</ref>).  <ref type="figure">Figure 13</ref>: Baseline algorithm. Left: Using road masks, we train CNN segmentation algorithms (such as PSPNet <ref type="bibr" target="#b32">[35]</ref> and U-Net <ref type="bibr" target="#b30">[31]</ref>) to infer road masks from SpaceNet imagery. Left center: These outputs masks are then refined using standard techniques such as thresholding, opening, closing, and smoothing. Right center: A skeleton is created from this refined mask (e.g.: sckit-image skeletonize <ref type="bibr">[36]</ref>. Right: This skeleton is subsequently rendered into a graph structure, such as with the sknw package <ref type="bibr">[32]</ref>. Competitors to the roads challenge generally followed the same approach as this baseline algorithm, albeit with different network architectures and more refined post-processing steps.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://registry.opendata.aws/spacenet" />
		<title level="m">AWS: Spacenet on aws</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A multi-task convolutional neural network for mega-city analysis using very high resolution satellite imagery and geospatial data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1702.07985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Worldpop, open data for spatial demography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Tatem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170004</biblScope>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Disaggregating census data for population mapping using random forests with remotely-sensed and ancillary data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Gaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Tatem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2" to="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<ptr target="https://tasks.hotosm.org/" />
		<title level="m">HOTOSM: Hot tasking manager</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<ptr target="http://www2.isprs.org/commissions/comm3/wg4/semantic-labeling.html" />
		<title level="m">ISPRS: 2d semantic labeling contest</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Torontocity: Seeing the world with a million eyes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>M?ttyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheverie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1612.00423</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Machine Learning for Aerial Image Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">A</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
		<idno>abs/1609.04453</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">xview: Objects in context in overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuzma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klaric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccord</surname></persName>
		</author>
		<idno>abs/1802.07856</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Kaggle: Dstl satellite imagery feature detection</title>
		<ptr target="https://www.kaggle.com/c/dstl-satellite-imagery-feature-detection" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Functional map of the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Christie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fendley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mukherjee</surname></persName>
		</author>
		<idno>abs/1711.07846</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">xview: Objects in context in overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kuzma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcgee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dooley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Laielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klaric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mccord</surname></persName>
		</author>
		<idno>abs/1802.07856</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deepglobe</surname></persName>
		</author>
		<ptr target="http://deepglobe.org/challenge.html" />
	</analytic>
	<monogr>
		<title level="j">Deepglobe -cvpr</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gerke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sohn</surname></persName>
		</author>
		<title level="m">Isprs semantic labeling contest</title>
		<imprint>
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title/>
		<ptr target="https://github.com/SpaceNetChallenge/utilities/" />
	</analytic>
	<monogr>
		<title level="j">CosmiQWorks: Spacenet challenge utilites</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A benchmark for building footprint classification using orthorectified rgb imagery and digital surface models from commercial satellites</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Applied Imagery Pattern Recognition Workshop</title>
		<meeting>IEEE Applied Imagery Pattern Recognition Workshop</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
		<ptr target="https://github.com/CosmiQ/apls" />
	</analytic>
	<monogr>
		<title level="j">CosmiQWorks: Apls metric</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning to detect roads in high-resolution aerial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th European Conference on Computer Vision: Part VI. ECCV&apos;10</title>
		<meeting>the 11th European Conference on Computer Vision: Part VI. ECCV&apos;10<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="210" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Inferring road maps from global positioning system traces: Survey and comparative evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Biagioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Record</title>
		<imprint>
			<biblScope unit="volume">2291</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="71" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A higher-order crf model for road network extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Montoya-Zegarra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1698" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A note on two problems in connexion with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Dijkstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="271" />
			<date type="published" when="1959-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Osmnx: New methods for acquiring, constructing, analyzing, and visualizing complex street networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boeing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers, Environment and Urban Systems</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="126" to="139" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<ptr target="https://spacenetchallenge.github.io/" />
	</analytic>
	<monogr>
		<title level="j">CosmiQWorks: Spacenet on aws</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">YOLO9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno>abs/1612.08242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">You only look twice: Rapid multi-scale object detection in satellite imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Etten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Submitted to KDD</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<ptr target="https://github.com/SpaceNetChallenge/BuildingDetectors/tree/master/wleite" />
		<title level="m">wleite: Spacenet round 1 winner: wleite&apos;s implementation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.04412</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<ptr target="https://github.com/SpaceNetChallenge/BuildingDetectors_Round2/tree/master/1-XD_XD" />
		<title level="m">Spacenet round 2 winner: Xd_xd&apos;s implementation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno>abs/1505.04597</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CosmiQWorks: Broad area satellite imagery semantic segmentation</title>
		<ptr target="https://github.com/CosmiQ/basiss" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A restricted access major divided highway, normally with 2 or more running lanes plus emergency hard shoulder</title>
		<imprint>
			<pubPlace>Autobahn, etc</pubPlace>
		</imprint>
	</monogr>
	<note>Access onto a motorway comes exclusively through ramps (controlled access). Equivalent to the Freeway</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">National roads connect the most important cities/towns in a country. In most countries, these roads will usually be tarmacked and show center markings</title>
	</analytic>
	<monogr>
		<title level="m">South Sudan, however, primary roads might also be unpaved</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Secondary roads are the second most important roads in a country&apos;s transport system. They typically link medium-sized places</title>
		<imprint/>
	</monogr>
	<note>They may be paved but in in some countries they are not</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">More often than not, these roads will be unpaved. However, this tag should be used only on roads wide enough to allow two cars to pass safely</title>
		<imprint/>
	</monogr>
	<note>tertiary -Tertiary roads are busy through roads that link smaller towns and larger villages</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Roads which serve as an access to housing, without function of connecting settlements. Often lined with housing</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The least important through roads in a country&apos;s system -i.e. minor roads of a lower classification than tertiary, but which serve a purpose other than access to properties. Often link villages and hamlets. (The word &quot;unclassified&quot; is a historical artifact of the UK road system and does not mean that the classification is unknown</title>
		<imprint/>
	</monogr>
	<note>you can use highway=road for that</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
