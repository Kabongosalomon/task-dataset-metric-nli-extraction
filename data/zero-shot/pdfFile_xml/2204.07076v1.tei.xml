<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Learning for Joint Depth and Image Reconstruction from Diffracted Rotation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mazen</forename><surname>Mel</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><surname>Siddiqui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Zanuttigh</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Engineering</orgName>
								<orgName type="institution">University of Padova</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Sony Europe B.V</orgName>
								<orgName type="institution">Zweigniederlassung Deutschland Stuttgart Technology Center</orgName>
								<address>
									<settlement>Stuttgart</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Learning for Joint Depth and Image Reconstruction from Diffracted Rotation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Monocular Depth Estimation</term>
					<term>RPSFs</term>
					<term>Image Deblurring</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Monocular depth estimation</head><p>is still an open challenge due to the ill-posed nature of the problem at hand. Deep learning based techniques have been extensively studied and proved capable of producing acceptable depth estimation accuracy even if the lack of meaningful and robust depth cues within single RGB input images severally limits their performance. Coded aperture-based methods using phase and amplitude masks encode strong depth cues within 2D images by means of depth-dependent Point Spread Functions (PSFs) at the price of a reduced image quality. In this paper, we propose a novel end-to-end learning approach for depth from diffracted rotation. A phase mask that produces a Rotating Point Spread Function (RPSF) as a function of defocus is jointly optimized with the weights of a depth estimation neural network. To this aim, we introduce a differentiable physical model of the aperture mask and exploit an accurate simulation of the camera imaging pipeline. Our approach requires a significantly less complex model and less training data, yet it is superior to existing methods in the task of monocular depth estimation on indoor benchmarks. In addition, we address the problem of image degradation by incorporating a non-blind and non-uniform image deblurring module to recover the sharp all-in-focus image from its RPSF-blurred counterpart.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Depth estimation from a single RGB image is an ill-posed inverse problem, thus, additional image priors or sophisticated imaging systems are generally needed to account for the lack of depth cues in captured images. For example, the camera optics may be modified accordingly, a more advanced image sensor design and task-specific post processing algorithms may be developed in order to capture information well beyond the capabilities of conventional imaging systems.</p><p>In this paper, we propose a computational camera wherein the Point Spread Function (PSF) is altered in order to encode depth information within a 2D image. The desired PSF is obtained via an optimized phase mask inserted at the aperture plane of the camera. Our system encodes meaningful and robust depth cues in single RGB images thus making it easier for post-processing algorithms to produce accurate depth data. However, such approach generally suffers from image quality degradation due to the poor light efficiency and/or the low Modulation Transfer Function (MTF) of the engineered PSF for high spatial frequency components. In fact, amplitude aperture masks block a significant amount of light from reaching the image sensor resulting in low light throughput, while using phase only masks solves such problem since it only acts on the phase component of the incoming light wave. Still, the MTF drops rapidly in high frequency regions and the depth-dependent blurring caused by the camera's engineered PSF produces low SNR and degrade image quality.</p><p>We propose a full pipeline that takes as input a single RGB image and produces an estimated depth map of the scene along with the recovered sharp image from its PSF-blurred counterpart. We introduce a novel end-to-end deep learning model that jointly deals with the PSF design optimization and the depth estimation task. To this aim, a differentiable physical model of the aperture mask is introduced together with an accurate simulation of the imaging pipeline including the optimized optics. In this way the learned model is able to firstly predict the optimal parameters of the phase mask design and then estimate the depth data from the RPSF-blurred input. Finally, in order to address the image quality issue, a deep learning based non-blind and non-uniform deblurring module is incorporated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Rotating Point Spread Functions. RPSFs have been theoretically shown to increase the Fisher Information along the depth dimension by at least an order of magnitude as they have a uniformly lower Cram?r-Rao bound across the axial dimension <ref type="bibr" target="#b0">[1]</ref>, which makes them highly sensitive to depth changes. RPSFs are obtained using pupil engineered cameras by means of phase and/or amplitude masks that are inspired by the concept of Orbital Angular Momentum (OAM) of light beams <ref type="bibr" target="#b1">[2]</ref>. A beam with a rotating light intensity distribution along the propagation axis can be generated by a linear superposition of a set of suitably chosen Gaussian-Laguerre (GL) modes <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b0">1]</ref> that can be optically encoded by the aperture mask. These masks generate a PSF with invariant features that continuously rotate with defocus. However, such approach suffers from poor light throughput, some works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> addressed this problem and proposed iterative optimization schemes to ensure higher light efficiency of the engineered RPSF. Depth dependent RPSFs can be also generated by phase only masks. For instance, a phase mask design inspired by Spiral Phase Plates (SPPs) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> was introduced in <ref type="bibr" target="#b7">[8]</ref> where the pupil is subdivided into a set of annular Fresnel zones with an azimuthally increasing thickness profile, the delay imposed on the incident light waves increases azimuthally generating a corkscrew like wave-front carrying an OAM with a rotating phase function. In <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref>, the authors generalized the previous phase mask design by considering a phase function that allows for generating multi-orderhelix RPSFs by introducing new design parameters: the number of rotating lobes within the RPSF and the confinement of each zone, i.e. the inner and outer radii of each annular region, in addition to the number of Fresnel zones. While <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b9">[10]</ref> used purely empirical approaches to determine the values for each design parameter, in this work we optimize those parameters in an end-to-end fashion jointly with the weights of a depth estimation deep neural network.</p><p>Depth estimation using coded apertures. Levin et al. <ref type="bibr" target="#b10">[11]</ref> proposed an amplitude modulation mask that was placed in front of a conventional camera lens to encode depth information via the diffracted pattern of the camera's PSF, as point-like sources move along a plane parallel to that of the camera sensor, the mask shadow would shift accordingly and as they move closer or farther from the camera, the pattern would expand or shrink. This information is later used to determine the object's distance from the camera. The mask introduced by <ref type="bibr" target="#b10">[11]</ref> has opaque regions blocking a significant amount of light from reaching the image sensor. Zhou et al. <ref type="bibr" target="#b11">[12]</ref> built upon the work of Levin et al. <ref type="bibr" target="#b10">[11]</ref> and introduced two complementary amplitude masks.</p><p>In <ref type="bibr" target="#b0">[1]</ref> the concept of depth from diffracted rotation was introduced: a superposition of a set of suitably chosen Gaussian-Laguerre modes generates a double helix RPSF that was used to estimate the depth of a planar scene by analyzing the blurring effects within the captured image. However, low MTF by the mask leads to poor SNR within the captured image, thus limiting the capability of signal-processing based algorithms to recover sharp images or accurate depth maps. In addition, earlier studies relied on a design paradigm based on the separate optimization of the camera optics and post-processing algorithms: they design the mask first and then tailor a reconstruction algorithm that fits the proposed physical design of the mask as in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. Such design methodology, however, leads to sub-optimal performance.</p><p>End-to-end learning for monocular depth estimation. Recently, an emerging trend appeared to tackle the separate design issue and new frameworks for joint optical and digital optimization using deep learning techniques have been introduced. These methods exploit end-to-end learning to optimize the mask's height map together with the trainable weights of a Convolutional Neural Network (CNN). Haim et al. <ref type="bibr" target="#b15">[16]</ref> proposed a differentiable phase mask consisting of concentric rings that introduce depth-dependent chromatic aberrations and en-coding depth cues within single captured images. In the work of Chang et al. <ref type="bibr" target="#b16">[17]</ref> and Wu et al. <ref type="bibr" target="#b17">[18]</ref> a free-form differentiable phase mask design parameterized using a set of superposed Zernike polynomials <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> was jointly optimized with the weights of a U-Net <ref type="bibr" target="#b20">[21]</ref>. However, the employed camera model was not realistic accounting only for additive Gaussian noise <ref type="bibr" target="#b17">[18]</ref>. Furthermore, unrestricted parameterized mask design and higher degrees of freedom may cause the optimization to converge towards local minimas as the objective function becomes too complex leading to sub-optimal performance. In this work, the mask is parameterized using only three design parameters two of which are optimized in an end-to-end fashion, and the PSF has clear and simple correlations with depth.</p><p>Image deblurring. Non-blind image deblurring has been extensively studied before (e.g., <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>), and a substantial performance gain has been made easier using deep learning based image deblurring techniques.</p><p>The authors of <ref type="bibr" target="#b25">[26]</ref> used the separability property of the pseudo-inverse kernel in Wiener deconvolution filter to design a dedicated CNN. In <ref type="bibr" target="#b26">[27]</ref>, a two-stage deblurring process was introduced using a Wiener deconvolution filter and a simple MLP architecture. A novel deconvolution approach was recently introduced by <ref type="bibr" target="#b27">[28]</ref> where a Wiener deconvolution filter is applied to the input data in feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Depth Estimation from Diffracted Rotation</head><p>We propose an end-to-end learning approach for depth from diffracted rotation using RPSF-coded images. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the full pipeline of the proposed solution encompasses three main stages. In the first stage, the height map of the phase mask is jointly learned with the weights of a neural network (DEPTH-DNN) trained to perform monocular depth estimation. This module is trained using noise-free RPSF-blurred synthetic images. In the second stage, the optimized phase mask is fitted within the optics module and a digital image formation pipeline is applied to the RPSF-coded synthetic images in order to simulate a realistic camera model. Finally, we used the demosaiced images from the previous stage as input to fine-tune the weights of DEPTH-DNN on noisy data obtaining a refined model (DEPTH-DNN-TUNED) and to recover the all-in-focus sharp image using a dedicated network (IMAGE-DNN) which performs non-blind and non-uniform image deblurring. Both of these modules make up the third and last stage of the proposed architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Engineered PSF</head><p>In this section we briefly describe the effect of modified optics on the system's PSF. For more details please refer to the supplementary materials.</p><p>For simplicity, consider an on-axis ideal point source situated at optical infinity, the light field U in with a constant amplitude P and a phase ? emanating from such source has the form: U in = P e i? . An optical element such as a lens or a phase mask with a refractive index n and a height profile h introduces a phase delay ? = 2?(n?1) ? h on incident light wave-fronts. If a phase mask is inserted at the entrance pupil plane of an imaging system, the total phase delay can be expressed as the sum of the delay due to the lens with the one due to the mask:</p><formula xml:id="formula_0">? optics = ? lens + ? mask<label>(1)</label></formula><p>The light field after the lens and mask system has the form:</p><formula xml:id="formula_1">U out = A ? P ? e i(?+?optics)<label>(2)</label></formula><p>Where A is the aperture mask simulating the finite aperture area. The field U out can be further propagated to the image plane and the PSF can be obtained by the field's intensity distribution. Choosing the appropriate height profile of the phase mask helps design specific PSF patterns depending on the target task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Phase Mask Design</head><p>Both annular and free-form mask designs have been studied in the context of joint optimization of camera optics with postprocessing algorithms <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b16">17]</ref>. We build upon the depthdependent RPSF introduced by <ref type="bibr" target="#b7">[8]</ref>. In this section, the mathematical model of the mask's phase profile is described and in the next section a differentiable approximation of the mask's height map is derived to allow for gradient back-propagation in our end-to-end learning framework.</p><p>We use a Fresnel-zone based design <ref type="bibr" target="#b7">[8]</ref> that has an outermost radius R and L phase plates in the form of concentric annular regions each of topological charge l = 1, ..., L and bounded by two radii R l?1 = R l?1 L and R l = R l L . The pupil plane phase can be written as in <ref type="bibr" target="#b7">[8]</ref>:</p><formula xml:id="formula_2">? mask (?, ?) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0 ? ? &lt; 1 L . . . l? l?1 L ? ? &lt; l L . . . L? L?1 L ? ? &lt; 1<label>(3)</label></formula><p>? mask is defined with the polar position vector ? normalized by the pupil's outermost radius R, and is a step function modeling the physical design property of concentric rings each with its own phase profile.</p><p>The phase profile in Eq. 3 can be further generalized to account for multi-order-helix RPSFs as in <ref type="bibr" target="#b9">[10]</ref> and <ref type="bibr" target="#b8">[9]</ref> by expanding it into:</p><formula xml:id="formula_3">? mask (?, ?) = ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? 0 ? ? &lt; ( 1 L ) . . . [(l ? 1)N + 1]? ( l?1 L ) ? ? &lt; ( l L ) . . . [(L ? 1)N + 1]? ( L?1 L ) ? ? &lt; 1<label>(4)</label></formula><p>Notice that the inner and outermost radii of each zone are now controlled by which lies in [0, 1], and the topological charge of each ring is now [(l ? 1)N + 1] instead of just l. Besides the number of rings L, N and are two new design parameters each having an effect over the resulting RPSF shape. More precisely, N defines the number of peaks or the main rotating lobes of the RPSF and L and both control the peak separation and confinement of each peak. In the case of a single helix rotating PSF (N = 1) the phase profile of each ring would be reduced to the original expression as in Eq. 3. Notice also that by increasing the number of peaks, the practical depth range would be reduced because of rotation ambiguities when peaks rotate beyond [? ? N , ? N ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Differentiable Phase Mask Design</head><p>The phase profile presented in Eq. 4 is not differentiable with respect to the design parameters N and . Thus, a differentiable approximation for this equation is necessary in order to simulate the camera's optical layer and enable both forward and backward propagation. The number of Fresnel zones L is considered as a hyper-parameter that can be manually tuned to achieve better depth estimation performance.</p><p>The steps in Eq. 4 can be approximated with a set of 2D rings in polar coordinates with increased radii as L increases as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, each Fresnel zone is obtained by subtracting the areas of two 2D tanh functions each with a radius corresponding to the one of the two radiis of the desired ring. We multiply the inner coordinates of each tanh by a large constant (100 in our case) in order to get sharp mask edges. The new approximated phase profile equation for L zones can be written as:?</p><formula xml:id="formula_4">mask (?, ?) = l=L l=0 (l ? 1)N + 1 ? l th ring phase ? 1 2 tanh[100(? ? r l )] ? tanh[100(? ? r l+1 )] l th ring mask<label>(5)</label></formula><p>Where</p><formula xml:id="formula_5">r 0 = 0, r l = R( l L ) ; ?l ? [1.</formula><p>.L], and R is the outermost radius of the mask. ? and ? are the polar coordinates.</p><p>Each phase profile ? l mask = (l ?1)N +1 ? is multiplied by the corresponding ring mask and the resulting zones are added up to produce the final phase profile of the mask. We produce the height map h of the mask as follows:</p><formula xml:id="formula_6">h(x, y) = ? 2?(n ? 1) ? arg{e i? mask } mod 2?<label>(6)</label></formula><p>where arg is the complex argument function, and the modulo accounts for the phase wrapping operation (phase values have a 2? periodicity as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>). Notice that the height profile of the phase mask is wavelength dependent: the resulting PSFs for the three RGB color primaries have different rotation rates which introduces chromatic aberrations. Still, it will not be problematic in the context of an end-to-end optimization framework since the network could learn the correlations between the rotation rate and the corresponding color channel. In fact, such aberrations can be seen as depth-dependent and can also relay valuable depth cues. For the physical design of the mask a reference wavelength value (? = 536.67nm) is used to produce the height map. <ref type="figure" target="#fig_3">Fig. 4</ref> shows a double helix RPSF generated by a phase mask with [N = 2, L = 5, = 0.9], it has two main lobes rotating counterclockwise as a function of defocus. Notice that even at the in-focus plane the RPSF has the same shape thus objects that are "in-focus" are also blurred. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training Data Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>A subset of FlyingThings3D dataset is used for the joint optimization of the phase mask and depth estimation network, and to train IMAGE-DNN. This dataset is a part of the Scene Flow synthetic datasets <ref type="bibr" target="#b28">[29]</ref>. This subset was previously used by <ref type="bibr" target="#b17">[18]</ref> in a similar joint optimization approach, it contains synthetic images of randomly placed objects with pixel-accurate disparity maps. The training, validation, and test sets contain respectively 5078, 555, and 420 images with a resolution of 278?278 pixels. Additionally, in order to evaluate the performance of our approach with the state-of-the-art in the task of monocular depth estimation on real data, NYUV2 <ref type="bibr" target="#b29">[30]</ref> depth dataset is used to train and evaluate DEPTH-DNN. Originally, the dataset contains 120k training images of indoor scenes with a resolution of 640 ? 480 pixels along with ground truth depth maps acquired by a Microsoft Kinect V1 depth sensor. The test set as defined by the split in <ref type="bibr" target="#b30">[31]</ref> contains 654 images. In this work, a subset of 50k samples of NYUV2 is used to train the depth estimation network as in <ref type="bibr" target="#b31">[32]</ref>. Finally, the test set of SUN-RGBD dataset <ref type="bibr" target="#b32">[33]</ref> is used to evaluate the generalization capability of DEPTH-DNN. This set contains 5050 test images of indoor scenes with ground truth depth maps acquired by four different depth sensors some of which use active illumination techniques and others incorporate passive stereo systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Formation Model</head><p>Light rays emanating from the scene are acquired by the camera and optically coded by the phase mask via a depthdependent blurring process with the camera's RPSF. For our simulations, the ground truth depth maps are approximated with a layered model in which only a finite number of depth planes are used to compose and render the RPSF-coded image using the following image formation model:</p><formula xml:id="formula_7">I sim = d=D d=1 (I aif * RP SF d ) M d<label>(7)</label></formula><p>Where I sim is the simulated blurred image, I aif is its all-infocus counterpart, RP SF d is the RPSF intensity distribution at the depth plane d, stands for element-wise multiplication, and {M d ; d = 1, ..., D} are the depth masks presenting the individual depth layers such that at each pixel location d M d = 1, i.e. only one pixel mask is set to one at each position.</p><p>Afterwards, the image formation pipeline is applied to the RPSF-blurred images to simulate real cameras. As illustrated by <ref type="figure" target="#fig_0">Fig. 1</ref>, a Bayer CFA receives the full color resolution RPSFblurred image and produces down-sampled RGGB color pattern. Although it is hard to accurately simulate the noise behaviour within the sensor chip, the final amount of noise is mainly caused by sensor shot and read noises. To this end, we simulated the read noise with an additive Gaussian N (0, ? 2 ) with zero mean and a fixed standard deviation ? = 0.01, photon shot noise follows a Poisson distribution, in practice it is modeled by a Gaussian distribution whose mean and variance depend on the expected photon count over the exposure time. The resulting noisy sensor image is quantized with an ADC module with a resolution of 8 bits. Finally, a linear interpolation-based demosaicing technique of <ref type="bibr" target="#b33">[34]</ref> is used to recover the full color channels from the CFA and produce the final output which will be used to fine-tune DEPTH-DNN and train IMAGE-DNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Deep Learning Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Monocular Depth Estimation</head><p>In the first stage of the proposed solution (see <ref type="figure" target="#fig_0">Fig. 1</ref>), the phase mask design parameters [N, ] are jointly learned with the weights of a U-Net <ref type="bibr" target="#b20">[21]</ref> which is trained on a subset of Fly-ingThings3D <ref type="bibr" target="#b28">[29]</ref>. Two different learning rates, L mask r = 0.1 and L dnn r = 1e ? 4 are set for the phase mask and for the depth estimation neural network. During the training process, the gradient error is back-propagated through the network as well as the mask's trainable parameters and the weights are updated accordingly using TensorFlow's automatic differentiation framework <ref type="bibr" target="#b34">[35]</ref>. The network of this stage is trained for 150k iterations using a batch size of 20. Adam optimizer <ref type="bibr" target="#b35">[36]</ref> is used with exponential decay rates of the first momentum and second momentum respectively set to ? 1 = 0.99 and ? 2 = 0.999.</p><p>Similar to Wu et al. <ref type="bibr" target="#b17">[18]</ref>, we used a combination of Root Mean Square Error (RMSE) loss L rmse and gradient loss L grad which forces the network to estimate accurate depth maps with well defined object boundaries at different depth planes. The total loss function is defined as:</p><formula xml:id="formula_8">L depth = L rmse + L grad (8)</formula><p>Where L rmse and L grad are defined as:</p><formula xml:id="formula_9">L rmse (?, ? * ) = 1 |T | ??T (? ? ? * ) 2 (9) L grad (?, ? * ) = L rmse ( ?? ?x , ?? * ?x ) + L rmse ( ?? ?y , ?? * ?y )<label>(10)</label></formula><p>? and ? * are respectively the predicted and the ground truth disparity maps and the subtraction is done pixel-by-pixel, x and y are the spatial dimensions, and |T | is the number of disparity maps.</p><p>In the third stage, the same U-Net <ref type="bibr" target="#b20">[21]</ref> which was previously trained on noise-free RPSF data is fine-tuned using 50k training iterations with noisy images simulated by the camera model of the second stage (see <ref type="figure" target="#fig_0">Fig. 1</ref>), the phase mask is fixed during this training pass and only the network's weights are updated. All hyper-parameters' values are fixed to the same values as in the first training stage.</p><p>For NYUV2 dataset <ref type="bibr" target="#b29">[30]</ref>, both the network and the mask are learned using 50k training samples, the input images to the network and the output depth maps have a resolution of 320 ? 240 which correspond to half of that of the original samples, a bilinear-upsampling is applied to the predicted depth maps to recover the original resolution of 640 ? 480 for evaluation purposes as in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref>. The network is trained for 150k iterations with a batch size of 20, the phase mask and the neural network learning rates as well as the optimizer used in the training are the same as the ones used for the subset of Fly-ingThings3D <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Image Deblurring</head><p>We used a modified version of the deblurring model proposed by <ref type="bibr" target="#b27">[28]</ref> wherein a Wiener deconvolution is applied to a set of feature maps extracted from the blurred input using a simple CNN architecture as feature extractor, the deconvolved feature maps are then fed into a multi-scale feature refinement stage in order to get the final deblurred image in a coarse-to-fine based reconstruction technique. Such approach proved capable of restoring very fine structural details allowing for accurate image reconstruction. In this work, we adapted the network proposed by <ref type="bibr" target="#b27">[28]</ref> for the case of non-uniform image deblurring as the blur kernel in our case is spatially variant. More precisely, a separate deconvolution is performed for every depth layer and the results are cropped using the corresponding depth masks M d as in Eq. 7. The deconvolved feature maps are then combined in order to get the final Wiener filter output. We found that, even though the Wiener filter module is applied in feature space, some undesirable deconvolution artifacts, most noticeably ringings, are visible especially around image boundaries and object edges. Since the F F T operator within the Wiener deconvolution filter supposes circular periodicity of the input.</p><p>To tackle this problem, an edgetaper operation <ref type="bibr" target="#b38">[39]</ref> was implemented on the blurred input image to smooth out its boundaries which can considerably reduce the ringing artifacts in the final reconstructed sharp image.</p><p>We trained the network using the subset of FlyingTh-ings3D <ref type="bibr" target="#b28">[29]</ref> for 500 epochs with Adam optimizer <ref type="bibr" target="#b35">[36]</ref> with exponential decay rates of the first momentum and second momentum respectively set to ? 1 = 0.99 and ? 2 = 0.999, and a learning rate L image r = 1e?4 which is halved after 250 epochs. The number of auto-encoders in the multi-scale feature refinement modules is set to 2 as in the original work of <ref type="bibr" target="#b27">[28]</ref>, the number of extracted feature maps from the blurry input is 16, and the batch size is set to 8. For the loss function, it was experimentally seen that L1 norm leads to better reconstruction results than the ones obtained with L2 norm.</p><formula xml:id="formula_10">L image (?, ? * ) = 1 |T | ??T |? ? ? * |<label>(11)</label></formula><p>Where ? and ? * are respectively the reconstructed and the ground truth sharp images and |T | is the number of images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Experimental Results on Synthetic Data</head><p>Monocular depth estimation on FlyingThings3D subset. For this set of experiments the phase mask's trainable parameters are initialized to [N = 1, = 0.1] and the number of Fresnel zones L is set to 7 as it was empirically observed that such value leads to a lower depth estimation error. In the case of noise-free RPSF-blurred inputs, the network achieved a RMSE of 0.392 on the test set. The corresponding learned phase mask parameters are [N = 1, = 0.92]: the generated RPSF as well as the height map of the mask are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. The network learned a single-helix RPSF with high confinement parameter meaning that the peak is spread out across a large area. Qualitative results are shown in <ref type="figure" target="#fig_5">Fig. 6</ref> (additional scenes are shown in the supplementary material): Notice how the network is able to predict accurate disparity maps with fine image details, e.g., the small leaves of the plant shown in the third row or the very fine parts of the headset shown in the last row. However, it becomes harder for the network to predict accurate object boundaries due to the blurring artifacts introduced by the RPSF especially when the blur kernel is larger than the image features. Some artifacts appear at object edges mainly due to the nature of the image formation model used to compose the RPSF-blurred images: the layered depth model used to render RPSF coded images does not accurately simulate the discontinuities around object edges as visible in the RPSF coded images <ref type="figure" target="#fig_5">(Fig. 6</ref>) due to the lack of accurate occlusion modeling. Such issue can be addressed by implementing a more advanced and sophisticated blending and matting approaches, e.g. Pyramidbased blending <ref type="bibr" target="#b39">[40]</ref>, at the expense of much higher computation time and complexity for a minor gain in performance.</p><p>As pointed out in Section 4.2, we also introduced an accurate noise simulation model for a more realistic evaluation. To handle noise we further fine-tune the DEPTH-DNN on noisy images. In this case we achieve a RMSE of 0.712 on the test set compared to 0.392 achieved on noise-free data. This is of course expected but at the same time the robustness of the system in real world applications should be enhanced.</p><p>In the first two columns of <ref type="figure" target="#fig_5">Fig. 6</ref>, one can observe image quality degradation after simulating the noisy images where the color down-sampling by the CFA and quantization artifacts by the ADC unit are visible (zoomed-in). The predicted disparity maps from noisy inputs are shown in the last column. Even though a small performance degradation is noticeable on the noisy predictions, the fine-tuned network is still able to learn fairly accurate disparity maps.</p><p>Image restoration. The sharp all-in-focus images are recovered by IMAGE-DNN trained on the subset of FlyingThings3D <ref type="bibr" target="#b28">[29]</ref>. Quantitative and qualitative results are shown in <ref type="table">Table.</ref> 1 and <ref type="figure" target="#fig_6">Fig. 7</ref>, respectively.</p><p>The simulated RPSF-coded images have a low mean PSNR of roughly 19 dB with respect to their sharp noise-free counterparts which were used as the ground truth images during training. As reported in <ref type="table">Table.</ref> 1, the mean PSNR of the recovered images increased by about 5.5 dB reaching 24.46 dB. Also, the Structural Similarity Index Measure (SSIM) achieved is 0.760 compared to 0.611 for the blurred and noisy images. Deblurring results from the traditional Wiener deconvolution filter <ref type="bibr" target="#b40">[41]</ref> are also shown in <ref type="figure" target="#fig_6">Fig. 7</ref>: even if we apply the deconvolution process independently for each depth plane and generated the final result following Eq. 7, it results in a low quality image reconstruction with heavy ringing artifacts (see <ref type="figure" target="#fig_6">Fig. 7</ref>). This happens since the Wiener filter fails to handle the spatially variant blur producing significant ringing artifacts.  <ref type="figure" target="#fig_6">Fig.7</ref> shows some recovered images along with the corresponding blurred inputs and the sharp all-in-focus ground truth. Upon visual inspection, one can notice that the model successfully restores very fine image details and high frequency components, e.g., the small tree leaves shown in the first row, or the various background details present in the second row. Notice also how large regions with smooth as well as textured structures are recovered. The quantization noise and color downsampling by the CFA make the task even more challenging resulting in some ringing artifacts on object edges. Although an edgetaper <ref type="bibr" target="#b38">[39]</ref> technique was used to limit such artifacts, few are still present in the recovered images, but are significantly reduced when comparing with the ones produced by the Wiener filter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PSNR (dB) SSIM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Experimental Results on Real Data</head><p>NYUV2 depth dataset. In order to compare our approach with state-of-the-art methods, the DEPTH-DNN is trained on a subset of NYUV2 indoor dataset <ref type="bibr" target="#b29">[30]</ref> in an end-to-end fashion with the phase mask's height map. In this experiment, only 10 depth planes are considered in the layered depth presentation (Eq. 7) due to memory constraints. The simulated camera lens parameters are the following: f /4.0 with 4 mm aperture diameter and 16 mm focal length focusing at a distance of 5 meters. The RGB images are directly convolved with a RPSF cube of the shape 10 ? 23 ? 23 ? 3. In the following evaluations, we applied the same crop used in competing works (e.g. <ref type="bibr" target="#b30">[31]</ref>) and excluded the invalid pixels from the Kinect V1 sensor as done by all competing approaches.   <ref type="figure" target="#fig_7">Fig. 8</ref> shows the learned RPSF and the corresponding phase mask. For this dataset, the learned RPSF is a double-helix [N = 2, = 0.99] with two main side lobes that rotate with defocus. We argue that such behaviour is mainly related to the characteristics of the training data: for more complex depth scenes, like in this case, the network tends to converge to higher number of peaks as it makes it easier to correlate between the rotation angle and the corresponding depth plane. Similar to the previous scenario, the network also converges towards a high confinement parameter = 0.99 producing more spread out peak regions.</p><p>Quantitative results on NYUV2 <ref type="bibr" target="#b30">[31]</ref> test set are reported in <ref type="table">Table.</ref>2. Coded-aperture based competing approaches <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> optimized a free-form phase mask parameterized with a superposition of a set of Zernike polynomials <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> using a U-Net <ref type="bibr" target="#b20">[21]</ref> architecture. Besides using a more accurate camera model, differently from <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>, our approach learns only a few design parameters for the mask and simultaneously tack-les the problem of image quality degradation. <ref type="table">Table.</ref> 2 shows quantitative results for the different error metrics used in <ref type="bibr" target="#b29">[30]</ref>. Our approach outperforms the competing methods of <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> in all but the last two accuracy metrics (where all top approaches including ours are very close to 1 making them not too significant). Note that for the more significant accuracy metric ? 1 (i.e., with the lowest threshold value), our approach achieves the highest score, even if we trained the network using a subset of 50k training samples which is less than half of the default split of 120k training samples <ref type="bibr" target="#b29">[30]</ref> used by <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref>. In particular, we achieve a significantly lower RMSE value of 0.267 which is down by respectively 0.166 and 0.115 from the ones achieved by <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b17">[18]</ref> and is the lowest yet achieved on this challenging dataset for the task of monocular depth estimation. This performance gain is primarily related to the optimized PSF shape, The one obtained by <ref type="bibr" target="#b17">[18]</ref> has a generic shape with no clear correlations between different defocus planes, while the one obtained by <ref type="bibr" target="#b16">[17]</ref> has an elliptical shape with varying section which increases with depth. On the other hand, the RPSF shape produces a clear and simple correlation between the defocus plane and the corresponding angle of rotation of the main peaks, thus encoding robust depth cues within input images. This, thanks also to the small number of parameters to be learned, explain why the network achieves better depth estimation accuracy with significantly less training data. <ref type="table">Table.</ref> 2 also presents quantitative results from the state-ofthe-art that mainly used sharp all-in-focus images as input. Our approach outperforms all competing methods in all evaluation metrics except for ? 3 accuracy metric where all the top approaches including ours are extremely close. More specifically, we achieve substantially lower error metrics compared <ref type="figure">Figure 9</ref>: Qualitative results from the proposed approach as well as those from AdaBins <ref type="bibr" target="#b31">[32]</ref> on the test set of NYUV2 <ref type="bibr" target="#b30">[31]</ref>.   <ref type="bibr" target="#b47">[48]</ref>, AdaBins <ref type="bibr" target="#b31">[32]</ref> and DPT-Hybrid <ref type="bibr" target="#b48">[49]</ref>, notice that the latter used a pre-trained model on a large combination of different datasets containing 1.4M samples and fine-tuned it on NYUV2 dataset <ref type="bibr" target="#b30">[31]</ref>. The rest of the competing approaches used less training samples but at the cost of more complex models with pre-trained weights, e.g. Hao et al. <ref type="bibr" target="#b37">[38]</ref> used a ResNet <ref type="bibr" target="#b49">[50]</ref> backbone pre-trained on Ima-geNet <ref type="bibr" target="#b50">[51]</ref>. Our model uses a smaller training set but the core network used for depth estimation is a simple U-Net <ref type="bibr" target="#b20">[21]</ref> architecture with approximately 8.6M trainable parameters. In contrast, AdaBins <ref type="bibr" target="#b31">[32]</ref> for example, like most other competing methods used a more complex network architecture with approximately 78M trainable parameters, making it slower in both training and inference. In our case, the RPSF encodes strong and robust depth cues making it easier for a simple network to predict accurate depth maps compared to those using conventional RGB inputs.</p><formula xml:id="formula_11">Method RMSE ? Rel ? Log10 ? ? 1 ? ? 2 ? ? 3 ?</formula><p>A qualitative comparison with AdaBins <ref type="bibr" target="#b31">[32]</ref> (trained with the same 50k samples as in this approach) is shown in <ref type="figure">Fig. 9</ref> while further qualitative results are shown in the supplementary materials. Upon visual inspection, one can easily see that our approach produces more accurate and realistic depth maps with respect to the ground truth (the ground truth depth maps are inpainted for visualization purposes, in the supplementary material the ground truth data used for evaluation are shown). Due to the scarcity of reliable depth cues in single all-in-focus input images, Adabins <ref type="bibr" target="#b31">[32]</ref> struggles to predict accurate and sometimes realistic depth values in a consistent manner and fails to predict correct values for images where depth values span large ranges, e.g. the results shown in the two last columns in <ref type="figure">Fig. 9</ref>. Moreover, sometimes Adabins <ref type="bibr" target="#b31">[32]</ref> produces erroneous depth predictions where the scene semantics are somehow confusing and the network fails to infer realistic values: such behaviour exposes the main limitation of semantic-based approaches, as visible in the last column of <ref type="figure">Fig. 9</ref> where the green carpet was misclassified. In contrast, our network consistently produces accurate depth maps for small and large depth ranges alike and is more agnostic to the scene's semantics. The main drawback of our method is that it sometimes fails to learn well defined object boundaries due to the blurring artifacts introduced by the RPSF kernel.</p><p>SUNRGBD dataset. We evaluate the generalization capability of our model for the task of monocular depth estimation where DEPTH-DNN that was previously trained on the subset of NYUV2 <ref type="bibr" target="#b29">[30]</ref> is evaluated using the test set of SUN-RGBD <ref type="bibr" target="#b32">[33]</ref> without any further fine-tuning. Quantitative and qualitative results are present in <ref type="table">Table.</ref> 3 and <ref type="figure" target="#fig_0">Fig. 10</ref> (additional qualitative results are in the supplementary material). Metric values for competitors shown in <ref type="table">Table.</ref> 3 are taken from <ref type="bibr" target="#b31">[32]</ref> where methods with publicly available pre-trained models on NYUV2 <ref type="bibr" target="#b29">[30]</ref> have been evaluated on the SUNRGBD dataset.</p><formula xml:id="formula_12">Method RMSE ? Rel ? Log10 ? ? 1 ? ? 2 ? ? 3 ?</formula><p>Chen et al. <ref type="bibr" target="#b36">[37]</ref> 0.494 0.166 0.071 0.757 0.943 0.984 Yin et al. <ref type="bibr" target="#b44">[45]</ref> 0.541 0.183 0.082 0.696 0.912 0.973 BTS <ref type="bibr" target="#b45">[46]</ref> 0.515 0.172 0.075 0.740 0.933 0.980 AdaBins <ref type="bibr" target="#b31">[32]</ref> 0.476 0.159 0.068 0.771 0.944 0.983 Ours 0.335 0.114 0.034 0.937 0.981 0.992 <ref type="table">Table 3</ref>: Quantitative comparison with the state-of-the-art methods for monocular depth estimation task on SUNRGBD <ref type="bibr" target="#b32">[33]</ref> test set.</p><p>As shown in <ref type="table">Table.</ref> 3, our approach outperforms the state-ofthe-art in all evaluation metrics with a significant reduction in error metrics, particularly the RMSE (where it achieved 0.335 compared to 0.476 of the best competitor) and Log10 (0.034 against 0.068). Notice also the accuracy metric ? 1 corresponding to the smallest threshold of 1.25 achieved by our approach which is up by 0.166 (? 1 = 0.937) compared to the best performing approach where the accuracy value achieved is 0.771, which indicates that ours produces a higher percentage of accurately predicted depth pixel values. Such results support the suitability of such engineered PSFs for depth acquisition applications enabling reliable and robust passive monocular depth estimation performance with real-time capabilities. <ref type="figure" target="#fig_0">Fig. 10</ref> shows some prediction samples from SUN-RGBD <ref type="bibr" target="#b32">[33]</ref> test set, as in the previous case, the network is able to predict overall accurate depth maps but with higher mean RMSE compared to the test set of NYUV2 test set <ref type="bibr" target="#b30">[31]</ref> which is expected due to the different statistical properties between the two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Ablation Study</head><p>A number of experiments were carried out as an ablation study to assess the contribution of the various components in our framework. Quantitative results are shown in <ref type="table">Table.</ref> 4, while the qualitative ones are in the Supplementary Material. In all simulations of the ablation study, the network architecture as well as the training settings and hyper-parameters are the same as indicated in the previous section except for the usage of a simple Gaussian additive noise model instead of the full image formation procedure. Furthermore, we set the number of Fresnel zones in the mask design to L = 5.  The baseline is a U-Net trained on all-in-focus sharp images from the subset of FlyingThings3D <ref type="bibr" target="#b28">[29]</ref>. The RMSE achieved in this first experiment is 2.649. In the second experiment, a coded aperture with a fixed phase mask design [N = 1, L = 5, = 0.5] is used to blur the sharp input images with a depth dependent single-helix RPSF. In fact, this particular mask design is the one which was first introduced by <ref type="bibr" target="#b7">[8]</ref>. The network trained with the fixed mask achieves a RMSE value of 1.117, i.e. down by 1.532 with respect to the baseline.</p><p>In the third experiment, the number of RPSF lobes is increased to N = 2. Both the number of Fresnel zones L and the confinement parameter are the same as in the second experiment. The network trained with such phase mask achieves even better RMSE value of 0.815 compared to the 1.117 achieved by the one in the previous experiment (see <ref type="table">Table.</ref> 4). Such performance gain could be due to the more discriminative shape of the double-helix RPSF compared to the single-helix RPSF generated by the previous mask as the rotation can be easily noticed from a depth plane to the other. As shown in <ref type="figure" target="#fig_0">Fig. 11</ref>, the RPSF generated by the second mask has two main side peaks rotating counterclockwise as a function of defocus, the same rotation aspect can be observed in the RPSF shape generated by the first fixed mask <ref type="figure" target="#fig_0">(Fig. 11a</ref>) except that in this case only one main side lobe is present. It suggest that, for a fixed phase mask, a double-helix RPSF conveys more discriminative depth cues than a single-helix one.</p><p>In the fourth and last experiment, the phase mask's trainable parameters N and are jointly optimized with the weights of the network, the number of Fresnel-zones is fixed to L = 5. As expected, the network was able to outperform the baseline as well as the ones trained with fixed masks, reaching a RMSE of 0.699. The learned phase mask parameters are [N = 1, = 0.91] meaning that the RPSF (shown in <ref type="figure" target="#fig_0">Fig. 11c</ref>) has a single side lobe that rotate with defocus. Notice that also the confinement parameter is high resulting in a more spread out lobe compared to the one generated by the fixed mask. It is therefore clear that a joint optimization approach helps the network to effectively learn the correlations between the rotation angle of the PSF and the corresponding depth plane leading to better estimation accuracy.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>In this paper we presented a novel computational camera model where an end-to-end learning framework is proposed for the joint optimization of camera optics and image processing algorithms for the tasks of monocular depth estimation from diffracted rotation. The learned phase mask generates multiorder helix rotating PSFs as a function of defocus, encoding strong depth cues within single 2D images and enabling reliable and accurate depth estimation. Experimental results confirmed the capability of the proposed model to outperform existing methods in the task of monocular depth estimation and to generalize well beyond the training environment. The depth estimation model complexity is significantly reduced compared to the state-of-the-art due to the 3D cues encoded by the RPSF, making it suitable for real-time applications without compromising accuracy. Finally, the sharp all-in-focus images are also recovered through a dedicated non-blind and non-uniform image deblurring module.</p><p>Further research will focus on the fabrication of the phase mask via photo-lithography which will be mounted on the back side of the camera's aperture, thus adding depth estimation capabilities to a standard RGB cameras. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>In this supplementary material document, we start by analyzing the focusing error in Out Of Focus (OOF) imaging in Section 1. Then in Section 2 we briefly present the wave-optics based analysis of light field propagation and the phase transformation introduced by the lens and the phase mask. We also show the effects of the phase mask design parameters on the shape of the resulting RPSF in Section 3.</p><p>Then, we present some additional qualitative results for the tasks of monocular depth estimation and image deblurring (Sections 4 and 5) that were not possible to fit in the main paper due to space limitations. Finally, a visual example of the results of the various tests made in the ablation study is also presented in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Out of Focus Imaging</head><p>Consider a simple imaging system consisting of a thin lens with focal length f . The light reflected from an object at distance z o in front of the lens is focused into an image plane at a distance z i behind it, i.e., all rays coming from a point-like object are focused into a point in the image plane that satisfy the well-known thin lens equation ( 1 zo + 1 zi = 1 f ). Away from the in-focus plane, where the thin lens equation is no longer satisfied, a quadratic phase error at the pupil plane is introduced to model OOF objects. Such error is measured by the defocus value <ref type="bibr" target="#b51">[52]</ref>:</p><formula xml:id="formula_13">? = ?R 2 ? ( 1 z o + 1 z i ? 1 f )<label>(12)</label></formula><p>Where R is the pupil radius and ? is the wavelength of incident light waves. ? indicates the severity of the focusing error and it increases in absolute value as objects move away from the in-focus plane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Field Propagation and the Point Spread Function</head><p>In this section, we discuss how a Fresnel diffraction model can be used to simulate near-field wave propagation. Consider a thin lens with a thickness profile h situated at the pupil plane of an imaging system (as depicted in <ref type="figure" target="#fig_0">Fig. 12</ref>), notice that h is a function of the spatial coordinates (x, y), and let h 0 be the thickest section of the lens. Generally the lens is made of glass * * Corresponding author Email address: mazen.mel@phd.unipd.it <ref type="table">(Mazen Mel)</ref> with a refractive index n = 1.5. For a convex thin lens with focal length f , h(x, y) is defined as: <ref type="figure" target="#fig_0">Figure 12</ref>: Light emitted by an object propagates through free space and its phase is delayed when it passes through the camera optics.</p><formula xml:id="formula_14">h(x, y) = h 0 ? x 2 + y 2 2f (n ? 1)<label>(13)</label></formula><p>Light wave fields passing through the lens are delayed by an amount proportional to the thickness of the lens h at each point. Thus, a thin lens applies a phase shift to the incident wavefront given by:</p><formula xml:id="formula_15">? lens (x, y) = 2?(n ? 1) ? h(x, y)<label>(14)</label></formula><p>Notice that the phase shift is wavelength-dependent hence using the same lens with polychromatic light results in chromatic aberrations.</p><p>By defining the phase shift at the pupil plane, the generalized pupil function, which is a complex function, can be written as:</p><formula xml:id="formula_16">P (x, y) = A(x, y)e i? lens (x,y)<label>(15)</label></formula><p>Where A(x, y) is the circular aperture mask simulating a finite aperture area.</p><p>In the case of OOF imaging, the pupil function would also introduce a phase error expressed as a quadratic phase term in the generalized pupil function showing the effects of defocus aberrations in the captured image. The new generalized pupil function would have the following form:</p><formula xml:id="formula_17">P (x, y) = A(x, y)e i(? lens (x,y)+? (x 2 +y 2 ) R 2 )<label>(16)</label></formula><p>Where R is the pupil radius and ? is the defocus parameter. In the case of a wave field generated by an ideal point source object, the PSF of the imaging system is expressed as:</p><formula xml:id="formula_18">P SF (x, y) ? |F (P (x, y))| 2<label>(17)</label></formula><p>Where F denotes the Fourier Transform.</p><p>Concerning the wave field, let the complex wave field U in propagating in free space just before the entrance pupil be:</p><formula xml:id="formula_19">U in (x, y) = C(x, y)e i?in(x,y)<label>(18)</label></formula><p>Where C(x, y) is the field's amplitude and ? in is its phase just before entering the pupil.</p><p>In the presence of a phase mask with a phase delay ? mask (as shown in <ref type="figure" target="#fig_0">Fig. 12</ref>), the phase delay introduced by the combination of both the lens and mask is expressed as:</p><formula xml:id="formula_20">? optics (x, y) = ? lens (x, y) + ? mask (x, y)<label>(19)</label></formula><p>Thus, the expression of the wave field just after the lens system U out in this case is: Using Fresnel propagation, U out can be further propagated of a distance z i until it reaches the image sensor (we use a coordinate system (u, v) for the sensor).</p><formula xml:id="formula_21">U sensor (u, v) = e ikzi i?z i +? ?? +? ?? U out (x, y) ? e ik 2z i [(u?x) 2 +(v?y) 2 ] dx dy<label>(21)</label></formula><p>Where k = 2? ? is the wave number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Visual Evaluation of the Impact of RPSF Design Parameters</head><p>In this section we show some visual examples of RPSF shapes obtained by using different values of the design parameters.</p><p>Recall that we denoted with N the number of rotating peaks within the RPSF: in the first row of <ref type="figure" target="#fig_0">Fig. 13</ref> we show the resulting RPSF shape for different values of N .</p><p>The design parameter controls the peak separation as well as the confinement of each peak as illustrated in the second row of <ref type="figure" target="#fig_0">Fig. 13</ref>. It was empirically hypothesized by <ref type="bibr" target="#b8">[9]</ref> that the depth range increases as decreases.</p><p>The number of Fresnel zones L within the phase mask also controls the peak separation as illustrated in the last row of <ref type="figure" target="#fig_0">Fig. 13</ref>. Additionally, the rate of rotation which is 1 L in the basic case of a single helix RPSF, can be easily extended to 1 N L in the more general case of multiple peaks. This indicates that the rate of rotation would decrease with higher L and/or N values leading to an increase in the practical depth range with no peak rotation ambiguity.</p><p>In summary, [N, L, ] can be jointly optimized depending on the target task: they all influence the shape of the RPSF and the practical depth range in which the PSF can be used to encode unambiguous depth information within the captured 2D images.   <ref type="bibr" target="#b28">[29]</ref>: the network achieves overall satisfactory reconstruction with minor artifacts (a bit of ringing and a few texture patterns not properly recovered are visible upon closer inspection). Some challenging complex structures are restored (e.g. the sample shown in the third row) while other fine details, such as the periodic pattern of the "floor" shown in the sample of the last row, proved difficult to be accurately recovered. <ref type="figure" target="#fig_0">Fig. 15</ref> shows some additional examples of the proposed depth estimation algorithm on sample images from the Fly-ingThings3D test set <ref type="bibr" target="#b28">[29]</ref>. These results correspond to the sim- ulated noisy input images where sensor noise, CFA, and quantization noise are added. The network was able to produce accurate disparity maps even for objects with very fine details (e.g. the motorcycles in the second and third columns in addition to the plant in the last column). On the other side, edge pixels are not always accurately estimated (e.g., see the fourth column) as discussed in Section VI in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Additional Visual Results for Image Deblurring</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Additional Visual Results for Monocular Depth Estimation</head><p>Then, <ref type="figure" target="#fig_0">Fig. 16</ref> shows some additional examples of the proposed depth estimation algorithm on sample images from the NYUV2 test set <ref type="bibr" target="#b30">[31]</ref> and compares them with those from Ad-aBins <ref type="bibr" target="#b31">[32]</ref> competing approach. Our approach proved capable of producing superior results for small and large depths ranges alike with no effects or ambiguities whatsoever originating from the scene's semantics.</p><p>Finally, <ref type="figure" target="#fig_0">Fig. 17</ref> shows some additional examples of the proposed depth estimation algorithm on sample images from the SUNRGBD test set <ref type="bibr" target="#b32">[33]</ref>. Even though the network was not fine-tuned for this specific dataset, it performed very well on this larger and challenging dataset. <ref type="figure" target="#fig_0">Study   Fig. 18</ref> shows a sample of the predicted depth maps for the four conducted ablation experiments discussed in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Qualitative Results for the Ablation</head><p>Quantitatively, the network trained using all-in-focus images as input fails to discriminate between positive and negative defocus values, i.e., it is very difficult for the network to accurately predict whether objects are close or far from the camera because of the lack of reliable depth cues within the clear aperture PSF. While the rest of the performed experiments significantly improve the depth estimation accuracy, the learned mask proved capable not only to predict accurate depth maps but also to preserve fine details around object edges at different depth planes as shown, for example, in <ref type="figure" target="#fig_0">Fig. 18</ref> around the plant edges. <ref type="figure" target="#fig_0">Figure 15</ref>: Additional qualitative results on RPSF-blurred images from the test set of FlyingThings3D <ref type="bibr" target="#b28">[29]</ref> subset. <ref type="figure" target="#fig_0">Figure 16</ref>: Additional qualitative results from the proposed approach as well as those from AdaBins <ref type="bibr" target="#b31">[32]</ref> on the test set of NYUV2 <ref type="bibr" target="#b30">[31]</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The full architecture of our end-to-end learning framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>A differentiable phase mask design where each Fresnel zone is simulated by a ring mask multiplied by the phase profile (? i mask ) corresponding to each zone.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The height map of the phase mask. From left to right, the phase distribution as the argument of e i? , the [0, 2?] wrapped phase distribution, and the obtained height map for [N = 2, L = 5, = 0.9].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>A double helix RPSF obtained by a mask with [N = 2, L = 5, = 0.9]. The RPSF's intensity distributions are shown as a function of defocus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Learned RPSF shape at different defocus planes (left), height map of the phase mask (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Qualitative results on RPSF-blurred images from the test set of FlyingThings3D<ref type="bibr" target="#b28">[29]</ref> subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results of the image deblurring model on the test set of FlyingThings3D<ref type="bibr" target="#b28">[29]</ref> subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Learned RPSF shape at different defocus planes (left), the height map of the phase mask (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Qualitative results from our proposed method on the test set SUNRGBD<ref type="bibr" target="#b32">[33]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>The generated RPSFs for the ablation experiments: (a) Single-helix RPSF generated by the fixed mask 1 and its height map. (b) Double-helix RPSF generated by the fixed mask 2 and its height map. (c) Single-helix RPSF generated by the learned mask and its height map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>U</head><label></label><figDesc>out (x, y) = P (x, y)U in (x, y) = A(x, y)C(x, y)e i(?in(x,y)+?optics(x,y))<ref type="bibr" target="#b19">(20)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :</head><label>13</label><figDesc>The effects of [N, L, ] design parameters on the shape of the RPSF: (first row) only the value of N is changed, i.e., [N = 1, 2, 3, 4, 5; L = 5; = 0.9], (second row) only the value of is changed, i.e., [N = 2; L = 5; = 0.1, 0.3, 0.5, 0.8, 1], (third row) only the value of L is changed with [N = 2; L = 2, 3, 5, 7, 10; = 0.9].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14</head><label>14</label><figDesc>shows some additional examples of the proposed image deblurring algorithm on the FlyingThings3D subset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 14 :</head><label>14</label><figDesc>Additional qualitative results of the image deblurring model on the test set of FlyingThings3D [29] subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 17 :</head><label>17</label><figDesc>Additional qualitative results from our proposed method on the test set of SUNRGBD [33] Figure 18: A sample from the qualitative results for the four conducted ablation experiments on the test set of the subset of FlyingThings3D [29].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative comparison with the state-of-the-art for monocular depth estimation task on NYUV2 test set<ref type="bibr" target="#b30">[31]</ref>.to the competing approaches which used similar or larger training sets, e.g. Eigen et al.<ref type="bibr" target="#b30">[31]</ref>, Laina et al.<ref type="bibr" target="#b41">[42]</ref>, DORN<ref type="bibr" target="#b42">[43]</ref>, DAV<ref type="bibr" target="#b46">[47]</ref>, Alhashim et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Quantitative results of the ablation experiments on the test set of Fly-ingThings3D<ref type="bibr" target="#b28">[29]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>End-to-end Learning for Joint Depth and Image Reconstruction from Diffracted RotationSupplementary Material Sony Europe B.V., Zweigniederlassung Deutschland Stuttgart Technology Center, Stuttgart, Germany.</figDesc><table><row><cell>Mazen Mel c , Muhammad Siddiqui d , Pietro Zanuttigh c</cell></row><row><cell>c Department of Information Engineering, University of Padova, Italy.</cell></row></table><note>d</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Depth from diffracted rotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Greengard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piestun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics letters</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="181" to="183" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Orbital angular momentum of light and the transformation of laguerre-gaussian laser modes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Beijersbergen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Spreeuw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woerdman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review A</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">8185</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Propagation-invariant wave fields with finite energy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piestun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Schechner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="294" to="303" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">High-efficiency rotating point spread functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R P</forename><surname>Pavani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piestun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="3484" to="3489" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimal point spread function design for 3d imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Sahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Backer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Moerner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">133902</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generation of phase singularity through diffracting a plane or gaussian beam by a spiral phase plate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Kotlyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Almazov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Khonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Soifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elfstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="849" to="861" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diffraction of conic and gaussian beams by a spiral phase plate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Kotlyar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Kovalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Khonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Skidanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Soifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Elfstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tossavainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied optics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2656" to="2665" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rotating point spread function via pupil-phase engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics letters</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="585" to="587" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">High-order-helix point spread functions for monocular three-dimensional imaging with superior aberration robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stallinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4873" to="4891" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Three-Dimensional Imaging using a Novel Rotating Point Spread Function Imager</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>The University of New Mexico</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">70</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coded aperture pairs for depth from defocus and defocus deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="53" to="72" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Psf rotation with changing defocus and applications to 3d imaging for space situational awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 AMOS Technical Conference</title>
		<meeting>the 2013 AMOS Technical Conference<address><addrLine>Maui, HI</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Axial superlocalisation using rotating point spread functions shaped by polarisationdependent phase modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jesacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ritsch-Marte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4029" to="4037" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single shot three-dimensional imaging using an engineered point spread function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berlich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Br?uer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stallinga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optics express</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="5946" to="5960" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Depth estimation from a single image using deep learned phase coded mask</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Elmalem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Imaging</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="298" to="310" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep optics for monocular depth estimation and 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10193" to="10202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Veeraraghavan, Phasecam3d-learning phase masks for passive single view depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boominathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Computational Photography (ICCP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal modeling of corneal surfaces with zernike polynomials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Iskander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on biomedical engineering</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="87" to="95" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Principles of optics: electromagnetic theory of propagation, interference and diffraction of light</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Born</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Elsevier</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modeling realistic degradations in non-blind deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delbracio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="978" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast motion deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="page" from="1" to="8" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep gradient descent optimization for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5468" to="5482" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fast image deconvolution using hyper-laplacian priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="1033" to="1041" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep convolutional neural network for image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1790" to="1798" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A machine learning approach for non-blind image deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Christopher</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1067" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.09962</idno>
		<title level="m">Deep wiener deconvolution: Wiener meets deep learning for image deblurring</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>H?usser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02134</idno>
		<ptr target="http://lmb.informatik.uni-freiburg.de/Publications/2016/MIFDB16" />
		<title level="m">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>IEEE International Conference on Computer Vision and Pattern Recognition (CVPR)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2283</idno>
		<title level="m">Depth map prediction from a single image using a multi-scale deep network</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14141</idno>
		<title level="m">Adabins: Depth estimation using adaptive bins</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="567" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">High-quality linear interpolation for demosaicing of bayer-patterned color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Malvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cutler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">485</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems, software available from tensorflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename></persName>
		</author>
		<ptr target="http://tensorflow.org/" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06023</idno>
		<title level="m">Structure-aware residual pyramid network for monocular depth estimation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detail preserving depth estimation from a single image using attention guided networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="304" to="313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fast image restoration without boundary artifacts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Reeves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1448" to="1453" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Depth-of-field rendering by pyramidal image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Strengert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1964" />
			<publisher>MIT press</publisher>
			<biblScope unit="volume">8</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep ordinal regression network for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Batmanghelich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2002" to="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Geonet: Geometric neural network for joint depth and surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="283" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Enforcing geometric constraints of virtual normal for depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5684" to="5693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-K</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Suh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10326</idno>
		<title level="m">From big to small: Multi-scale local planar guidance for monocular depth estimation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Guiding monocular depth estimation using depth-attention volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen-Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heikkil?</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="581" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Alhashim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wonka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.11941</idno>
		<title level="m">High quality monocular depth estimation via transfer learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13413</idno>
		<title level="m">Vision transformers for dense prediction</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Introduction to Fourier optics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Roberts and Company Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
