<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Satya</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gorti</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">No?l</forename><surname>Vouitsis</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>AI</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<addrLine>3 Vector Institute 4 NVIDIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyvan</forename><surname>Golestan</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>AI</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Animesh</forename><surname>Garg</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Toronto</orgName>
								<address>
									<addrLine>3 Vector Institute 4 NVIDIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<address>
									<country>AI</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">X-Pool: Cross-Modal Language-Video Attention for Text-Video Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In text-video retrieval, the objective is to learn a crossmodal similarity function between a text and a video that ranks relevant text-video pairs higher than irrelevant pairs. However, videos inherently express a much wider gamut of information than texts. Instead, texts often capture subregions of entire videos and are most semantically similar to certain frames within videos. Therefore, for a given text, a retrieval model should focus on the text's most semantically similar video sub-regions to make a more relevant comparison. Yet, most existing works aggregate entire videos without directly considering text. Common text-agnostic aggregations schemes include mean-pooling or self-attention over the frames, but these are likely to encode misleading visual information not described in the given text. To address this, we propose a cross-modal attention model called X-Pool that reasons between a text and the frames of a video. Our core mechanism is a scaled dot product attention for a text to attend to its most semantically similar frames. We then generate an aggregated video representation conditioned on the text's attention weights over the frames. We evaluate our method on three benchmark datasets of MSR-VTT, MSVD and LSMDC, achieving new state-of-the-art results by up to 12% in relative improvement in Recall@1. Our findings thereby highlight the importance of joint textvideo reasoning to extract important visual cues according to text. Full code and demo can be found at: layer6ailabs.github.io/xpool/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The advent of video content platforms like TikTok, YouTube and Netflix have enabled the mass outreach of videos around the world. The ability to retrieve videos that are most semantically similar to a provided text-based search query allows us to quickly find relevant information and to make sense of massive amounts of video data. * Authors contributed equally to this work.</p><p>A bus is going on the road and it is fallen in the big dig.</p><p>Man in black suit is having meeting with group of people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A pope traveling on open body vehicle and blessing the peoples.</head><p>A female reporter gives updates on foreign relations and tragedies. <ref type="figure">Figure 1</ref>. Illustration of the joint text and visual representations for a single video and its captions taken verbatim from the MSR-VTT dataset. Since the video is capturing more content than each individual text, aggregating the entire video regardless of the input text can be misleading.</p><p>The task of text-video retrieval is an approach to solve this problem wherein the objective is for a model to learn a similarity function between texts and videos. To compute the similarity between both modalities, a common technique is to first embed a text and a video into a joint latent space and then apply a distance metric such as the cosine similarity between the text and video embeddings <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>However, there is an important discrepancy between both modalities that makes such a direct comparison challenging. Videos inherently express a much wider gamut of information than texts, so a text generally cannot fully capture the entire contents of a video. Instead, texts are most semantically similar to sub-regions of videos, represented as a subset of frames. Depending on the given text, the frames that are the most semantically similar would differ, so multiple equally valid texts can match a particular video. For example, in <ref type="figure">Figure 1</ref>, we show frames of a sample video from the MSR-VTT dataset <ref type="bibr" target="#b43">[44]</ref>. The frames depict various scenes from international news and express different visual content. Moreover, we show multiple captions associated with this video, and observe that each caption best matches a different video frame but can seem irrelevant to others. In this example, we would expect the same video to be re-trieved for any of these queries, even though the relevant content is limited to sub-regions of the video.</p><p>Based on this observation, we want a retrieval model to focus on the video sub-regions that are most relevant to the given text during retrieval. A model should therefore directly reason between texts and the frames of videos to extract the most relevant information as described in each text. However, most existing works do not apply direct cross-modal reasoning, and instead utilize the entire contents of a video such as through mean-pooling or selfattention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>. By encoding a video independently from a given text, a model is likely to encode superfluous or even distracting visual information that is not described in the text, which can reduce retrieval performance.</p><p>To address this gap, we design a cross-modal attention model that we call X-Pool to allow for joint reasoning between a text and a video's frames. Unlike previous works that pool the entire frames of a video, our model provides flexibility for a text to attend to its most semantically similar frames and then generates an aggregated video representation conditioned on those frames.</p><p>Our main contributions can be summarized as follows: (i) We show empirically through a proof of concept that text-conditioned video pooling allows a model to reason about the most relevant video frames to a given text, which outperforms baselines that use text-agnostic video pooling; (ii) We propose a cross-modal attention model that extends our proof of concept with parametric capacity for a text to attend to its most semantically similar video frames for aggregation which we call X-Pool. X-Pool obtains state-of-the-art results across the popular benchmark datasets of MSR-VTT <ref type="bibr" target="#b43">[44]</ref>, MSVD <ref type="bibr" target="#b7">[8]</ref> and LSMDC <ref type="bibr" target="#b37">[38]</ref>; (iii) We demonstrate the robustness of X-Pool to videos with increasing amounts of content diversity, such as videos with many scene transitions. We show how text-agnostic pooling methods are much more sensitive to such videos compared to our textconditioned X-Pool model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Joint Language-Image Understanding. Joint language-image models are a form of multimodal learning <ref type="bibr" target="#b5">[6]</ref> that aim to understand and relate the text and image modalities. Methods in text-image understanding such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref> are pre-trained to jointly reason about language and image semantics which make them suitable for downstream cross-modal tasks like visual question answering (VQA) <ref type="bibr" target="#b2">[3]</ref>, image captioning <ref type="bibr" target="#b44">[45]</ref> and textimage retrieval <ref type="bibr" target="#b15">[16]</ref>. Most recently, methods such as CLIP <ref type="bibr" target="#b36">[37]</ref>, ALIGN <ref type="bibr" target="#b13">[14]</ref>, DeCLIP <ref type="bibr" target="#b21">[22]</ref> and ALBEF <ref type="bibr" target="#b18">[19]</ref> employ unimodal encoders to learn a joint latent space that matches relevant text-image pairs via a contrastive loss. Our goal is to bootstrap from a pre-trained joint text-image model and extend it towards a joint text-video model for the task of text-video retrieval.</p><p>Text-Video Retrieval. The prototypical approach to text-video retrieval has been through a pre-trained language expert and often a combination of video experts pre-trained for various tasks and modalities, after which the language and vision streams are consolidated through late fusion. MoEE <ref type="bibr" target="#b32">[33]</ref>, CE <ref type="bibr" target="#b24">[25]</ref>, MMT <ref type="bibr" target="#b12">[13]</ref> MDMMT <ref type="bibr" target="#b11">[12]</ref>, and Teach-Text <ref type="bibr" target="#b10">[11]</ref> are all such works. The motivation for using pre-trained experts stems from the small-scale nature of the datasets used in text-video retrieval.</p><p>Some works have also benefited from pre-training their own models on either large-scale text-video datasets <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50]</ref> or through text-image pre-training <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b32">33]</ref>. Among them, ActBERT <ref type="bibr" target="#b49">[50]</ref> and ClipBERT <ref type="bibr" target="#b16">[17]</ref> are both single stream models that jointly embed text-video pairs through BERT-like architectures for early cross-modal fusion. However, these works do not allow for direct reasoning about the most semantically similar video sub-regions to a given text.</p><p>Recently, the works of CLIP4Clip <ref type="bibr" target="#b28">[29]</ref> and Straight-CLIP <ref type="bibr" target="#b35">[36]</ref> use the joint language-vision model of CLIP <ref type="bibr" target="#b36">[37]</ref> pre-trained on a large-scale text-image dataset as a backbone. Even the trivial use of CLIP in a zero-shot manner outperforms most of the above recent works <ref type="bibr" target="#b35">[36]</ref>, highlighting how the rich joint text-image understanding of CLIP can be expanded towards videos. CLIP4Clip <ref type="bibr" target="#b28">[29]</ref> proposes several video aggregation schemes including mean-pooling, self-attention and a multimodal transformer, yet none allow for direct matching of a text with its most relevant video sub-regions which motivates our cross-modal attention model. Cross-modal attention has been explored in previous related work such as <ref type="bibr">[9, 17, 19-21, 28, 31, 41, 42, 46, 49, 50]</ref>. We design a cross-modal attention mechanism for the task of text-video retrieval that shows significant improvement over previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Statement</head><p>In text-video retrieval, the objective is for a model to learn a scalar similarity function s(t, v) between a text t and a video v. We want to assign higher similarity to relevant text-video pairs and assign lower similarity to irrelevant pairs. We define two retrieval tasks, text-to-video retrieval denoted as t2v and video-to-text retrieval denoted as v2t. In t2v, we are given a query text t and a video index set V. The goal is to rank all videos v ? V according to their similarities with the query text. Analogously, in v2t, we are given a query video v and a text index set T . The goal is to rank all texts t ? T according to their similarities with the query video. In both of these tasks, we are under the assumption that only the index set is known ahead of time.</p><p>The inputs to our problem are a video v and a text t. We define a video v ? R F ?3?H?W as a sequence of F sampled image frames in time.</p><formula xml:id="formula_0">That is, v = [v 1 , v 2 , ? ? ? , v F ] T where v f is the f th image frame of resolution H ? W .</formula><p>We define a text t as a sequence of tokenized words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Methodology</head><p>In this section, we incrementally introduce the insights and methodologies that motivate our final model X-Pool. We first describe in Section 4.1 how the use of a pre-trained joint text-image model is an essential component of our model to match texts and images which we extend to match texts and videos. We then explain the drawbacks of aggregating a video into a text-agnostic embedding in Section 4.2, and present an alternative framework that aggregates frames conditioned on a given text in Section 4.3. We then introduce our X-Pool model in Section 4.4, a cross-modal attention model that enables joint reasoning between a text and the frames of a video. Our model learns to aggregate videos using the most semantically similar frames to a given text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Expanding Joint Text-Image Models</head><p>Bootstrapping From Joint Text-Image Models. Jointly pre-trained text-image models have demonstrated the ability to match semantically similar texts and images <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b36">37]</ref>. We can leverage the existing text-image reasoning of such models to bootstrap a joint text-video model. This allows us to learn language-video interactions with substantially less video data and offers a more compute efficient solution during training, while benefiting from the rich cross-modal understanding of pre-trained joint text-image models. In general, the idea of bootstrapping video models from image models stems from the importance of first understanding images in order to understand videos, as shown in <ref type="bibr" target="#b6">[7]</ref>.</p><p>CLIP as a Backbone. We bootstrap from CLIP <ref type="bibr" target="#b36">[37]</ref> due to its strong downstream performance, its simplicity, and to more objectively compare with recent works that also leverage CLIP as a backbone <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>, although other pre-trained text-image models may be suitable backbone candidates. To bootstrap from CLIP for text-video retrieval, we first embed a text and individual video frames into its joint latent space and then pool the frame embeddings to obtain a video embedding <ref type="bibr" target="#b35">[36]</ref>. Since the existing information extracted from a pre-trained CLIP model contains rich text-image semantics, we use CLIP as a backbone to learn a new joint latent space to match texts and videos instead of just images.</p><p>More precisely, given a text t a video frame v f as input, CLIP outputs a text embedding c t ? R D and a frame embedding c f v ? R D in a joint latent space:</p><formula xml:id="formula_1">c t = ?(t)<label>(1)</label></formula><formula xml:id="formula_2">c f v = ?(v f )<label>(2)</label></formula><p>where ? is CLIP's text encoder and ? is CLIP's image encoder. By computing equation <ref type="formula" target="#formula_2">(2)</ref> for each frame in a video v, we obtain a sequence of frame embeddings</p><formula xml:id="formula_3">C v = [c 1 v , c 2 v , ? ? ? , c F v ] T ? R F ?D .</formula><p>Computing Text and Video Embeddings. As mentioned, we want to embed our given text and video into a joint space to compute similarity. That is, we want to compute a text embedding z t ? R D and a video embedding z v ? R D . The text embedding is directly taken as the output from CLIP. On the other hand, we compute the video embedding by aggregating the frame embeddings in C v using a temporal aggregation function ?:</p><formula xml:id="formula_4">z t = c t (3) z v = ?(C v )<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Gap: Text-Agnostic Pooling</head><p>In most existing works, the aggregation function ? does not directly consider the input text and is purely a function of the frames of the videos such as through mean-pooling, self-attention or an LSTM <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>While defining the temporal aggregation function as agnostic to text forms a simple baseline, there are important drawbacks with this approach. Videos are inherently much more expressive than texts, so the information captured in text generally cannot fully capture that of an entire video. Instead, texts are most semantically similar to certain subregions of videos which we define as subsets of frames, as shown in <ref type="figure">Figure 1</ref>. As such, common text-agnostic aggregation schemes that pool entire videos like mean-pooling and self-attention might encode spurious information that is not described in the input text.</p><p>We note that this effect is exacerbated when we consider videos that exhibit significant diversity in their visual content <ref type="bibr" target="#b22">[23]</ref> which we refer to as content diversity. To elaborate, it is natural to find videos with scene transitions such as when the actor moves from an indoor setting to an outdoor setting, abrupt scene cuts like in movies, occlusions of key subjects or noise in the form of distractors for example. Since this is an intrinsic property of many videos "in the wild" <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>, we want a retrieval model to be robust to such content diversity by focusing its attention to the most relevant video sub-regions described in a given text. Intuitively, any text-agnostic pooling method will fail under this setting since it aggregates information from all scenes of the video, disregarding the input text for retrieval, as we empirically show in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Key Insight: Text-Conditioned Pooling</head><p>We note that it is therefore important to match texts not with the entire contents of a video, but with those video frames that are most semantically similar to a given text. Depending on the given text, the frames that are most semantically similar would differ, so there could be multiple equally valid texts that match a particular video. As such,  <ref type="figure">Figure 2</ref>. Diagram of X-Pool. For the given text t1, we embed it with the text encoder ? and then apply a query projection to obtain Qt 1 . We similarly embed the frames of the given video v1 with the image encoder ? and then apply a key projection to obtain Kv 1 . We compute the dot product attention between them as illustrated by the horizontal bar plot in the middle of the figure. Our attention mechanism allows X-Pool to focus on the most relevant frames given an input text. We aggregate a separate set of value-projected frame embeddings that we weight by the previously computed dot product attention scores to obtain an aggregated video embedding that we then pass through a fully connected layer (FC) with a residual connection to obtain z v 1 |t 1 . We compute the similarity score s(t1, v1) as the cosine similarity between z v 1 |t 1 and zt 1 = ?(t1). Finally, we compute a cross entropy loss after obtaining s(ti, vj) as just described for each pair (ti, vj) within a batch of size B.</p><p>our temporal aggregation function should directly reason between a given text and the frames of a video.</p><p>To that end, we formulate a new temporal aggregation function ? that allows us to aggregate the video frames that are most semantically similar to a given text t. By conditioning ? on t, we can extract from a video v the most relevant information as described in t while suppressing noisy and misleading visual cues. We denote the resulting aggregated video embedding as z v|t and define our similarity function s(t, v) as:</p><formula xml:id="formula_5">z v|t = ?(C v | t) (5) s(t, v) = z t ? z v|t z t z v|t<label>(6)</label></formula><p>To demonstrate the efficacy of our idea, we first propose a top-k aggregation function ? top-k (C v | t) as:</p><formula xml:id="formula_6">? top-k (C v | t) = 1 k f ?K c f v<label>(7)</label></formula><p>where the set K is defined as:</p><formula xml:id="formula_7">K = arg max K?{1,...,F } |K|=k f ?K c t ? c f v c t c f v<label>(8)</label></formula><p>and the selected frames are those with the highest cosine similarity. Here, we directly select only the frames with the highest cosine similarity to a given text as a proxy for semantic similarity. Only the top-k most semantically similar frames to a given text are pooled while lower similarity frames are completely ignored. We observe that even by just applying top-k pooling, there is already a significant improvement over baselines where the temporal aggregation function is text-agnostic. Detailed experiments can be found in Section 5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Our Model: X-Pool</head><p>Towards Parametric Text-Conditioned Pooling. However, there are still drawbacks with the top-k method. Firstly, the tuning of the k hyperparameter can be task and instance specific as we show in Section 5.3. Secondly, deciding which frames to aggregate from can require more complex reasoning than simple cosine similarity. Lastly, completely suppressing frames with lower similarity may be too restrictive. As such, we propose a parametric approach to address these additional considerations while incorporating our insights from applying textconditioned pooling.</p><p>Cross-Modal Language-Video Attention. Our idea is to design a learned frame aggregation function with parametric capacity for cross-modal reasoning about a text's most semantically similar frames in a video, which we call X-Pool. The core mechanism is our adaptation of a scaled dot product attention <ref type="bibr" target="#b42">[43]</ref> between a text and the frames of a video. Conditioned on these frames, we generate a video embedding that learns to capture the most semantically similar video sub-regions as described in a given text. Since the frames with highest semantic similarity can differ depending on the text, our scaled dot product attention mechanism can learn to highlight relevant frames to a given text while suppressing frames not described in said text. Our model's capacity to selectively pick frames based on relevance to a given text is motivated by the same text-conditioning insights as outlined in the previously described top-k approach. However, unlike the top-k approach, our proposed model learns the optimal amount of information to extract for a text-video pair, thereby removing the need to manually specify a k value. Furthermore, our cross-attention module handles both high and low relevancy frames rather than adopting a hard selection of relevant frames as in the top-k approach.</p><p>To elaborate, in our cross-modal attention module, we first project a text embedding c t ? R D into a single query Q t ? R 1?Dp and a video's frame embeddings</p><formula xml:id="formula_8">C v ? R F ?D into key K v ? R F ?Dp and value V v ? R F ?Dp matrices,</formula><p>where D is the size of our model's latent dimension and D p is the size of the projection dimension. The projections are defined as:</p><formula xml:id="formula_9">Q t = LN(c T t )W Q<label>(9)</label></formula><formula xml:id="formula_10">K v = LN(C v )W K<label>(10)</label></formula><formula xml:id="formula_11">V v = LN(C v )W V<label>(11)</label></formula><p>where LN is a Layer Normalization layer <ref type="bibr" target="#b3">[4]</ref> and W Q , W K and W V are projection matrices in R D?Dp . In order to learn flexible conditioning between the given text and the frames, we then adapt scaled dot product attention from the queryprojected text embedding to the key-projected frame embeddings. The dot product attention gives relevancy weights from a text to each frame which we leverage to aggregate the value-projected frame embeddings:</p><formula xml:id="formula_12">Attention(Q t , K v , V v ) = softmax Q t K T v D p V v (12)</formula><p>As such, the Q t , K v and V v matrices can be interpreted akin to those in the original scaled dot product attention proposed in <ref type="bibr" target="#b42">[43]</ref> except with cross-modal interactions. That is, the query-projected text embedding is used to seek from the key-projected frame embeddings to attend to frames with highest relevance. The value-projected embeddings represent the video's context from which we want to aggregate only certain sub-regions depending on the text.</p><p>To embed a video into a joint space with a text, we project the aggregated video representation from the attention module back into R D by applying a weight W O ? R Dp?D to obtain:</p><formula xml:id="formula_13">r v|t = LN(Attention(Q t , K v , V v )W O )<label>(13)</label></formula><p>where the resulting output r t|v is an aggregated video embedding conditioned on the text t. We can thereby learn this embedding such that a text can attend to its most semantically similar frames through parametric reasoning in the dot product attention. Our final text-conditioned pooling is defined as:</p><formula xml:id="formula_14">? X-Pool (C v | t) = LN(FC(r v|t )) + r v|t ) T<label>(14)</label></formula><p>where FC is a fully connected network which together with the residual connection provides additional capacity for more complex reasoning in our aggregation function. <ref type="figure">Figure 2</ref> shows a diagram of our model. We show how X-Pool performs text-conditioned video aggregation over frames by allowing a text to learn to attend to its most semantically similar frames for pooling. In the top example, the input text t 1 is most relevant to the first few frames displayed of video v 1 of a man yelling at and punching a sink, whereas the final displayed frames of a man near a car do not capture what is described in the text and instead act as misleading visual distractors. We show how our model can reason about semantic similarity by assigning higher attention weights to the text's most relevant frames for aggregation. We emphasize that any text-agnostic pooling method such as mean-pooling would have aggregated the contents from this entire video. The resulting aggregation would thereby capture noisy distractors not described in the input text which could hamper the similarity score for retrieval. In the bottom example, we show a similar behaviour wherein X-Pool can attend to the most relevant frames of two guys jumping in an elevator as described in the text, whereas textagnostic methods would capture non-relevant content from this video.</p><p>Loss. We train models using a dataset D consisting of N text and video pairs</p><formula xml:id="formula_15">{(t i , v i )} N i=1 .</formula><p>In each pair, the text t i is a matching text description of the corresponding video v i . We employ the cross entropy loss from <ref type="bibr" target="#b47">[48]</ref> by considering matching text-video pairs as positives and by considering all other pairwise text-video combinations in the batch as negatives. Specifically, we jointly minimize the symmetric text-to-video and video-to-text losses: </p><formula xml:id="formula_16">L t2v = ? 1 B B i=1</formula><formula xml:id="formula_17">L = L t2v + L v2t<label>(17)</label></formula><p>where s(t i , v j ) is the cosine similarity between the text t i and the video v j , B is the batch size and ? is a learnable scaling parameter. By bootstrapping from a pre-trained CLIP model and through our cross-modal attention mechanism, training with this loss enables our model to learn to match a text with its most semantically similar sub-regions of the ground-truth video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We perform experiments on the commonly used benchmark text-video retrieval datasets of MSR-VTT <ref type="bibr" target="#b43">[44]</ref>, MSVD <ref type="bibr" target="#b7">[8]</ref> and LSMDC <ref type="bibr" target="#b37">[38]</ref> and evaluate our performance following existing literature <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47]</ref> by reporting Recall@1 (R@1), Recall@5 (R@5), Recall@10 (R@10), Median Rank (MdR), and Mean Rank (MnR).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>MSR-VTT is comprised of 10,000 videos, each paired with about 20 human-labeled captions. We note that the multiple captions for each video in MSR-VTT often describe different video sub-regions, which supports our motivation for matching a given text with its most relevant frames in a video. The lengths of videos in this dataset range from 10 to 32 seconds, and we use two training splits which we call 7k-Train and 9k-Train to effectively compare with previous works. 7k-Train is a subset of roughly 7k videos as defined in <ref type="bibr" target="#b33">[34]</ref>, while 9k-Train consists of approximately 9k videos following the split in <ref type="bibr" target="#b12">[13]</ref>. Unless otherwise stated, we use the 9k-Train split for training. To evaluate our models, we use the 1K-A test set from <ref type="bibr" target="#b46">[47]</ref> consisting of 1,000 selected caption-video pairs.</p><p>MSVD contains about 120k captions that each describe one of 1,970 videos ranging in length from 1 to 62 seconds. Again, videos are paired with multiple captions and each may describe different sub-regions of the same video. In MSVD, the training, validation and test splits are comprised of 1,200, 100 and 670 videos respectively. Our final results are evaluated on the test split that has a varying number of captions per video. To that end, we follow recent methods for evaluation by treating all the provided caption-video pairs as separate instances for evaluation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref>.</p><p>LSMDC is a movie clip dataset containing 118,081 videos each paired with a single caption description. The lengths of videos range from 2 to 30 seconds. 101,079 videos are used for training while 7,408 and 1,000 videos are used for validation and testing respectively. We report all results on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We use CLIP's ViT-B/32 image encoder as ? and CLIP's transformer base text encoder as ?, and initialize all encoder parameters from CLIP's pre-trained weights. We set the query, key and value projection dimension size as D p = 512 to match CLIP's output dimension and initialize our logit scaling paramter ? with that from a pre-trained CLIP model. We apply a linear layer with D = 512 output units and dropout <ref type="bibr" target="#b39">[40]</ref> of 0.3 as our FC. Finally, we initialize all new projection weight matrices with identity and all new biases with zeros to bootstrap our entire model from the existing text-image semantic reasoning of a pre-trained CLIP. Our models are fine-tuned end-to-end on each dataset. To that end, we set our batch size to 32 for all experiments and set the learning rate for CLIP-initialized weights to 1e-6 and for all other parameters to 1e-5. We optimize our model for 5 epochs using the AdamW optimizer <ref type="bibr" target="#b26">[27]</ref> with weight decay set to 0.2 and decay the learning rate using a cosine schedule <ref type="bibr" target="#b25">[26]</ref> following CLIP <ref type="bibr" target="#b36">[37]</ref>. For all experiments, we uniformly sample 12 frames from every video and resize each frame to 224x224 following previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>To evaluate our method, we compare its performance with recent works from the literature. We tabulate the t2v retrieval performance of our model trained on the MSR-VTT 9k-Train and 7k-Train splits in <ref type="table">Table 1 and Table 2</ref> respectively. <ref type="table" target="#tab_3">Tables 3 and 4</ref> similarly compare the performance of X-Pool on the MSVD and LSMDC datasets respectively. We note that on all datasets and across all metrics, our text-conditioned X-Pool model outperforms all other works that use text-agnostic pooling <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36]</ref>    ing those using video experts in multiple video modalities <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32]</ref>. Most notably, our model outperforms the hitherto state-of-the-art methods CLIP4Clip-meanP and CLIP4Clip-seqTransf <ref type="bibr" target="#b28">[29]</ref> which are the most directly comparable to X-Pool since they also use CLIP as a backbone. Therefore, we can directly attribute the performance gains of our model to the fact that we use text-conditioned pooling compared to the text-agnostic pooling schemes of CLIP4Clip-meanP and CLIP4Clip-seqTransf. More precisely, on the MSR-VTT dataset, we observe a relative improvement of 5% in Recall@1 compared to CLIP4Clip-seqTransf. For the MSVD dataset, we outperform CLIP4Clip-meanP by over 2% in relative improvement in Recall@1. In the case of the LSMDC dataset, the retrieval problem is more challenging since the movie scene text descriptions are much more ambiguous, which can be observed by the overall lower retrieval scores of all previous methods. Yet, our method notably outperforms CLIP4Clip-seqTransf by 12% in relative improvement in Recall@1. Our results thereby highlight the importance of our model's text-conditioned aggregation that can learn to match a text with its most relevant frames while suppressing distracting visual cues from other video sub-regions.</p><p>Top-k Experiments. To better understand the merits and intuition for our X-Pool model, we first revisit our topk temporal aggregation function defined in equation <ref type="formula" target="#formula_6">(7)</ref> that we introduce as a proof of concept for our proposed idea of text-conditioned video pooling. To validate this idea, we compare top-k pooling with a mean-pooling baseline as in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b35">36]</ref> across two settings: first we apply a pretrained CLIP model in a zero-shot manner similar to <ref type="bibr" target="#b35">[36]</ref>    to compare mean-pooling and top-k aggregation, and second we fine-tune a pre-trained CLIP model on the MSR-VTT dataset and then measure retrieval performance for mean-pooling and top-k pooling. In both settings, we set k=3 which empirically yields the best overall performance. We compare the t2v results in <ref type="table" target="#tab_3">Table 3a</ref> and observe that even by using cosine similarity in top-k pooling as a proxy for semantic similarity between a text and frames, we can outperform mean-pooling across all listed metrics by up to 6% of relative improvement in Recall@1 through our textconditioned pooling scheme.</p><p>Yet, the top-k aggregation function still presents some drawbacks as mentioned in Section 4.2, most notably relating to the tuning of the k hyperparameter. To analyze this shortcoming, we run an experiment wherein for a zero-shot pre-trained CLIP, we find the optimal k of each individual text-video pair in the MSR-VTT test set and report the results in a histogram in <ref type="figure" target="#fig_0">Figure 3b</ref>. Here, we define optimal as the the k value that yields the highest similarity score between a ground-truth text-video pair as defined in equation <ref type="bibr" target="#b5">(6)</ref>. We observe that the optimal choice of k varies widely between text-video pairs, which makes k difficult to select in general. Our proposed X-Pool model therefore addresses the drawbacks of top-k pooling while being motivated by our derived insights of text-conditioned pooling.</p><p>Robustness to Content Diversity in Videos. We now analyze the robustness of our model to content diversity as we described in Section 4.2. As explained, many videos inherently exhibit diverse visual content such as scene transitions or changes in object appearance for example. While current datasets such as MSR-VTT, LSMDC and MSVD already display these traits to an extent, they are curated by choosing only small video clip segments extracted from larger videos. Therefore, in order to more effectively test the robustness of text-video retrieval methods to content diversity, one way is to introduce additional diversity in visual content with more scene transitions. That is, we augment a video's visual content by randomly injecting another video from the dataset to simulate an abrupt scene transition. By performing retrieval on such augmented videos and their original text captions, we can better evaluate a retrieval model's ability to handle diverse videos in the wild.</p><p>To that end, we construct augmented versions of the MSR-VTT test set by adding scene transitions from each video to other videos in the test set. The number of transitions is defined as the number of random videos that are added to the original video at a random location. We compare the t2v retrieval performance of our X-Pool model to the baseline of mean-pooling, and plot the results in <ref type="figure" target="#fig_1">Figure  4</ref>. Here, we measure performance using the metric of Median Rank. We can clearly observe that as the number of video transitions increases and we add video content diversity, there is a sharp performance decline in mean-pooling as the Median Rank increases from 2 to 46, whereas our X-Pool model is significantly more robust to content diversity as Median rank only increases from 2 to 9. The performance gap is because any text-agnostic pooling method like mean-pooling aggregates content from all scenes of a video regardless of their relevance to an input text. Therefore, the more diverse a video is in terms of scene transitions, the more possibly noisy distractors are being aggregated. Conversely, X-Pool can extract only the most relevant visual cues as described in a text through text-conditioned pooling.</p><p>Qualitative Results. In <ref type="figure" target="#fig_1">Figure 4</ref>, we show qualitative examples of our X-Pool model. For each example, we show four sampled frames from a video along with a bar plot representing the associated attention weights of X-Pool from the given text to each frame. In the top example, we can see that our model outputs a higher attention weight for the middle frames when the input text describes a brain animation and lower attention weights everywhere else. On the other hand, when the input text instead describes a fictional character looking at a machine, the attention weight correspondingly activates for the last frame where the text is most relevant. The second example in the middle shows a singing competition. Here, the text of "a judge hearing the voice of competitors" describes an event that requires reasoning over all of the frames. Indeed, we observe that X-Pool attends to the entire video, indicating the flexibility two women and a female lion lay down in a street while music plays man is crossing the street with big lion and friend a judge hearing the voice of competitors three kids sing together on the voice animation where fictional characters look at a complicated machine an animation of a brain of our approach. Finally, in the bottom example, we observe that our model correspondingly activates on the most relevant frames to each text despite the more subtle nuances in the language and video semantics of this lion example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we highlight the drawbacks of text-agnostic video pooling and present an alternative framework for textconditioned pooling for text-video retrieval. We then extend our idea and derived insights to design a parametric model for cross-modal attention between a text and video frames called X-Pool. We show how X-Pool can learn to attend to the most relevant frames to a given a text, which also makes our model substantially more robust to video content diversity such as in the form of scene transitions, a property that is common in videos in the wild. As part of future work, we plan on applying text-conditioned video pooling to other cross-modal tasks like video question answering.   <ref type="table" target="#tab_3">Table A3</ref>. v2t results on the LSMDC dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Number of Frames Experiment</head><p>Our experiments use 12 sampled frames by default following recent text-video retrieval literature <ref type="bibr" target="#b28">[29]</ref>, and we run additional experiments on the MSR-VTT-9K dataset by varying the number of sampled frames for both training and inference as shown in <ref type="figure">Figure B1</ref>. We observe worse performance for 6 frames likely due to important information being missing at this scale. As we increase the number of frames 1 , we observe performance saturation which is consistent with findings in <ref type="bibr" target="#b28">[29]</ref>. However, we note that the optimal number of sampled frames remains a dataset specific hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Online Inference in a Large-Scale Production System</head><p>Since our model computes an aggregated video embedding conditioned on a given text, the embeddings from a 1 "All" indicates inference with all frames at inference time after training on 12 sampled frames. <ref type="bibr" target="#b5">6</ref> 12 24 All 40 <ref type="bibr" target="#b43">44</ref> 48</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Frames</head><p>Recall@1 X-Pool Mean-Pooling <ref type="figure">Figure B1</ref>. t2v Recall@1 results on the MSR-VTT-9K dataset when varying the number of frames. "All" indicates inference with all frames.</p><p>video index set in t2v cannot be entirely pre-computed because query texts are not a priori known during online inference. Instead, we can only pre-compute the frame embeddings of each index video, so fast nearest neighbour retrieval techniques <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24]</ref> cannot be readily applied. To address this in a production system with large-scale index sets, one commonly used approach is to use a high recall method to obtain a set of top retrieval candidates using using a nearest-neighbour search, and then use another method yielding high precision to re-rank the candidates <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>In our case, we can first mean-pool the pre-computed frame embeddings coming from X-Pool and then very efficiently obtain a set of P most similar candidates from the index set given a retrieval query. We can then run X-Pool's text-conditioned attention mechanism only on said candidates and then re-rank them for retrieval. That way, given T text queries and V index videos in t2v, instead of an O(T V) complexity, we can achieve an O(T P + V) complexity where P &lt;&lt; V while maintaining good performance. In fact, we evaluated the performance of our model on the MSR-VTT dataset using the top-100 candidates from mean-pooling (i.e. P = 100) and obtained the same performance in Recall@1, Recall@5 and Recall@10 as listed in our main results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Top-k analysis on MSR-VTT. (a) t2v retrieval performance comparing mean-pooling with top-k text-conditioned pooling. (b) Histogram showing the k value where each ground truth text-video pair in the MSR-VTT test set achieves the highest cosine similarity when using top-k pooling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Robustness to content diversity. We show the t2v Median Rank results on MSR-VTT for different amounts of content diversity measured by the number of scene transitions. Our X-Pool approach remains robust whereas mean-pooling significantly deteriorates as we increase the content diversity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of X-Pool from the MSR-VTT dataset. For each displayed frame above, the bar plot shows its attention weights in our model given a particular text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>... ... ... ... ... ... .. .</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Element-Wise</cell><cell>Cosine</cell></row><row><cell></cell><cell></cell><cell>Key Proj.</cell><cell></cell><cell></cell><cell>Value Proj.</cell><cell></cell><cell></cell><cell>Weighted</cell><cell>Similarity</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Addition</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.26</cell><cell></cell><cell></cell><cell></cell><cell>Add</cell></row><row><cell>A man yells at a faucet</cell><cell>Query</cell><cell>...</cell><cell></cell><cell>...</cell><cell>...</cell><cell>FC</cell><cell>+ Norm</cell></row><row><cell>then punches it.</cell><cell>Proj.</cell><cell></cell><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cross</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entropy Loss</cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.</cell><cell></cell><cell></cell><cell>0.03</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.04</cell><cell></cell><cell></cell><cell></cell><cell>Add</cell></row><row><cell>Two guys jumped up and down in a glass elevator really high in the air.</cell><cell>Query Proj.</cell><cell>...</cell><cell>0.25</cell><cell>...</cell><cell>...</cell><cell>FC</cell><cell>+ Norm</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.20</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Value Proj.</cell><cell></cell><cell>Initialized</cell><cell>Positive</cell><cell>Negative</cell><cell>Embedding</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>from CLIP</cell><cell>Text-Video</cell><cell>Text-Video</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Similarity</cell><cell>Similarity</cell></row></table><note>? Key Proj.?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 1. t2v results on the MSR-VTT-9K dataset.Table 2. t2v results on the MSR-VTT-7K dataset.</figDesc><table><row><cell>Methods</cell><cell cols="5">R@1 ? R@5 ? R@10 ? MdR ? MnR ?</cell></row><row><cell>CE [25]</cell><cell>19.8</cell><cell>49.0</cell><cell>63.8</cell><cell>6.0</cell><cell>23.1</cell></row><row><cell>Support Set [35]</cell><cell>28.4</cell><cell>60.0</cell><cell>72.9</cell><cell>4.0</cell><cell>-</cell></row><row><cell>NoiseE [2]</cell><cell>20.3</cell><cell>49.0</cell><cell>63.3</cell><cell>6.0</cell><cell>-</cell></row><row><cell>Straight-CLIP [36]</cell><cell>37.0</cell><cell>64.1</cell><cell>73.8</cell><cell>3.0</cell><cell>-</cell></row><row><cell>Frozen [5]</cell><cell>33.7</cell><cell>64.7</cell><cell>76.3</cell><cell>3.0</cell><cell>-</cell></row><row><cell>TeachText-CE+ [11]</cell><cell>25.4</cell><cell>56.9</cell><cell>71.3</cell><cell>4.0</cell><cell>-</cell></row><row><cell>CLIP4Clip-meanP [29]</cell><cell>46.2</cell><cell>76.1</cell><cell>84.6</cell><cell>2.0</cell><cell>10.0</cell></row><row><cell cols="2">CLIP4Clip-seqTransf [29] 45.2</cell><cell>75.5</cell><cell>84.3</cell><cell>2.0</cell><cell>10.3</cell></row><row><cell>X-Pool (ours)</cell><cell>47.2</cell><cell>77.4</cell><cell>86.0</cell><cell>2.0</cell><cell>9.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>includ-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell>R@1 ? R@5 ? R@10 ? MdR ? MnR ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CE [25]</cell><cell>20.9</cell><cell>48.8</cell><cell>62.4</cell><cell>6.0</cell><cell>28.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MMT [13]</cell><cell>26.6</cell><cell>57.1</cell><cell>69.6</cell><cell>4.0</cell><cell>24.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Straight-CLIP [36]</cell><cell>31.2</cell><cell>53.7</cell><cell>64.2</cell><cell>4.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Support Set [35]</cell><cell>30.1</cell><cell>58.5</cell><cell>69.3</cell><cell>3.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MDMMT [12]</cell><cell>38.9</cell><cell>69.0</cell><cell>79.7</cell><cell>2.0</cell><cell>16.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Frozen [5]</cell><cell>31.0</cell><cell>59.5</cell><cell>70.5</cell><cell>3.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TeachText-CE+ [11]</cell><cell>29.6</cell><cell>61.6</cell><cell>74.2</cell><cell>3.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLIP4Clip-meanP [29]</cell><cell>43.1</cell><cell>70.4</cell><cell>80.8</cell><cell>2.0</cell><cell>16.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CLIP4Clip-seqTransf [29] 44.5</cell><cell>71.4</cell><cell>81.6</cell><cell>2.0</cell><cell>15.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X-Pool (ours)</cell><cell>46.9</cell><cell>72.8</cell><cell>82.2</cell><cell>2.0</cell><cell>14.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell>R@1 ? R@5 ? R@10 ? MdR ? MnR ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>HowTo100M [34]</cell><cell>14.9</cell><cell>40.2</cell><cell>52.8</cell><cell>9.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ActBERT [50]</cell><cell>8.6</cell><cell>23.4</cell><cell>33.1</cell><cell>36.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>NoiseE [2]</cell><cell>17.4</cell><cell>41.6</cell><cell>53.6</cell><cell>8.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ClipBERT [17]</cell><cell>22.0</cell><cell>46.8</cell><cell>59.9</cell><cell>6.0</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CLIP4Clip-meanP [29]</cell><cell>42.1</cell><cell>71.9</cell><cell>81.4</cell><cell>2.0</cell><cell>15.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CLIP4Clip-seqTransf [29] 42.0</cell><cell>68.6</cell><cell>78.7</cell><cell>2.0</cell><cell>16.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>X-Pool (ours)</cell><cell>43.9</cell><cell>72.5</cell><cell>82.3</cell><cell>2.0</cell><cell>14.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>t2v results on the MSVD dataset.</figDesc><table><row><cell>Methods</cell><cell cols="5">R@1 ? R@5 ? R@10 ? MdR ? MnR ?</cell></row><row><cell>CE [25]</cell><cell>11.2</cell><cell>26.9</cell><cell>34.8</cell><cell>25.3</cell><cell>-</cell></row><row><cell>MMT [13]</cell><cell>12.9</cell><cell>29.9</cell><cell>40.1</cell><cell>19.3</cell><cell>75.0</cell></row><row><cell>NoiseE [2]</cell><cell>6.4</cell><cell>19.8</cell><cell>28.4</cell><cell>39.0</cell><cell>-</cell></row><row><cell>Straight-CLIP [36]</cell><cell>11.3</cell><cell>22.7</cell><cell>29.2</cell><cell>56.5</cell><cell>-</cell></row><row><cell>MDMMT [12]</cell><cell>18.8</cell><cell>38.5</cell><cell>47.9</cell><cell>12.3</cell><cell>58.0</cell></row><row><cell>Frozen [5]</cell><cell>15.0</cell><cell>30.8</cell><cell>39.8</cell><cell>20.0</cell><cell>-</cell></row><row><cell>TeachText-CE+ [11]</cell><cell>17.2</cell><cell>36.5</cell><cell>46.3</cell><cell>13.7</cell><cell>-</cell></row><row><cell>CLIP4Clip-meanP [29]</cell><cell>20.7</cell><cell>38.9</cell><cell>47.2</cell><cell>13.0</cell><cell>65.3</cell></row><row><cell cols="2">CLIP4Clip-seqTransf [29] 22.6</cell><cell>41.0</cell><cell>49.1</cell><cell>11.0</cell><cell>61.0</cell></row><row><cell>X-Pool (ours)</cell><cell>25.2</cell><cell>43.7</cell><cell>53.5</cell><cell>8.0</cell><cell>53.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>t2v results on the LSMDC dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Aggr. R@1 ? R@5 ? R@10 ? MdR ? MnR ?</figDesc><table><row><cell cols="2">Zero-Shot CLIP</cell><cell></cell><cell></cell></row><row><cell>Mean 31.5 52.8</cell><cell>63.6</cell><cell>5.0</cell><cell>42.9</cell></row><row><cell>Top-k 33.6 54.0</cell><cell>64.3</cell><cell>4.0</cell><cell>42.5</cell></row><row><cell cols="2">Fine-Tuned CLIP</cell><cell></cell><cell></cell></row><row><cell>Mean 42.1 69.8</cell><cell>80.7</cell><cell>2.0</cell><cell>15.7</cell></row><row><cell>Top-k 44.6 70.9</cell><cell>82.4</cell><cell>2.0</cell><cell>14.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>A. Video-to-Text Retrieval ResultsTable A1. v2t results on the MSR-VTT-9K dataset.</figDesc><table><row><cell>Methods</cell><cell cols="5">R@1 ? R@5 ? R@10 ? MdR ? MnR ?</cell></row><row><cell>CE [25]</cell><cell>20.6</cell><cell>50.3</cell><cell>64.0</cell><cell>5.3</cell><cell>25.1</cell></row><row><cell>MMT [13]</cell><cell>27.0</cell><cell>57.5</cell><cell>69.7</cell><cell>3.7</cell><cell>21.3</cell></row><row><cell>Straight-CLIP [36]</cell><cell>27.2</cell><cell>51.7</cell><cell>62.6</cell><cell>5.0</cell><cell>-</cell></row><row><cell>Support Set [35]</cell><cell>28.5</cell><cell>58.6</cell><cell>71.6</cell><cell>3.0</cell><cell>-</cell></row><row><cell>TeachText-CE+ [11]</cell><cell>32.1</cell><cell>62.7</cell><cell>75.0</cell><cell>3.0</cell><cell>-</cell></row><row><cell>CLIP4Clip-meanP [29]</cell><cell>43.1</cell><cell>70.5</cell><cell>81.2</cell><cell>2.0</cell><cell>12.4</cell></row><row><cell cols="2">CLIP4Clip-seqTransf [29] 42.7</cell><cell>70.9</cell><cell>80.6</cell><cell>2.0</cell><cell>11.6</cell></row><row><cell>X-Pool (ours)</cell><cell>44.4</cell><cell>73.3</cell><cell>84.0</cell><cell>2.0</cell><cell>9.0</cell></row><row><cell>Methods</cell><cell cols="5">R@1 ? R@5 ? R@10 ? MdR ? MnR ?</cell></row><row><cell>Straight-CLIP [36]</cell><cell>59.9</cell><cell>85.2</cell><cell>90.7</cell><cell>1.0</cell><cell>-</cell></row><row><cell>TeachText-CE+ [11]</cell><cell>27.1</cell><cell>55.3</cell><cell>67.1</cell><cell>4.0</cell><cell>-</cell></row><row><cell>CLIP4Clip-meanP [29]</cell><cell>56.6</cell><cell>79.7</cell><cell>84.3</cell><cell>1.0</cell><cell>7.6</cell></row><row><cell cols="2">CLIP4Clip-seqTransf [29] 62.0</cell><cell>87.3</cell><cell>92.6</cell><cell>1.0</cell><cell>4.3</cell></row><row><cell>X-Pool (ours)</cell><cell>66.4</cell><cell>90.0</cell><cell>94.2</cell><cell>1.0</cell><cell>3.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A2 .</head><label>A2</label><figDesc>v2t results on the MSVD dataset.</figDesc><table><row><cell>JSFusion [47]</cell><cell>12.3</cell><cell>28.6</cell><cell>38.9</cell><cell>20.0</cell><cell>-</cell></row><row><cell>Straight-CLIP [36]</cell><cell>6.8</cell><cell>16.4</cell><cell>22.1</cell><cell>73.0</cell><cell>-</cell></row><row><cell>TeachText-CE+ [11]</cell><cell>17.5</cell><cell>36.0</cell><cell>45.0</cell><cell>14.3</cell><cell>-</cell></row><row><cell>CLIP4Clip-meanP [29]</cell><cell>20.6</cell><cell>39.4</cell><cell>47.5</cell><cell>13.0</cell><cell>56.7</cell></row><row><cell cols="2">CLIP4Clip-seqTransf [29] 20.8</cell><cell>39.0</cell><cell>48.6</cell><cell>12.0</cell><cell>54.2</cell></row><row><cell>X-Pool (ours)</cell><cell>22.7</cell><cell>42.6</cell><cell>51.2</cell><cell>10.0</cell><cell>47.4</cell></row></table><note>Methods R@1 ? R@5 ? R@10 ? MdR ? MnR ?</note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03186</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">G?l Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal machine learning: A survey and taxonomy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltru?aitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="423" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Uniter: Universal image-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM conference on recommender systems</title>
		<meeting>the 10th ACM conference on recommender systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for textvideo retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part IV 16</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling up visual and vision-language representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhsuan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duerig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05918</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Quality assessment of in-the-wild videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2351" to="2359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven Chu Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Visualbert: A simple and performant baseline for vision and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liunian Harold</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03557</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Supervision exists everywhere: A data efficient contrastive language-image pre-training paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.05208</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recognizing realistic actions from videos &quot;in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1996" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An investigation of practical approximate nearest neighbor algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note>Citeseer</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.02265</idno>
		<title level="m">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Cross-class relevance learning for temporal concept localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishna</forename><surname>Satya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksims</forename><surname>Gorti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Volkovs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Stanevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08548</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thinking fast and slow: Efficient text-to-visual retrieval with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9826" to="9836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Learning a text-video embedding from incomplete and heterogeneous data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02516</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s Andr?s</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning crossmodality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Look at what i&apos;m doing: Self-supervised spatial grounding of narrations in instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Describing videos by exploiting temporal structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atousa</forename><surname>Torabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4507" to="4515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Yu</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12649</idno>
		<title level="m">Classification is a strong baseline for deep metric learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Temporal query networks for fine-grained video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankush</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4486" to="4496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
