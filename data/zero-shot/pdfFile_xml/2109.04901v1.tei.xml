<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Document-level Entity-based Extraction as Template Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kung-Hsiang</forename><surname>Huang</surname></persName>
							<email>khhuang3@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Illinois Urbana</orgName>
								<address>
									<settlement>Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Information Sciences Institute</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Los Angeles</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Document-level Entity-based Extraction as Template Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level entity-based extraction (EE), aiming at extracting entity-centric information such as entity roles and entity relations, is key to automatic knowledge acquisition from text corpora for various domains. Most document-level EE systems build extractive models, which struggle to model long-term dependencies among entities at the document level. To address this issue, we propose a generative framework for two document-level EE tasks: role-filler entity extraction (REE) and relation extraction (RE). We first formulate them as a template generation problem, allowing models to efficiently capture crossentity dependencies, exploit label semantics, and avoid the exponential computation complexity of identifying N-ary relations. A novel cross-attention guided copy mechanism, TOPK COPY, is incorporated into a pre-trained sequence-to-sequence model to enhance the capabilities of identifying key information in the input document. Experiments done on the MUC-4 and SCIREX dataset show new stateof-the-art results on REE (+3.26%), binary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score 1 . arXiv:2109.04901v1 [cs.CL] 10 Sep 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Document-level entity-based extraction (EE) are tasks that extract entity-centric information, such as entities and their relations, from unstructured text across multiple sentences. With the rise of big data in recent years, document-level EE is growing in importance with applications such as understanding clinical reports <ref type="bibr" target="#b20">(Nye et al., 2020)</ref>, extracting document-level events , and building knowledge graphs from journals . In this work, we focus on two classic tasks of document-level EE: role-filler entity extraction (REE) and relation extraction (RE).</p><p>Recent works on document-level EE usually build <ref type="bibr">1</ref> The source code is publicly available at https:// github.com/PlusLabNLP/TempGen Ours SciREX-P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gold Relations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI-accuracy</head><p>We conducted experiments on the Stanford Natural Language Inference ( SNLI ) dataset ... (39 sentences) ... Our best score on this task is 87.3 % accuracy. ... (20 sentences ) ... we adopt MAP and MRR as the evaluation metrics for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SNLI-accuracy</head><p>SNLI-MAP SNLI-MRR <ref type="figure">Figure 1</ref>: A comparison between our approach and a competitive extractive system, SCIREX-P <ref type="bibr" target="#b12">(Jain et al., 2020)</ref>, on a relation extraction example from SCIREX. The task is to extract entities and identify which entities are related from the given scientific article. Due to the long distances between entities, SCIREX-P struggles to extract the right entity pair that has a relation, while our approach correctly identifies them. This reflects our method's advantage in modeling long-term cross-entity dependencies.</p><p>task-specific classifiers on top of large pre-trained language models. For example, <ref type="bibr" target="#b6">Du and Cardie (2020a)</ref> builds a sequence tagging framework with multi-granularity representations based on BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> for role-filler entity extraction. <ref type="bibr" target="#b12">Jain et al. (2020)</ref> builds a relation extraction pipeline upon SCIBERT <ref type="bibr" target="#b1">(Beltagy et al., 2019)</ref>. However, there are a few drawbacks of this model architecture. First, as the size of the document increases, it becomes increasingly difficult for extractive methods to capture cross-entity dependencies among entitiy types due to long distances between entities, as shown in <ref type="figure">Figure 1</ref>. Additionally, discriminative models have no information regarding the semantics of the labels when classifying relations or entity types. Thus, it is unable to take advantage of the label semantics embedded in the pre-trained encoders.</p><p>Motivated by these challenges, we propose to formulate REE and RE tasks as template generation. Due to the autoregressive nature of generative setup, this formulation makes dependencies among the output entities easier to capture compared to sequence tagging methods. Moreover, label names are incorporated into the decoder targets for exploiting label semantics not present in the extractive counterparts. Furthermore, for tasks that involve the identification of N -ary relations, this formulation significantly alleviates the computational complexity of comparing exponential combinations of entities. A generative framework, Cross-attention Guided Template Generation (TEMPGEN), that incorporates a novel copy mechanism into a pretrained sequence-to-sequence model is proposed to solve the template generation problem effectively.</p><p>Our contributions can be summarized as follows:</p><p>? We propose to formulate document-level EE tasks as a template generation problem, which allows our generative framework to effectively capture cross-entity dependencies, better identify entities with label semantics, and avoid the exponential computation complexity of identifying N -ary relations.</p><p>? We devise a novel copy mechanism based on cross-attention to enable our model to better learn how to copy key information from the input document.</p><p>? Our approach achieves state-of-the-art results on MUC role-filler entity extraction task and SCIREX relation extraction task, while being data efficient compared to previous systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Tasks</head><p>This section gives an overview of the two document-level EE tasks we tackled in this work: role-filler entity extraction (REE) and relation extraction (RE).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Role-filler Entity Extraction</head><p>The REE task aims to extract all entities involved in events from the input article <ref type="bibr" target="#b8">(Du et al., 2021)</ref>. It differs from the event template extraction task introduced by the MUC-4 dataset <ref type="bibr">(muc, 1992)</ref> in that only one event template, as opposed to many, is outputted for each input document. For documents associating with multiple event templates (all events in MUC-4 are of ATTACK type), the event templates are collapsed as one -systems are required to identify all entities associate with different events for each role type. An event template consists of a set of pre-defined roles, and each role is filled with zero to many entities, as shown in <ref type="figure">Figure 2</ref>. An entity is characterized by a group of mentions, which are spans of text in the input document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relation Extraction</head><p>We focus on end-to-end document-level relation extraction where systems first extract entities from the input document and then identify the N -ary non-typed relations among the extracted entities. The SCIREX <ref type="bibr" target="#b12">(Jain et al., 2020)</ref> dataset is the only dataset that supports such end-to-end configurations that we know of. Thus, we follow the definition of document-level RE in SCIREX, which contains binary and 4-ary relation annotation. A binary relation contains two typed entities, and a 4ary relation contains four typed entities. An entity is represented by a cluster of mentions, similar to the REE task. Systems should first extract salient entities of pre-defined types 2 . Then, binary and 4-ary relations among salient entities are identified. A binary relation example is shown in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Methods</head><p>In this section, we first illustrate how the REE and RE tasks can be framed as a template generation problem. This formulation then allows us to capture cross-entity dependencies easily with our proposed generative model, a pre-trained sequence-tosequence model integrated with a copy mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Template Generation Formulation</head><p>We frame the REE and RE tasks as template generation problem, as shown in <ref type="figure">Figure 2</ref>. A template is composed of slot names and slot values. For both tasks, slot names are entity types, and slot values are all entity mentions corresponding to such entity types. Similar to previous works on REE <ref type="bibr" target="#b11">(Huang and Riloff, 2011;</ref><ref type="bibr" target="#b6">Du and Cardie, 2020a;</ref><ref type="bibr" target="#b8">Du et al., 2021)</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Role-filler Entity Extraction Relation Extraction</head><p>Method: We ... denote the modified ESIM as aESIM...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments:</head><p>The accuracy ( ACC ) of each method is measured by the commonly used precision score ... It also achieved 88.01 % on Quora ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Template Filling</head><p>Template Generation <ref type="figure">Figure 2</ref>: An overview of how document-level EE tasks can be transformed into template generation tasks. Special tags are defined as follows: &lt;SOT&gt; : start of template, &lt;EOT&gt; : end of template, &lt;SOSN&gt;: start of slot name, &lt;EOSN&gt; : end of slot name, &lt;SOE&gt; : start of entity (slot value), &lt;EOE&gt;: end of entity.</p><p>be transformed into template sequences with special tags delimiting templates, slot names, and slot values.</p><p>Formally, a document of tokens D = {D i } n i=1 may correspond to a decoding target of zero to many template sequences</p><formula xml:id="formula_0">{T i } l i=1 . A template sequence T i is characterized by multiple slot se- quences {S i,j } m j=1 , Ti = &lt;SOT&gt; Si,1, ..., Si,m &lt;EOT&gt; .</formula><p>A slot sequence S i,j is represented by slot names and entities,</p><formula xml:id="formula_1">Si,j = &lt;SOSN&gt; L &lt;EOSN&gt; &lt;SOE&gt; D (e k ) 1 ...D (e k ) n &lt;EOE&gt; .</formula><p>where L is the slot name 3 , and D (e k ) 1 , ..., D (e k ) n is the token sequence that correspond to one mention randomly sampled from entity e k . Special tokens, such as &lt;SOSN&gt; and &lt;EOSN&gt; , are to indicate whether a tag-enclosed string is a slot name or an entity mention. In the first row of the REE example from <ref type="figure">Figure 2</ref>, L would be PERPIND and D (e k ) 1 , ..., D (e k ) n are "group of terrorists". Using this formulation, scalability challenges of modeling cross-entity dependencies is alleviated due to the significantly reduced distances between entities in template sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-attention Guided Template Generation</head><p>The template generation problem can be broken down into two sub-goals: (1) generating valid tem-3 Slot name corresponds to role in REE and entity type in RE. plate structures while capturing the dependencies between the input document and decoder targets, and (2) ensuring that salient mentions in the input document are correctly identified and outputted by the decoder. To achieve the first sub-goal, we leverage BART <ref type="bibr" target="#b15">(Lewis et al., 2020)</ref>, a pre-trained sequence-to-sequence model. The second sub-goal is achieved using a novel copy mechanism incorporated into BART.</p><p>Seq2Seq Model for Template Generation BART <ref type="bibr" target="#b15">(Lewis et al., 2020</ref>) is a pre-trained language model that combines bidirectional and auto-regressive transformers. Pre-training with multiple denoising objectives, BART has demonstrated significant advantages in various text generation tasks, especially on summarization <ref type="bibr">(Lewis et al., 2020) 4</ref> . The template generation problem much resembles summarization, except that generated template sequences contain implicit structures. With the various denoising pre-training objectives, we believe that BART can capture the implicit structure within template sequences, effectively model the dependencies among predicted entities, and produce rich semantics to reason over between slot names and entities.</p><p>Cross-attention guided copy mechanism To enhance BART's capabilities to identify salient mentions in the input documents, we incorporate a copy mechanism based on cross-attention. As cross-attentions often imply saliency of input to-kens, a naive approach of computing copy distributions P copy at time step t over the input tokens is taking the mean of the last decoder layer's crossattention across all heads, as mentioned in <ref type="bibr" target="#b29">Xu et al. (2020)</ref>,</p><formula xml:id="formula_2">? t,h = softmax( (Wsst) T Wee ? d k ) (1) Pcopy = h ? t,h |H| ,<label>(2)</label></formula><p>where ? t,h is the attention scores over input tokens at decoding step t for head h. W s and W e are the projection matrices for the encoder and the decoder. s t is the decoder hidden states at step t, and e denotes the encoder hidden states.</p><p>However, recent studies have shown that attention heads are not equally important, and that some heads can be pruned out with a marginal decrease in overall performance <ref type="bibr" target="#b26">(Voita et al., 2019;</ref><ref type="bibr" target="#b18">Michel et al., 2019)</ref>. We hypothesize that the attention probabilities produced by insignificant attention heads may be noisy. Thus, computing copy distributions without these heads could improve the model's ability to infer the importance of each token in the input document. Motivated by this hypothesis, we propose TOPK COPY, a copy mechanism where only the Top-k important attention heads are used for computing copy distributions. Consider the formulation of multi-head attention, following the notation from <ref type="bibr" target="#b24">Vaswani et al. (2017)</ref>:</p><formula xml:id="formula_3">MultiHead(Q, K, V ) = Concat(head1, ..., head h )W O (3) headi = Attention(QW Q i , KW K i , V W V i ). (4) W Q i , W K i , W V i ? R d model ?d</formula><p>are the projection matrices for computing attention. W O ? R hdv?d model is the matrix that allows interaction between different attention heads, where h is the number of heads. To determine the importance of each attention head, we first transform <ref type="formula">(5)</ref>), and then sum over the last two dimensions of W O (Equation <ref type="formula" target="#formula_5">(6)</ref>),</p><formula xml:id="formula_4">W O to dimension h ? d v ? d model (Equation</formula><formula xml:id="formula_5">W O ? R hdv ?d model ? W O ? R h?dv ?d model (5) scorei = j,k |W O i,j,k |.<label>(6)</label></formula><p>where score i denotes the significance score for head i. We take the attention heads with Topk highest significance scores in the last crossattention layer, and use the mean of the attention probabilities outputted by these heads as the copy distribution as shown in equations 7 and 8,</p><formula xml:id="formula_6">K = Top-k(score) (7) Pcopy = h?K ? t,h k . (8)</formula><p>Objective function. The final probability P final of a word w t is a weighted sum of vocabulary distribution computed by BART P vocab and copy distribution P copy ,</p><formula xml:id="formula_7">Pfinal(wt) = pgenPvocab(wt) + (1 ? pgen)Pcopy(wt). (9)</formula><p>where p gen ? [0, 1] is the generation probability computed by passing the dot product of the mean encoder hidden state e = n i=0 e i n and decoder hidden state s t at time step t through the sigmoid function ?,</p><formula xml:id="formula_8">pgen = ?(e ? st)<label>(10)</label></formula><p>Using the final probability distribution P final , we can then compute the loss function as the average negative log likelihood of the target word y t over all timesteps, following <ref type="bibr" target="#b23">See et al. (2017)</ref>, </p><formula xml:id="formula_9">L = 1 T T t=0 ? log Pfinal(yt).<label>(11</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We compare our method with the following competitive baseline systems.</p><p>NST (Du and Cardie, 2020a) builds multigranularity representations on documents, and utilizes gate mechanism to fuse representations of different granularity.</p><p>TANL <ref type="bibr" target="#b21">(Paolini et al., 2021)</ref> augments sequential labels with input sentences, allowing it to be applied to various structured prediction tasks 7 .</p><p>GRIT <ref type="bibr" target="#b8">(Du et al., 2021)</ref> shares transformer parameters between the encoder and the pointer network decoder, and is the SOTA system for the REE task 5 https://github.com/allenai/SciREX 6 There are 4 entity types: MATERIAL, METRIC, TASK, and METHOD. <ref type="bibr">7</ref> Since the source code of TANL has not been released by the time we conducted experiments, we re-implemented it by closely following the method described in <ref type="bibr" target="#b21">Paolini et al. (2021)</ref>. on the MUC dataset.</p><p>DYGIE++ <ref type="bibr" target="#b27">(Wadden et al., 2019)</ref> is a span-based multi-task IE framework jointly trained on relation extraction, named entity recognition, and coreference resolution. <ref type="bibr" target="#b12">(Jain et al., 2020)</ref> is the SOTA framework for end-to-end binary and 4-ary relation extraction on SCIREX. The pipeline is composed of 4 components: mention identification, mention clustering, salient entity cluster identification, and relation classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCIREX-P</head><p>In terms of the pre-trained language models used, BERT-BASE <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> is used for NST, DYGIE++, and GRIT. SCIREX-P is fine-tuned on SCIBERT <ref type="bibr" target="#b1">(Beltagy et al., 2019)</ref>. We replace T5 <ref type="bibr" target="#b22">(Raffel et al., 2020)</ref> with BART-BASE for TANL for a fair comparison with our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Implementation details</head><p>The proposed models are optimized using AdamW <ref type="bibr" target="#b17">(Loshchilov and Hutter, 2019)</ref> with learning rate 5e-5 and weight decay 1e-5. We used grid search to find the best k for TOPK COPY and found that k = 10 yields the best overall performance across REE and RE. The maximum input sequence length for RE and REE are 1024 and 512, respectively. During inference time, all generative models used beam search with a beam width of 4.  Additionally, we set the maximum input sequence length to 512 for TEMPGEN for fairer comparisons with SCIREX-P. We obtain F1 scores of 11.94% and 2.18% on binary and 4-ary relation extraction, respectively. This confirms the advantage of our model on the relation extraction tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Main Results</head><p>While TANL performs worse than our model on REE, it is still able to achieve a higher score than GRIT. This suggests that augmenting decoding targets with label names provides useful semantics, whereas adding input documents to decoding targets may not yield better results in the REE task. We also observe that TANL scores extremely low on both RE tasks, where 58% of the binary relations and 26 % of the 4-ary relations in the decoding targets are filtered out due to exceeding maximum sequence length of BART. Out of the remaining relations, 57% of the binary relations and 78% of the 4-ary relations have at least one entity removed in the decoding targets due to its long distance from the first-appearing entity 8 , suggesting that TANL's poor performance on RE tasks is due to scarcity of gold labels. This reflects that TANL is ill-suited for document-level EE tasks. We observe extremely low performances across all systems on both tasks of SCIREX, even though TEMPGEN outperforms the baseline systems significantly. This is mainly caused by the characteristics of the SCIREX dataset. First, syntactic 8 Please refer to Appendix B for more details.</p><p>......The police also stopped 8,000 cars in the search for assassins, who are presumably members of the maoist terrorist organization shining path...... The dircote (counterterrorism divison) has identified one of the terrorists as Gerardo Olivos Silva through a composite made from witness' reports.......</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated PerpInd entity: terrorists</head><p>(a) Copy distribution produced by NAIVE COPY.</p><p>......The police also stopped 8,000 cars in the search for assassins, who are presumably members of the maoist terrorist organization shining path...... The dircote (counterterrorism divison) has identified one of the terrorists as Gerardo Olivos Silva through a composite made from witness' reports.......</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generated PerpInd entity: Gerardo Olivos Silva</head><p>(b) Copy distribution produced by TOPK COPY.</p><p>1e-2 5e-1 (c) The darker the color, the higher the probability. <ref type="figure">Figure 3</ref>: TOPK COPY produces a more reliable copy distribution P copy than that computed by NAIVE COPY in a MUC-4 example. Given an input document and decoded tokens "&lt;s&gt;&lt;SOT&gt; &lt;SOSN&gt; PERPIND &lt;EOSN&gt; &lt;SOE&gt;", the gold PERPIND entity is "Gerado Olivos Silva". However, "terrorist" is assigned the highest copy probability computed by NAIVE COPY, leading to incorrect entity extracted. Conversely, TOPK COPY assigns the highest P copy to the head token of the gold entity, "Ger", resulting in successful extraction of the correct entity eventually.</p><p>characteristics specific to scientific journals, such as algorithm blocks, result in the unusually long sequences in the SCIREX dataset despite best parsing efforts. Additionally, another feature frequently seen in scientific journals is the use of table and figure captions. Since captions are not included as part of the input text, the number of accepted relations decreases drastically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Performance Analysis</head><p>Ablation Study We conducted ablation studies by replacing the TOPK COPY module with other copy mechanisms. NAIVE COPY refers to computing copy distributions with the attentions from all cross-attention heads. SAGCopy <ref type="bibr" target="#b29">(Xu et al., 2020)</ref> utilizes encoder self-attention to compute centrality scores for measuring the saliency of each input token. As shown in <ref type="table" target="#tab_3">Table 2</ref>, we found that NAIVE COPY leads to performance drop on all three tasks, especially on binary and 4-ary relation extractions. NAIVE COPY achieving scores even lower than fine-tuning BART alone (i.e. w/o TOPK COPY ) reflects that copy mechanisms may mislead models to copy incorrect input tokens. A qualitative example of the difference between TOPK COPY and NAIVE COPY demonstrated in <ref type="figure">Figure 3</ref>   We also experimented with replacing the original slot names with numeric slot names (i.e. converting PERPIND to &lt;ROLE_1&gt;, PERPORG to &lt;ROLE_2&gt;, and etc). This conversion removes the semantics of slot names in the decoding targets. While little performance drop was observed on the REE task, using numeric slot names resulted in the worst performance on binary and 4-ary relation extraction tasks, which could be a result of strong slot dependencies in RE in comparison with REE. In RE, slots are directly semantically related to other slots in each template whereas slots in REE are relatively independent. This shows that slot name semantics are useful for template generation tasks with strong slot dependencies in each template. Finally, we conducted ablation studies on different variations of templates as decoding targets. Specifically, three variations are tested on the REE task: (1) We merge entities of the same role names into the same "slot". (e.g. transforming the decoding targets from " &lt;SOSN&gt; PerpInd &lt;EOSN&gt; &lt;SOE&gt; Alice &lt;EOE&gt; &lt;SOSN&gt; PerpInd &lt;EOSN&gt; &lt;SOE&gt; Bob &lt;EOE&gt; " to " &lt;SOSN&gt; PerpInd &lt;EOSN&gt; &lt;SOE&gt; Alice; Bob &lt;EOE&gt; ").</p><p>(2) Based on (1), all slot names, such as "PerpInd" and "PerpOrg", are replaced with the same special token "&lt;ROLE&gt;". (3) We use the same decoding targets as GRIT's. These three settings achieve test set F1 scores of 56.65, 54.16, and 52.55, respectively. The results suggest that differentiating entities with different entity types helps improve the performance. Furthermore, comparing with the results in <ref type="table" target="#tab_1">Table 1</ref>, we found that GRIT performs better than our system, reflecting that a pointer network-based model, which has with smaller search space than ours, is more advantageous when using the same decoding targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of the Amount of Training Data</head><p>To test the data efficiency of our approach, we compared TEMPGEN and TEMPGEN -TOPK COPY with GRIT on the REE task using different amount of MUC training data. As seen in <ref type="figure" target="#fig_1">Figure 5</ref>, both TEMPGEN and TEMPGEN -TOPK COPY outperform GRIT across all settings with a slightly larger performance margin in low resource settings. This indicates that our approach is more data-efficient compared to the previous SOTA system on REE.</p><p>Impact of K Cross-attention Heads <ref type="figure">Figure 4</ref> shows our model's change in performance conditioned on various values of K in the TOPK COPY mechanism. Consistent with our results in Section 5.1, we see that removing some of the cross-attention heads (12 ? 10) can lead to performance gain due to the filtered noise brought by unimportant attention heads. However, we noticed a drop in performance across all three tasks for lower values of K, suggesting that beneficial cross-attention heads are removed. Interestingly, performance drops immediately as K decreases below 10, suggesting that only a small portion of the cross-attention heads are unimportant. The trend is consistent with <ref type="bibr" target="#b18">Michel et al. (2019)</ref>'s results where pruning cross-attention heads to a certain extent can easily result in performance drop. Additionally, the model with no copy mechanism (K = 0) outperforms the model with few attention heads  (K ? {2, 4, 6}), suggesting that the copy distributions obtained from not sufficiently informative cross-attentions can mislead the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Qualitative Analysis</head><p>The following qualitative analysis provides intuition for our model's ability to capture dependencies across entities and utilize slot name semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-entity Dependencies</head><p>To validate our approach's capability to capture cross-entity dependencies, we considered binary relations on SCIREX where at least one of the associated entities is involved in multiple relations. The dependencies among entities are better captured by the model that predicts fewer unlikely relations. Comparing the test set outputs of TEMPGEN and SCIREX-P, we see that 13131 errors made by SCIREX-P are corrected by our model, which only introduces 604 errors. This result demonstrates the strength of TEMPGEN in modeling cross-entity dependencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance of Label Semantics</head><p>Comparing the test set predictions between TEMPGEN and GRIT on the MUC-4 REE task, we see that our approach better distinguishes confusing entities such as VIC-TIM and TARGET entities. As shown in the example in <ref type="figure" target="#fig_2">Figure 6</ref>, GRIT incorrectly predicts the two victims, "Miguel Soler Rodrigues" and "Martha Luz Lopez", as TARGET entities. It also misidentifies "El Espectador", a newspaper company, as a victim of the attack. In contrast, TEMPGEN is able to recognize the roles of the two victims. Even though it's not an exact match, the predicted TARGET entity had a correctly identified role type with similar semantic meaning compared to the gold label. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Inference time comparison</head><p>As discussed earlier, TEMPGEN can significantly reduce the exponential computational complexity of document-level N-ary relation identification. To illustrate this, we compared the inference time between TEMPGEN and two other systems, TANL and SCIREX-P, on the SCIREX 4-ary RE task. As shown in <ref type="figure" target="#fig_3">Figure 7</ref>, TEMPGEN drastically shortens the inference time by around 39 times compared to SCIREX-P. TANL also runs much faster than SCIREX-P, but is still around 4 times slower than TEMPGEN. This is resulted from the fact that TANL generates the entire input document in addition to entity and relation labels, which is much longer than TEMPGEN's generated sequences. <ref type="figure" target="#fig_4">Figure 8</ref> shows the number of parameters of different models. GRIT, with the same size of BERT-BASE, has the least number of parameters among all models. DYGIE++ and SCIREX-P have slightly more parameters than GRIT due to the additional linear layers for constructing classifiers. The two generative models, TANL and TEMPGEN, have the most parameters, thanks to the larger vocab size (30522 ? 50265), larger positional embedding matrix (512 ? 1024), and cross-attention modules in BART-BASE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Number of Parameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Related Works</head><p>In the following sections, we will first discuss a few important works on the REE task and document-level RE task. Then, we will dive into a few works that uses a similar sequence generative approach to various document-level IE tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Role-filler Entity Extraction</head><p>Document-level REE has been explored in recent works using a variety of model architectures. <ref type="bibr" target="#b7">Du and Cardie (2020b)</ref> formulates the task as a sequence tagging problem, and trains layered classifiers as sequence readers on multiple granularities. In contrast, GRIT <ref type="bibr" target="#b8">(Du et al., 2021)</ref> formulates the problem as sequence generation, and employs a single transformer layer whose parameters are shared between encoder and decoder to enrich semantics in the shared parameters. A pointer selection network is used for the final layer of decoding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Document-level Relation Extraction</head><p>Due to long-term dependencies that often span over hundreds of tokens, capturing entity relations have proven to be a challenging task. One approach was constructing a document-level graph from sentence encoding, then extracting entity relations from edge representations in the graph <ref type="bibr" target="#b4">(Christopoulou et al., 2019)</ref>. Other works such as <ref type="bibr" target="#b13">Jia et al. (2019)</ref> layer classifiers in a pipeline architecture to obtain hierarchical representation of N -ary relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">IE as Sequence Generation</head><p>Recently, there has been an increasing number of works framing information extraction tasks as sequence generation problem. <ref type="bibr" target="#b32">Zeng et al. (2018)</ref> formed triple extraction as a sequence generation task and adopted a RNN-based model with copy mechanisms. To encourage the faithfullness of the extracted triplets, <ref type="bibr">Ye et al. (2021)</ref> designed a triplet contrastive training objective. These works focus on sentence-level triplet extraction, while our work extracts role-filler entities and entity relations at the document level. <ref type="bibr" target="#b16">Li et al. (2021)</ref>; <ref type="bibr" target="#b9">Hsu et al. (2021)</ref> formulates the document-level event argument extraction task as a conditional generation problem by providing event ontology. However, their work cannot be applied to REE or RE due to the lack of ontology for role-filler entities and relations. <ref type="bibr" target="#b8">Du et al. (2021)</ref> relied on a pointer-network-based decoder <ref type="bibr" target="#b25">(Vinyals et al., 2015)</ref> to extract event rolefiller entities, and the parameters of BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> is shared between the encoder and the decoder. Nevertheless, their method cannot incorporate role labels, whereas our approach can take advantage of the label semantics. <ref type="bibr" target="#b21">Paolini et al. (2021)</ref> uses a very similar generative approach, which constructs decoder targets by inserting text markers and labels around entity mentions in the input sentence. The key idea is that augmenting the decoder targets with original input sentence and labels provides stronger semantics to the model. Unfortunately, modeling cross-entity dependencies remains a challenge as entities are further apart in their decoding targets. We instead transform annotations into template sequences as decoding targets, where distances between entities are significantly shortened. Thus, our approach alleviates the scalability challenge of capturing crossentity dependencies at the scale of documents. Additionally, our approach differs in that the length of our decoder targets is significantly shorter, allowing the non-truncated decoder targets to fit in pre-trained language models. In contrast, for their method, the gold decoder targets are guaranteed to be longer than corresponding input document. Since the length of input tokens are often greater than the max sequence length of pre-trained language models for document-level EE, a great portion of the gold labels will be skipped using <ref type="bibr" target="#b21">Paolini et al. (2021)</ref>'s method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>We have proposed TEMPGEN, a framework that frames document-level REE and RE tasks as a template generation task. A copy mechanism that takes the top-k important cross-attentions as copy distributions is incorporated into BART for capturing key information in the input document. Experimental results on MUC-4 and SCIREX showed that TEMPGEN outperforms prior approaches on rolefiller entity extraction and end-to-end documentlevel relation extraction tasks. Under different amount of training data, TEMPGEN demonstrates robustness across all settings, while being advantageous in lower-resource regime. <ref type="table" target="#tab_8">Table 3</ref> demonstrates the per-role performance comparison between TEMPGEN and other baselines. We observe that:</p><p>? TEMPGEN achieves the best precision across all roles.</p><p>? Except for PERPIND, TEMPGEN obtain substantial improvement in F1 over other baselines.</p><p>? While TEMPGEN has higher precision over GRIT in extracting PERPIND entities, it scores slightly lower in recall, leading to worse F1 performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B TANL Decoding Target Formulation</head><p>In this section, we illustrate how we formulate the TANL <ref type="bibr" target="#b21">(Paolini et al., 2021)</ref> decoding targets for REE and RE. The formulation for REE is simple due to its similarity to the NER task. We produce REE decoding targets exactly the same way as how NER decoding targets are formed in <ref type="bibr" target="#b21">Paolini et al. (2021)</ref>. Given the REE example in <ref type="figure">Figure 2</ref>, the corresponding TANL decoding target is: As for RE, we follow how <ref type="bibr" target="#b21">Paolini et al. (2021)</ref> handles nested entities and multiple relations, but we made a small modification on decoding targets. Since SCIREX does not contain relation type annotation, we use the related entities' types as the relation type in the decoding targets. With their formulation, the decoding target is created by inserting each relation annotation around the first-appearing entity in the input document. Take the RE instance in <ref type="figure">Figure 2</ref> as an example. The corresponding TANL decoding target would be:</p><p>Introduction: [Natural language inference | Task | Method = aESIM] (NLI) is an important andsignificant task in natural language processing (NLP)...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method:</head><p>We ... denote the modified ESIM as aESIM ... Experiments: The [accuracy | Metric | Material = Quora] (ACC) of each method is measured by thecommonly used precision score ... It also achieved 88.01 % on Quora ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Hardware and Software configurations</head><p>All experiments are conducted on a CentOS Linux 7 (Core) machine with NVIDIA RTX 2080. We use PyTorch 1.6.0 with CUDA 10.1 as the Deep Learning framework and utilize Transformers 4.3.0 to load all pre-trained language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Implementation Details</head><p>We conducted grid search to find the best learning rate over {1 ? 10 ?5 , 3 ? 10 ?5 , 5 ? 10 ?5 , 7 ? 10 ?5 , 9 ? 10 ?5 } using TEMPGEN w/o TOPK COPY on the MUC-4 REE task. The best learning rate, 5 ? 10 ?5 , is fixed for all other experiments. Models are trained for 150 epochs for REE and binary RE experiments, and 50 epochs for 4-ary RE experiments. To reproduce our results, please follow the README.md file in https://github. com/PlusLabNLP/TempGen. The weights of the trained models are also included for reproduction purposes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Validation Performance</head><p>For all reported test set results in <ref type="table" target="#tab_1">Table 1</ref>, the corresponding development set performance are listed in <ref type="table" target="#tab_9">Table 4</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>validates our hypothesis. Quantitatively, examining MUC-4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>REE test set performance on MUC-4 with regard to different amount of training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>An example showing how GRIT misidentifies the VICTIM entities and TARGET entities, likely due to the lack of role type semantics. Here, VICTIM entities are the people attacked, and TARGET entities are the objects compromised.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Inference time comparison on the SCIREX 4-ary RE task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Number of parameters of different systems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Two U.S. mormon missionaries --aged 19 and 21 --were shot to death last night by a group of terrorists from the Zarate Wilka Armed Forces of Liberation (FAL). ... blew up the lines providing power to La Paz, ... the U.S. citizens --Todd Ray Wilson Burdenson and Jeffrey Brent Ball --.... they were killed with two bursts of machinegun fire. ...Introduction:Natural language inference ( NLI ) is an important and significant task in natural language processing ( NLP ) ...</figDesc><table><row><cell cols="2">PerpInd group of terrorists</cell><cell>Task</cell><cell>Natural Language Inference</cell></row><row><cell cols="2">PerpOrg Zarate Wilka Armed Forces of Liberation</cell><cell>Method</cell><cell>aESIM</cell></row><row><cell>Target</cell><cell>lines</cell><cell></cell><cell></cell></row><row><cell>Victim</cell><cell>Toddy Ray Wilson Burderson</cell><cell>Material</cell><cell>Quora</cell></row><row><cell cols="2">Weapon machinegun</cell><cell>Metric</cell><cell>accuracy</cell></row><row><cell cols="2">Template Sequences:</cell><cell cols="2">Template Sequences:</cell></row><row><cell cols="2">&lt;SOT&gt;&lt;SOSN&gt;PerpInd&lt;EOSN&gt;&lt;SOE&gt;group of terroists&lt;EOE&gt;</cell><cell cols="2">&lt;SOT&gt;&lt;SOSN&gt;Task&lt;EOSN&gt;&lt;SOE&gt;Natural Language Inference&lt;EOE&gt;</cell></row><row><cell cols="2">&lt;SOSN&gt;PerpOrg&lt;EOSN&gt;&lt;SOE&gt;Zarate Wilka Armed Forces of</cell><cell cols="2">&lt;SOSN&gt;Method&lt;EOSN&gt;&lt;SOE&gt;aESIM&lt;EOE&gt;&lt;EOT&gt;</cell></row><row><cell cols="2">Liberation&lt;EOE&gt;...&lt;SOSN&gt;Weapon&lt;EOSN&gt;&lt;SOE&gt;</cell><cell cols="2">&lt;SOT&gt;&lt;SOSN&gt;Material&lt;EOSN&gt;&lt;SOE&gt;Quora&lt;EOE&gt;</cell></row><row><cell cols="2">machinegun&lt;EOE&gt;&lt;EOT&gt;</cell><cell cols="2">&lt;SOSN&gt;Metric&lt;EOSN&gt;&lt;SOE&gt;accuracy&lt;EOE&gt;&lt;EOT&gt;</cell></row><row><cell></cell><cell></cell><cell></cell><cell>, we only generate one template</cell></row><row><cell></cell><cell></cell><cell cols="2">per document without differentiating which event</cell></row><row><cell></cell><cell></cell><cell cols="2">template each entity mention associates with. In</cell></row><row><cell></cell><cell></cell><cell cols="2">contrast, for RE, we generate multiple templates,</cell></row><row><cell></cell><cell></cell><cell cols="2">each corresponding to a relation. A binary relation</cell></row><row><cell></cell><cell></cell><cell cols="2">can be represented by a template of 2 slots, whereas</cell></row><row><cell></cell><cell></cell><cell cols="2">a 4-ary relation forms a 4-slot template. A relation</cell></row><row><cell></cell><cell></cell><cell cols="2">template consists of typed mentions of correspond-</cell></row><row><cell></cell><cell></cell><cell cols="2">ing salient entities. After transforming REE and</cell></row><row><cell></cell><cell></cell><cell cols="2">RE annotation to templates, each template can then</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison on role-filler entity extraction, binary and 4-ary relation extraction tasks. TANL results are re-implemented and evaluated by ourselves. TEMPGEN outperforms all previous systems on REE, binary RE, and 4-ary RE. Statistical significance over previous best systems computed using the paired bootstrap procedure<ref type="bibr" target="#b2">(Berg-Kirkpatrick et al., 2012)</ref> is indicated with * (p &lt; .01). , relations are not typed in SCIREX. Hence, the official SCIREX evaluator<ref type="bibr" target="#b12">(Jain et al., 2020)</ref> only considers the correctness of predicted entities and entity types 6 in each relation. Predicted entities are aligned with gold entities based on mention overlap. When the entities are aligned, predicted relations are aligned with gold relations accordingly. A predicted relation is correct if and only if both the associated entities and the entity types match the aligned gold relation.</figDesc><table><row><cell>)</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc>summarizes the main results on role-filler entity extraction, binary, and 4-ary relation extraction. TEMPGEN establishes new state-of-the-art scores on all three tasks, outperforming the previous best models by an absolute F1 of 3.26%, 4.8%, and 2.7%. The improvements demonstrate the effectiveness of our approach in formulating</figDesc><table><row><cell>Model</cell><cell>REE</cell><cell cols="2">Binary RE 4-ary RE</cell></row><row><cell>TEMPGEN</cell><cell>57.76</cell><cell>14.47</cell><cell>3.55</cell></row><row><cell>? NAIVE COPY</cell><cell>56.45</cell><cell>11.22</cell><cell>1.42</cell></row><row><cell>? SAGCopy</cell><cell>54.47</cell><cell>11.17</cell><cell>1.41</cell></row><row><cell>w/o TOPK COPY</cell><cell>55.76</cell><cell>12.63</cell><cell>3.00</cell></row><row><cell cols="2">numeric slot name 56.31</cell><cell>8.22</cell><cell>0.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on removing and replacing different components of TEMPGEN. document-level EE tasks into template generation tasks. Although TEMPGEN scores the highest F1 across all three tasks, SCIREX-P does achieve the highest recall on both RE tasks. This can be explained by the fact that our model can only encode the first 1024 sub-tokens of each SCIREX document, which is merely 17% of the average subtoken count per document. This makes it challeng-</figDesc><table /><note>ing for TEMPGEN to identify relations that lie in the latter 83% of each document. In the future, we can extend BART's positional embedding matrix to enable TEMPGEN to encode longer documents.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Figure 4: Effect of K. We compare the test set F1 score on MUC-4 REE, SCIREX binary and 4-ary RE tasks with regard to different K. K = 0 is equivalent to removing TOPK COPY.</figDesc><table><row><cell>test set predictions, there are 79 cases where TOPK</cell></row><row><cell>COPY corrects the misguidance of Naive Copy,</cell></row><row><cell>while only 32 cases where new errors are intro-</cell></row><row><cell>duced by TOPK COPY. For both REE and RE,</cell></row><row><cell>adding SAGCopy leads to performance drop, sug-</cell></row><row><cell>gesting that the centrality scores of input tokens</cell></row><row><cell>may not be an ideal feature for these tasks.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Miguel Soler Rodriguez, El Espectator's circulation manager and Martha Luz Lopez, the correspondent's office administrator, were killed today .... the main offices in Bogota of El Espectador, ..., were partially destroyed.</figDesc><table><row><cell></cell><cell>Victim</cell><cell>Target</cell></row><row><cell>Gold</cell><cell>Miguel Soler Rodriguez Martha Luz Lopez</cell><cell>the main offices in Bogota of El Espectador</cell></row><row><cell>Ours</cell><cell>Miguel Soler Rodriguez Martha Luz Lopez</cell><cell>El Espectador's office</cell></row><row><cell>GRIT</cell><cell>El Espectador</cell><cell>Miguel Soler Rodriguez Martha Luz Lopez</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Two U.S. mormon missionaries -aged 19 and 21 -were shot to death last night by [a group of terrorists| PerpInd] from the [Zarate Wilka Armed Forces of Lib-eration| PerpOrg] (FAL) ... blew up the [lines| Target] providing power to La Paz, ... the U.S. citizens -[Todd Ray Wilson Burdenson| Victim] and Jeffrey Brent Ball -... they were killed with two bursts of [machinegun| Weapon] fire ...</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>32.61/ 43.32 56.00/ 34.15/ 42.42 53.49/ 50.74/ 52.08 60.00/ 66.32/ 63.00 57.14/ 53.33/ 55.17 (Wadden et al., 2019) GRIT 65.48/ 39.86/ 49.55 66.04/ 42.68/ 51.85 55.05/ 44.12 / 48.98 76.32/ 61.05/ 67.84 61.82/ 56.67 / 59.13 (Du et al., 2021) TEMPGEN 67.12/ 35.51/ 46.45 67.12/ 59.76/ 63.23 64.13/ 43.38/ 51.75 77.22/ 64.21/ 70.11 67.27/ 61.67/ 64.35</figDesc><table><row><cell>Model</cell><cell>PERPIND</cell><cell>PERPORG</cell><cell>TARGET</cell><cell>VICTIM</cell><cell>WEAPON</cell></row><row><cell>DYGIE++</cell><cell>48.39/</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Performance breakdown with regard to each role in CEAF-REE (Precision/ Recall /F1) on the MUC-4 REE task.</figDesc><table><row><cell></cell><cell cols="9">Role-filler Entity Extraction Binary Relation Extraction 4-ary Relation Extraction</cell></row><row><cell>Model</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell cols="2">Precision Recall</cell><cell>F1</cell></row><row><cell>TANL</cell><cell>58.42</cell><cell>46.74</cell><cell>51.93</cell><cell>3.12</cell><cell>2.11</cell><cell>2.39</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>TEMPGEN</cell><cell>61.34</cell><cell>46.11</cell><cell>52.64</cell><cell>22.04</cell><cell>19.24</cell><cell>19.60</cell><cell>1.38</cell><cell>2.77</cell><cell>1.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Corresponding development set performance of the reported test set results in Table 1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Salient entities are entities needed to describe the results of corresponding scientific article.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">We have considered the SOTA abstractive summarization LM, PEGASUS<ref type="bibr" target="#b33">(Zhang et al., 2019</ref>). Yet, the GPU memory consumption is too high for us to test it.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We appreciate insightful feedback from PLUSLab members and the anonymous reviewers. This research was sponsored by the Intelligence Advanced Research Projects Activity (IARPA), via Contract No. 2019-19051600007. The views and conclusions of this paper are those of the authors and do not reflect the official policy or position of IARPA or the US government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<title level="m">Fourth Message Uunderstanding Conference</title>
		<meeting><address><addrLine>McLean, Virginia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
	<note>Proceedings of a Conference Held in</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">SciB-ERT: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An empirical investigation of statistical significance in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Burkett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012</title>
		<meeting>the 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<title level="m">Natural Language Processing and Computational Natural Language Learning</title>
		<meeting><address><addrLine>Jeju Island, Korea</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="995" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1498</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4925" to="4936" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<title level="m">Proceedings of the 2019 Conference of the North</title>
		<meeting>the 2019 Conference of the North</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Document-level event role filler extraction using multi-granularity contextualized encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.714</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8010" to="8020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Documentlevel event role filler extraction using multigranularity contextualized encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.06579</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Grit: Generative role-filler transformers for document-level event entity extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinya</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online. EACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hao</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elizabeth</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Boschee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prem</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.12724</idno>
		<title level="m">Event extraction as natural language generation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Document-level event extraction with efficient end-toend learning of cross-event dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang</forename><surname>Kung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 3rd Workshop on Narrative Understanding (NAACL 2021</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Peeling back the layers: Detecting event role fillers in secondary contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Portland, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1137" to="1147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SciREX: A challenge dataset for document-level information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7506" to="7516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3693" to="3704" />
		</imprint>
		<respStmt>
			<orgName>Long and Short Papers</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Documentlevel event argument extraction by conditional generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Are sixteen heads really better than one?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Algorithms for the assignment and transportation problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Munkres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the society for industrial and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="38" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding clinical trial reports: Extracting medical entities and their relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><forename type="middle">E</forename><surname>Nye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Deyoung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><forename type="middle">J</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byron</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structured prediction as translation between augmented natural languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Paolini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Athiwaratkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Krone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cicero</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Get to the point: Summarization with pointergenerator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abigail</forename><surname>See</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1099</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1073" to="1083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meire</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2692" to="2700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Analyzing multi-head selfattention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5784" to="5789" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Extracting summary knowledge graphs from long documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeqiu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rik</forename><surname>Koncel-Kedziorski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Self-attention guided copy mechanism for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youzheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.125</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1355" to="1362" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Contrastive triple extraction with generative transformer</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extracting relational facts by an end-to-end neural model with copy mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1047</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="506" to="514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Pegasus: Pre-training with extracted gap-sentences for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
