<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Hsuan</forename><surname>Li</surname></persName>
							<email>chiahsuan.li@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szu-Lin</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Liang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">College of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: Spoken Question Answering</term>
					<term>SQuAD</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Reading comprehension has been widely studied. One of the most representative reading comprehension tasks is Stanford Question Answering Dataset (SQuAD), on which machine is already comparable with human. On the other hand, accessing large collections of multimedia or spoken content is much more difficult and time-consuming than plain text content for humans. It's therefore highly attractive to develop machines which can automatically understand spoken content. In this paper, we propose a new listening comprehension task -Spoken SQuAD. On the new task, we found that speech recognition errors have catastrophic impact on machine comprehension, and several approaches are proposed to mitigate the impact.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Machine comprehension (MC) on text has significant progress in the recent years. On Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b0">[1]</ref>, deep neural network (DNN) based models are comparable with human. The achievements of the state-of-theart models demonstrate that MC models already contain decent reasoning ability. On the other hand, accessing large collections of multimedia or spoken content is much more difficult and time-consuming than plain text content for humans. It is therefore highly attractive to develop Spoken Question Answering (SQA) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>, which requires machine to find the answer from spoken content given a question in either text or spoken form. We focus on text query in the following discussions.</p><p>Comparing to the significant progress in MC on text documents, MC on spoken content is a much less investigated field. Different kinds of DNN systems have been used in slot filling task including Recurrent Neural Network (RNN) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, bidirectional RNN <ref type="bibr" target="#b7">[8]</ref> and Convolutional Neural Network(CNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>. However, all these previous models work on sequence labeling task, while SQA requires machine to perform more sophisticated reasoning. There were also works trying to estimate word confidence scores or error probability on ASR transcriptions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b19">19,</ref><ref type="bibr" target="#b20">20]</ref>. ASR confidence measures have been introduced as additional features for spoken language understanding (SLU) <ref type="bibr" target="#b21">[21]</ref>.</p><p>SQA usually worked with automatic speech recognition (ASR) transcripts of the spoken content, and typical approaches used information retrieval (IR) techniques <ref type="bibr" target="#b13">[14]</ref> or used knowledge bases <ref type="bibr" target="#b22">[22]</ref> to find the proper answer. Recently, deep learning is used to answer TOEFL listening comprehension test which is a task highly related to SQA. TOEFL is an English examination that tests the knowledge and skills of academic English for English learners whose native languages is not English. On this task, attention-based RNN is used to construct sentence representation considering the word order <ref type="bibr" target="#b23">[23]</ref>, and <ref type="figure">Figure 1</ref>: Flow diagram of the SQA and two evaluation methods. Given spoken document and text question, SQA system can have two types of outputs: a text answer or a time span of audio segments including the answer. The two types of outputs will be evaluated by two different approaches.</p><p>tree-structured RNN is used to construct sentence representation from their syntactic structure <ref type="bibr" target="#b24">[24]</ref>. However, the scale of TOEFL dataset used in the previous study is too limited to develop powerfully expressive models. In addition, the test is multi-select, in which machine does not provide an answer by itself, so it is still one step away from SQA.</p><p>To further push the boundary of MC on spoken content, we propose a new listening comprehension task -Spoken SQuAD. The contributions of our work are three-folds:</p><p>? We propose the Spoken SQuAD task, which is an extraction-based SQA task, together with a new evaluation approach.</p><p>? We found that ASR errors have catastrophic impact on QA. We tested numbers of state-of-the-art SQuAD models on Spoken SQuAD dataset and reported their degrading performance on ASR transcriptions.</p><p>? Last but not least, we propose several approaches to mitigate the impact of ASR errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Task Definition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Corpus Description</head><p>In this paper, we propose a new listening comprehension task named Spoken SQuAD, in which document is in spoken form, and the input question is in the form of text. SQuAD is one of the largest MC dataset on a large set of Wikipedia articles, with more than 100,000 human-written questions. The answer to each question is always a span in the document. The authors randomly partitioned articles into a training set (80%), a To test the comprehension ability of machine in real life scenario under worse audio quality, we further added two different levels of white noise into the audio files of testing set to obtain different WERs. The resulting three versions of testing sets and corresponding WERs are listed in <ref type="table" target="#tab_0">Table 1</ref>. The synthesized speech makes the task easier than its real application, but in the following experiments, we found that the comprehension capability already seriously degraded in the above scenario. We leave the study on real speech recording as the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Evaluation Metrics</head><p>In this task, when the model is given a spoken article, it needs to find the answer of a text-formed question. SQA can be solved by the concatenation of ASR module and reading comprehension module. The flow diagram of spoken SQuAD is illustrated in <ref type="figure">Figure 1</ref>. With the ASR transcriptions of spoken documents, the reading comprehension module can return a text answer based on the questions. The most intuitive way to evaluate the text answer is to directly compute the Exact Match (EM) and F1 scores between the predicted text answer and the ground-truth text answer. If the predicted text answer and the ground-truth text answer are exactly the same, then the EM score is 1, otherwise 0. The F1 score is based on the precision and recall. Precision is the percentage of words in the predicted answer existing in the ground-truth answer, while the recall is the percentage of words in the ground-truth answer also appearing in the predicted answer.</p><p>The above evaluation on text answer not only considers the performance of the reading comprehension system but also the ASR system. If the reading comprehension system correctly identifies the word sequence in the transcription that corresponds to the answer, but the words in the answers are misrecognized, then the answer would be considered as completely incorrect. This is very possible because most answers are name entities which are usually Out-of-Vocabulary. Therefore, we propose a second evaluation approach specifically designed for SQA. In the second approach, instead of returning a text answer, machine returns the time span corresponding to the answer. In other words, it outputs an audio segment extracted from spoken document as the answer. The time span of the correct answer in the spoken document is available. Because in SQuAD a correct answer always exists in the document, we can force-align the spoken document with its reference transcription to obtain the time span of the correct answer. We compute Audio Overlapping Score (AOS) between the time span of system output and the ground-truth time span as in <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_0">AOS = X ? Y X ? Y ,<label>(1)</label></formula><p>where X is the audio time interval of predicted answer, and Y is the audio time interval of ground-truth answer. AOS rewards the high portion of overlapping and punishes the model for predicting too long time spans. The second evaluation approach is necessary. In a listening comprehension test, it is extremely difficult, even for human, to correctly transcribe every single word in the spoken document. However, if one comprehends the whole spoken document in the right way, he/she can easily select the right segments corresponding to the answer in the audio file. Therefore, we believe the second approach better evaluates the reasoning capability of SQA system. In the following experiments, because the models always select the answers from the ASR transcriptions, to compute AOS, we simply used time stamp for each word given by ASR module to find the time span of the predicted answers. We believe in the future an end-to-end SQA system can probably directly output time span without generating a text answer first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>The SQA system is the cascade of an ASR module and reading comprehension module. In this section, we first briefly introduce the reading comprehension models we used. Then we introduce how we used subword sequence embedding to mitigate the impact of ASR errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reading Comprehension Models</head><p>We chose several competitive models that have acquired stateof-the-art results on SQuAD.The models are listed as follow:</p><p>? BiDirectional Attention Flow (BiDAF) <ref type="bibr" target="#b27">[26]</ref> ? R-NET <ref type="bibr" target="#b28">[27]</ref> ? Mnemonic Reader <ref type="bibr" target="#b29">[28]</ref> ? FusionNet <ref type="bibr" target="#b30">[29]</ref> ? Dr.QA <ref type="bibr" target="#b31">[30]</ref> In our task, during the testing, those models take a machinetranscribed spoken document and a question as input, and the output is an extracted text span from the ASR transcription.</p><p>We first train these models on SQuAD training set and compare the testing results between SQuAD dev set and Spoken SQuAD testing set. Furthermore, the models can be trained on text documents or the ASR transcriptions of spoken documents. We will also compare the results from the two kinds of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Subword Units</head><p>ASR errors are inevitable. However, when a transcribed word is wrong, some subword units in the word may still be correctly transcribed. Therefore, enhancing the word embedding with subword units may mitigate the effect of ASR errors. The output F is further fed into max-pooling layer, and a scalar value is generated. All the scalars from various filters will form the phoneme sequence embedding. Then the phoneme sequence embedding is further concatenated with word embedding as the input of reading comprehension model. In this illustration, E ? R 6?7 , F ? R 3?7 and stride is 1.</p><p>Phoneme sequences of words are used here. We adopt CNN to generate the embeddings from the phoneme sequence of a word, and this network is called Phoneme-CNN. Our proposed approach is the reminiscent of Char-CNN <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b33">32]</ref>, which apply CNN on characters to generate distributed representation of word for text classification task. Phoneme-CNN is illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>. We explain how we obtain feature for one word with one filter. Suppose that a word W consists of a sequence of phonemes P = [p1, ..., p l ], where l is the number of phonemes of this word. Let H ? R C?d be the lookup table phoneme embedding matrix, where C is the number of phonemes, and d is the dimension of the phoneme embedding. In other words, each phoneme corresponds to a d-dimensional vector. After looking up table, the embeddings of the phonemes in the word form the intermediate phoneme embedding E ? R l?d . The convolution between E and a filter F ? R k?d is performed with stride 1 to obtain one-dimension vector Z ? R l?k+1 . After max pooling over Z, we obtain a scalar value. Since we concatenate all the output scalars from different filters, the number of filters determine the size of phoneme sequence embedding. The filter is essentially scanning phoneme n-gram, where the size of n-gram is the height of the filter (the number of k above). All the parameters of filters and phoneme embedding matrix H are end-to-end learned with reading comprehension model. It is also possible to incorporate other sub-word units like syllable <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b36">35]</ref> by the same CNN architecture described above. We will experiment with syllable sequences of words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments Setup and Results</head><p>In this section, we first show the performance of the published models from SQuAD leader board on this Spoken SQuAD dataset. Then we compare the performance of models trained on text or ASR transcriptions. Following that with subword  sequence embedding ablation study on BiDAF, we verify that phoneme sequence embedding and syllable sequence embedding improved BiDAF performance on testing sets with different WERs. We also provide qualitative analysis to show how typical QA model suffers from ASR errors and how phoneme sequence embedding prevent modes from these errors on spoken SQuAD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Investigating the Impact of ASR Errors</head><p>We trained five reading comprehension models mentioned in Section 3.1 on the SQuAD training set, and they were tested on SQuAD dev set and Spoken SQuAD testing set. The number of questions in the Spoken SQuAD testing set is less than the original SQuAD dev set because some of the examples are removed as mentioned in Section 2.1. To make the comparison fair, on SQuAD dev set, we only report the results on the questions also in Spoken SQuAD testing set. In <ref type="table" target="#tab_1">Table 2</ref>, across the five models, average F1 score on the text document is 75.42%. The average F1 score fell to 55.4% when there are ASR errors. Similar phenomenon is observed on EM. The impact of ASR errors is significant for machine comprehension models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Mitigating ASR errors by Subword Units</head><p>We utilized CMU LOGIOS Lexicon Tool to convert each word into sequence of phonemes and then fed the phonemes into Phoneme-CNN network to obtain phoneme sequence embedding. The network details are listed as follow : phoneme embedding size 6, filter size 3x6 and numbers of filters 80. Different from <ref type="bibr" target="#b37">[36]</ref> using one-hot vector, we choose distributed representation vectors to represent phonemes. For syllables, we utilized a python module Pyphen that hyphenate text uses existing Hunspell hyphenation dictionaries to convert word to sequence of syllables. It is reported that in English, hyphenation output of word is often the same as sequence of syllables in that word. We fed the syllables into Phoneme-CNN network to obtain the syllable sequence embedding. The network details are listed as follow : syllable embedding size 20, filter size 2x20 and numbers of filters 100. Both phoneme sequence embedding and syllable sequence embedding will be further concatenated with other representations of word to be the inputs of reading comprehension model. The experimental results with various mitigation approaches which were trained on SQuAD training set are in <ref type="table" target="#tab_3">Table 4</ref>. We chose BiDAF as the reading comprehension model. Because ASR errors can be considered as noise, we can see that adding dropout offered improvements (rows (b) v.s. (a)), while using phoneme or syllable sequence embedding is even better (rows (c), (d) v.s. (b)).</p><p>We compare the performance of the proposed subword unit sequence embedding with two strong baselines, word embedding and combination of word and character embedding. The results on Spoken SQuAD testing set without noise and different levels of noises are listed in <ref type="table" target="#tab_4">Table 5</ref>. We see from <ref type="table" target="#tab_4">Table 5</ref>, phoneme and syllable sequence embedding mostly performed better (row (c)(d) vs. (a)(b)). In addition, model which concatenates character, phoneme and syllable sequence embeddings together (row (e)) achieves the best performance across EM/F1/AOS over all kinds of WER ASR data. <ref type="table">Table 6</ref>: An example of generated predictions from BiDAF with proposed phoneme sequence embedding and BiDAF with only word embedding. The capitalization was added on ASR transcriptions for easy reading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text Document</head><p>To the east, the United States Census Bureau considers the San Bernardino and Riverside County areas, Riverside-San Bernardino area as a separate metropolitan area from Los Angeles County.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ASR Transcription</head><p>To the east, the United States Census Bureau considers the San Bernardino and Riverside County areas, Riverside San Bernardino harry as a separate separate metropolitan area from Los Angeles county.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question</head><p>Who considers Los Angeles County to be a separate metropolitan area?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground truth</head><p>United States Census Bureau WORD in <ref type="table" target="#tab_4">Table 5</ref> riverside san bernardino harry WORD+PHONEME United States Census Bureau in <ref type="table" target="#tab_4">Table 5</ref> 4.4. Qualitative Analysis <ref type="table">Table 6</ref> is a selected example from Spoken SQuAD testing set. According to the generated predictions, we observe that BiDAF with only word embedding is not robust to ASR errors. We can check on ASR transcriptions, the word "area" is misrecognized to "harry". The BiDAF model with only word embedding is confused by the word "harry". The corresponding question starts with "Who", so the model is trying to search for a person name neighboring the key word "Los Angeles county". However, model with our proposed phoneme sequence embedding correctly selects the answer span despite of the ASR transcription errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper we propose a new extraction based spoken question answering task named Spoken SQuAD. We demonstrate that ASR errors significantly degraded the performance of reading comprehension models. Then we propose to use different kinds of subword units to mitigate the impact of ASR errors.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of enhanced word embedding. For a given input word W at the bottom, a sequence of phonemes P = [p1, ..., p l ] are obtained by looking up in the pronunciation dictionary. Each phoneme is mapped to a vector R d and concatenated to form intermediate matrix E. E is fed into the temporal convolutional module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Three testing sets with different levels of WER.</figDesc><table><row><cell cols="4">Testing Set No noise Noise V1 Noise V2</cell></row><row><cell>WER(%)</cell><cell>22.73</cell><cell>44.22</cell><cell>54.82</cell></row><row><cell cols="4">development set (10%), and a testing set (10%). The testing set</cell></row><row><cell cols="4">is not yet publicly available. To build a spoken version SQuAD</cell></row><row><cell cols="4">dataset, we conducted the following procedures to generate spo-</cell></row><row><cell cols="4">ken documents from the original SQuAD dataset. First, we used</cell></row><row><cell cols="4">Google text-to-speech system to generate the spoken version of</cell></row><row><cell cols="4">the articles in SQuAD. Then we utilized CMU Sphinx [25] to</cell></row><row><cell cols="4">generate the corresponding ASR transcriptions. In this study,</cell></row><row><cell cols="4">we left the questions in the text form. We used SQuAD training</cell></row><row><cell cols="4">set to generate the training set of Spoken SQuAD, and SQuAD</cell></row><row><cell cols="4">development set was used to generate the testing set for Spo-</cell></row><row><cell cols="4">ken SQuAD. If the answer of a question did not exist in the</cell></row><row><cell cols="4">ASR transcriptions of the associated article, we removed the</cell></row><row><cell cols="4">question-answer pair from the dataset because these examples</cell></row><row><cell cols="4">are too difficult for listening comprehension machine at this</cell></row><row><cell cols="4">stage. In this way, we collected 37,111 question answer pairs</cell></row><row><cell cols="4">as the training set and 5,351 as the testing set. The WER on the</cell></row><row><cell cols="4">training and testing sets are 22.77% and 22.73%, respectively 1 .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experiment results for state-of-the-art models demonstrating degrading performance under spoken data. All models were trained on the full SQuAD training set. Mnemonic Reader and FusionNet are denoted by Mreader and F-NET, respectively. SQuAD dev set and Spoken SQuAD testing set are denoted by SQuAD-dev and SpokenS-test, respectively.</figDesc><table><row><cell>MODEL</cell><cell cols="2">SQuAD-dev EM F1</cell><cell cols="2">SpokenS-test EM F1</cell></row><row><cell>BiDAF</cell><cell>58.4</cell><cell>69.9</cell><cell>37.02</cell><cell>50.9</cell></row><row><cell>R-NET</cell><cell cols="4">66.34 76.20 44.75 58.68</cell></row><row><cell>Mreader</cell><cell cols="4">64.00 73.35 40.36 52.87</cell></row><row><cell>Dr.QA</cell><cell cols="4">62.84 73.74 41.16 54.51</cell></row><row><cell>F-Net</cell><cell cols="4">70.47 79.51 46.51 60.06</cell></row><row><cell>Average</cell><cell cols="4">64.41 74.54 41.96 55.40</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance</figDesc><table><row><cell></cell><cell cols="5">comparison of training on text documents</cell></row><row><cell cols="6">(full SQuAD Train Set, denoted as Text) and ASR transcriptions</cell></row><row><cell cols="6">(Spoken SQuAD Train Set, denoted as Speech). The testing data</cell></row><row><cell cols="6">is from Spoken SQuAD testing set (SpokenS-test) with ASR er-</cell></row><row><cell>rors.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Data</cell><cell>Score</cell><cell cols="2">BiDAF Text Spoken</cell><cell cols="2">Dr.QA Text Spoken</cell></row><row><cell>SpokenS-test</cell><cell>EM F1</cell><cell>37.02 50.9</cell><cell>44.45 57.6</cell><cell>41.16 54.51</cell><cell>49.07 61.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison experiments demonstrating that the proposed approaches improved EM/F1 scores over ASR transcriptions. All experiments were conducted with BiDAF, which originally take word and character sequence embedding as inputs.</figDesc><table><row><cell>MODEL</cell><cell>EM</cell><cell>F1</cell></row><row><cell>WORD+CHAR</cell><cell>(a) 37.02</cell><cell>50.9</cell></row><row><cell>WORD+CHAR+Dropout</cell><cell cols="2">(b) 38.83 53.07</cell></row><row><cell>WORD+PHONEME+Dropout</cell><cell cols="2">(c) 39.82 53.76</cell></row><row><cell cols="3">WORD+SYLLABLE+Dropout (d) 39.71 53.72</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison of of BiDAF with various embeddings over Spoken SQuAD testing set 29.28 43.21 0.2922 20.07 33.16 0.2258 WORD+PHONEME (c) 45.58 58.25 0.3818 29.09 43.56 0.2899 20.31 33.42 0.2253 WORD+SYLLABLE (d) 45.61 58.25 0.3824 29.37 43.46 0.2974 20.23 33.53 0.2316 WORD+CHAR+PHONEME+SYLLABLE (e) 45.78 58.71 0.3846 29.73</figDesc><table><row><cell>MODEL</cell><cell>EM</cell><cell>No noise F1</cell><cell>AOS</cell><cell>EM</cell><cell>NoiseV1 F1</cell><cell>AOS</cell><cell>EM</cell><cell>NoiseV2 F1</cell><cell>AOS</cell></row><row><cell>WORD</cell><cell cols="9">(a) 44.34 57.37 0.3775 28.64 42.35 0.2915 19.82 32.89 0.2287</cell></row><row><cell>WORD+CHAR</cell><cell>(b) 44.45</cell><cell>57.6</cell><cell cols="3">0.3772 44.2</cell><cell cols="4">0.2967 20.66 33.86 0.2295</cell></row><row><cell>4.2. Training on ASR Transcriptions</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">We further trained BiDAF and Dr.QA on either SQuAD training</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">set (text documents) or Spoken SQuAD training set (ASR tran-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">scriptions), and tested on the testing set of Spoken SQuAD. The</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">results are in Table 3. The results for training on text docuemtns</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">are labelled with Text, while training on ASR transcriptions are</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">labelled as Speech. The scale of SQuAD training data is almost</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">two times of Spoken SQuAD training data because some docu-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ments were removed 2 . However, when testing documents have</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">ASR errors, models trained on ASR transcriptions are remark-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ably better than on text documents.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Spoken SQuAD dataset : https://github.com/chiahsuan156/Spoken-SQuAD</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In these documents, the correct answers do not exist in the ASR transcriptions due to ASR errors, so they cannot be used to train BiDAF and Dr.QA.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R C</forename><surname>Umbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Borr?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Villodre</surname></persName>
		</author>
		<title level="m">Spoken question answering</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Factoid question answering for spoken documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Umbert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Overview of qast</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mostefa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of the Cross-Language Evaluation Forum for European Languages</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="314" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sibyl, a factoid question-answering system for spoken documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Comas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turmo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>M?rquez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Using recurrent neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="530" to="539" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Exploring the use of attention-based recurrent neural networks for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Del?glise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Spoken Language Understanding and Interaction NIPS 2015 workshop (SLU-NIPS 2015)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-domain joint semantic frame parsing using bi-directional rnn-lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y.</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="715" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Sequential convolutional neural networks for slot filling in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07783</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint discriminative decoding of words and semantic tags for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deoras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarikaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1612" to="1621" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beyond asr 1-best: Using word confusion networks in spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>B?chet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="514" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving spoken language understanding using word confusion networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riccardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hakkani-T?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Spoken Language Processing</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Abstractive headline generation for spoken content by attentive recurrent neural networks with asr error modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="151" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spoken question answering using tree-structured conditional random fields and two-layer random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-R</forename><surname>Shiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fifteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acoustic word embeddings for asr error detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Del?glise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1330" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining continuous word representation and prosodic features for asr error prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Esteve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dutrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Santiago</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Adda-Decker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Statistical Language and Speech Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="84" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Asr error detection using recurrent neural network language model and complementary asr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Tam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="page" from="2312" to="2316" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Asr error detection in a conversational spoken language translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="7418" to="7422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving the understanding of spoken referring expressions through syntactic-semantic and contextual-phonetic error-correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Zukerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Partovi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="284" to="310" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asr error segment localization for spoken recovery strategy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>B?chet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Favre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6837" to="6841" />
		</imprint>
	</monogr>
	<note>2013 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Asr error management for improving spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simonnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghannay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Camelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Est?ve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R. De</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09515</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning knowledge graphs for question answering through conversational dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hixon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="851" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Towards machine comprehension of spoken content: Initial toefl listening comprehension test by machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-H</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y.</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06378</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical attention model for improved machine comprehension of spoken content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ieee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="232" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sphinx-4: A flexible open source framework for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gouvea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Woelfel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gated selfmatching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reinforced mnemonic reader for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<idno>abs/1705.02798</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Fusionnet: Fusing via fully-aware attention with application to machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07341</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reading wikipedia to answer open-domain questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Character-aware neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2741" to="2749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Better word representations with recursive neural networks for morphology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth Conference on Computational Natural Language Learning</title>
		<meeting>the Seventeenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Compositional morphology for word representations and language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Botha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1899" to="1907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowledge-powered deep learning for word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="132" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Phoneme embedding and its application to speech driven talking avatar synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1472" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
