<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Colar: Effective and Efficient Online Action Detection by Consulting Exemplars</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation</orgName>
								<orgName type="institution">Northwestern Polytechnical University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Colar: Effective and Efficient Online Action Detection by Consulting Exemplars</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Code is available at https://github.com/VividLe/Online-Action-Detection.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T11:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Online action detection has attracted increasing research interests in recent years. Current works model historical dependencies and anticipate the future to perceive the action evolution within a video segment and improve the detection accuracy. However, the existing paradigm ignores category-level modeling and does not pay sufficient attention to efficiency. Considering a category, its representative frames exhibit various characteristics. Thus, the category-level modeling can provide complimentary guidance to the temporal dependencies modeling. This paper develops an effective exemplar-consultation mechanism that first measures the similarity between a frame and exemplary frames, and then aggregates exemplary features based on the similarity weights. This is also an efficient mechanism, as both similarity measurement and feature aggregation require limited computations. Based on the exemplarconsultation mechanism, the long-term dependencies can be captured by regarding historical frames as exemplars, while the category-level modeling can be achieved by regarding representative frames from a category as exemplars. Due to the complementarity from the categorylevel modeling, our method employs a lightweight architecture but achieves new high performance on three benchmarks. In addition, using a spatio-temporal network to tackle video frames, our method makes a good trade-off between effectiveness and efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the development of mobile communications, video has become a powerful medium to record life and transform information. As a result, video understanding technologies have aroused increasing research interests. Among these technologies, temporal action detection <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b61">62]</ref> can discover action instances from untrimmed videos and extract valuable information. Well-performed action detec- <ref type="bibr">Figure 1</ref>. Comparison between existing state-of-the-art method OadTR <ref type="bibr" target="#b41">[42]</ref> and our proposed Colar. Unlike OadTR, Colar consults historical exemplars to model long-term dependencies and consults category exemplars to capture category-level particularity, forming an effective and efficient method. tion algorithms can benefit smart surveillance <ref type="bibr" target="#b31">[32]</ref>, anomaly detection <ref type="bibr" target="#b3">[4]</ref> etc. In recent years, along with action detection technologies becoming mature, a more challenging but more practical task, namely online action detection, has been proposed <ref type="bibr" target="#b7">[8]</ref>. The online action detection algorithm tackles a streaming video, reports the occurrence of an action instance, and keeps alarming until the action ends <ref type="bibr" target="#b7">[8]</ref>. In inference, the algorithm only employs historical frames that have been observed, but has no access to future frames.</p><p>As an early exploration, Geest et al. <ref type="bibr" target="#b7">[8]</ref> discovered the importance of modeling long-term dependencies. Later, Xu et al. <ref type="bibr" target="#b44">[45]</ref> revealed the value of anticipating future status to enhance the long-term dependencies modeling. OadTR <ref type="bibr" target="#b41">[42]</ref> recently utilized the multi-head self-attention module to jointly model historical dependencies and anticipate the future, which achieved promising online action detection results.</p><p>As an under-explored domain, there are three core challenges for online action detection: How to model long-term dependencies? How to associate a frame with representative frames from the same category? How to conduct detection efficiently? Existing works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> primarily focus on the long-term dependencies modeling, but ignore the other two challenges. However, as shown in <ref type="figure">Figure 1</ref> (a), both analyzing historical frames and anticipating future status only model relationships within a video segment, leaving the category-level modeling under-explored. Because an action category contains multiple instances and each instance exhibits special appearance and motion characteristic, the guidance of exemplary frames can make the online detection algorithm more robust to resist noises within a video segment. In addition, a practical online action detection algorithm should always consider the computational efficiency, including both the efficiency to perform online detection and the efficiency to extract video features.</p><p>This paper develops an exemplar-consultation mechanism to tackle above three challenges in a unified framework. The exemplar-consultation mechanism first jointly transforms a frame and its exemplary frames to the key space and value space. Then, it measures the similarity in the key space and employs the similarity to aggregate information in the value space. As both feature transformation and similarity measurement require limited computations, the proposed exemplar-consultation mechanism is efficient. Considering a video segment, we can effectively model long-term dependencies by using historical frames as exemplars based on the exemplar-consultation mechanism. As we only compare one frame with its historical frames, rather than performing self-attention on all frames, the computational burden is alleviated. Similarly, we can also regard representative frames of each category as exemplars and conduct category-level modeling based on the exemplar-consultation mechanism. Compared with a video segment, category exemplars can provide complementary guidances and make the algorithm more robust.</p><p>By consulting exemplars, we build a unified framework, namely Colar, to perform online action detection, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Colar maintains the dynamic branch and the static branch in parallel, where the former models long-term dependencies within a video segment and the latter models category-level characteristics. In the dynamic branch, Colar consults previous frames and aggregates historical features. In the static branch, Colar first obtains category exemplars via clustering, then consults exemplars and aggregate category features. Finally, two classification scores are fused to detect actions. Moreover, we analyze the running time bottleneck of existing works and discover the expensive costs to extract flow features. Thus, we employ a spatio-temporal network to only dispose of video frames and perform endto-end online action detection, which only takes 9.8 seconds to tackle a one-minute video. To sum up, this paper makes the following contributions:</p><p>? We make an early attempt to conduct category-level modeling for the online action detection task, which provides holistic guidance and makes the detection algorithm more robust.</p><p>? We propose the exemplar-consultation mechanism to compare similarities and aggregate information, which can efficiently model long-term dependencies and category particularities.</p><p>? Due to the effectiveness of the exemplar-consultation mechanism and the complimentary guidance from category-level modeling, our method employs a lightweight architecture. Still, it achieves superior performance and builds new state-of-the-art performance on three benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Modeling temporal dependencies. Different from imagebased task, e.g. detection <ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b47">48]</ref>, localization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref> and segmentation <ref type="bibr" target="#b55">[56]</ref>, it is crucial to model temporal dependencies for online action detection. Existing works rely on recurrent networks, including both LSTMbased methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b49">50]</ref> and GRU-based methods <ref type="bibr" target="#b10">[11]</ref>. Specifically, Geest et al. <ref type="bibr" target="#b8">[9]</ref> proposed a two-stream LSTM <ref type="bibr" target="#b16">[17]</ref> network. Similarly, TRN <ref type="bibr" target="#b44">[45]</ref> employed LSTM blocks to model historical temporal dependencies. Recently, OadTR <ref type="bibr" target="#b41">[42]</ref> drove the recurrent-network paradigm into a transformer-based paradigm and effectively captured the long-term relationship via self-attention. Although OadTR <ref type="bibr" target="#b41">[42]</ref> effectively models long-term dependencies, the self-attention process for all frames leads to the computational burden problem. This work regards historical frames as exemplars and utilizes the exemplar-consultation mechanism to model long-term dependencies. Anticipating future. Although online action detection algorithms cannot access future frames, anticipating future features can assist the decision of current frame. In RED <ref type="bibr" target="#b21">[22]</ref>, Gao et al. estimated features for future frames and calculated the classification loss and the feature regression loss to improve the anticipation quality, which is further developed by TRN <ref type="bibr" target="#b44">[45]</ref> and OadTR <ref type="bibr" target="#b41">[42]</ref>. In this paper, the static branch employs the exemplar-consultation mechanism to compare a frame with representative exemplars of each category and brings complementary information to the dynamic branch.</p><p>Offline action detection. The offline action detection algorithm aims to discover action instances from untrimmed videos <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, where all video frames can be utilized. Some algorithms <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref> tackled video frames to perform localization. In addition, a majority of works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b61">62]</ref> first extracted video features from powerful backbone networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>, then performed action localization based on video features. From the view of anchor mechanism, the representative works include anchor-based methods <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b61">62]</ref> and anchor-free methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b46">47]</ref>. Besides, multiple effective modules have been proposed, e.g. graph convolutional module <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref>. Moreover, action detection under the weakly supervised setting <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b57">58]</ref> was also well explored. The primary difference between online action detection and offline action detection algorithms lies in whether future frames can be accessed. In offline algorithms, Xu et al. <ref type="bibr" target="#b43">[44]</ref> performed data augmentation via playing the video in reverse order, while Zhu <ref type="bibr" target="#b61">[62]</ref> modeled the relationship among multiple proposals within a video. However, these procedures are unsuitable for the studied online action detection task. Space-time memory network. Oh et al. <ref type="bibr" target="#b32">[33]</ref> proposed the space-time memory network to efficiently connect a frame and its previous frames via space-time memory read. It has verified effective performance on modeling temporal information, and has been extended to multiple tasks, e.g. video object detection <ref type="bibr" target="#b5">[6]</ref>, video object segmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b30">31]</ref>, tracking <ref type="bibr" target="#b24">[25]</ref>. In contrast to the space-time memory network, our static branch employs the exemplarconsultation mechanism to model the intra-category relationship. Specifically, it first aggregates particular features from each category, and then combines multiple features to obtain the category feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Given a video stream, the online action detection algorithm should report the occurrence once the action starts and keep alarming until the action ends. The learning process is guided by the frame-level classification label y = [y 0 , y 1 , ..., y C ], where y c ? {0, 1} indicates whether frame f 0 belongs to the c th category. As shown in <ref type="figure" target="#fig_0">Figure  2</ref>, we first employ a backbone network to extract video features. Then, we propose the dynamic branch to model longterm dependencies within a segment and propose the static branch to capture the holistic particularity for each category. Finally, two detection results are fused to perform the online action detection task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dynamic branch</head><p>As neighboring frames can provide rich contextual cues to determine the category label of the current frame, the core idea of the dynamic branch is to model local evolution by comparing a frame with its previous historical frames and dynamically aggregating the local features. The upper part of <ref type="figure" target="#fig_0">Figure 2</ref> exhibits the detailed operations in the dynamic branch. Compared with the standard multi-head self-attention mechanism of OadTR <ref type="bibr" target="#b41">[42]</ref>, our proposed dynamic branch makes two reasonable designs, which sufficiently benefit the online action detection task. First, we use temporal convolution with kernel size 3 to model local cues among historical frames, which is complementary to the global modeling of self-attention. Second, we make two simplifications over OadTR, i.e. removing the class to-ken and replacing multi-head self-attention with one-head attention on the current frame. The simplifications reduce learning difficulty and benefit the performance when training data is not rich enough.</p><p>Given a video feature sequence, we first transform a feature f t to the key space and the value space, where the former is responsible for comparing similarity, and the latter can be used for feature aggregation.</p><formula xml:id="formula_0">f k t = ? k (f t ), f v t = ? v (f t ),<label>(1)</label></formula><p>where ? k and ? v indicate two convolutional layers in the dynamic branch. Then, we measure the pair-wise affinity between f k 0 and other key features (e.g. f k t ) via calculating the cosine similarity:</p><formula xml:id="formula_1">? t = cos(f k 0 , f k t ) = f k 0 ? f k t ||f k 0 || ? ||f k t || .<label>(2)</label></formula><p>Given a series of affinity values [? ?T , ..., ? ?1 , ? 0 ], we perform softmax normalization and obtain the attention mask [? ?T , ...,? ?1 ,? 0 ]. As each element ? t indicates the similarity between the previous t th frame and the current frame, we can aggregate value features among previous frames and obtain the historical feature f :</p><formula xml:id="formula_2">f = 0 t=?T? t ? f v t .<label>(3)</label></formula><p>In the end, the dynamic branch jointly considers value feature f v 0 and the historical feature f (e.g. via summation) and conduct online action detection:</p><formula xml:id="formula_3">s d = ? d (f v 0 , f |? d ),<label>(4)</label></formula><p>where ? d is the classifier in the dynamic branch with parameter ? d , and s d ? R C+1 is the classification score from the dynamic branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Static branch</head><p>Considering action instances from the same category, some instances with distinctive appearance characteristics and clear motion patterns can be selected as exemplars to represent this category. We employ the K-means clustering algorithm for each category, carry out clustering, and obtain M exemplary features. On this basis, the online action detection task can be formulated as comparing a frame with representative exemplars of each category. As a result, the static branch can provide complementary cues to the dynamic branch and makes the online detection algorithm robust to noises within the local video segment.</p><p>Before stepping to detailed operations in the static branch, it is necessary to analyze its efficiency. First, using another branch increases a certain computation. However, compared with OadTR <ref type="bibr" target="#b41">[42]</ref>, we not only simplify the attention computation but also remove decoder layers. Thus, our holistic computation is smaller than OadTR <ref type="bibr" target="#b41">[42]</ref>, and we require less memory as well (see experiments in Sec.4.2). In addition, even given a dataset with millions of samples and thousands of categories, the modern implementation <ref type="bibr" target="#b22">[23]</ref> of the K-Means algorithm can still efficiently generate exemplars, as verified by DeepCluster <ref type="bibr" target="#b1">[2]</ref>.</p><p>As shown in the bottom part of <ref type="figure" target="#fig_0">Figure 2</ref>, the static branch operates with the category exemplars {E c = [e c,1 , e c,2 , ..., e c,M ]} C c=0 to classify feature f 0 , where each category contains M representative exemplars. At first, we convert each exemplar e c,i to the key space and the value space:</p><formula xml:id="formula_4">e k c,i = ? k (e c,i ), e v c,i = ? v (e c,i ),<label>(5)</label></formula><p>and convert the frame feature f 0 to the key space and value space as well:</p><formula xml:id="formula_5">e k 0 = ? k (f 0 ), e v 0 = ? v (f 0 ),<label>(6)</label></formula><p>where ? k , ? v , ? k and ? v indicate convolutional layers. In the key space, we can measure the similarity between feature f 0 and exemplar E c from the c th category:</p><formula xml:id="formula_6">? c,i = cos(e k 0 , e k c,i ) = e k 0 ? e k c,i e k 0 ? e k c,i .<label>(7)</label></formula><p>Based on the pair-wise similarity between e k 0 and all exemplars [e c,1 , e c,2 , ..., e c,M ] from the c th category, we can first calculate the attention mask [? c,1 , ...,? c,M ] via softmax normalization, and then aggregate all exemplars to represent the current frame from the perspective of the c th category:</p><formula xml:id="formula_7">e c = M i=1? c,i ? e v c,i .<label>(8)</label></formula><p>After comparing the current frame with representative exemplars of all categories, we obtain category-specific features [e 0 , e 1 , ..., e C ]. Considering a feature from the c th category, it would be similar to exemplars from the c th category while be different from other exemplars. Thus, we use a convolutional layer to estimate the attention weight a ? R C+1 and aggregate category feature e:</p><formula xml:id="formula_8">e = C c=0 a c ? e c .<label>(9)</label></formula><p>The exemplary feature e is generated from all exemplars and can reveal the category characteristics. In the end, the static branch employs both value feature e v 0 and category feature e to predict the classification score s s :</p><formula xml:id="formula_9">s s = ? s (e v 0 , e|? s ),<label>(10)</label></formula><p>where ? s is the classifier with parameter ? s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Efficient online action detection</head><p>Given a series of pre-extracted video features, the dynamic branch connects a frame with its historical neighbors and models local evolution, while the static branch compares a frame with representative exemplars and models category particularity. It is convenient to fuse the predictions of two branches and detect actions online. However, the feature extraction process, especially calculating optical flows, requires heavy computations, which prevents us from conducting online action detection in practical scenarios.</p><p>To alleviate the computational burdens, we can employ a spatio-temporal network to tackle video frames and provide representative features for the dynamic and the static branch. Considering the video recognition performance and calculation efficiency, we utilize the ResNet-I3D network <ref type="bibr" target="#b40">[41]</ref>, discard the last classification layer, and construct our feature extraction backbone. Given a video sequence with T frames, the output of the backbone network is x ? R D?T /8 , where D indicates the feature dimension. In practice, as the benchmark datasets contain limited training videos, we find frozen the first three blocks can produce more accurate detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and inference</head><p>Given a frame, the dynamic branch and the static branch predict its classification score s d and s s , respectively. We calculate the cross-entropy loss to guide the learning process:</p><formula xml:id="formula_10">L d cls = ? C c=0 y c log(? c d ), L s cls = ? C c=0 y c log(? c s ),<label>(11)</label></formula><p>where? c d and? c s indicate scores after softmax normalization. Besides, as the dynamic and static branches tackle the same frame, two classification scores should be consistent. Thus, we introduce the consistency loss L cons to enable mutual guidance among two branches:</p><formula xml:id="formula_11">L cons = L KL (? d ? s ) + L KL (? s ? d ),<label>(12)</label></formula><p>where L KL indicates the KL-divergence loss. As verified by Zhang et al. <ref type="bibr" target="#b56">[57]</ref>, the consistency loss can lead to a robust model with better generalization. To sum up, the training process is guided by the following loss:</p><formula xml:id="formula_12">L = L d cls + L s cls + ?L cons ,<label>(13)</label></formula><p>where ? is a trade-off parameter.</p><p>In inference, the dynamic classification score s d and the static classification score s s are fused via a balance coefficient ? to perform online action detection: </p><formula xml:id="formula_13">s = ?? s + (1 ? ?)? d .<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setups</head><p>Dataset. We carry out experiments on three widely used benchmarks, THUMOS14 <ref type="bibr" target="#b20">[21]</ref>, TVSeries <ref type="bibr" target="#b7">[8]</ref> and HDD <ref type="bibr" target="#b33">[34]</ref>. THUMOS14 <ref type="bibr" target="#b20">[21]</ref> includes sports videos from 20 action categories, where the validation set and test set contain 200 and 213 videos, respectively. On THUMOS14, challenges for online action detection include drastic intracategory varieties, motion blur, short action instances etc. We follow previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, train the model on the validation set, and evaluate performance on the test set.</p><p>TVSeries <ref type="bibr" target="#b7">[8]</ref> collects about 16 hours of videos from 6 popular TV series. The dataset contains 30 daily actions, where the total instance number is 6231. The TVSeries dataset exhibits some challenging characteristics, e.g. temporal overlapping action instances, a large proportion of background frames and unconstrained perspectives.</p><p>HDD <ref type="bibr" target="#b33">[34]</ref> contains 104 hours of human driving video, belonging to 11 action categories. The videos were collected from 137 driving sessions using an instrumented vehicle equipped with different sensors. Following existing works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, we use 100 sessions for training and 37 sessions for testing.</p><p>Metric. We adopt mean average precision (mAP) and calibrated mean average precision (cmAP) to measure the performance of online action detection algorithms. As for mAP, we first collect classification scores for all frames and then calculate precision and recall based on sorted results. Afterward, we calculate interpolated average precision to obtain AP scores for a category and finally regard the mean  <ref type="bibr" target="#b7">[8]</ref> proposed to calibrate the mAP score. In particular, we first calculate the ratio w between background frames and action frames and then calculate the calibrated precision as:</p><formula xml:id="formula_14">cP re(i) = w ? T P (i) w ? T P (i) + F P (i) .<label>(15)</label></formula><p>Afterward, the calibrated average precision cAP for a category can be calculated as:</p><formula xml:id="formula_15">cAP = i cP re(i) ? 1(i) i 1(i) ,<label>(16)</label></formula><p>where 1(?) indicates whether the i th frame belongs to the considered action category. Finally, cmAP can be obtained via calculating the mean value among all cAPs. Implementation details. Following previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref>, we first conduct experiments with pre-extracted features. The feature extractor uses the two-stream network <ref type="bibr" target="#b42">[43]</ref>, whose spatial stream adopts ResNet-200 <ref type="bibr" target="#b15">[16]</ref> and temporal stream adopts BN-Inception <ref type="bibr" target="#b19">[20]</ref>. We report two experiments where the two-stream network <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b42">43]</ref> is trained on the ActivityNet v1.3 dataset <ref type="bibr" target="#b0">[1]</ref> or the Kinetics-400 <ref type="bibr" target="#b2">[3]</ref> dataset to verify the generalization of the proposed Colar method. As for end-to-end online action detection, our backbone network is based on the ResNet50-I3D architecture <ref type="bibr" target="#b40">[41]</ref>, where the last average pooling layer and classification layer are removed. The ResNet50-I3D network is pretrained on Kinetics-400 <ref type="bibr" target="#b2">[3]</ref> dataset, and we use the weight file provided by MMAction2 <ref type="bibr" target="#b6">[7]</ref>. In training, we freeze the first three blocks of the backbone network. Video frames are extracted with a frame rate of 25fps, where the spatial size is set as 224?224. We use the Adam <ref type="bibr" target="#b23">[24]</ref> algorithm to optimize the whole network and set the batchsize as 16. The initial learning rate is 3 ? 10 ?4 and decays every five epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison experiments</head><p>Quantitative comparisons. We make a comparison with current state-of-the-art methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> and consistently build new high performance on THUMOS14 <ref type="bibr" target="#b20">[21]</ref>, TVSeries <ref type="bibr" target="#b7">[8]</ref>, and HDD <ref type="bibr" target="#b33">[34]</ref> benchmarks. As shown in Table 1, based on TSN-ActivityNet features, our Colar brings an mAP gain of 1.1% over OadTR <ref type="bibr" target="#b41">[42]</ref>, and the improvements would be an mAP of 1.7% if the comparison is based on TSN-Kinetics features. The consistent improvements </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setups</head><p>Method Portion of actions 0-0.1 0.1-0.2 0.2-0.3 0.3-0.4 0.4-0.5 0.5-0.6 0.6-0.7 0.7-0.8 0.8-0.9 0.9-1 over current state-of-the-art methods verify the efficacy of our proposed exemplar-consultation mechanism. In addition, the proposed Colar can directly tackle video frames and perform online action detections, which achieves 58.6% mAP. In addition to THUMOS14, experiments on TVSeries <ref type="bibr" target="#b7">[8]</ref> and HDD <ref type="bibr" target="#b33">[34]</ref> benchmarks also verify the superiority of our method, as shown in <ref type="table" target="#tab_1">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref>.</p><p>Effectiveness and efficiency. <ref type="table">Table 4</ref> analyzes the performance and running time under different setups. When using pre-extracted features, Colar performs superior to existing methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b41">42]</ref>. It is worth noting that extracting RGB and flow features takes 46.5 seconds, of which calculating optical flow costs the majority of the time. When only flow features or RGB features are available, the feature extraction cost is reduced, but both OadTR <ref type="bibr" target="#b33">[34]</ref> and our Colar observe performance drop. In particular, it costs 44.2s to extract flow features, where OadTR <ref type="bibr" target="#b33">[34]</ref> and Colar observe 7.4% and 7.3% performance drops, respectively. The cost to extract RGB features is small, but the online detection performance decreases a lot.</p><p>Given the ResNet50-I3D network <ref type="bibr" target="#b40">[41]</ref>, we first extract features from video frames and then train the proposed Colar method, which gets 53.4%. In contrast, the proposed end-to-end learning paradigm achieves 58.8%. To sum up, our proposed Colar method achieves a good balance between effectiveness and efficiency. Given pre-extracted features, Colar makes accurate detection results. Given only video frames, Colar costs 9.8 seconds to tackle a oneminute video and achieves comparable performance. In addition, we measure the memory cost under identical setups, where Colar requires 2235M memory and OadTR <ref type="bibr" target="#b41">[42]</ref> requires 4375M memory.</p><p>Performance under different action portions. <ref type="table" target="#tab_3">Table 5</ref> elaborately studies the online action detection performance when different action portions are observed. The proposed Colar achieves promising accuracy when using the TSN-ActivityNet feature, the TSN-Kinetics feature, and only video frames. In particular, considering the most severe cases that only the first 10% portion of actions are observed, the previous state-of-the-art method OadTR <ref type="bibr" target="#b41">[42]</ref> shows inferior performance to IDU <ref type="bibr" target="#b10">[11]</ref>. However, the proposed Colar consistently exceeds OadTR, due to that the static branch effectively connects a frame with representative exemplars of each category and provides complimentary guidance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation experiments</head><p>Efficacy of each component. The proposed Colar method consists of the dynamic branch and the static branch, as well as a consistency loss L cons to enable mutual guidance between two branches.   <ref type="figure" target="#fig_1">Figure 3</ref> (a), we first study the influence of temporal scope T in modeling temporal dependencies and find 64 is a proper choice for the dynamic branch. The too-short temporal scope is insufficient to perceive evolvement within a video segment, while too long temporal scope would bring noises. In addition, we vary the number of convolutional layers and choose two layers, as shown in <ref type="figure" target="#fig_1">Figure 3 (b)</ref>. Ablations about the static branch. Based on K-Means clustering, the number of exemplars is an influential parameter for the static branch. As shown in <ref type="figure" target="#fig_1">Figure 3</ref> (c), the ability of limited exemplars is insufficient and overwhelming exemplars would damage the performance as well.</p><p>Ablations about the complete method. Given the complete method, <ref type="figure" target="#fig_1">Figure 3</ref> (d) studies the performance under different feature channels and verifies 1024 is a proper choice. <ref type="figure" target="#fig_1">Figure 3</ref> (e) studies the coefficient ? for the consistency loss in the training phase, while <ref type="figure" target="#fig_1">Figure 3</ref> (f) verifies the influence of coefficient ? in the inference phase. We find ? = 1 and ? = 0.3 are proper choices.  <ref type="figure" target="#fig_2">Figure 4</ref> (a) exhibits the static and dynamic scores within a video segment. Because the Volleyball Spiking instance shows dramatic viewpoint changes, the dynamic branch predicts low confident scores for some unique frames (shown in the yellow dotted box). In contrast, the static branch consults representative exemplars from the Volleyball Spiking category and consistently predicts high scores for these unique frames. <ref type="figure" target="#fig_2">Figure 4</ref>   segment containing multiple action instances, exhibits the similarity between the current frame (the last one) and its historical frames. The similarity weights clearly highlight historical action frames and suppress background frames, which contributes to aggregating temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Qualitative analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b) presents a video</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes Colar, based on the exemplarconsultation mechanism, to conduct category-level modeling for each frame and capture long-term dependencies within a video segment. Colar compares a frame with exemplar frames, aggregates exemplar features, and carries out online action detection. In the dynamic branch, Colar regards historical frames as exemplars and models long-term dependency with a lightweight network structure. In the static branch, Colar employs representative exemplars of each category and captures the category particularity. The prominent efficacy of Colar would inspire future works to pay attention to category-level modeling. In addition, as Colar has made a good trade-off between effectiveness and efficiency, it is a promising direction to conduct online action detection directly from streaming video data, which can benefit practical usage. Limitations. Because Colar is only verified on the benchmark datasets, it may observe performance drop in practical scene due to new challenges, e.g. long-tail distribution, open-set action categories. Besides, the unintended usage of Colar for surveillance may violate individual privacy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Framework of the proposed Colar method for online action detection. Given a video, the dynamic branch compares a frame with its historical exemplars and models temporal dependencies, while the static branch compares a frame with category exemplars and captures the category particularity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Ablation studies about hyper-parameters in the proposed Colar method, measured by mAP (%) on THUMOS14 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4</head><label>4</label><figDesc>qualitatively analyzes the proposed Colar method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Detection scores from dynamic branch and static branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative analysis of the proposed Colar method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison experiments on THUMOS14 dataset, measured by mAP (%).</figDesc><table><row><cell>Setups</cell><cell>Method</cell><cell>mAP(%)</cell></row><row><cell></cell><cell>CNN [38]ICLR15</cell><cell>34.7</cell></row><row><cell></cell><cell>CNN [37]NIPS14</cell><cell>36.2</cell></row><row><cell>Offline</cell><cell>LRCN [10]CVPR15</cell><cell>39.3</cell></row><row><cell></cell><cell>MultiLSTM [49]IJCV18</cell><cell>41.3</cell></row><row><cell></cell><cell>CDC [35]CVPR17</cell><cell>44.4</cell></row><row><cell></cell><cell>RED [22]BMVC17</cell><cell>45.3</cell></row><row><cell></cell><cell>TRN [45]ICCV19</cell><cell>47.2</cell></row><row><cell>Online</cell><cell>IDU [11]CVPR20</cell><cell>50.0</cell></row><row><cell>(TSN-Anet)</cell><cell>OadTR [42]ICCV21</cell><cell>58.3</cell></row><row><cell></cell><cell>Colar</cell><cell>59.4</cell></row><row><cell></cell><cell>IDU [11]CVPR20</cell><cell>60.3</cell></row><row><cell>Online</cell><cell>OadTR [42]ICCV21</cell><cell>65.2</cell></row><row><cell>(TSN-Kinetics)</cell><cell>Colar</cell><cell>66.9</cell></row><row><cell>RGB end-to-end</cell><cell>Colar</cell><cell>58.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison experiments on TVSeries dataset, measured by mcAP(%).</figDesc><table><row><cell>Setups</cell><cell>Method</cell><cell>mcAP(%)</cell></row><row><cell></cell><cell>LRCN [10]CVPR15</cell><cell>64.1</cell></row><row><cell></cell><cell>RED [22]BMVC17</cell><cell>71.2</cell></row><row><cell>RGB</cell><cell>2S-FN [9]WACV18</cell><cell>72.4</cell></row><row><cell></cell><cell>TRN [45]ICCV19</cell><cell>75.4</cell></row><row><cell></cell><cell>IDU [11]CVPR20</cell><cell>76.6</cell></row><row><cell>Flow</cell><cell>FV-SVM [8]ECCV2016 IDU [11]CVPR20</cell><cell>74.3 80.3</cell></row><row><cell></cell><cell>RED [22]BMVC17</cell><cell>79.2</cell></row><row><cell></cell><cell>TRN [45]ICCV19</cell><cell>83.7</cell></row><row><cell>Online</cell><cell>IDU [11]CVPR20</cell><cell>84.7</cell></row><row><cell>(TSN-Anet)</cell><cell>OadTR [42]ICCV21</cell><cell>85.4</cell></row><row><cell></cell><cell>Colar</cell><cell>86.0</cell></row><row><cell></cell><cell>IDU [11]CVPR20</cell><cell>86.1</cell></row><row><cell>Online</cell><cell>OadTR [42]ICCV21</cell><cell>87.2</cell></row><row><cell>(TSN-Kinetics)</cell><cell>Colar</cell><cell>88.1</cell></row><row><cell>RGB end-to-end</cell><cell>Colar</cell><cell>86.8</cell></row></table><note>value of AP scores among all categories as mAP. Consid- ering the drastically imbalanced frame numbers of different categories, Geest et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison experiments on HDD dataset, measured by mAP (%).</figDesc><table><row><cell cols="2">Setups</cell><cell>Method</cell><cell></cell><cell cols="2">mAP(%)</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">CNN [8]ICLR15</cell><cell></cell><cell>22.7</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">LSTM [34]CVPR18</cell><cell></cell><cell>23.8</cell><cell></cell></row><row><cell cols="2">Sensors</cell><cell cols="2">ED [22]BMVC17</cell><cell></cell><cell>27.4</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">TRN [45]ICCV19</cell><cell></cell><cell>29.2</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">OadTR [42]ICCV21</cell><cell></cell><cell>29.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Colar</cell><cell></cell><cell>30.6</cell><cell></cell></row><row><cell cols="7">Table 4. Comparison between our proposed Colar method and ex-</cell></row><row><cell cols="7">isting methods. The inference time (in second) is measured on</cell></row><row><cell cols="7">a 1080Ti GPU when tackling the same one-minute video. Both</cell></row><row><cell cols="7">"Colar*" and Colar ? directly tackle video frames, where the for-</cell></row><row><cell cols="7">mer uses a fixed backbone and the latter is end-to-end trained.</cell></row><row><cell>Method</cell><cell cols="6">RGB Optical Flow Action Inference mAP Feature Flow Feature Detection Time (%)</cell></row><row><cell cols="7">Given pre-extracted features, Colar is faster and more accurate.</cell></row><row><cell>IDU [11]</cell><cell>2.3</cell><cell>39.8</cell><cell>4.4</cell><cell>52.8</cell><cell cols="2">99.3 60.3</cell></row><row><cell cols="2">OadTR [42] 2.3</cell><cell>39.8</cell><cell>4.4</cell><cell>4.7</cell><cell cols="2">51.2 65.2</cell></row><row><cell>Colar</cell><cell>2.3</cell><cell>39.8</cell><cell>4.4</cell><cell>4.2</cell><cell cols="2">50.7 66.9</cell></row><row><cell>OadTR-Flow</cell><cell>-</cell><cell>39.8</cell><cell>4.4</cell><cell>4.5</cell><cell cols="2">48.7 57.8</cell></row><row><cell>Colar-Flow</cell><cell>-</cell><cell>39.8</cell><cell>4.4</cell><cell>4.0</cell><cell cols="2">48.2 59.6</cell></row><row><cell cols="2">OadTR-RGB 2.3</cell><cell>-</cell><cell>-</cell><cell>4.5</cell><cell>6.8</cell><cell>51.2</cell></row><row><cell>Colar-RGB</cell><cell>2.3</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell>6.3</cell><cell>52.1</cell></row><row><cell cols="7">Given frames, Colar provides a trade-off between speed and accuracy.</cell></row><row><cell>Colar*</cell><cell>5.8</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell>9.8</cell><cell>53.4</cell></row><row><cell>Colar ?</cell><cell>5.8</cell><cell>-</cell><cell>-</cell><cell>4.0</cell><cell>9.8</cell><cell>58.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Detailed online action detection performances under different action portions, measured by mcAP (%) on TVSeries dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies about the efficacy of each component, measured by mAP(%) on three benchmarks.</figDesc><table><row><cell cols="3">Dynamic Static L cons THUMOS14 TVSeries HDD</cell></row><row><cell>65.2</cell><cell>86.3</cell><cell>29.5</cell></row><row><cell>58.1</cell><cell>83.5</cell><cell>26.4</cell></row><row><cell>65.8</cell><cell>86.9</cell><cell>29.9</cell></row><row><cell>66.9</cell><cell>88.1</cell><cell>30.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6</head><label>6</label><figDesc>studies the efficacy of each component on all three benchmark datasets. Firstly, the dynamic branch performs superior to the static branch, demonstrating the necessity of carefully modeling temporal dependencies. Besides, without the consistency loss, directly fusing prediction scores of two branches (e.g. using Eq. (14)) only observes limited improvements, while L cons can further improve the detection performance.Ablations about the dynamic branch. As shown in</figDesc><table><row><cell>mAP (%)</cell><cell>67.0 63.0 65.0</cell><cell>62.3</cell><cell>63.7</cell><cell cols="3">64.9 65.2 64.6</cell><cell>mAP (%)</cell><cell>66.0 64.0</cell><cell>64.2</cell><cell>65.2</cell><cell>64.5</cell><cell cols="2">63.9 63.6</cell></row><row><cell></cell><cell>61.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>62.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>16</cell><cell>32</cell><cell>48</cell><cell>64</cell><cell>80</cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell cols="7">(a) Temporal length T in the dy-</cell><cell cols="7">(b) Number of convolutional lay-</cell></row><row><cell cols="4">namic branch.</cell><cell></cell><cell></cell><cell></cell><cell cols="6">ers in the dynamic branch.</cell><cell></cell></row><row><cell>mAP (%)</cell><cell>58.4 57.6 58.0</cell><cell>57.7</cell><cell>57.8</cell><cell>58.1</cell><cell>58.0</cell><cell>57.8</cell><cell>mAP (%)</cell><cell>67.0 65.0 66.0</cell><cell>64.3</cell><cell>65.4</cell><cell>65.8</cell><cell>66.9</cell><cell>66.3</cell></row><row><cell></cell><cell>57.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>64.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>1</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell></cell><cell></cell><cell>128</cell><cell>256</cell><cell cols="3">512 1024 2048</cell></row><row><cell cols="7">(c) Number of representative fea-</cell><cell cols="7">(d) Channel number for the com-</cell></row><row><cell cols="6">tures in the static branch.</cell><cell></cell><cell cols="5">plete Colar method.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>67.0</cell><cell></cell><cell></cell><cell>66.9</cell><cell></cell><cell></cell><cell></cell><cell>68.0</cell><cell cols="3">66.5 66.9 66.33</cell><cell></cell><cell></cell></row><row><cell>mAP (%)</cell><cell>66.0</cell><cell>66.1</cell><cell>65.8</cell><cell></cell><cell cols="2">66.0 66.1</cell><cell>mAP (%)</cell><cell>62.0 65.0</cell><cell></cell><cell></cell><cell></cell><cell>64.57</cell><cell>60.9</cell></row><row><cell></cell><cell>65.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>59.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell><cell>1.2</cell><cell>1.4</cell><cell></cell><cell></cell><cell>0.1</cell><cell>0.3</cell><cell>0.5</cell><cell>0.7</cell><cell>0.9</cell></row><row><cell cols="7">(e) Influence of coefficient ? in the</cell><cell cols="7">(f) Influence of coefficient ? when</cell></row><row><cell cols="4">loss function.</cell><cell></cell><cell></cell><cell></cell><cell cols="6">fusing prediction scores.</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arindam</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="58" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Relation attention for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Memory enhanced global-local aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="10337" to="10346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Openmmlab&apos;s next generation video understanding toolbox and benchmark</title>
		<idno>2020. 6</idno>
		<ptr target="https://github.com/open-mmlab/mmaction2" />
	</analytic>
	<monogr>
		<title level="m">MMAction2 Contributors</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Roeland De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyang</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cees</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="269" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Modeling temporal structure with lstm for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roeland</forename><surname>De Geest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to discriminate information for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjun</forename><surname>Eun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyoung</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanho</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changick</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Progressive contextual instance refinement for weakly supervised object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="8002" to="8012" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tcanet: Triple context-aware network for weakly supervised object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Saenet: Self-supervised adversarial and equivariant network for weakly supervised object detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strengthen learning tolerance for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7403" to="7412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Scribble-supervised video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/CAA Journal of Automatica Sinica</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="353" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Clrnet: Component-level refinement network for deep face parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TNNLS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<ptr target="http://crcv.ucf.edu/THUMOS14/" />
		<title level="m">THUMOS challenge: Action recognition with a large number of classes</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Red: Reinforced encoder-decoder networks for action anticipation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Billionscale similarity search with gpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TBD</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mast: A memoryaugmented self-supervised tracker</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="6479" to="6488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning salient boundary feature for anchor-free temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3320" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bmn: Boundary-matching network for temporal action proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3889" to="3898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Single shot temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="988" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-shot temporal event localization: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12596" to="12606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end temporal action detection with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qimeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10271</idno>
		<editor>Song Bai, and Xiang Bai</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="661" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">An embedded computer-vision system for multi-object detection in traffic surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ala</forename><surname>Mhalla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thierry</forename><surname>Chateau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sami</forename><surname>Gazzah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Najoua Essoukri Ben</forename><surname>Amara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-ITS</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4006" to="4018" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9226" to="9235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Toward driving scene understanding: A dataset for learning driver behavior and causal reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasili</forename><surname>Ramanishka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teruhisa</forename><surname>Misu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuyuki</forename><surname>Miyazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5734" to="5743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1049" to="1058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Rgb stream is enough for temporal action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxiang</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.04362</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Oadtr: Online action detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengrong</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nong</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Cuhk &amp; ethz &amp; siat submission to activitynet challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00797</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">R-c3d: Region convolutional 3d network for temporal activity detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Temporal recurrent networks for online action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingze</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingfei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Larry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Background-click supervision for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianwei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Revisiting anchor mechanisms for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8535" to="8548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Automatic weakly supervised object detection from high spatial resolution remote sensing images via dynamic curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiwen</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TGRS</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Every moment counts: Dense detailed labeling of actions in complex videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2-4</biblScope>
			<biblScope unit="page" from="375" to="389" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal dynamic graph lstm for action-driven video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1801" to="1810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Marginalized average attentional network for weakly-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueming</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Breaking winner-takesall: Iterative-winners-out networks for weakly supervised temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5797" to="5808" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7094" to="7103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Graph convolutional module for temporal action localization in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhao</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization and detection: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Weakly supervised semantic segmentation via alternative self-dual teaching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lechao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09459</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep mutual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Soda: Weakly supervised temporal action localization based on astute background response and selfdistillation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Binglu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingwen</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">129</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2474" to="2498" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Few-shot common-object reasoning using commoncentric localization network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4253" to="4262" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Temporal cross-layer correlation mining for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehe</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Label independent memory for semi-supervised few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="273" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Enriching local and global contexts for temporal action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Nanning Zheng, and Gang Hua</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
