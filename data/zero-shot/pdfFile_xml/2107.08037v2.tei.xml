<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CCVS: Context-aware Controllable Video Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Le Moing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria and Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL Research University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria and Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL Research University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Inria and Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">ENS</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<orgName type="institution" key="instit3">PSL Research University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CCVS: Context-aware Controllable Video Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This presentation introduces a self-supervised learning approach to the synthesis of new video clips from old ones, with several new key elements for improved spatial resolution and realism: It conditions the synthesis process on contextual information for temporal continuity and ancillary information for fine control. The prediction model is doubly autoregressive, in the latent space of an autoencoder for forecasting, and in image space for updating contextual information, which is also used to enforce spatio-temporal consistency through a learnable optical flow module. Adversarial training of the autoencoder in the appearance and temporal domains is used to further improve the realism of its output. A quantizer inserted between the encoder and the transformer in charge of forecasting future frames in latent space (and its inverse inserted between the transformer and the decoder) adds even more flexibility by affording simple mechanisms for handling multimodal ancillary information for controlling the synthesis process (e.g., a few sample frames, an audio track, a trajectory in image space) and taking into account the intrinsically uncertain nature of the future by allowing multiple predictions. Experiments with an implementation of the proposed approach give very good qualitative and quantitative results on multiple tasks and standard benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Feeding machines with extensive video content, and teaching them to create new samples on their own, may deepen their understanding of both the physical and social worlds. Video synthesis has numerous applications from content creation (e.g., deblurring, slow motion) to human-robot interaction (e.g., motion prediction). Despite the photo-realistic results of modern image synthesis models <ref type="bibr" target="#b37">[38]</ref>, video synthesis is still lagging behind due to the increased complexity of the additional temporal dimension.</p><p>An emerging trend is to use autoregressive models, for example transformer architectures [68], for their simplicity, and their ability to model long-range dependencies and learn from large volumes of data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. First introduced for natural language processing (NLP) and then succesfully applied to visual data <ref type="bibr" target="#b17">[18]</ref>, the strength of transformers is grounded in a self-attention mechanism which considers all pairwise interactions within the data. The price to pay is a computational complexity which grows quadratically with the data size, which itself depends linearly on the temporal dimension and quadratically on the spatial resolution in the image domain. Although there have been some efforts to reduce the complexity of self-attention <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref>, using such methods directly on visual data is still limited to low resolutions and impractical without considerable computational power <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b78">79]</ref>.</p><p>Some recent works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b52">53]</ref> address this problem by using an autoencoder to compress the visual data, and apply the autoregressive model in the latent space. This greatly reduces the memory footprint and computational cost, yet, the greater the compression, the harder it is to faithfully reconstruct frames. The corresponding trade-offs may undermine the practical usability of these approaches. GANs <ref type="bibr" target="#b25">[26]</ref> mitigate this issue by "hallucinating" plausible local details in image synthesis <ref type="bibr" target="#b19">[20]</ref>. But latent video</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>transformers <ref type="bibr" target="#b52">[53]</ref> decode frames independently, which prevents local details from being temporally coherent. Hence, using GANs in this setting may result in flickering effects in the synthesized videos.</p><p>We follow the problem decomposition from <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b52">53]</ref>, but introduce a more elaborate compression strategy with CCVS (for Context-aware Controllable Video Synthesis), an approach that takes advantage of "context" frames (i.e., both input images and previously synthesized ones) to faithfully reconstruct new ones despite lossy compression. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, CCVS relies on optical flow estimation between context and new frames, within temporal skip connections, to let information be shared across timesteps. New content, which cannot be retrieved from context, is synthesized directly from latent compressed features, and adversarial training <ref type="bibr" target="#b25">[26]</ref> is used to make up realistic details. Indeed, information propagates in CCVS to new frames as previously synthesized ones become part of the context. Like other video synthesis architectures based on autoregressive models <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b78">79]</ref>, CCVS can be used in many tasks besides future video prediction. Any data which can be expressed in the form of a fixed-size sequence of elements from a finite set (aka, tokens) can be processed by a transformer. This applies to video frames (here, via compression and quantization) and to other types of data. Hence, one can easily fuse modalities without having to build complex or task-specific architectures. This idea has been used to control image synthesis <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b53">54]</ref>, and we extend it to a variety of video synthesis tasks by guiding the prediction with different annotations as illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. Code, pretrained models, and video samples synthesized by our approach are available at the url https://16lemoing.github.io/ccvs. Our main contributions are as follows:</p><p>1. an optical flow mechanism within an autoencoder to better reconstruct frames from context, 2. the use of ancillary information to control latent temporal dynamics when synthesizing videos, 3. a performance on par with or better than the state of the art, while being more memory-efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Video synthesis. In its simplest form, videos are produced without prior information about their content. GAN-based approaches map Gaussian noise into a visually plausible succession of frames. For example, VGAN <ref type="bibr" target="#b70">[71]</ref> and ProgressiveVGAN <ref type="bibr" target="#b0">[1]</ref>, adapt the traditional GAN framework <ref type="bibr" target="#b25">[26]</ref> from image to video synthesis by simply using 3D instead of 2D convolutions. These approaches, including recent attempts such as G 3 AN <ref type="bibr" target="#b75">[76]</ref>, are computationally expensive, and, by nature, restricted to synthesizing a fixed number of frames due to the constraints of their architecture. Other approaches predict latent motion vectors with a CNN <ref type="bibr" target="#b55">[56]</ref>, or a recurrent neural network (RNN) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">65]</ref>, and generate frames with individual 2D operations. To avoid the shortcomings of information loss in the sequential processing of RNNs, we use an attention-based autoregressive model instead. Since we forecast temporal dynamics in a compressed space, a large temporal window can be used when predicting new frames, without having to resort to expensive 3D computations. Previous works have attempted to scale video synthesis to higher resolution by using progressive training <ref type="bibr" target="#b0">[1]</ref>, subsampling <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b56">57]</ref>, reducing the dimension of the discriminator <ref type="bibr" target="#b35">[36]</ref>, or redefining the task as finding a trajectory in the latent space of a pre-trained image generator <ref type="bibr" target="#b62">[63]</ref>. Compression, together with efficient context-aware reconstruction allows us to synthesize videos at high resolution.</p><p>Controllable video synthesis. Some of the aforementioned works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b75">76]</ref> handle synthesis conditioned on a class label. Another popular control is to use a few priming frames to set off the generation process. This task, known as future video prediction, has received a lot of attention recently. Methods based on variational autoencoders (VAE) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> have been proposed to account for the stochastic nature of future forecasting, i.e., the plurality of possible continuations, disregarded in deterministic predictive models <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b69">70</ref>]. Yet, their blurry predictions have motivated the incorporation of adversarial training <ref type="bibr" target="#b43">[44]</ref>, hierarchical architectures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b79">80]</ref>, fully latent temporal models <ref type="bibr" target="#b21">[22]</ref>, or normalizing flows <ref type="bibr" target="#b41">[42]</ref>. Another line of work infers spatial transformation parameters (e.g., optical flow), and predicts the future by warping past frames (i.e., grid sampling and interpolation as in <ref type="bibr" target="#b32">[33]</ref>) in RGB space <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b71">72,</ref><ref type="bibr" target="#b80">81]</ref> or in feature space <ref type="bibr" target="#b44">[45]</ref>, typically using a refinement step to handle occlusions. Lately, autoregressive methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b78">79]</ref> leveraging a self-attention mechanism <ref type="bibr" target="#b67">[68]</ref> have also been applied to this task. Our method benefits from the context-efficiency of approaches based on spatial transformation modules and the modeling power of autoregressive networks. In the meantime, other forms of control with interesting applications have emerged. Point-to-point generation <ref type="bibr" target="#b74">[75]</ref>, a variant of future video prediction, specifies both start and end frames. State or action-conditioned synthesis <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b47">48]</ref> guides the frame-by-frame evolution with high-level commands. Here X c and X s respectively stand for the (input) context and (output) synthesized video. Learnable encoder and decoder modules E and D are linked by a learnable flow estimation module F ensuring spatio-temporal consistency between context and synthesized frames. The architecture is doubly autoregressive, with a transformer T responsible for predicting the features Z s associated with future frames X s from the features Z c associated with context frames X c , and a simple, parameterless, "shift-and-add" module S updating X c as each new frame is generated. The architecture is trained in two steps: The parameters of E, D and F are first estimated (without any future forecasting from the transformer) in an adversarial manner using two discriminators ? i and ? t to ensure that the frames synthesized are both realistic (? i ) and temporally coherent (? t ). The two discriminators are then discarded, and the parameters of the transformer T are estimated with E, F and D frozen. At inference time, the transformer is used only once, latter frames being estimated in an autoregressive manner by the quadruple D S, E, F. See text for more details.</p><p>Additional works consider video synthesis based on one <ref type="bibr" target="#b50">[51]</ref> or multiple layouts <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b73">74]</ref>, another video <ref type="bibr" target="#b7">[8]</ref>, or sound <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b72">73]</ref>. To account for the variety of potential controls, we leverage the flexibility of transformers, and propose an unifying approach to tackle all of these tasks.</p><p>3 Context-aware controllable video synthesis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of the proposed approach</head><p>We consider video data, such that an individual frame x is natively an element of X = R H?W ?3 , which can be encoded as some feature z in Z = R h?w?F with reduced spatial resolution and increased number of channels. We assume that we are given X c in X m corresponding to m successive frames, and our goal is to synthesize a plausible representation of the following n frames X s which lies in X n . Given X c , we can compute the corresponding features Z c in Z m using some encoder E : X ? Z on each frame individually, then use an autoregressive model (a transformer coupled with a quantization step in our case) to predict features Z s in Z n formed by the n features corresponding to time ticks m + 1 to m + n. (Note that the values of m and n can be arbitrary, using [by now] traditional masking and sliding window techniques.) These features can finally be converted one by one into the corresponding frames X s using some decoder D : Z ? X .</p><p>The overall approach is illustrated by <ref type="figure" target="#fig_0">Figure 1</ref>. The use of a quantizer over a learned codebook in our implementation complicates the architecture a bit, but has several advantages, including the reuse of familiar NLP technology <ref type="bibr" target="#b67">[68]</ref> and, perhaps more importantly, affording simple mechanisms for handling different types of inputs (from images to sound for example <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31]</ref>) and the intrinsically uncertain and multimodal nature of future prediction (by sampling different codebook elements according to their likelihood <ref type="bibr" target="#b29">[30]</ref>). Concretely, this choice simply amounts to inserting in our architecture, right after the encoder E, a nearest-neighbor quantizer Q : R F ? 1, q parameterized by an F ? q codebook which, given the embedding z of a frame, returns a h ? w matrix of corresponding tokens, that is, the indices of the closest entry in the codebook for feature vector z i,j for all spatial locations (i, j) in 1, h ? 1, w . We abuse notation, and identify Q with its parameterization by this codebook, so we can optimize over Q just as we optimize over E instead of naming explicitly its parameters in the rest of this presentation. An "un-quantizer" U : 1, q ? R F , also parameterized (implicitly) by the codebook and associating with each token the corresponding entry of the codebook, is also inserted right before the decoder D. In this setting, T : 1, q m?h?w ? 1, q n?h?w takes as input a sequence of tokens, and outputs the tokens for subsequent frames, each one of them chosen among q possibilities as either the one with the highest score, or drawn randomly from the top-k scores to account for the multimodal nature of future forecasting (here k ? q is some predefined constant, see <ref type="bibr" target="#b29">[30]</ref> for related approaches).</p><p>The elements of the architecture described so far are by now (individually) rather classical, with learnable, parametric functions E, D, Q and T . Besides putting them all together, and as detailed in the rest of this section, we add several original elements: (a) The encoding/decoding scheme is improved by the use of two discriminators, ? i and ? t respectively, trained in an adversarial manner to ensure that the predicted frames are realistic and temporally consistent. (b) The context frames X c are themselves updated each time a new frame is predicted in an autoregressive manner (iteratively fill the sequence up to some predefined capacity, then shift to the left, forgetting the first frame and adding the latest synthetic one on the right). (c) The encoder and decoder are linked through a learnable flow module F, allowing the context frames to guide the prediction of the synthetic ones.</p><p>(d) Additional control variables, ranging from object trajectories to audio tracks, can be used in the form of sequence-or frame-level annotations to drive the synthesis by adding the corresponding tokens to the ones passed on to the autoregressive model T .</p><p>We detail in this section the concrete components of the approach sketched above, including the autoencoder and quantizer architectures and their training procedure (Section 3.2), and the implementation of the autoregressive model by a transformer <ref type="bibr" target="#b67">[68]</ref>, illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>, which we adapt to account for outside control signals (Section 3.3), once again with the corresponding procedure. Further architectural choices are also detailed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">First stage: training the context-aware autoencoder and the quantizer</head><p>Architecture. E and D respectively decreases and increases the spatial resolution by using convolutional residual blocks <ref type="bibr" target="#b28">[29]</ref>, with (r k ) k? 1;K the K corresponding resolution levels (r k = h k ? w k ). It is common practice to augment, as in U-Net <ref type="bibr" target="#b54">[55]</ref>, the autoencoder with long skip connections between E and D to share information across the two models at these intermediate levels and escape the lossy compression bottleneck. Although we cannot apply this directly to our setting since information only flows through D for predicted timesteps, such skip connections can be established from the encoding stage of a context frame x c to the decoding stage of a new frame x s (resulting from features z s ). Similar mechanisms <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> have been proposed in the past for video synthesis but they are only copying static background features from a single context frame. We follow works on semantic segmentation <ref type="bibr" target="#b22">[23]</ref> and face frontalization <ref type="bibr" target="#b77">[78]</ref> and use a flow module F to warp features and produce temporally consistent outputs despite motion. We extend this to multi-frame contexts, with significant performance gains and no additional parameter to be learned.</p><p>Concretely, let e k c be features being encoded from x c , and d k s features being decoded from z s at a given intermediate resolution r k . We first compute all intermediate context features e k c by applying E to x c . We then progressively decode features d k s for the new frame from low (k = 1) to high resolution (k = K) by iterating over the following steps: (a) apply one decoding sub-module to get (see Appendix A for further details). Temporal skip connections at a given resolution level r k are defined as the following in-place modification of d k s :</p><formula xml:id="formula_0">d k s = ?(m k c ) ? d k s + (1 ? ?(m k c )) ? e k c ,<label>(1)</label></formula><p>where ? is the Sigmoid function, and ? the element-wise product. We note that update rule (1) is quite standard for warping and fusing two streams of spatial information <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b73">74]</ref>. For concrete implementation of F, we build upon LiteFlowNet <ref type="bibr" target="#b31">[32]</ref>, an optical flow estimation model which also combines pyramidal extraction and progressive warping of features. For simple integration into our framework, we use features from E and D both in the mask and flow estimation, and in the update (1). This process readily generalizes to multi-frame contextual information (see Appendix D for details). Similar to spatial transformers <ref type="bibr" target="#b32">[33]</ref>, the warping operation W is differentiable. As a result, gradients from the training losses can backpropagate from D to E through F. This allows end-to-end training of the autoencoder even with information from different timesteps in E and D.</p><p>Training procedure. The global objective is the linear combination of four auxiliary ones:</p><formula xml:id="formula_1">L = ? q L q + ? r L r + ? a L a + ? c L c ,<label>(2)</label></formula><p>namely a quantization loss (L q ), a reconstruction loss (L r ), an adversarial loss (L a ), and a contextual loss (L c ), detailed in the next paragraphs.</p><p>The codebook is trained by minimizing the reconstruction error between encoded features z = E(x) and quantized features z q = U(Q(z)) (with notations introduced in Section 3.1):</p><formula xml:id="formula_2">L q (E, Q) = sg(z) ? z q 2 2 + ? sg(z q ) ? z 2 2 ,<label>(3)</label></formula><p>where sg(.) is the stop gradient operation which constrains its operand to remain constant during backpropagation. The first part moves the codebook entries closer to the encoded features. The second part, known as the commitment loss <ref type="bibr" target="#b66">[67]</ref>, reverses the roles played by the two variables.</p><p>In regions with complex textures and high frequency details, local patterns shifted by a few pixels in x and its reconstruction x = D(z q ) may result in large pixel-to-pixel errors, while being visually satisfactory. We thus define the recovery objective as the L 1 loss in both RGB space and between features from a VGG network <ref type="bibr" target="#b58">[59]</ref> pretrained on ImageNet <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_3">L r (E, Q, F, D) = x ? x 1 + VGG(x) ? VGG( x) 1 .<label>(4)</label></formula><p>To tackle cases where information cannot be recovered from context due to occlusion, and the compressed features are insufficient to create plausible reconstructions due to lossy compression, we supplement our architecture with an image discriminator ? i , made of downsampling residual blocks <ref type="bibr" target="#b28">[29]</ref>, to encourage realistic outputs. ? i tries to distinguish real images from reconstructed ones (L d ), while E and D fools ? i into assuming reconstructed images are as good as real ones (L a ):</p><formula xml:id="formula_4">L d (? i ) = ln(1 + e ?i(x) ) + ln(1 + e ??i( x) ), (5) L a (E, Q, F, D) = ln(1 + e ?i( x) ).<label>(6)</label></formula><p>We employ a similar strategy on sequences of consecutive frames to improve the temporal consistency using a 3D temporal discriminator ? t , a direct extension of 2D image discriminator ? i .</p><p>The success of our method relies on accurate motion estimation in F, a difficult task which benefits from self-supervision <ref type="bibr" target="#b34">[35]</ref>. Therefore, we train the autoencoder with augmented views of the input frames as context. Custom augmentations functions A : X ? X include: rotation, scaling, translation, elastic deformation, and combinations of these. Augmented views are obtained by warping x by the suitable flow a c : A(x) = W(x, a c ), and the inverted flow f c from A(x) to x can be approximated. <ref type="bibr" target="#b0">1</ref> We resort to flow inversion because directly reconstructing distorted views may encourage similar defects during inference. Moreover, we balance between self-recovery and context-recovery objectives by additionally applying a blurring function B : X ? X and an occlusion mask o c to the augmented frames x c = o c ? B(A(x)). This augmentation strategy is illustrated in Appendix F. We define the contextual loss as:</p><formula xml:id="formula_5">L c (E, Q, F, D) = f c ? f c 2 2 + o c ? ?( m c ) 2 2 ,<label>(7)</label></formula><p>where o c = W(o c , f c ), and f c and m c are the flow and mask estimated by F. In practice, this loss is applied at intermediate resolution levels r k for improved training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Second stage: predicting temporal dynamics with transformers</head><p>Architecture. We follow <ref type="bibr" target="#b19">[20]</ref> and adopt an architecture similar to Image-GPT <ref type="bibr" target="#b8">[9]</ref> for T . Instead of modeling a single annotated frame as in <ref type="bibr" target="#b53">[54]</ref>, we design our model to account for sequences of N such frames to allow prediction of temporal dynamics controlled by ancillary information in the form of video-and frame-level annotations. We have shown, in Section 3.2, how to represent a frame as a sequence of h ? w tokens (indices in 1, q ) through encoding and quantization. Similar strategies can be applied to cater to other types of data, with or without compression depending on their complexity, and, thereby, turn ancillary information into tokens as well. The capacity of the transformer (the maximum sequence length it can process) is L = l v + N * (l f + h * w), where l v and l f are the size of video-and frame-level annotations respectively. The final layer of the model predicts, for every i in 1, L ? 1 , a vector o i (of size q, the number of possible tokens) which scores the likelihood of the i + 1 th token given all preceding ones. For clarity, we omit the flow module F between E and D. Input video-level annotation, frame-level annotations, and conditioning frames are encoded and quantized to form the initial token sequence, and mapped to corresponding embeddings. Subsequent frame tokens are obtained autoregressively with T , while the ones corresponding to video-and frame-level annotations guide the prediction.</p><p>Training procedure. To learn the parameters of T , we load a complete sequence (N frames corresponding to L tokens), and try to predict the L ? 1 last tokens based on the L ? 1 first ones. This is done by maximizing the log-likelihood of the data using the cross-entropy loss:</p><formula xml:id="formula_6">L(T ) = ? L i=2 log exp( o i?1 , e ?(i) ) j exp( o i?1 , e j ) ,<label>(8)</label></formula><p>where ?(i) is the i th ground-truth token, and e j a vector of 0's with a 1 in its j th coordinate.</p><p>Inference. During inference we use T to complete autoregressively an input sequence of tokens.</p><p>To avoid known pitfalls (e.g., repetitive synthesis) and allow diverse outcomes, we use top-k sampling whereby the next token is randomly chosen among the k most likely ones, weighted by their scores o.</p><p>Although T processes sequences of N annotated frames, we predict ones of arbitrary length by using a temporal sliding window. Tokens corresponding to video-and frame-level annotations are given as input to T (not predicted, even for future frames) to guide the synthesis. We show in the experiments how this control can translates into a variety of interesting tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We assess the merits of CCVS in the light of extensive experiments on various video synthesis tasks.</p><p>Datasets. BAIR Robot Pushing <ref type="bibr" target="#b18">[19]</ref> consists of 43k training and 256 test videos of a robotic arm interacting with objects from a fixed viewpoint. A high-resolution version has recently been released. We manually annotate, in 500 frames, the (x, y) location of the arm in image space to train a position estimator, which we use for state-conditioned synthesis. To account for real world scenarios, we evaluate on Kinetics-600 <ref type="bibr" target="#b4">[5]</ref>, a large and diverse action-recognition dataset with approximately 500K videos. We also test our method on AudioSet-Drums <ref type="bibr" target="#b24">[25]</ref> for sound-conditioned synthesis on music performance, containing 6k and 1k video clips in train and test splits respectively. Other datasets and tasks are covered in Appendix E.</p><p>Metrics. We use the Fr?chet video distance (FVD) <ref type="bibr" target="#b65">[66]</ref> which measures the distribution gap between real and synthetic videos in the feature space of an Inception3D network <ref type="bibr" target="#b5">[6]</ref> pretrained on Kientics-400 <ref type="bibr" target="#b39">[40]</ref>. It estimates the visual quality and temporal consistency of samples as well as the diversity in unconditioned scenarios. In conditioned ones, we use another metric for diversity (DIV) which is the mean pixel-wise distance among synthetic trajectories conditioned on the same input. For near-deterministic motions (e.g., in reconstructions, or constrained tasks), there is a one-to-one mapping between real video frames and synthetic ones, and we include pairwise image quality assessments: the structural similarity index measure (SSIM) <ref type="bibr" target="#b76">[77]</ref> which evaluates a per-frame <ref type="table">Table 1</ref>: Ablation study of the autoencoder on BAIR (256 ? 256). We evaluate self-and contextrecovery modules in different scenarios: synthesizing 16-frame videos from known compressed features ("Reconstruction"), by inferring compressed features with T given the real trajectory of the robotic arm ("State-conditioned"), or without the trajectory ("Pred." and "Unc."). The first real frame is used as initial context in all cases, except for "Unc." where it is synthesized by StyleGAN2 <ref type="bibr" target="#b37">[38]</ref>.    conformity (combination of luminance, contrast and structure), and the peak signal-to-noise ratio (PSNR) which is directly related to the root mean squared error. For each metric, we compute the mean and standard deviation (std) over 5 evaluation runs (2 for Kinetics due to its voluminous 50k video test set). For clarity, the std value is shown only when it is greater than the reported precision.</p><p>Training details. All our models are trained on 4 Nvidia V100 GPUs (32GB VRAM each), using ADAM <ref type="bibr" target="#b40">[41]</ref> optimizer, for multiple 20 hour runs. We adapt the batch size to fill the memory available. We use a learning rate of 0.02 to train the autoencoder, and exponential moving average <ref type="bibr" target="#b82">[83]</ref> to obtain its final parameters. We use weighting factors (1, 10, 1, 1) and 0.25 for (? q , ? r , ? a , ? c ) and ? in Equations <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_2">(3)</ref> respectively. We use a learning rate of 10 ?5 to train the transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ablation study</head><p>We conduct an ablation study of CCVS to show the individual contribution of the key components of the proposed autoencoder <ref type="table">(Table 1)</ref>, transformer <ref type="table" target="#tab_1">(Table 2)</ref>, and the effect of compression ( <ref type="figure" target="#fig_4">Figure 3</ref>).</p><p>First, we fix the transformer and observe the incremental improvements in synthesis quality when adding self-and context-recovery modules to the autoencoder <ref type="table">(Table 1</ref>). In particular, L 1 loss in the feature space of a VGG net <ref type="bibr" target="#b58">[59]</ref> produces sharper videos than using the same loss in the RGB space alone. Predicted frames using image and temporal discriminators (? i and ? t ) display greater realism and temporal consistency. The flow module F significantly improves the performance on all metrics by allowing context frames to guide the reconstruction of synthetic ones. The self-supervision of F, the use of larger context windows, and longer training times, further improve the quality of the synthesis. ? t seems to deteriorate the FVD at first, but when all modules are combined it improves FVD by almost a factor 2 as it encourages better temporal consistency in the presence of context. </p><formula xml:id="formula_7">t = 1 t = 2 t = 4 t = 6 t = 8 t = 10 t = 12 t = 14 t = 16</formula><p>Future prediction Point-topoint</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>State-conditioned</head><p>Real video We fix the autoencoder, and compare different architectures and sampling strategies for the transformer ( <ref type="table" target="#tab_1">Table 2</ref>). Increasing the model capacity by adding layers and increasing expressivity (number of attention heads) along with a spatio-temporal decomposition of positional embeddings (shown in <ref type="figure" target="#fig_2">Figure 2</ref>, detailed in supplementary material) yields small improvements. Top-k sampling is beneficial for stochastic tasks (video prediction and unconditional synthesis), whereas always selecting the most probable tokens results in little to no motion. Guiding temporal dynamics with state-conditioned synthesis reduces the advantage of sampling by narrowing down possible outcomes.</p><p>Finally, we explore the effect of compression (in terms of the number of tokens for each frame) on synthesis speed (FPS) and quality (FVD) of reconstructed and predicted videos ( <ref type="figure" target="#fig_4">Figure 3</ref>). We use a compression of 64 tokens in our default setup since it gives the best FVD while retaining a reasonable speed. The critical drop of FPS for low compression ratios is due to the pairwise consideration of all input tokens in T . Note that using a smaller temporal window in T may allow additional speed-ups. Method FVD ? GPU/TPU Mem. LVT <ref type="bibr" target="#b52">[53]</ref> 225 128GB, 48h Video Transformer <ref type="bibr" target="#b78">[79]</ref> 170?5 2TB, 336h * DVD-GAN-FP <ref type="bibr" target="#b11">[12]</ref> 69?1 2TB, 144h * CCVS (ours) 55?1 128GB, 300h TriVD-GAN-FP <ref type="bibr" target="#b44">[45]</ref> 26?1 16TB, 160h * * : value confirmed by authors.   <ref type="table" target="#tab_2">(Table 3)</ref>, CCVS trained at 64?64 resolution (low res.) is on par with the best method (L-size version of <ref type="bibr" target="#b78">[79]</ref>), but requires much less computing resources, and outperforms <ref type="bibr" target="#b78">[79]</ref> under similar resources. We also propose high res. CCVS which is not strictly comparable to the prior arts as we use 256?256 image resolution for training and test, and resize the synthesized frames to 64?64 for computing FVD. However, using this variant demonstrates the performance gains that can arise by scaling CCVS. We additionally address point-to-point synthesis (with the end frame as a video-level annotation) and state-conditioned synthesis (with the estimated 2D position of the arm as a frame-level annotation). Point-to-point synthesis is more difficult than video prediction: Not only does it requires realistic video continuations, but also ones which explain the end position of all visible objects. Hence, FVD score is constant despite the additional input. Still, this yields better SSIM for mid-point and one-before-last frames. State-conditioned synthesis improves on FVD and mid-synthesis SSIM as motion becomes near-deterministic. Some synthetic frames for these tasks are shown in <ref type="figure" target="#fig_5">Figure 4</ref>. CCVS creates plausible high-quality videos in various settings, and true interactions with objects compared to previous attempts <ref type="bibr" target="#b47">[48]</ref> at the same resolution.  Kinetics. CCVS ranks second on Kinetics video prediction benchmark. Kinetics contains more diversity than BAIR, and the reconstruction is thus more difficult. A solution <ref type="bibr" target="#b53">[54]</ref> is to increase the codebook size <ref type="table" target="#tab_4">(Table 5</ref>) but it stops translating into better prediction FVD at some point. We also try removing the quantization step (equivalent to an infinite codebook), and directly regressing latent features with T (instead of ranking the likelihood of possible tokens). It allows better reconstructions, yet the prediction FVD is high. <ref type="figure" target="#fig_7">Figure 5</ref> shows examples of synthetic continuations conditioned on AudioSet-Drums Whole frames 2.25?0.26 Moving parts 65.45?6.00 * : the pixel-wise distances are obtained by applying a 1.0 ? 10 ?3 factor (omitted for clarity). <ref type="figure">Figure 6</ref>: Diversity results for conditioned scenarios. The diversity metric (DIV) measures the mean pixel-wise distance among 10 15-frame synthetic trajectories conditioned on the same frame. We report the mean and std over 100 runs: on whole frames as in <ref type="bibr" target="#b81">[82]</ref>, or moving parts only by masking static regions (optical flow magnitude between consecutive frames &lt; 20% of the max magnitude) as in <ref type="bibr" target="#b68">[69]</ref>. We show, for the same input, the 15 th frame of various randomly seeded synthetic trajectories.</p><p>5-frame unseen test sequences. CCVS produces realistic and temporally coherent outputs which display various types of motion (e.g., body, hand, camera).</p><p>AudioSet-Drums. CCVS achieves top performance on sound-conditioned video synthesis on Au-dioSet Drums <ref type="table" target="#tab_5">(Table 6</ref>). <ref type="figure">Figure 6</ref> shows quantitative and qualitative insights on the diversity of the synthetic trajectories conditioned on the same input for the three datasets. The diversity metrics (DIV) computed on whole frames is lower on AudioSet Drums than on the other two datasets. This is explained by the fact that motion is quite repetitive and involves a limited portion of the frame. The same metric on moving parts only, and the end position of the drummer's hand and upper left cymbal in qualitative samples, show the diversity of synthetic trajectories. An ablation of CCVS with/without audio guidance as well as more qualitative results on diversity can be found in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>CCVS is on par or better than the state-of-the-art on standard benchmarks, uses less computational resources, and scales to high resolution. Training neural networks is environmentally costly, due to the carbon footprint to power processing hardware <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b60">61]</ref>. Methods sparing GPU-hours like ours are crucial to make AI less polluting <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b60">61]</ref>, and move from a "Red" to a "Green" AI <ref type="bibr" target="#b57">[58]</ref>. Future work will include exploring new codebook strategies and synthesis guided by textual information.</p><p>Limitations. CCVS uses a complex architecture and a two-stage training strategy. Simplification of both is an interesting direction for improving the method. Moreover, CCVS lacks global regularization of motion (flow computed on pairs of timesteps), and its efficiency relies on recycling context information such that synthesizing content from scratch (i.e., no input frame given) remains difficult.</p><p>Broader impact. The increased accessibility and the many controls CCVS offers could accelerate the emergence of questionable applications, such as "deepfakes" (e.g., a video created from someone's picture and an arbitrary audio) which could lead to harassment, defamation, or dissemination of fake news. On top of current efforts to automate their detection <ref type="bibr" target="#b48">[49]</ref>, it remains our responsibility to grow awareness of these possible misuses. Despite these worrying aspects, our contribution has plenty of positive applications which outweigh the potential ethical harms. Our efficient compression scheme is a step in the direction of real-time solutions: e.g., enhancing human-robot interactions, or improving the safety of self-driving cars by predicting the trajectories of people and vehicles nearby.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Detailed architecture design</head><p>(1) Conversion from image to feature space (E) and vice versa (D).</p><p>In the later case, context frames help producing new ones via F.</p><p>(2) Computations in F at one resolution level. We describe the implementation choices for the different functional blocks which compose CCVS. Note that different parts of the framework may benefit from future advances in various research areas: autoregressive modeling, image synthesis, codebook learning and optical flow estimation for transformer T , decoder D, quantizer Q and flow module F respectively.</p><p>Encoder and decoder. Both the encoder E and the decoder D, illustrated in <ref type="figure" target="#fig_0">Figure A1</ref>-(1), use a sequence of K modules handling information at different intermediate resolution levels (r k ) k? 1;K (with reverse ordering from E to D). Each module is implemented by a residual block <ref type="bibr" target="#b28">[29]</ref> which increases or decreases the spatial resolution by a factor 2. In practice, we use the succession of two 3 ? 3 2D convolutions with LeakyReLU activations and a skip connection in the form of a 1 ? 1 convolution.</p><p>Flow module. The flow module F, also illustrated in <ref type="figure" target="#fig_0">Figure A1-(1)</ref>, allows the sharing of information between the encoding and decoding stages across different timesteps. Computations for one resolution level r k are detailed in <ref type="figure" target="#fig_0">Figure A1</ref>-(2): The "match" operation computes the correlation between features in d k s and its closest spatial neighbours in e k c . We thus obtain a 3D cost volume of size N ? h k ? w k , where N is the number of neighbours considered and (h k ? w k ) = r k is the spatial resolution. We apply 2D convolutions to this cost volume to deduce the residual flow and mask. The "refine" operation is also a residual update of f k c and m k c . It is made of 2D convolutions and takes as input the concatenation of features, mask and flow. Finally, the "fuse" operation update features from D with information from the context in E according to the update rule given in <ref type="figure" target="#fig_0">Figure A1</ref>-(1) and detailed in Section 3.2. For these operations, we take our inspiration from LiteFlowNet <ref type="bibr" target="#b31">[32]</ref> which we adapt to handle not only the optical flow but also the proposed fusion mask. An extension to multi-frame context for this temporal skip connection mechanism is proposed in Appendix D.</p><p>Image and temporal discriminators. The architecture of the image discriminator ? i is similar to that of StyleGAN2 <ref type="bibr" target="#b37">[38]</ref>, and the temporal discriminator ? t is essentially the same, with 3D instead of 2D convolutions to account for the temporal dimension. To avoid the propagation of errors when decoding frames autoregressively during inference (see the "shift-and-add" module introduced for this purpose in <ref type="figure" target="#fig_0">Figure 1</ref>), we mimic this iterative process at train time: decode frames one by one, then encode them back to obtain context features for the next steps. The resulting synthetic videos are fed to ? t . To avoid excessive memory consumption due to the recursive encoding / decoding operations, we only keep the gradients for the context features from the immediate preceding frame.</p><p>Quantizer. The implementation of the quantizer Q is straightforward since it is just using a codebook of learnable embeddings. Yet, the non-differentiable nature of this operation prevents gradients from training losses (introduced in Section 3.2) to backpropagate through U and Q, and from D to E at the compression bottleneck. To solve this in practice, we use a straight-through gradient estimator <ref type="bibr" target="#b2">[3]</ref> which ignores the quantization step during backward-pass.</p><p>Transformer. The transformer T is described in Section 3.3. Its input is obtained by transforming a sequence of tokens into a 2D matrix of size L ? D. For that, T associates with each possible token and each of the L positions in the sequence a learnable embedding of size D. To leverage the prior information that successive fixed-sized portions of the sequence represent annotated frames, we propose a spatio-temporal decomposition of the positional embeddings with a term indicating the timestep, and another one indicating the position within the frame-related portion. This is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. The input matrix is computed by adding the token embedding and the positional embedding for each token of the sequence. T thus consists in the succession of multi-head self-attention layers, and position-wise fully connected layers.</p><p>Position estimator. In the experiments (Section 4), we mention that we manually annotated the (x, y) location of the robotic arm (more precisely, its gripper) in 500 frames of the BAIR dataset. We use these to train a position estimator so that we can extract the trajectory of the arm from any new video (and use this trajectory to guide the state-conditioned synthesis of new videos). To facilitate training and accelerate inference, we design our position estimator to exploit latent features (obtained from images with E) and to map them to the (x, y) position of the arm. We use a simple architecture with a few downsampling convolutional layers and a fully connected layer to output the estimated position. The position estimator is trained by regressing the target 2D coordinates by minimizing the corresponding mean squared error.</p><p>Sound autoencoder. To allow multimodal synthesis of video and sound, we first compute the short-time Fourier transform (STFT) of the raw audio to obtain a time-frequency representation of the sound. We then employ an encoding / quantization / decoding strategy similar to that used to construct E, Q and D for video frames (without F in this case) to reduce dimension and obtain corresponding tokens for audio. These tokens are used as frame-level annotations to guide the synthesis, just as in state-conditioned synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Role of the flow module</head><p>Looking at <ref type="figure" target="#fig_0">Figure A1</ref>, one may wonder whether the flow module F alone is sufficient for predicting features for future frames which would result in z s potentially being completely ignored. This would only apply if a frame was fully determined from the preceding one, yet, this is not the case due to the inherent stochastic nature of future prediction in the considered datasets. We have conducted a simple experiment to demonstrate that features z s are actually used and that they actively drive the dynamics of the scene: We have repeated the "reconstruction" experiment from <ref type="table">Table 1</ref> for which we compare the final model (last line) with an ablation with fixed z s (i.e., fixing z s = z c for all predicted timesteps). Results are available in <ref type="table" target="#tab_7">Table B1</ref>.</p><p>We observe that the reconstruction performance is much poorer when we force z s to remain constant for all timesteps. Qualitatively, looking at the synthesized videos, we see that they do not exhibit any motion, as one would expect when fixing the encoded features. Moreover, thanks to the fusion masks estimated by the flow module F and used in Eq. (1), it is possible to observe how much information comes from z s and from F respectively when generating x s . <ref type="figure" target="#fig_0">Figure F1</ref> shows some examples of estimated masks on Cityscapes. Those are white when the source is z s , and black when it is context through F. In practice, we see that they are white when the context is occluded and mostly grey in other regions. Thus, final videos are as much the result of context warping than direct decoding in non-occluded regions for this dataset. We observe similar behaviours on other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Flow inversion approximation</head><p>For affine transformations, the exact inverse flow can be determined analytically. This is not the case for elastic deformations for which we propose to approximate this inverse flow f ? R 2?H?W from an input flow g ? R 2?H?W (where H ? W is the image spatial resolution). Here, g maps cells from a grid 1; H ? 1; W to cells from the same grid (we ignore cells for which g points out of the grid since they do not help in computing f ). Such a mapping is not surjective, that is, all the cells in the grid are not necessarily reached. We compute the pixel-accurate inversion in cells for which there exists a direct mapping, and approximate others by iterative interpolation. That is, we keep track of inverted cells with a completion mask c which we update with the closest cells in cardinal directions at every step. For the elastic deformations that we use when training CCVS, around 80% of the grid can be directly inverted in average, which is enough to accurately reconstruct the missing cells by interpolation.</p><p>Algorithm C1: Flow inversion approximation. We note that an alternative to flow inversion is to use the flow module F to compute the backward flow. This requires an extra forward pass in the flow module to compute both forward and backward flows and would result in additional GPU computations. Not to slow down training we prefer to run flow inversion on parallelized CPU processes as part of data loading. While both alternatives take approximately the same amount of time (0.15s on a GPU for backward flow estimation for a batch of 16 images at resolution 256 ? 256, 0.12s on parallelized CPU for flow inversion for the same input) flow inversion is advantageous because, in our setup, GPU consumption is the limiting factor for speed (fully utilized GPUs, data loading on CPUs can run in the background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Extension of the flow module to multi-frame context</head><p>We extend the flow-module (introduced in Section 3.2) which aims at reusing contextual information from single-to multi-frame context. We thus consider a context (x i ) i? 1;c consisting of c frames. Just as before, for a given level r k and i in 1; c , we use F to compute the corresponding fusion mask m k i and optical flow f k i between intermediate encoded features e k i (from context frame x i ) and the decoded features d k s (that we wish to update). We recall that fusion masks m k i handle occlusion by indicating for each spatial location the relevance of warped context features e k i = W(e k i , f k i ), that is, whether features e k i correspond to d k s at that location in terms of content, or not. Hence, we can derive from the fusion masks a confidence score s k i defined as the context-wise normalization of features relevance, and use it to aggregate all the information that can be recovered from context by applying a weighted average:</p><formula xml:id="formula_8">s k i = (1 ? ?(m k i )) ? ? c j=1 (1 ? ?(m k j )) ? ? , m k a = c i=1 s k i ? m k i e k a = c i=1 s k i ? e k i .<label>(9)</label></formula><p>where represents the element-wise division. In other words, given a spatial location, m k a is an estimate for the likelihood that the content of d k s matches any of the c context frames, and e k a is used to reconstruct this content faithfully by weighting proposals from the context by their relevance. This brings us back to the problem setup of Section 3.2, so that the aggregated fusion mask m k a and warped features e k a can be used to update d k s , just as in single-frame fusion. The key benefits of this extension are the use of redundancy within context to better reconstruct the current frame, and the greater robustness to occlusion as different views are combined. This is demonstrated by the ablation study of the autoencoder in <ref type="table">Table 1</ref>. Unconditional synthesis. We apply CCVS to unconditional video synthesis (i.e., the production of new videos without prior information about their content) on UCF-101 <ref type="bibr" target="#b59">[60]</ref>, an action-recognition dataset of 101 categories for a total of approximately 13k video clips. Our approach leverages recent progress made in image synthesis by using StyleGAN2 <ref type="bibr" target="#b37">[38]</ref> to produce the first frame. CCVS is able to improve over the state of the art on the FVD metric (computed on 2048 videos, under the same evaluation process as related approaches), but has an average performance (24.47) on the inception score (IS) <ref type="bibr" target="#b56">[57]</ref> (computed on 10k videos, again with the appropriate protocol). IS measures the coverage of the different categories, and whether one of these is clearly identifiable in each of the synthetic videos, through the lens of the Softmax scores of a C3D network <ref type="bibr" target="#b63">[64]</ref> trained for action recognition on the Sports-1M dataset <ref type="bibr" target="#b36">[37]</ref> and fine-tuned on UCF-101. We also evaluate the performance of CCVS when considering a perfect image synthesizer (or one that has overfitted the training data) by using a real frame instead of a synthetic one to launch the prediction. This yields similar FVD but considerable improvements on IS <ref type="bibr">(41.37)</ref>. Note that CCVS favorably contribute to IS in this setting because videos made from the same real frames (by repeating them along the temporal axis) have much lower IS <ref type="bibr">(28.41)</ref>. A plausible explanation why MoCoGAN-HD <ref type="bibr" target="#b62">[63]</ref> achieves a good IS score is due to it sampling multiple times from StyleGAN2 <ref type="bibr" target="#b37">[38]</ref> (as opposed to once in our case, we use the same pretrained model). Indeed, MoCoGAN-HD synthesizes videos by finding a trajectory in the latent space of StyleGAN2. In this case, it may be easier to recognize a category because different modes can be interpolated (greater IS) but the outcome is less temporally realistic than ours (FVD near the one of still videos for MoCoGAN-HD). Future work will include trying to reconciliate both approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional results</head><p>Layout-conditioned synthesis. We explore here layout-conditioned synthesis where one controls the semantic structure of new frames by attributing a class to each of their spatial locations. To this end, we apply CCVS on image and layout sequence pairs on the Cityscapes dataset <ref type="bibr" target="#b12">[13]</ref> which contains 3475 and 1525 30-frame sequences for train and test respectively. A single layout is annotated for each sequence, and we use a pretrained segmentation model <ref type="bibr" target="#b61">[62]</ref> to obtain the remaining ones. Layouts are used as frame-level annotations for processing in T . They are encoded, quantized, and decoded just as it is done for images. To save memory consumption during the decoding stage and ensure greater consistency between the two, frame and layout are decoded simultaneously with a single decoder D (we just supplement D with another prediction head to output layout masks). We show some qualitative results of CCVS on the synthesis of 30-frame videos given 3 starting frames and layouts for all timesteps in <ref type="figure" target="#fig_0">Figure D1</ref>. Synthetic videos closely follow the target semantic structure, remain temporally consistent, and can handle complex motions. We repeat this synthesis process in <ref type="figure" target="#fig_2">Figure D2</ref>, this time without layouts at subsequent timesteps. CCVS forecasts the semantic evolution of the scene on its own, predict corresponding layouts, and translate these into high-quality images. Both approaches (with or without layout at subsequent timesteps) are compared in <ref type="table" target="#tab_1">Table D2</ref>.</p><p>Having this additional information allows to produce videos which are closer to the real ones. A common architectural choice in layout-conditioned synthesis is to replace ResNet blocks <ref type="bibr" target="#b28">[29]</ref> by SPADE blocks <ref type="bibr" target="#b51">[52]</ref> to strengthen the layout guidance in the decoding stage <ref type="bibr" target="#b45">[46]</ref>. Quantitatively, we see that it significantly improves the fidelity of the synthesized videos to the real ones in terms of SSIM and PSNR. Qualitatively, side-by-side reconstructions with and without SPADE in <ref type="figure" target="#fig_4">Figure D3</ref> show that the flow module allows temporally consistent textures (similar to the 3D world model used in <ref type="bibr" target="#b45">[46]</ref> but without being limited to static objects here) while SPADE enhances the compliance to the semantic structure, especially for small fast-moving objects.</p><p>Sound ablation study. A simple ablation of CCVS on AudioSet Drum <ref type="bibr" target="#b24">[25]</ref> is presented in <ref type="table" target="#tab_2">Table D3</ref>. It shows that, as time goes by, the performance gap between unimodal and multimodal synthesis (when considering audio) increases. However, the improvement on account of the audio information seems relatively small compared to the overall performance. We suspect this is due to a minor part of the scene being animated (the drummer upper body) and to relatively fast motion (it is difficult to strictly minimize the image-to-image error between real and synthetic videos in this case).  <ref type="figure" target="#fig_0">Figure D1</ref>: Layout-conditioned synthesis of 30-frame videos given 3 starting frames and layouts for all timesteps on Cityscapes (256 ? 512). We show different samples of real layout and synthetic image pairs at intermediate timesteps. We see that synthesized frames follow both the target semantic structure specified by the layouts and the texture extracted from the conditioning frames.  <ref type="figure" target="#fig_4">Figure D3</ref>: Reconstruction of the 30 th frame given the real 1 st frame and both compressed features and layouts for subsequent timesteps. We compare our method with and without SPADE decoding blocks <ref type="bibr" target="#b51">[52]</ref> to the expected real image and layout at the same 30 th timestep. We zoom in specific areas to facilitate comparisons. We observe better alignment and more precise details for objects displaying complex motion when SPADE is used.  <ref type="table" target="#tab_3">Table D4</ref>: Point-to-point video synthesis on BAIR (64?64). We follow <ref type="bibr" target="#b74">[75]</ref> and synthesize 30-frame videos conditioned on the start and end frames. For each of the real test videos we produce synthetic ones (100 predicted samples) and compute the pairwise SSIM between real and synthetic frames. As a quality assessment, we check if the real video is represented among synthetic ones by reporting the best SSIM among samples ("Best"). We estimate the diversity of the predictions by computing the variance of the SSIM across samples with the true video as reference ("Div."). We evaluate the control point consistency by measuring the SSIM between the last synthesized frame and the corresponding real one for all samples ("C.P.C."). Finally, we assess the reconstruction quality for the different methods by using latent features extracted from real frames at intermediate timesteps instead of predicting them ("Rec."). Metrics are computed in a 95% confidence interval like in <ref type="bibr" target="#b74">[75]</ref>. Point-to-point synthesis. We explore further the performance of CCVS on the point-to-point synthesis task which we introduced in Section 4 of the article. We show some quantitative results against different baselines in <ref type="table" target="#tab_3">Table D4</ref>. CCVS is on par or significantly outperforms the state of the art on 3 out of 4 metrics. It produces more faithful reconstructions when latent features are known. CCVS is more likely to synthesize the true outcome among 100 predicted samples compared to baselines. It has a good control point consistency (i.e., the last synthetic frame is close to the corresponding ground-truth), but lower diversity than prior arts. We note, however, that SSIM diversity may capture the capacity of producing different trajectories between start and end points, but may also increase due to visual quality degradations. Like previous works <ref type="bibr" target="#b74">[75]</ref>, CCVS is able to produce videos of arbitrary length between the target start and end frames. We recall that the end frame is used as a video-level annotation in the transformer model T <ref type="figure" target="#fig_2">(Figure 2</ref>) so that it is taken into account when predicting frames at earlier timesteps. To allow any timestep for the end frame (possibly greater than the capacity of T ), we make its positional embedding reflect the gap with current predicted frames. Namely, the positional embedding of the end frame is updated as the sliding window moves forward in time and gets closer to the end timestep. We show qualitative results for different video lengths in <ref type="figure" target="#fig_5">Figure D4</ref>.</p><p>Class-conditioned synthesis. Next, we conduct a qualitative study of video synthesis conditioned on a class label on the Weizmann dataset <ref type="bibr" target="#b26">[27]</ref>, which consists of short clips captured from a fixed viewpoint from 9 subjects, each performing the same 10 actions. We use 8 subjects for training and leave 1 out for testing. CCVS tackles this task by specifying the class label as a video-level annotation in T <ref type="figure" target="#fig_2">(Figure 2</ref>). We can make the held-out subject perform the learned actions accurately given a single input image <ref type="table" target="#tab_4">(Table D5</ref>). Or we can synthesize new videos without any prior knowledge other than the desired class label <ref type="table" target="#tab_5">(Table D6)</ref>.</p><p>Qualitative ablation. We repeat the ablation study of the autoencoder conducted in <ref type="table">Table 1</ref> on BAIR <ref type="bibr" target="#b18">[19]</ref>, by giving some qualitative samples of the reconstruction this time. The reconstruction results of 30-frame videos given the first frame and the compressed features at subsequent timesteps can be found in <ref type="table" target="#tab_4">Table D5</ref>. To best compare different ablations of CCVS, we use the same input data for all. A model trained with the L 1 loss in RGB space reconstructs the static background and the robotic arm well, while other objects look very blurry. When using the same loss in the feature space of a VGG net <ref type="bibr" target="#b58">[59]</ref>, objects display higher-frequency details, but look transparent. Having the image discriminator ? i allows much more realistic outcomes, although these are still  <ref type="figure" target="#fig_5">Figure D4</ref>: Videos of various lengths (L ? {20, 25, 30}) synthesized with our point-to-point method given a pair of start and end frames on BAIR (256 ? 256). Note that the same model is used to synthesize videos of different lengths. Our approach generalizes to arbitrary lengths between start and end frames by taking the length into account in the positional embedding of the end frame.  <ref type="figure" target="#fig_7">Figure D5</ref>: Qualitative samples of class-conditioned synthesis from a single image on Weizmann (128?128). We use one conditioning frame from a held-out subject (not seen during training), that is, a side view for lateral actions ("walk", "run", "jump", "bend" and "skip") and a front view for frontal actions ("side", "wave1", "wave2", "pjump" and "jack"). not temporally consistent, nor do they faithfully correspond to the ground-truth video. Having the temporal discriminator ? t seems to improve over the temporal consistency issue. Thanks to the flow module F, reconstructed videos are closer to the ground-truth ones, in particular at early timesteps. However, we see that flow imperfections cause synthesis artefacts which add up as time goes by, and result in quality degradation. The self-supervised technique used to learn F mitigates this shortcoming. Finally, larger context windows and longer training times help to better recover details (see the more realistic texture and shape of manipulated objects for instance). The final proposal creates high-quality videos almost indistinguishable from real ones, and shows true interactions with objects (a ball and a colorful gun in this example). We see that in case of disocclusion (around t = 20 the upper right corner is revealed), our model is able to synthesize plausible arrangements of objects despite lossy compression. These new objects remain consistent as the synthesis process unfolds (see t = 30) thanks to the autoregressive nature of our approach.</p><p>Long-term synthesis. We assess the ability of CCVS to generalize to long-term prediction through a qualitative study. We compare, for a model trained on sequences of length 16, the synthesis of 90-frame videos conditioned on a single frame and using different context windows for the flow module: 0 (not using the flow), 1, 4 and 16. Results on BAIR, Kinetics and AudioSet-Drums are available in <ref type="figure">Figure D7</ref>. When the flow is not used (size 0, i.e., a na?ve extension of <ref type="bibr" target="#b19">[20]</ref> or a setup close to <ref type="bibr" target="#b52">[53]</ref>) videos are quite unstable and we see colors shifting over time. With a single-frame context window (size 1), there is a greater temporal consistency (e.g., the drum sticks are better preserved on AudioSet-Drums, the semantic structure shows higher fidelity to the input frame on BAIR or Kinetics), but in the long-term, synthesis artefacts add up due to the autoregressive decoding which leads to visual deterioration and saturation. These effects stand out the most on BAIR and Kinetics. The proposed multi-frame context extension (Appendix D) reduces this issue (size 4), and the latter becomes barely perceptible with an even larger context window (size 16). We found that even small context windows on AudioSet-Drums are sufficient thanks to the high inter-frame similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Augmentation strategy</head><p>As discussed in Section 3.2, the self-supervision process used to train F reconstructs a static image x from both its latent representation and an augmented view as context by estimating the flow and mask corresponding to that augmentation. We illustrate this process on the Cityscapes dataset in <ref type="figure" target="#fig_0">Figure F1</ref>. We recall that, in this case, the context image can be writen as x c = o c ? B(A(x)), where o c is an occlusion mask, and B and A some blurring and augmentation (i.e., spatial transformation) functions respectively. We can invert the flow corresponding to transformation A thanks to Algorithm C1. The output corresponds to the target "Real Flow" in the illustration. As for the target "Real Mask", it is obtained by warping the occlusion mask with the inverted flow. Looking at synthetic reconstructions, we see that visible details in the context are well recovered despite the augmentation. Indeed, real and synthetic flows match in non occluded areas (indicated by the dashed red line). Moreover, adversarial training produces high-quality images from blurred and occluded contexts. We notice that textures do not always match in real and synthetic images (e.g., building windows, car appearance). Yet, thanks to recovery losses, the semantic structure of the scene is preserved. Finally, we see that darker and lighter areas of the synthetic masks correspond to the same areas in the real ones. The darker the synthetic masks, the more features are updated from context. The whitish part indicates that in occluded areas the final image is reconstructed from its latent representation alone, and the greyish part shows that for other regions context is effectively reused (and latent representations still contribute to the final outcome). It is interesting to see that edges are visible in the synthetic masks. This indicates that those regions may be easier to detect (darker color) and play an important role in the image alignment process (estimation of the flow).   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Proposed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>d k s from d k? 1 s,</head><label>1</label><figDesc>(b) use F to refine the optical flow f k c (in R 2?r k ) which estimates the displacement field from e k c to d k s (as a proxy to the one from x c to x s ), and a fusion mask m k c (in R 1?r k ) which indicates the expected similarity between aligned features e k c = W(e k c , f k c ) and d k s (also as a proxy for the one in image domain) with W corresponding to a standard warping operation, (c) use e k c and m k c to update d k s with context information (see update rule (1) below), (d) move to resolution level r k+1 by going back to (a). We note that F estimates f k c and m k c in a coarse-to-fine fashion by refining f k?1 c and m k?1 c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of the transformer to predict latent temporal dynamics with control-related data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>"</head><label></label><figDesc>Dec.": spatio-temporal decomposition of positional embeddings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Quality and speed of synthesis vs. compression on a Nvidia V100 GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative samples for different types of control on BAIR (256 ? 256). Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative samples on future video prediction on Kinetics (64 ? 64). Zoom in for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure A1 : 1 s,f k c and m k c from e k c , d k s , f k? 1 c and m k? 1 c</head><label>A1111</label><figDesc>In(1), illustration of the encoding (E) and decoding (D) architectures, and the interactions between the two thanks to the progressive optical flow and fusion mask estimation operator (F). As opposed to E, D processes features from low to high spatial resolution. To allow feature updates from context frame x c in the decoding phase of a latent embedding z s , we first compute encoded features e k c for all intermediate resolution levels (r k ) k? 1,K by applying E to x c . We then decode x s = D(z s ) from low (k = 1) to high resolution (k = K) by iterating over the following steps:(a) apply one decoding sub-module to get d k s from d k?(b) use d k s and e k c (at corresponding levels) to estimate the flow f k c from e k c to d k s and a fusion mask m k c , (c) use f k c and m k c to update d k s with e k c (see update rule), and (d) move to the next resolution level by going back to (a). The residual computation of is shown in (2) and detailed in the text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Data:</head><label></label><figDesc>Flow g ? R 2?H?W Result: Approximation of inverted flow f ? R 2?H?W /* initialization of flow and cell-completion mask */ f = 0 ? R 2?H?W ; c = 0 ? {0; 1} H?W ; /* invert flow in cells for which there exists a direct mapping */ for (h, w) ? 1; H ? 1;W do dh = g 0,h,w ; dw = g 1,h,w ; h = round(h + dh); w = round(w + dw); if (h , w ) ? 1; H ? 1; W then f 0,h ,w = ?dh; f 1,h ,w = ?dw; c h ,w = 1; end end /* fill empty cells iteratively by interpolating neighbours' flow */ while ?(h, w) ? 1; H ? 1; W | ? c h,w do c = D(c)? ? c; i = B(f ); w = B(c); for (h, w) ? 1; H ? 1; W | c h,w do f 0,h,w = i 0,h,w /w h,w ; f 1,h,w = i 1,h,w /w h,w ; end c = c ? c ; endThe blurring function B : R H?W ? R H?W is a 2D convolution with 3 ? 3 Gaussian kernel (and 0-valued 1-sized padding to preserve the spatial resolution), and interpolates flow values from neighbouring cells weighted by the their proximity; Dilation function D : {0; 1} H?W ? {0; 1} H?W propagates 1-valued cells in a 2D boolean mask according to the cardinal directions (up, down, left, right); ?, ? and ? are notations which designate logical AND, OR and NOT respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure D2 :</head><label>D2</label><figDesc>Future video synthesis of 30-frame videos given 3 starting layout-frame pairs on Cityscapes (256 ? 512). We show different samples of synthetic layout and image pairs at intermediate timesteps. In this case, layouts are a byproduct of the synthesis process, i.e., they are synthesized alongside predicted frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure D6 :</head><label>D6</label><figDesc>Qualitative samples of class-conditioned synthesis without input image on Weizmann (128?128). Videos are synthesized from the class label alone. The diversity of brightness, contrast and saturation levels reflects the augmentations we use during training. This is necessary to avoid severe overfitting due to the small size of the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>"</head><label></label><figDesc>Sup.": self-supervision of F ; "Ctxt.": number of context frames taken into account (in F ) when decoding current frame (in D).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Ctxt. = 1 Ctxt. = 4 Ctxt. = 16 (Ctxt. = 1 Ctxt. = 4 Ctxt. = 16 (Ctxt. = 1 Ctxt. = 4 Ctxt. = 16 Figure D7 :Figure F1 :</head><label>141614161416D7F1</label><figDesc>b) Kinetics (64 ? 64). c) AudioSet-Drums (128 ? 128). Qualitative samples for long-term generalization of a model trained on 16-frame videos to the synthesis of 90-frame ones conditioned on one frame. We compare different context windows ("Ctx.") for the flow module: 0 (not using the flow), 1, 4 and 16. Zoom in for details. Some examples of reconstructed image, fusion mask and optical flow from self-augmented views on Cityscapes (256 ? 512). For better visualization, we use a dashed red line in the synthetic mask and flow to indicate the outline of the real mask. Training losses encourage real and synthetic masks to be close within occluded context areas (white region), and real and synthetic flows to be close within the visible context areas (black region).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of the transformer on BAIR. We adopt notations and evaluation setups fromTable 1.</figDesc><table><row><cell cols="7">Architecture Top-k State-conditioned Pred.</cell><cell>Unc.</cell></row><row><cell cols="5">Layer Head Dec. Frame State FVD?</cell><cell>PSNR?</cell><cell>FVD?</cell><cell>FVD?</cell></row><row><cell>6</cell><cell>4</cell><cell>1</cell><cell>1</cell><cell>73?2</cell><cell cols="2">23.0 281?7 474?17</cell></row><row><cell cols="2">12 8</cell><cell>1</cell><cell>1</cell><cell>73?3</cell><cell cols="2">23.1 262?7 435?8</cell></row><row><cell cols="2">24 16</cell><cell>1</cell><cell>1</cell><cell>70?3</cell><cell cols="2">23.3 331?9 521?19</cell></row><row><cell cols="2">24 16</cell><cell>1</cell><cell>1</cell><cell>69?2</cell><cell cols="2">23.2 321?9 479?34</cell></row><row><cell cols="2">24 16</cell><cell cols="3">10 1 65?2</cell><cell cols="2">22.4 127?7 308?19</cell></row><row><cell cols="2">24 16</cell><cell cols="2">100 1</cell><cell>67?1</cell><cell cols="2">22.3 121?2 314?12</cell></row><row><cell cols="2">24 16</cell><cell cols="3">100 10 67?1</cell><cell cols="2">22.3 100?2 293?7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Future video prediction on BAIR (64 ? 64), synthesizing 16-frame videos given a few conditioning frames ("Cond."). We include some extensions of our method at higher resolution. * : training / inference / SSIM at 256 ? 256, FVD at 64 ? 64.</figDesc><table><row><cell>Method</cell><cell cols="2">Cond. FVD ?</cell><cell cols="2">Code Avail. Memory, compute</cell></row><row><cell>MoCoGAN [65]</cell><cell>4</cell><cell>503</cell><cell></cell><cell>16GB, 23h  *</cell></row><row><cell>SVG-FP [15]</cell><cell>2</cell><cell>315</cell><cell></cell><cell>12GB, 6 to 24h  *</cell></row><row><cell>CDNA [21]</cell><cell>2</cell><cell>297</cell><cell></cell><cell>10GB, 20h  *</cell></row><row><cell>SV2P [2]</cell><cell>2</cell><cell>263</cell><cell></cell><cell>16GB, 24 to 48h  *</cell></row><row><cell>SRVP [22]</cell><cell>2</cell><cell>181</cell><cell></cell><cell>36GB, 168h  *</cell></row><row><cell>VideoFlow [42]</cell><cell>3</cell><cell>131</cell><cell></cell><cell>128GB, 336h  *</cell></row><row><cell>LVT [53]</cell><cell>1</cell><cell>126?3</cell><cell></cell><cell>128GB, 48h</cell></row><row><cell>SAVP [44]</cell><cell>2</cell><cell>116</cell><cell></cell><cell>32GB, 144h</cell></row><row><cell>DVD-GAN-FP [12]</cell><cell>1</cell><cell>110</cell><cell></cell><cell>2TB, 24h  *</cell></row><row><cell>Video Transformer (S) [79]</cell><cell>1</cell><cell>106?3</cell><cell></cell><cell>256GB, 33h  *</cell></row><row><cell>TriVD-GAN-FP [45]</cell><cell>1</cell><cell>103</cell><cell></cell><cell>1TB, 280h  *</cell></row><row><cell>Low res. CCVS (ours)</cell><cell>1</cell><cell>99?2</cell><cell></cell><cell>128GB, 40h</cell></row><row><cell>Video Transformer (L) [79]</cell><cell>1</cell><cell>94?2</cell><cell></cell><cell>512GB, 336h  *</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">SSIM ? (t = 8) SSIM ? (t = 15)</cell></row><row><cell>High res.  *  *  CCVS (ours)</cell><cell>1</cell><cell>80?3</cell><cell>0.729</cell><cell>0.683</cell></row><row><cell>+ end frame</cell><cell>2</cell><cell>81?2</cell><cell>0.766</cell><cell>0.839</cell></row><row><cell>+ state</cell><cell>1</cell><cell>50?1</cell><cell>0.885</cell><cell>0.863</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : value confirmed by authors;*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Future video prediction on Kinetics (64?64) of 16-frame videos from 5 consecutive input frames.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Codebook size on Kinetics.</figDesc><table><row><cell>Size</cell><cell>Reconstruction</cell><cell>Pred.</cell></row><row><cell>1024</cell><cell cols="2">58?1 0.907 31.2 66?2</cell></row><row><cell>4096</cell><cell cols="2">54?1 0.917 31.6 64?1</cell></row><row><cell>16384</cell><cell cols="2">49?1 0.923 32.0 55?1</cell></row><row><cell>65536</cell><cell cols="2">45?1 0.928 32.2 61?1</cell></row><row><cell>?</cell><cell cols="2">12?1 0.963 34.5 229?1</cell></row></table><note>FVD ? SSIM ? PSNR ? FVD ?</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Sound-conditional video synthesis on AudioSet-Drums (64?64). 971?0.017 0.661?0.010 0.510?0.008 30.0?1.1 16.6?0.3 13.5?0.1 Vougioukas et al. [73] 15 0.940?0.017 0.904?0.007 0.896?0.015 26.2?1.0 23.8?0.2 23.3?0.3 Sound2Sight [10] 15 0.984?0.009 0.954?0.007 0.947?0.007 33.2?0.1 27.9?0.5 27.0?0.3 CCVS (ours) 15 0.987?0.001 0.956?0.006 0.945?0.008 33.7?0.4 28.4?0.6 27.3?0.</figDesc><table><row><cell>Method</cell><cell>Cond. Audio</cell><cell>t = 16</cell><cell>SSIM? t = 30</cell><cell>t = 45</cell><cell>t = 16</cell><cell>PSNR? t = 30</cell><cell>t = 45</cell></row><row><cell>SVG-LP [15]</cell><cell>15</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>5 4.2 Quantitative and qualitative studies BAIR. For future video prediction on BAIR</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B1 :</head><label>B1</label><figDesc>Reconstruction of 16-frame videos from compressed features (z s ) and the first frame on BAIR (256 ? 256). Features z s are either fixed (copied from the first timestep for subsequent timesteps) or dynamic (the true ones, obtained by applying E frame-by-frame to the video).</figDesc><table><row><cell>zs</cell><cell cols="2">FVD? PSNR?</cell></row><row><cell>Fixed</cell><cell cols="2">556?12 18.9</cell></row><row><cell>Dynamic</cell><cell>45?1</cell><cell>26.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table D1 :</head><label>D1</label><figDesc>Unconditional video synthesis of 16-frame videos on UCF-101 (128?128).</figDesc><table><row><cell>Method</cell><cell>IS ?</cell><cell>FVD ?</cell></row><row><cell>StyleGAN2 [38] (repeat ?16)</cell><cell>17.98?.12</cell><cell>990?33</cell></row><row><cell>Real frame (repeat ?16)</cell><cell>28.41?.11</cell><cell>838?27</cell></row><row><cell>VGAN [71]</cell><cell>8.31?.09</cell><cell>-</cell></row><row><cell>TGAN [56]</cell><cell>11.85?.07</cell><cell>-</cell></row><row><cell>MoCoGAN [65]</cell><cell>12.42?.07</cell><cell>-</cell></row><row><cell>ProgressiveVGAN [1]</cell><cell>14.56?.05</cell><cell>-</cell></row><row><cell>LDVD-GAN [36]</cell><cell>22.91?.19</cell><cell>-</cell></row><row><cell>TGANv2 [57]</cell><cell cols="2">26.60?.47 1209?28</cell></row><row><cell>DVD-GAN [12]</cell><cell>27.38?.53</cell><cell>-</cell></row><row><cell>MoCoGAN-HD [63]</cell><cell>33.95?.25</cell><cell>700?24</cell></row><row><cell cols="2">StyleGAN2 [38] + CCVS (ours) 24.47?.13</cell><cell>386?15</cell></row><row><cell>Real frame + CCVS (ours)</cell><cell>41.37?.39</cell><cell>389?14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table D2 :</head><label>D2</label><figDesc>Layout-conditioned video synthesis on Cityscapes (256?512). 750?0.002 0.639?0.003 0.582?0.005 20.7?0.1 18.8?0.1 17.9?0.1 CCVS (ours) 3 0.783?0.002 0.690?0.003 0.640?0.005 21.5?0.1 19.6?0.1 18.7?0.1 CCVS + [52] (ours) 3 0.776?0.002 0.742?0.002 0.723?0.005 22.1?0.1 20.6?0.1 19.9?0.1</figDesc><table><row><cell>Method</cell><cell>Cond. Layout</cell><cell>t = 10</cell><cell>SSIM? t = 20</cell><cell>t = 30</cell><cell>t = 10</cell><cell>PSNR? t = 20</cell><cell>t = 30</cell></row><row><cell>CCVS (ours)</cell><cell>3</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table D3 :</head><label>D3</label><figDesc>Sound-conditioned video synthesis on AudioSet-Drums (64?64). 952?0.005 0.942?0.009 0.925?0.002 28.1?0.5 26.8?0.5 25.8?0.6 CCVS (ours) 1 0.955?0.006 0.943?0.008 0.931?0.010 28.2?0.5 27.1?0.4 26.3?0.4</figDesc><table><row><cell>Method</cell><cell>Cond. Audio</cell><cell>t = 16</cell><cell>SSIM? t = 30</cell><cell>t = 45</cell><cell>t = 16</cell><cell>PSNR? t = 30</cell><cell>t = 45</cell></row><row><cell>CCVS (ours)</cell><cell>1</cell><cell>0.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table D5 :</head><label>D5</label><figDesc>Qualitative ablation study of the autoencoder on BAIR (256?256). Reconstruction of 30-frame videos given the first frame and the compressed features at subsequent timesteps.</figDesc><table><row><cell>Self-recovery</cell><cell>Ctxt.-recovery</cell><cell>Reconstructed frames</cell></row><row><cell>RGB VGG ?i</cell><cell>?t</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our implementation of flow inversion approximation is detailed in Appendix C.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was granted access to the HPC resources of IDRIS under the allocation 2020-AD011012227 made by GENCI. It was funded in part by the French government under management of Agence Nationale de la Recherche as part of the "Investissements d'avenir" program, reference ANR-19-P3IA-0001 (PRAIRIE 3IA Institute). JP was supported in part by the Louis Vuitton/ENS chair in artificial intelligence and the Inria/NYU collaboration. We thank the reviewers for useful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Towards high resolution video generation with progressive growing of sliced wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Acharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danda</forename><surname>Pani Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Stochastic variational video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><forename type="middle">H</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons for conditional computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>L?onard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ilya Sutskever, and Dario Amodei</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A short note about Kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Improved conditional VRNNs for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluis</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Everybody dance now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiry</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sound2Sight: Generating visual dynamics from sound and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moitreya</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial video generation on complex datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Stochastic video generation with a learned prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The carbon impact of artificial intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Payal</forename><surname>Dhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Self-supervised visual planning with temporal skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<editor>CoRL</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bj?rn</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stochastic latent residual video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Yves</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Delasalles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micka?l</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallinari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic video cnns through representation warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghudeep</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Disentangling propagation and generation for video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lena</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Controllable video generation with sparse trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The curious case of neural text degeneration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Buys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Music transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Dinculescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">LiteFlowNet: A lightweight convolutional neural network for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You said that?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Jamaludin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Synthesising talking faces from audio. IJCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">What matters in unsupervised optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Jonschkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Stone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Lower dimensional kernels for video discriminators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Kahembwe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subramanian</forename><surname>Ramamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nikolaos Pappas, and Fran?ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">VideoFlow: A conditional flow-based model for stochastic video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Durk</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Quantifying the carbon emissions of machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Luccioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Dandres</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Stochastic adversarial video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">X</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Ebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Diego de Las Casas, Yotam Doron, Albin Cassirer, and Karen Simonyan. Transformation-based adversarial video prediction on large-scale data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">World-consistent video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Playable video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willi</forename><surname>Menapace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Lathuili?re</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksandr</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Duc Thanh Nguyen, and Saeid Nahavandi. Deep learning for deepfakes creation and detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><forename type="middle">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Viet Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuong</forename><forename type="middle">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dung</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Video generation from single semantic label map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Semantic image synthesis with spatiallyadaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Rakhimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Volkhonskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Latent video transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Temporal generative adversarial nets with singular value clipping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eiichi</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Train sparsely, generate densely: Memory-efficient unsupervised training of high-resolution temporal GAN. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunta</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sosuke</forename><surname>Kobayashi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><forename type="middle">Etzioni</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">I</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Energy and policy considerations for deep learning in NLP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananya</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Hierarchical multi-scale attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Sapra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A good image generator is what you need for high-resolution video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Olszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">MoCoGAN: Decomposing motion and content for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Towards accurate generative models of video: A new metric &amp; challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Sjoerd Van Steenkiste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Kurach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Marinier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruben</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xunyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">End-to-end speech-driven facial animation with temporal GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Vougioukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Video-to-video synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Point-to-point video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsun-Hsuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">G3AN: Disentangling appearance and motion for video generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bilinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Bremond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antitza</forename><surname>Dantcheva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning flow-based feature warping for face frontalization with illumination inconsistent supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Scaling autoregressive video models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>T?ckstr?m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Greedy hierarchical variational autoencoders for large-scale video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Future video synthesis with object motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Diversity-sensitive conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">The unusual effectiveness of averaging in GAN training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasin</forename><surname>Yaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan-Sheng</forename><surname>Foo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim-Hui</forename><surname>Yap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Piliouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Chandrasekhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
