<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
							<email>1yuxinxia@cs.cmu.edu2zecheng@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecheng</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
							<email>hanj@illinois.edu4j.carlyang@emory.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SAIS: Supervising and Augmenting Intermediate Steps for Document-Level Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stepping from sentence-level to documentlevel, the research on relation extraction (RE) confronts increasing text length and more complicated entity interactions. Consequently, it is more challenging to encode the key information sources-relevant contexts and entity types. However, existing methods only implicitly learn to model these critical information sources while being trained for RE. As a result, they suffer the problems of ineffective supervision and uninterpretable model predictions.</p><p>In contrast, we propose to explicitly teach the model to capture relevant contexts and entity types by Supervising and Augmenting Intermediate Steps (SAIS) for RE. Based on a broad spectrum of carefully designed tasks, our proposed SAIS method not only extracts relations of better quality due to more effective supervision, but also retrieves the corresponding supporting evidence more accurately so as to enhance interpretability. By assessing model uncertainty, SAIS further boosts the performance via evidence-based data augmentation and ensemble inference while reducing the computational cost. Eventually, SAIS delivers state-of-the-art RE results on three benchmarks (DocRED, CDR, and GDA) and outperforms the runner-up by 5.04% relatively in F1 score in evidence retrieval on DocRED.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Playing a crucial role in the continuing effort of transforming unstructured text into structured knowledge, RE <ref type="bibr" target="#b0">(Bach and Badaskar, 2007)</ref> seeks to identify relations between an entity pair based on a given piece of text. Earlier studies mostly pay attention to sentence-level RE <ref type="bibr" target="#b47">(Zhang et al., 2017;</ref><ref type="bibr" target="#b11">Hendrickx et al., 2019)</ref> (i.e., the targeting entity pair co-occur within a sentence) and achieve promising results . Based on an extensive empirical analysis,  reveals that textual contexts and entity types are the major information sources that lead to the success of prior approaches.</p><p>Given that more complicated relations are often expressed by multiple sentences, recent focus of RE has been largely shifted to the document level <ref type="bibr" target="#b41">(Yao et al., 2019;</ref><ref type="bibr" target="#b4">Cheng et al., 2021)</ref>. Existing document-level RE methods  utilize advanced neural architectures such as heterogeneous graph neural networks <ref type="bibr" target="#b40">(Yang et al., 2020)</ref> and pre-trained language models <ref type="bibr" target="#b37">(Xu et al., 2021b)</ref>. However, although documents typically include longer contexts and more intricate entity interactions, most prior methods only implicitly learn to encode contexts and entity types while being trained for RE. As a result, they deliver inferior and uninterpretable results.</p><p>On the other hand, it has been a trend that many recent datasets support the training of more powerful language models by providing multi-task annotations such as coreference and evidence <ref type="bibr" target="#b41">(Yao et al., 2019;</ref><ref type="bibr" target="#b15">Li et al., 2016;</ref><ref type="bibr" target="#b34">Wu et al., 2019)</ref>. Therefore, in contrast to existing methods, we advocate for explicitly guiding the model to capture textual contexts and entity type information by Supervising and Augmenting Intermediate Steps (SAIS) for RE. More specifically, we argue that, from the input document with annotated entity mentions to the ultimate output of RE, there are four intermediate steps involved in the reasoning process. Consider the motivating example in <ref type="figure" target="#fig_0">Figure 1:</ref> (1) Coreference Resolution (CR): Although Sentence 0 describes the "citizenship" of "Carl Linnaeus the Younger" and Sentence 1 discusses the "father" of "Linnaeus filius", the two names essentially refer to the same person. Hence, given a document, we need to first resolve various contextual roles represented by different mentions of the same entity via CR. (2) Entity Typing (ET): After gathering contextual information from entity mentions, ET reg- ularizes entity representations with the corresponding type information (e.g., Entity A, "Linnaeus filius", is of type "PER" (person)).</p><p>Within an entity pair, the type information of the head and tail entities can be used to filter out impossible relations, as the relation "year_of_birth" can never appear between two entities of type "PER", for instance. (3) Pooled and (4) Fine-grained Evidence Retrieval (PER and FER): A unique task for locating the relevant contexts within a document for an entity pair with any valid relation is to retrieve the evidence sentences supporting the relation. Nonetheless, some entity pairs may not express valid relations within the given document (e.g., Entities D and B in the example). Meanwhile some entity pairs possess multiple relations (e.g., Entity A is both "ed-ucated_at" and an "employee" of Entity D), each with a different evidence set. Therefore, we use PER to distinguish entity pairs with and without valid supporting sentences and FER to output more interpretable evidence unique to each valid relation of an entity pair.</p><p>In this way, the four intermediate steps are complementary to RE, in the sense that CR, PER, and FER capture textual contexts while ET preserves entity type information. Consequently, by explicitly supervising the model's outputs in these intermediate steps via carefully designed tasks, we extract relations of improved quality.</p><p>In addition, based on the predicted evidence, we filtrate relevant contexts by augmenting specific intermediate steps with pseudo documents or attention masks. By assessing model confidence, we apply these two kinds of evidence-based data augmentation together with ensemble inference, only when the model is uncertain about its original predictions. Eventually, we further boost the performance with negligible computational cost.</p><p>Altogether, our SAIS method achieves state-ofthe-art RE performance on three benchmarks (Do-cRED <ref type="bibr" target="#b41">(Yao et al., 2019)</ref>, CDR <ref type="bibr" target="#b15">(Li et al., 2016)</ref>, and GDA <ref type="bibr" target="#b34">(Wu et al., 2019)</ref>) due to more effective supervision and enhances interpretability by improving the evidence retrieval (ER) F1 score on DocRED by 5.04% relatively compared to the runner-up.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consider a document d containing sentences</head><formula xml:id="formula_0">S d = {s i } |S d | i=1 and entities E d = {e i } |E d | i=1</formula><p>where each entity e is assigned an entity type c ? C and appears at least once in d by its mentions M e = {m i } |Me| i=1 . For a pair of head and tail entities (e h , e t ), document-level RE aims to predict if any relation r ? R exists between them, based on whether r is expressed by some pair of e h 's and e t 's mentions in d. Here, C and R are pre-defined sets of entity and relation types, respectively. Moreover, for (e h , e t ) and each of their valid relations r ? R h,t , ER aims to identify the subset V h,t,r of S d that is sufficient to express the triplet (e h , e t , r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Related Work</head><p>Early research efforts on RE <ref type="bibr" target="#b0">(Bach and Badaskar, 2007;</ref><ref type="bibr" target="#b22">Pawar et al., 2017)</ref> center around predicting relations for entity pairs at the sentence level <ref type="bibr" target="#b47">(Zhang et al., 2017;</ref><ref type="bibr" target="#b11">Hendrickx et al., 2019)</ref>. Many pattern-based <ref type="bibr" target="#b3">(Califf and Mooney, 1999;</ref><ref type="bibr" target="#b24">Qu et al., 2018;</ref> and neural network-based <ref type="bibr" target="#b2">(Cai et al., 2016;</ref><ref type="bibr" target="#b8">Feng et al., 2018;</ref> models have shown impressive results. A recent study  attributes the success of these models to their ability to capture textual contexts and entity type information.</p><p>Nevertheless, since more complicated relations can only be expressed by multiple sentences, there has been a shift of focus lately towards documentlevel RE <ref type="bibr" target="#b41">(Yao et al., 2019;</ref><ref type="bibr" target="#b15">Li et al., 2016;</ref><ref type="bibr" target="#b4">Cheng et al., 2021;</ref><ref type="bibr" target="#b34">Wu et al., 2019)</ref>. According to how an approach models contexts, there are two general trends within the domain. Graph-based approaches <ref type="bibr" target="#b43">Zeng et al., 2021;</ref><ref type="bibr">Xu et al., 2021c,d;</ref><ref type="bibr" target="#b25">Sahu et al., 2019;</ref><ref type="bibr" target="#b10">Guo et al., 2019)</ref> typically infuse contexts into heuristic-based document graphs and perform multi-hop reasoning via advanced neural techniques. Transformer-based approaches <ref type="bibr" target="#b48">Tang et al., 2020;</ref> leverage the strength of pre-trained language models <ref type="bibr" target="#b6">(Devlin et al., 2019;</ref> to encode long-range contextual dependencies. However, most prior methods only implicitly learn to capture contexts while being trained for RE. Consequently, they experience ineffective supervision and uninterpretable model predictions.</p><p>On the contrary, we propose to explicitly teach the model to capture textual contexts and entity type information via a broad spectrum of carefully designed tasks. Furthermore, we boost the RE performance by ensembling the results of evidenceaugmented inputs. Compared to EIDER , we leverage the more precise and interpretable FER for retrieving evidence and present two different kinds of evidence-based data augmentation. We also save the computational cost by applying ensemble learning only to the uncertain subset of relation triplets. As a result, our SAIS method not only enhances the RE performance due to more effective supervision, but also retrieves more accurate evidence for better interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Supervising Intermediate Steps</head><p>This section describes the tasks that explicitly supervise the model's outputs in the four intermediate steps. Together they complement the quality of RE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Encoding</head><p>Given the promising performance of pre-trained language models (PLM) in various downstream tasks, we resort to PLM for encoding the document. More specifically, for a document d, we insert a classifier token "[CLS]" and a separator token "[SEP]" at the start and end of each sentence s ? S d , respectively. Each mention m ? M d is wrapped with a pair of entity markers "*" <ref type="bibr" target="#b47">(Zhang et al., 2017)</ref> to indicate the position of entity mentions. Then we feed the document, with alternating segment token indices for each sentence <ref type="bibr" target="#b17">(Liu and Lapata, 2019)</ref>, into a PLM:</p><formula xml:id="formula_1">H, A = PLM(d),<label>(1)</label></formula><p>to obtain the token embeddings H ? R N d ?H and the cross-token attention</p><formula xml:id="formula_2">A ? R N d ?N d .</formula><p>A is the average of the attention heads in the last transformer layer <ref type="bibr" target="#b27">(Vaswani et al., 2017</ref>) of the PLM. N d is the number of tokens in d, and H is the embedding dimension of the PLM. We take the embedding of "*" or "[CLS]" before each mention or sentence as the corresponding mention or sentence embedding, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Coreference Resolution (CR)</head><p>As a case study, it is reported by <ref type="bibr" target="#b41">Yao et al. (2019)</ref> that around 17.6% of relation instances in DocRED require coreference reasoning. Hence, after encoding the document, we resolve the repeated contextual mentions to an entity via CR. In particular, consider a pair of mentions (m i , m j ), we determine the probability of whether m i and m j refer to the same entity by passing their corresponding embeddings m i and m j through a group bilinear layer <ref type="bibr" target="#b49">(Zheng et al., 2019)</ref>. The layer splits the embeddings into K equal-sized groups ([m 1 i , . . . , m K i ] = m i , similar for m j ) and applies bilinear with parameter W k m ? R H/K?H/K within each group:</p><formula xml:id="formula_3">P CR i,j = ? K k=1 m k i W k m m k j + b m ,<label>(2)</label></formula><p>where b m ? R and ? is the sigmoid function.</p><p>Since most mention pairs refer to distinct entities (each entity has only 1.34 mentions on average in DocRED), we adopt the focal loss <ref type="bibr">(Lin et al., 2017)</ref> on top of the binary cross-entropy to mitigate this extreme class imbalance:</p><formula xml:id="formula_4">CR d = ? m i ?M d m j ?M d y CR i,j (1 ? P CR i,j ) ? CR log P CR i,j + (1 ? y CR i,j )(P CR i,j ) ? CR log(1 ? P CR i,j ) w CR i,j ,<label>(3)</label></formula><p>where y CR i,j = 1 if m i and m j refer to the same entity, and 0 otherwise. Class weight w CR i,j is inversely proportional to the frequency of y CR i,j , and ? CR is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Entity Typing (ET)</head><p>In a pair of entities, the type information can be used to filter out impossible relations. Therefore, we regularize entity embeddings via ET. More specifically, we first derive the embedding of an entity e by integrating the embeddings of its mentions M e via logsumexp pooling <ref type="bibr" target="#b13">(Jia et al., 2019)</ref>: e = log m?Me exp(m). Since entity e could appear either at the head or tail in an entity pair, we distinguish between the head entity embedding e h and the tail entity embedding e t via two separate linear layers:</p><formula xml:id="formula_5">e h = W e h e + b e h , e t = W et e + b et , (4) where W e h , W et ? R H?H and b e h , b et ? R H .</formula><p>However, no matter where e appears in an entity pair, its head and tail embeddings should always preserve e's type information. Hence, we calculate the probability of which entity type e belongs to by passing e ? for ? ? {h, t} through a linear layer</p><formula xml:id="formula_6">P ET e = ?(W c tanh(e ? ) + b c ) ,<label>(5)</label></formula><p>followed by the multi-class cross-entropy loss:</p><formula xml:id="formula_7">ET d = ? e?E d c?C y ET e,c log P ET e,c ,<label>(6)</label></formula><p>where W c ? R |C|?H , b c ? R |C| , and ? is the softmax function. y ET e,c = 1 if e is of entity type c, and 0 otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Pooled Evidence Retrieval (PER)</head><p>To further capture textual contexts, we explicitly guide the attention in the PLM to the supporting sentences of each entity pair via PER. That is, we want to identify the pooled evidence set V h,t = ? r?R h,t V h,t,r in d that is important to an entity pair (e h , e t ), regardless of the specific relation expressed by a particular sentence s ? V h,t . In this case, given (e h , e t ), we first compute a unique context embedding c h,t based on the cross-token attention from Equation 1:</p><formula xml:id="formula_8">c h,t = H A h ? A t 1 (A h ? A t ) .<label>(7)</label></formula><p>Here, ? is the element-wise product. A h is e h 's attention to all the tokens in the document (i.e., the average of e h 's mention-level attention). Similar for A t . Then we measure the probability of whether a sentence s ? S d is part of the pooled supporting evidence V h,t by passing (e h , e t )'s context embedding c h,t and sentence s' embedding s through a group bilinear layer:</p><formula xml:id="formula_9">P PER h,t,s = ? K k=1 c k h,t W k p s k + b p ,<label>(8)</label></formula><p>where W k p ? R H/K?H/K and b p ? R. Again, we face a severe class imbalance here, since most entity pairs (97.1% in DocRED) do not have valid relations or supporting evidence. As a result, similar to Section 3.2, we also use the focal loss with the binary cross-entropy:</p><formula xml:id="formula_10">PER d = ? e h ?E d et?E d s?S d y PER h,t,s (1 ? P PER h,t,s ) ? PER log P PER h,t,s + (1 ? y PER h,t,s )(P PER h,t,s ) ? PER log(1 ? P PER h,t,s ) w PER h,t,s ,<label>(9)</label></formula><p>where y PER</p><formula xml:id="formula_11">h,t,s = 1{s ? V h,t }, class weight w PER h,t,s</formula><p>is inversely proportional to the frequency of y PER h,t,s , and ? PER is a hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Fine-grained Evidence Retrieval (FER)</head><p>In addition to PER, we would like to further refine V h,t , since an entity pair could have multiple valid relations and, correspondingly, multiple sets of evidence. As a result, we explicitly train the model to recover contextual evidence unique to a triplet (e h , e t , r) via FER for better interpretability. More specifically, given (e h , e t , r), we first generate a triplet embedding l h,t,r by merging e h , e t , c h,t , and r's relation embedding r via a linear layer:</p><formula xml:id="formula_12">l h,t,r = tanh(W l [e h e t c h,t r] + b l ) ,<label>(10)</label></formula><p>where W l ? R H?4H , b l ? R H , represents concatenation, and r is initialized from the embedding matrix of the PLM.</p><p>Similarly, we use a group bilinear layer to assess the probability of whether a sentence s ? S d is included in the fine-grained evidence set V h,t,r :</p><formula xml:id="formula_13">P FER h,t,r,s = ? K k=1 l k h,t,r W k f s k + b f ,<label>(11)</label></formula><p>where W k f ? R H/K?H/K and b f ? R. Since FER only involves entity pairs with valid relations, the class imbalance is milder here than in PER. Hence, let y FER h,t,r,s = 1{s ? V h,t,r }, we deploy the standard binary cross-entropy loss:</p><formula xml:id="formula_14">FER d = ? e i ?E d e j ?E d r?R h,t s?S d y FER h,t,r,s log P FER h,t,r,s + (1 ? y FER h,t,r,s ) log(1 ? P FER h,t,r,s ) .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Relation Extraction (RE)</head><p>Based on the four complementary tasks introduced above, for an entity pair (e h , e t ), we encode relevant contexts in c h,t and preserve entity type information in e h and e t . Ultimately, we acquire the contexts needed by the head and tail entities from c h,t via two separate linear layers:</p><formula xml:id="formula_15">c h = W c h c h,t + b c h , c t = W ct c h,t + b ct , (13) where W c h , W ct ? R H?H and b c h , b ct ? R H ,</formula><p>and then combine them with the type information to generate the head and tail entity representations:</p><formula xml:id="formula_16">e h = tanh(e h + c h ), e t = tanh(e t + c t ). (14)</formula><p>Next, a group bilinear layer is utilized to calculate the logit of how likely a relation r ? R exists between e h and e t :</p><formula xml:id="formula_17">L RE h,t,r = K k=1 e k h W k r e k t + b r ,<label>(15)</label></formula><p>where W k r ? R H/K?H/K and b r ? R. As discussed earlier, only a small portion of entity pairs have valid relations, among which multiple relations could co-exist between a pair. Therefore, to deal with the problem of multi-label imbalanced classification, we follow  by introducing a threshold relation class TH and adopting an adaptive threshold loss:</p><formula xml:id="formula_18">RE d = ? e h ?E d et?E d ? ? r?P h,t log exp L RE h,t,r r ?P h,t ?{TH} L RE h,t,r + log exp L RE h,t,TH r ?N h,t ?{TH} L RE h,t,r .<label>(16)</label></formula><p>In essence, we aim to increase the logits of valid relations P h,t and decrease the logits of invalid relations N h,t , both relative to TH. Overall, with the goal of improving the model's RE performance by better capturing entity type information and textual contexts, we have designed four tasks to explicitly supervise the model's outputs in the corresponding intermediate steps. To this end, we visualize the entire pipeline SAIS O All in Appendix A and integrate all the tasks by minimizing the multi-task learning objective</p><formula xml:id="formula_19">= d?D train RE d + Task ? Task Task d ,<label>(17)</label></formula><p>where Task ? {CR, ET, PER, FER}. ? Task 's are hyperparameters balancing the relative task weight.</p><p>During inference with the current pipeline SAIS O All , we predict if a triplet (e h , e t , r) is valid (i.e., if relation r exists between entity pair (e h , e t )) by checking if its logit is larger than the corresponding threshold logit (i.e., L RE h,t,r &gt; L RE h,t,TH ). For each predicted triplet (e h , e t , r), we assess if a sentence s belongs to the evidence set V h,t,r by checking if P FER h,t,r,s &gt; ? FER where ? FER is a threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Augmenting Intermediate Steps</head><p>We further improve RE after training the pipeline SAIS O All by augmenting the intermediate steps in SAIS O All with the retrieved evidence from FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">When to Augment Intermediate Steps</head><p>The evidence predicted by FER is unique to each triplet (e h , e t , r). However, consider the total number of all possible triplets (around 40 million in the develop set of DocRED), it is computationally prohibitive to augment the inference result of each triplet with individually predicted evidence. Instead, following the idea of selective prediction <ref type="bibr" target="#b7">(El-Yaniv et al., 2010)</ref>, we identify the triplet subset U for which the model is uncertain about its relation predictions with the original pipeline SAIS O All . More specifically, we set the model's confidence for (e h , e t , r) as L O h,t,r = L RE h,t,r ? L RE h,t,TH . Then, the uncertain set U consists of triplets with the lowest ?% absolute confidence |L O h,t,r |. Consequently, we reject the original relation predictions for (e h , e t , r) ? U and apply evidence-based data augmentation to enhance the performance (more details in Section 4.2).</p><p>To determine the rejection rate ?% (note that ?% is NOT a hyperparameter), we first sort all the triplets in the develop set based on their absolute confidence |L O h,t,r |. When ?% increases, the risk (i.e., inaccuracy rate) of the remaining triplets that are not in U is expected to decrease, and vice versa. On the one hand, we wish to reduce the risk for more accurate relation predictions; on the other hand, we want a low rejection rate so that data augmentation on a small rejected set incurs little computational cost. To balance this trade-off, we set ?% as the rate that achieves the minimum of risk 2 + rejection rate 2 . As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we find ?% ? 4.6% in the develop set of DocRED. In practice, we can further limit the maximum number of rejected triplets per entity pair. By setting it as  10 in experiments, we reduce the size of U to only 1.5% of all the triplets in the DocRED develop set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">How to Augment Intermediate Steps</head><p>Consider a triplet (e h , e t , r) ? U. We first assume its validity and calculate the probability P FER h,t,r,s of a sentence s being part of V h,t,r based on Section 3.5. Then in a similar way to how L O h,t,r is generated with SAIS O All , we design two types of evidencebased data augmentation as follows: </p><formula xml:id="formula_20">P B h,t,r = ?(L B h,t,r ) = ?(L O h,t,r + L D h,t,r + L M h,t,r ? ? r ).<label>(18)</label></formula><p>These parameters are trained by minimizing the binary cross-entropy loss on U of the develop set:</p><formula xml:id="formula_21">B = ? (e h , et, r) ? U y RE h,t,r log P B h,t,r + (1?y RE h,t,r ) log(1 ? P B h,t,r ) ,<label>(19)</label></formula><p>where y RE h,t,r = 1 if (e h , e t , r) is valid, and 0 otherwise. When making relation predictions for (e h , e t , r) ? U, we check whether its blended confidence is positive (i.e., L B h,t,r &gt; 0). In this way, we improve the RE performance when the model is uncertain about its original predictions and save the computational cost when the model is confident. The overall steps for evidencebased data augmentation and ensemble inference SAIS B</p><p>All are summarized in Appendix B. These steps are executed only after the training of SAIS O All and, therefore, adds negligible computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>We evaluate the proposed SAIS method on the following three document-level RE benchmarks. DocRED <ref type="bibr" target="#b41">(Yao et al., 2019)</ref> is a large-scale crowdsourced dataset based on Wikipedia articles. It consists of 97 relation types, seven entity types, and 5,053 documents in total, where each document has 19.5 entities on average. CDR <ref type="bibr" target="#b15">(Li et al., 2016)</ref> and GDA <ref type="bibr" target="#b34">(Wu et al., 2019)</ref> are two biomedical datasets where CDR studies the binary interactions between disease and chemical concepts with 1,500 documents and GDA studies the binary relationships between gene and disease with 30,192 documents. We follow  for splitting the train and develop sets.</p><p>We run our experiments on one Tesla A6000 GPU and carry out five trials with different seeds to report the mean and one standard error. Based on Huggingface <ref type="bibr" target="#b32">(Wolf et al., 2019)</ref>, we apply cased BERT-base <ref type="bibr" target="#b6">(Devlin et al., 2019)</ref> and RoBERTalarge  for DocRED and cased SciBERT  for CDR and GDA. The embedding dimension H of BERT or SciBERT is 768, and that of RoBERTa is 1,024. The number of groups K in all group bilinear layers is 64.</p><p>For the general hyperparameters of language models, we follow the setting in . The learning rate for fine-tuning BERT is 5e?5, that for fine-tuning RoBERTa or SciBERT is 2e?5, and that for training the other parameters is 1e?4. All the trials are optimized by AdamW <ref type="bibr" target="#b19">(Loshchilov and Hutter, 2019)</ref> for 20 epochs with early stopping and a linearly decaying scheduler <ref type="bibr">(Goyal et al., 2017</ref>) whose warm-up ratio = 6%. Each batch contains 4 documents and the gradients of model parameters are clipped to a maximum norm of 1.</p><p>For the unique hyperparameters of our method, we choose 2 from {1, 1.5, 2} for the focal hyperparameters ? CR and ? PER based on the develop   59.14 61.22 -59.00 61.24 -DRN-BERT base <ref type="bibr" target="#b38">(Xu et al., 2021c)</ref> 59.33 61.39 -59.15 61.37 -SIRE-BERT base <ref type="bibr" target="#b43">(Zeng et al., 2021)</ref> 59   55.22 58.72 47.14 ---SSAN-BERT base  57.03 59.19 -56.06 58.41 -ATLOP-BERT base  59.22 61.09 -59.31 61.30 -DocuNet-BERT base  59.86 61.83 -59.93 61.86 -Eider-BERT base  60. <ref type="formula" target="#formula_1">51</ref>  set. We also follow  for setting the FER prediction threshold ? FER as 0.5 and all the relative task weights ? Task for Task ? {CR, ET, PER, FER} as 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Quantitative Evaluation</head><p>Besides RE, DocRED also suggests to predict the supporting evidence for each relation instance. Therefore, we apply SAIS B All to both RE and ER. We report the results of SAIS B All as well as existing graph-based and transformer-based baselines in <ref type="table">Table 1</ref> 2 (full details in Appendix C). Generally, thanks to PLMs' strength in modeling long-range dependencies, transformer-based methods perform better on RE than graph-based methods. Moreover, most earlier approaches are not capable of ER despite the interpretability ER adds to the predictions. In contrast, our SAIS B All method not only establishes a new state-of-the-art result on RE, but also outperforms the runner-up significantly on ER.</p><p>Since neither CDR nor GDA annotates evidence sentences, we apply SAIS O RE+CR+ET here. It is trained with RE, CR, and ET and infers without data augmentation. As shown in <ref type="table" target="#tab_5">Table 2</ref> (full details in Appendix C), our method improves the prior best RE F1 scores by 2.7% and 1.8% absolutely on CDR and GDA, respectively. It indicates that our proposed method can still improve upon the baselines even if only part of the four complementary tasks are annotated and operational.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Study</head><p>To investigate the effectiveness of each of the four complementary tasks proposed in Section 3, we carry out an extensive ablation study on the Do-cRED develop set by training SAIS with all possible combinations of those tasks. As shown in <ref type="table" target="#tab_7">Table 3</ref>, without any complementary tasks, the RE performance of SAIS is comparable to ATLOP  due to similar neural architectures. When only one complementary task is allowed, PER is the most effective single task, followed by ET. Although FER is functionally analogous to PER, since FER only involves the small subset of entity pairs with valid relations, the performance gain brought by FER alone is limited. When Model CDR GDA LSR  64.8 82.2 SciBERT  65.1 82.5 DHG  65.9 83.1 SSAN-SciBERT  68.7 83.7 ATLOP-SciBERT  69.4 83.9 SIRE-BioBERT <ref type="bibr" target="#b43">(Zeng et al., 2021)</ref> 70.8 84.7 DocuNet-SciBERT  76  two tasks are used jointly, the pair of PER and ET, which combines textual contexts and entity type information, delivers the most significant improvement. The pair of PER and FER also performs well, which reflects the finding in  that context is the most important source of information. The version with all tasks except CR sees the least drop in F1, indicating that CR's supervision signals on capturing contexts can be covered in part by PER and FER. Last but not least, the SAIS pipeline with all four complementary tasks achieves the highest F1 score. Similar trends are also recognized on CDR and GDA in <ref type="table" target="#tab_5">Table 2</ref>, where SAIS trained with both CR and ET (besides RE) scores higher than its single-task counterpart.</p><p>Moreover, as compared to the original pipeline SAIS O All , pseudo document-based data augmentation SAIS D All acts as a hard filter by directly removing predicted non-evidence sentences, while attention mask-based data augmentation SAIS M All distills the context more softly. Therefore, we observe in <ref type="table" target="#tab_8">Table 4</ref> that SAIS D All earns a higher precision, whereas SAIS M All attains a higher recall. By ensembling SAIS O All , SAIS D All , and SAIS M All , we improve the RE F1 score by 0.57% absolutely on the DocRED develop set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qualitative Analysis</head><p>To obtain a more insightful understanding of how textual contexts and entity type information help with RE, we present a case study in <ref type="figure" target="#fig_3">Figure 3</ref> (a). Here, SAIS O RE+ET is trained with the task (i.e., ET) related to entity type information while SAIS O RE+CR+PER+FER is trained with the tasks (i.e., CR, PER, and FER) related to textual contexts.    not least, SAIS O All effectively models contexts spanning multiple sentences and regularizes them with entity type information. As a result, it is the only SAIS variant that correctly predicts the relation "country_of_origin" between Entities D and C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CR ET PER FER RE F1</head><p>Furthermore, to examine why SAIS (which uses FER for retrieving evidence) outperforms Eider  (which uses PER) significantly on ER in <ref type="table">Table 1</ref>, we compare the performance of FER and PER based on a case study in <ref type="figure" target="#fig_3">Figure 3 (b)</ref>. More specifically, PER identifies the same set of evidence for both relations between Entities A and B, among which Sentence 2 describes "place_of_birth" while Sentence 6 discusses "place_of_death". In contrast, FER considers an evidence set unique to each relation and outputs more interpretable results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose to explicitly teach the model to capture the major information sources of RE-textual contexts and entity types by Supervising and Augmenting Intermediate Steps (SAIS). Based on a broad spectrum of carefully designed tasks, SAIS extracts relations of enhanced quality due to more effective supervision and retrieves more accurate evidence for improved interpretability. SAIS further boosts the performance with evidence-based data augmentation and ensemble inference while preserving the computational cost by assessing model uncertainty. Experiments on three benchmarks demonstrate the state-of-theart performance of SAIS on both RE and ER.</p><p>If given a plain document, we shall utilize existing tools (e.g., spaCy) to get noisy annotations and apply our method afterward. It is also interesting to investigate how other tasks (e.g., named entity recognition) could be incorporated into the multitask learning pipeline of our SAIS method. We plan to explore these extensions in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Multi-Task Learning Pipeline by Supervising Intermediate Steps (SAIS O All )</head><p>To explicitly teach the model to capture relevant contexts and entity type information for RE, we design four tasks to supervise the model's outputs in the corresponding intermediate steps. We illustrate the overall multi-task pipeline SAIS O All in <ref type="figure" target="#fig_4">Figure 4</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Ensemble Inference Algorithm with Evidence-based Data Augmentation (SAIS B All )</head><p>After training the multi-task pipeline SAIS O All proposed in Section 3, we further boost the model performance by evidence-based data augmentation and ensemble inference as discussed in Section 4. The detailed steps are explained in Algorithm 1 below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experiment Details</head><p>We compare the proposed SAIS method against existing baselines based on three benchmarks: CDR <ref type="bibr" target="#b15">(Li et al., 2016)</ref> and GDA <ref type="bibr" target="#b34">(Wu et al., 2019)</ref> in <ref type="table">Table 5</ref>, and DocRED <ref type="bibr" target="#b41">(Yao et al., 2019)</ref> in <ref type="table">Table 6</ref>. The details are explained in Section 5.</p><p>In particular, DocRED uses the MIT License, CDR is freely available for the research community, and GDA uses the GNU Affero General Public License. DocRED is constructed from Wikipedia and Wikidata and, therefore, contains information that names people. However, since our research focuses on identifying relations among real-world entities (including public figures) based on a given document, it is impossible to fully anonymize the dataset. We ensure that we only use publicly available information in our experiments. Our use of these datasets is consistent with their intended use. Although our method achieves state-of-the-art performance for RE and ER, using the predicted relations and evidence directly for downstream tasks without manual validation may increase the risk of errors carried forward due to the incorrect predictions. The experiments in this paper focus on English documents from biomedical and general domains, but our proposed framework can be easily extended to documents of other languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>CDR GDA</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Motivating example adapted from DocRED. From the input document with annotated entity mentions to the RE output, there are four intermediate steps involved in the reasoning process. These steps are complementary to RE, in the sense that CR, PER, and FER capture textual contexts while ET preserves entity type information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Trade-off between risk and rejection rate on the develop set of DocRED.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Pseudo Document-based (SAIS D All ): Construct a pseudo document using sentences with P FER h,t,r,s &gt; ? FER and feed it into the original pipeline to get the confidence L D h,t,r . Attention Mask-based (SAIS M All ): Formulate a mask P FER h,t,r ? R N d based on P FER h,t,r,s and modify the context embedding to c h,t = H A h ?At?P FER h,t,r 1 (A h ?At?P FER h,t,r ) . Maintain the rest of the pipeline and get the confidence L M h,t,r . Following Xie et al. (2022), we ensemble L D h,t,r , L M h,t,r , and the original confidence L O h,t,r with a blending parameter ? r ? R (Wolpert, 1992) for each relation r ? R as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) Case study on the effectiveness of textual contexts and entity type information based on models' extracted relations from the DocRED develop set. By capturing contexts across sentences and regularizing them with entity type information, SAIS O All extracts relations of better quality. (b) Case study on the difference between FER and PER based on retrieved evidence from the DocRED develop set. FER considers evidence unique to each relation for better interpretability. Irrelevant sentences are omitted here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The overall multi-task learning pipeline of the proposed SAIS method (SAIS O All ). By explicitly supervising the model's outputs in the intermediate steps via carefully designed tasks, we improve the RE performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>? 0.15 65.17 ? 0.08 55.84 ? 0.23 63.44 65.11 55.67Table 1: RE and ER results (%) on DocRED. Ign F1 refers to the F1 score excluding the relation instances mentioned in the train set. Baselines using BERT base are separated into the graph-based (upper) and transformer-based (lower) groups. We report the test scores from the official scoreboard and the baseline scores from the corresponding papers. SAIS B</figDesc><table><row><cell></cell><cell></cell><cell>62.48</cell><cell>50.71</cell><cell cols="2">60.42 62.47 51.27</cell></row><row><cell>SAIS B All -BERT base (Ours)</cell><cell cols="5">59.98 ? 0.13 62.96 ? 0.11 53.70 ? 0.21 60.96 62.77 52.88</cell></row><row><cell>RoBERTa large (Ye et al., 2020)</cell><cell>57.19</cell><cell>59.40</cell><cell>-</cell><cell>57.74 60.06</cell><cell>-</cell></row><row><cell>SSAN-RoBERTa large (Xu et al., 2021a)</cell><cell>60.25</cell><cell>62.08</cell><cell>-</cell><cell>59.47 61.42</cell><cell>-</cell></row><row><cell>E2GRE-RoBERTa large (Huang et al., 2020)</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="2">60.30 62.50 50.50</cell></row><row><cell>ATLOP-RoBERTa large (Zhou et al., 2021)</cell><cell>61.32</cell><cell>63.18</cell><cell>-</cell><cell>61.39 63.40</cell><cell>-</cell></row><row><cell>DocuNet-RoBERTa large (Zhang et al., 2021)</cell><cell>62.23</cell><cell>64.12</cell><cell>-</cell><cell>62.39 64.55</cell><cell>-</cell></row><row><cell>Eider-RoBERTa large (Xie et al., 2022)</cell><cell>62.34</cell><cell>64.27</cell><cell>52.54</cell><cell cols="2">62.85 64.79 53.01</cell></row><row><cell>SAIS B All -RoBERTa large (Ours)</cell><cell>62.23</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>All achieves state-of-the-art performance on both RE and ER. Full details in Appendix C.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: RE F1 results (%) on the CDR and GDA</cell></row><row><cell>test sets. The baseline scores are from the correspond-</cell></row><row><cell>ing papers. SAIS O RE+CR+ET scores the highest on both</cell></row><row><cell>datasets. Full details in Appendix C.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Ablation study (%) using SAIS O -BERT base to assess the effectiveness of the four complementary tasks (i.e., CR, ET, PER, and FER) for RE based on the DocRED develop set.</figDesc><table><row><cell cols="2">SAIS O All SAIS D All SAIS M All Precision Recall F1</cell></row><row><cell>66.58</cell><cell>58.70 62.39</cell></row><row><cell>73.21</cell><cell>45.59 56.19</cell></row><row><cell>53.14</cell><cell>67.49 59.46</cell></row><row><cell>71.14</cell><cell>54.35 61.62</cell></row><row><cell>61.61</cell><cell>62.90 62.25</cell></row><row><cell>67.76</cell><cell>58.79 62.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4</head><label>4</label><figDesc>Case Study on the Effectiveness of Textual Contexts and Entity Type Information: He wrote the novels that formed the basis of two ? films, Kiss of Death ? and The People Against O'Hara ? [3] Lipsky, ?, was an assistant district attorney ? and served as legal counsel to the Mystery Writers of America.</figDesc><table><row><cell>: Ablation study (%) using BERT base to assess</cell></row><row><cell>the effectiveness of data augmentation (i.e., original</cell></row><row><cell>(SAIS O All ), pseudo document-based (SAIS D All ), and at-tention mask-based (SAIS M All )) for RE based on the Do-</cell></row><row><cell>cRED develop set.</cell></row><row><cell>Compared to SAIS O All , which is trained with all</cell></row><row><cell>four complementary tasks, they both exhibit draw-</cell></row><row><cell>backs qualitatively. In particular, SAIS O RE+ET can</cell></row><row><cell>easily infer the relation "country" between Enti-</cell></row><row><cell>ties E and C based on their respective types "ORG"</cell></row><row><cell>and "LOC", whereas SAIS O RE+CR+PER+FER may mis-</cell></row><row><cell>interpret Entity E as of type "PER" and infer the</cell></row><row><cell>relation "citizenship" wrongly. On the other hand,</cell></row><row><cell>SAIS O RE+CR+PER+FER can directly predict the rela-</cell></row><row><cell>tion "place_of_birth" between Entities A and B by</cell></row><row><cell>pattern matching, while overemphasizing the type</cell></row><row><cell>"LOC" of Entity B may cause SAIS O RE+ET to deliver</cell></row><row><cell>the wrong relation prediction "location". Last but</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>Algorithm 1: Evidence-based Data Augmentation and Ensemble Inference (SAIS B All ) input: trained pipeline SAIS O All from Section 3, FER threshold ? FER , develop set D dev , test set D test for D ? {D dev , D test } do Original RE Prediction with SAIS O All (Section 3.6): For (e h , e t , r) ? D, get L O h,t,r from SAIS O All . Identify the Uncertain Set U (Section 4.1): If D is D dev , calculate ?% by minimizing (risk 2 + rejection rate 2 ). U contains triplets with the lowest ?% absolute confidence |L O h,t,r |. Predict Evidence Probability for (e h , e t , r) ? U with SAIS O All (Section 3.5): For (e h , e t , r) ? U and s ? S d in the corresponding document d, get P FER h,t,r,s from SAIS O All . Pseudo Document-based Data Augmentation SAIS D All (Section 4.2): For (e h , e t , r) ? U, get L D h,t,r by feeding the corresponding pseudo document into SAIS O All . Attention Mask-based Data Augmentation SAIS M All (Section 4.2): For (e h , e t , r) ? U, get L M h,t,r by applying the corresponding attention mask to SAIS O All . Ensemble Inference SAIS B All (Section 4.2): If D is D dev , train ? r for r ? R based on L O h,t,r , L D h,t,r , and L M h,t,r for (e h , e t , r) ? U. For (e h , e t , r) ? U, get L B h,t,r = L O h,t,r + L D h,t,r + L M h,t,r ? ? r . Ultimate RE Prediction with SAIS B All and SAIS O All (Section 4.2 and 3.6): For (e h , e t , r) ? U, extract relation r for entity pair (e h , e t ) if L B h,t,r &gt; 0. For (e h , e t , r) / ? U, extract relation r for entity pair (e h , e t ) if L O h,t,r &gt; 0. Ultimate ER Prediction with SAIS O All (Section 3.5): For predicted (e h , e t , r), retrieve s ? S d in the corresponding document d if P FER h,t,r,s &gt; ? FER . output: sets of predicted triplet (e h , e t , r) and corresponding evidence V h,t,r for D dev and D test</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Our code is available at https://github.com/ xiaoyuxin1002/SAIS.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For a fair comparison, we report the scores of SSAN without being pretrained on an extra dataset.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">A review of relation extraction. Literature review for Language and Statistics II</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nguyen</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Badaskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Scibert: A pretrained language model for scientific text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent convolutional neural network for relation classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Relational learning of pattern-match rules for information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hacred: A largescale relation extraction dataset toward hard cases in practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juntao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoye</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhefeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxing</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><forename type="middle">Jing</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Connecting the dots: Document-level neural relation extraction with edge-oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">On the foundations of noisefree selective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reinforcement learning for relation classification from noisy data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention guided graph convolutional networks for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10422</idno>
		<title level="m">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Entity and evidence guided relation extraction for docred</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.12283</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph enhanced dual attention network for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Biocreative v cdr task corpus: a resource for chemical disease relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueping</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Hsuan</forename><surname>Sciaky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><forename type="middle">Peter</forename><surname>Leaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carolyn</forename><forename type="middle">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mattingly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wiegers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. 2017. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reasoning with latent structure refinement for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Sekuli?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for chemical-disease relation extraction are improved with character-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karin</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Verspoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BioNLP Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Pawar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Girish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushpak</forename><surname>Palshikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05191</idno>
		<title level="m">Relation extraction: A survey</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning from context or names? an empirical study on neural relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Weakly-supervised relation extraction by pattern-enhanced embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inter-sentence relation extraction with document-level graph convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenia</forename><surname>Sunil Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sophia</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ananiadou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fang Fang, Shi Wang, and Pengfei Yin. 2020. Hin: Hierarchical inference network for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangxia</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAKDD</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Simultaneously self-attending to all mentions for full-abstract biological relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Verga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Strubell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Global-to-local neural networks for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Difeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fine-tune bert for docred with two-step process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christfried</forename><surname>Focke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nilesh</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11898</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wolpert</surname></persName>
		</author>
		<title level="m">Stacked generalization. Neural networks</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Renet: A deep learning approach for extracting gene-disease associations from literature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibang</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hing-Fung</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wah</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RECOMB</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Eider: Evidence-enhanced document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (Findings)</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Entity structure within and throughout: Modeling mention dependencies for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benfeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhengyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gu</forename><surname>Yuxian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huo</forename><surname>Yuqi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiu</forename><surname>Jiezhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Wentao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang Minlie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.07139</idno>
		<title level="m">Pre-trained models: Past, present and future</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Heterogeneous network representation learning: A unified framework with survey and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>IEEE TKDE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Docred: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Coreferential reasoning learning for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaju</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Sire: Separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Double graph based reasoning for documentlevel relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Document-level relation extraction as semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Positionaware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with dual-tier heterogeneous graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengzhu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning deep bilinear transformation for fine-grained image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with adaptive thresholding and localized context pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Nero: A neural rule grounding framework for label-efficient relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<meeting><address><addrLine>Leonardo Neves, and Xiang Ren</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bran (verga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="63" to="69" />
		</imprint>
		<respStmt>
			<orgName>Nguyen and Verspoor</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsr (nan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">64</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Scibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beltagy</surname></persName>
		</author>
		<idno>65.1 82.5</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhg (zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="65" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glre (wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="68" to="75" />
		</imprint>
	</monogr>
	<note type="report_type">5 -SSAN-SciBERT</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atlop-Scibert (</forename><surname>Zhou</surname></persName>
		</author>
		<idno>2021) 69.4 83.9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>Sire-Biobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeng</surname></persName>
		</author>
		<idno>2021) 70.8 84.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Docunet-Scibert ;</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sais O Re+cr+et -Scibert</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Ours</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
				<title level="m">Table 5: RE F1 results (%) on the CDR and GDA test sets. We report the baseline performances from the corresponding papers. SAIS O RE+CR+ET using three training tasks (i.e., RE, CR, and ET) scores the highest on both datasets and better than its variants with fewer training tasks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">2020) 59.14 61.22 -59.00 61.24 -DRN-BERT base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geda-Bert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno>2021) 59.22 61.09 - 59.31 61.30 - DocuNet-BERT base</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="18" to="57" />
		</imprint>
	</monogr>
	<note>45 -GAIN-BERT base. Tang et al., 2020) 54.29 56.31 -53.70 55.60 -CorefBERT base. Zhang et al., 2021) 59.86 61.83 -59.93 61.86 -Eider-BERT base</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">83 -RoBERTa large</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert Large (ye</surname></persName>
		</author>
		<idno>60.30 62.50 50.50</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page">57</biblScope>
		</imprint>
	</monogr>
	<note>RoBERTa large</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">40 -DocuNet-RoBERTa large</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atlop-Roberta Large (</forename><surname>Zhou</surname></persName>
		</author>
		<idno>2021) 62.23 64.12 - 62.39 64.55 - Eider-RoBERTa large</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="61" to="93" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ign F1 refers to the F1 score excluding the relation instances mentioned in the train set. Baselines using BERT base are separated into the graph-based (upper) and transformer-based (lower) groups. We report the test set scores from the official scoreboard and the baseline scores from the corresponding papers</title>
	</analytic>
	<monogr>
		<title level="m">Table 6: RE and ER results (%) on the develop and test sets of DocRED</title>
		<imprint/>
	</monogr>
	<note>SAIS B All achieves state-of-the-art performance on both RE and ER</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
