<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">The Majority Can Help the Minority: Context-rich Minority Oversampling for Long-tailed Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seulki</forename><surname>Park</surname></persName>
							<email>seulki.park@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkyu</forename><surname>Hong</surname></persName>
							<email>yk.hong@kaist.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
							<email>sangdoo.yun@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
							<email>jychoi@snu.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">ASRI, ECE</orgName>
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">The Majority Can Help the Minority: Context-rich Minority Oversampling for Long-tailed Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The problem of class imbalanced data is that the generalization performance of the classifier deteriorates due to the lack of data from minority classes. In this paper, we propose a novel minority over-sampling method to augment diversified minority samples by leveraging the rich context of the majority classes as background images. To diversify the minority samples, our key idea is to paste an image from a minority class onto rich-context images from a majority class, using them as background images. Our method is simple and can be easily combined with the existing long-tailed recognition methods. We empirically prove the effectiveness of the proposed oversampling method through extensive experiments and ablation studies. Without any architectural changes or complex algorithms, our method achieves stateof-the-art performance on various long-tailed classification benchmarks. Our code is made available at https: //github.com/naver-ai/cmo.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Real-world data are likely to be inherently imbalanced <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, where the number of samples per class differs greatly. If models are trained on an imbalanced dataset, they can be easily biased toward majority classes and tend to have a poor generalization ability on recognizing minority classes (i.e., overfitting).</p><p>A simple and straightforward method to overcome the class imbalance problem is to repeatedly oversample the minority classes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b47">47]</ref>. However, these naive oversampling can intensify the overfitting problem, since the repeatedly selected samples have less diversity but almost similar image contexts <ref type="bibr" target="#b42">[42]</ref>. For example, consider a minority class of 'snow goose,' in which the geese always stand upon grass in the training images. If samples are drawn from these limited training samples <ref type="bibr" target="#b47">[47]</ref> or even if new samples are produced <ref type="bibr">Figure 1</ref>. Concept of context-rich minority oversampling. In the real-world long-tailed dataset iNaturalist 2018 <ref type="bibr" target="#b20">[21]</ref>, the number of samples from the head class and the tail class is extremely different (Upper). Simple random oversampling method repeatedly produces context-limited images from minority classes. We propose a novel context-rich oversampling method to generate diversified minority images. To this end, we oversample the tail-class images with various sizes. Then, these patches are pasted onto the headclass images to have various backgrounds. Our key idea is to bring rich contexts from majority samples into minority samples. by interpolating within the class <ref type="bibr" target="#b5">[6]</ref>, only context-limited images will be created as in <ref type="figure">Figure 1</ref>. Our goal is to solve the aforementioned problem by introducing a simple contextrich oversampling method. distributions; that is, majority class samples are data-rich and information-rich. Unlike the existing re-sampling methods that ignore (i.e., undersample) majority samples, our method uses the affluent information of the majority samples to generate new minority samples. Specifically, our idea is to leverage the rich major-class images as the background for the newly created minor-class images. <ref type="figure">Figure 1</ref> illustrates the concept of our proposed context-rich oversampling strategy. Given an original image from a minority class, the object is cropped in various sizes and pasted onto the various images from majority classes. Then, we can create images with more diverse contexts (e.g., 'snow goose' images with the sky, road, roof, crows, etc). Since this is an interpolation of the majority and minority class samples, it generates diversified data around the decision boundary, and as a result, it improves the generalization performance for minority classes.</p><p>To this end, we adopt an image-mixing data augmentation method, CutMix <ref type="bibr" target="#b52">[52]</ref>. As our key idea is to transfer rich contexts from majority to minority samples, we apply a simple and effective data sampling method to generate new minority-centric images with majority's contexts. However, naive use of CutMix may exacerbate the overfitting problem in favor of the majority classes because it may generate more majority-centric samples than minority samples. We solve this problem by sampling the background images and the foreground patches from different distributions to achieve the desired minority oversampling.</p><p>Our key contributions can be summarized as follows: (1) We propose a novel context-rich minority oversampling method that generates various samples by leveraging the rich context of the majority classes as background images. (2) Our method requires little additional computational cost and can be easily integrated into many end-toend deep learning algorithms for long-tailed recognition. <ref type="bibr" target="#b2">(3)</ref> We demonstrate that significant performance improvements and state-of-the-art performance can be achieved by applying the proposed oversampling to existing commonly used loss functions without any architectural changes or complex algorithms. (4) We empirically prove the effectiveness of the proposed oversampling method through extensive experiments and ablation studies. We believe that our study offers a useful and universal minority oversampling method for research into long-tailed classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Long-tailed Recognition</head><p>Re-weighting methods. Re-weighting aims to assign different weights to training samples to adjust their importance either at the class level or at the instance level. Classlevel re-weighting methods include re-weighting samples by inverse class frequency <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b51">51]</ref>, Class-balanced loss <ref type="bibr" target="#b11">[12]</ref>, LDAM loss <ref type="bibr" target="#b4">[5]</ref>, Balanced Softmax <ref type="bibr" target="#b40">[40]</ref>, LADE loss <ref type="bibr" target="#b19">[20]</ref>. Instance-level re-weighting methods include focal loss <ref type="bibr" target="#b30">[31]</ref> and influence-balanced loss <ref type="bibr" target="#b38">[38]</ref>.</p><p>Re-sampling methods. Resampling methods aim to modify the training distributions to decrease the level of imbalance <ref type="bibr" target="#b22">[23]</ref>. Resampling methods include undersampling and oversampling. Undersampling methods <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b55">55]</ref> that discard the majority samples can lose valuable information, and undersampling is infeasible when the imbalance between classes is too high. The simplest form of oversampling is random oversampling (ROS) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">47]</ref>, which oversamples all minority classes until class balance is achieved. This method is simple and can be easily used in any algorithm, but since the same sample is repeatedly drawn, it can lead to overfitting <ref type="bibr" target="#b42">[42]</ref>. As a more advanced method, the synthetic minority over-sampling technique (SMOTE) <ref type="bibr" target="#b5">[6]</ref>, which oversamples minority samples by interpolating between existing minority samples and their nearest minority neighbors, was proposed. Following the success of SMOTE, several variants have been developed: Borderline-SMOTE <ref type="bibr" target="#b16">[17]</ref>, which oversamples the minority samples near class borders, and Safe-level-SMOTE <ref type="bibr" target="#b3">[4]</ref>, which defines safe regions not to oversample samples from different classes. These methods have been widely used by classical machine learning algorithms, but there are difficulties in using them for large-scale image datasets due to the high computational complexity of calculating the K-Nearest Neighbor for every sample. Generative adversarial minority oversampling (GAMO) <ref type="bibr" target="#b36">[37]</ref> solves this issue by producing new minority samples by training a convex generator, inspired by the success of generative adversarial networks (GANs) <ref type="bibr" target="#b14">[15]</ref> in image generation. However, training the generator incurs high additional training cost; moreover, GAMO can suffer from the infamous mode collapse of GANs <ref type="bibr" target="#b1">[2]</ref>. To generate diverse minority data, recent works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> have proposed adversarial augmentations by adding small noise to the input images. To this end, Majorto-minor Translation (M2m) <ref type="bibr" target="#b26">[27]</ref> transfers knowledge from majority classes using a pre-trained network, and Balancing Long-Tailed datasets (BLT) <ref type="bibr" target="#b27">[28]</ref> uses a gradient-ascent image generator based on the confusion matrix.</p><p>Another recent line of research is oversampling in the feature space rather than in the input space: Deep Oversampling (DOS) <ref type="bibr" target="#b0">[1]</ref>, Feature-space Augmentation (FSA) <ref type="bibr" target="#b7">[8]</ref>, and Meta Semantic Augmentation (MetaSAug) <ref type="bibr" target="#b29">[30]</ref>. These methods aim to augment minority classes in the feature space by sampling from the in-class neighbors in the linear subspace <ref type="bibr" target="#b0">[1]</ref>, using learned features from pretrained networks <ref type="bibr" target="#b7">[8]</ref>, or using an implicit semantic data augmentation (ISDA) algorithm <ref type="bibr" target="#b50">[50]</ref>. However, DOS <ref type="bibr" target="#b0">[1]</ref> requires finding the nearest neighbors in the feature space, FSA <ref type="bibr" target="#b7">[8]</ref> requires a pre-trained feature sub-network and a classifier for feature augmentation procedure. Lastly, MetaSAug <ref type="bibr" target="#b29">[30]</ref> demands additional uniform validation samples that outnumber the number of samples in the tail classes and hundreds and thousands of iterations for training. Consequently, these methods are less cost-efficient and technically more difficult to perform. On the other hand, our method oversamples diverse minority samples using a simple data augmentation technique and outperforms all previous methods while maintaining reasonable training costs.</p><p>Other long-tailed methods. Recently, significant improvement has been achieved by two-stage algorithms: Deferred re-weighting (DRW) <ref type="bibr" target="#b4">[5]</ref>, classifier re-training (cRT), learnable weight scaling (LWS) <ref type="bibr" target="#b24">[25]</ref>, and the Mixup shifted label-aware smoothing model (MiSLAS) <ref type="bibr" target="#b56">[56]</ref>. Meanwhile, a bilateral branch network (BBN) <ref type="bibr" target="#b57">[57]</ref> uses an additional network branch for re-balancing, and RIDE <ref type="bibr" target="#b49">[49]</ref> uses multiple branches called experts, each of which learns to specialize in different classes. Another line of recent research employs meta-learning methods: Meta-Weight-Net <ref type="bibr" target="#b43">[43]</ref> learns an explicit loss-weight function, and a meta sampler <ref type="bibr" target="#b40">[40]</ref> estimates the optimal class sample rate. PaCo <ref type="bibr" target="#b10">[11]</ref> proposes supervised contrastive learning with parametric class-wise centers for long-tailed classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data Augmentation and Mixup Methods</head><p>Spatial-level augmentation methods have performed satisfactorily in the computer vision fields. Cutout <ref type="bibr" target="#b12">[13]</ref> removes random regions whereas CutMix <ref type="bibr" target="#b52">[52]</ref> fills the removed regions with patches from another training image. In addition, mixup methods <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b54">54]</ref> linearly interpolate two images in a training dataset. Since the data augmentation method is closely related to the oversampling methods, some recent long-tailed recognition methods have used the mixup method. Zhou et al. <ref type="bibr" target="#b57">[57]</ref> use the mixup as a baseline method, and MiSLAS <ref type="bibr" target="#b56">[56]</ref> uses mixup in its Stage-1 training. However, these methods apply mixup without any adjustments, and little work has been done to explore appropriate data augmentation techniques for a long-tailed dataset. Recently, for an imbalanced dataset, the Remix <ref type="bibr" target="#b6">[7]</ref> assigned a label in favor of the minority classes when mixing two samples. Unlike these methods, our method samples images from different distributions, which takes into account the specificity of long-tailed data distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Context-rich Minority Oversampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Algorithm</head><p>We propose a new oversampling method called Contextrich Minority Oversampling (CMO). CMO utilizes the contexts of the majority samples to diversify the limited context of the minority samples. As shown in the <ref type="figure">Figure 1</ref>, the background images are sampled from majority classes and combined with foreground images of minority classes. Let x ? R W ?H?C and y denote a training image and its label, respectively. We aim to generate a new sample (x,?) by combining two training samples (x b , y b ) and (x f , y f ). Here, the image x b is used as a background image, and the image x f provides the foreground patch to be pasted onto</p><formula xml:id="formula_0">(x b , y b ).</formula><p>For the image combining method, we chose CutMix <ref type="bibr" target="#b52">[52]</ref> data augmentation due to its simplicity and effectiveness. Following CutMix <ref type="bibr" target="#b52">[52]</ref> settings, the image and label pairs are augmented as</p><formula xml:id="formula_1">x = M ? x b + (1 ? M) ? x f y = ?y b + (1 ? ?)y f ,<label>(1)</label></formula><p>where (1?M) ? {0, 1} W ?H denotes a binary mask indicating where to select the patch and paste it onto a background image. 1 means a binary mask filled with ones, and ? is element-wise multiplication. The combination ratio ? ? R between two images is sampled from the beta distribution Beta(?, ?). To sample the mask and its coordinates, we apply the original CutMix <ref type="bibr" target="#b52">[52]</ref> setting. An experiment on using a different ? is included in the Supplementary Material.</p><p>Since CutMix was originally designed for data augmentation on a class-balanced dataset, Eq. 1 does not represent the majority or minority class of samples. To change the method to CMO, we include sampling data distributions for foreground (x f , y f ) and background samples (x b , y b ). In our design, the background samples (x b , y b ) should be biased to the majority classes. Therefore, we sample the background samples from the original data distribution P . Meanwhile, the foreground samples (x f , y f ) are sampled from minor-class-weighted distribution Q to be biased to the minority classes. In short, CMO consists of data sampling from two distributions, (x b , y b ) ? P and (x f , y f ) ? Q, and combining the images using Eq. 1. The pseudo-code of the training procedure is presented in Algorithm 1. Algorithm 1 Context-rich Minority Oversampling (CMO)</p><formula xml:id="formula_2">Require: Dataset D N i=1 , model parameters ?, P , Q, any loss function L(?). 1: Randomly initialize ?. 2: Sample weighted datasetD N i=1 ? Q. 3: for epoch = 1, . . . , T do 4: for batch i = 1, . . . , B do 5: Draw a mini-batch (x b i , y b i ) from D N i=1 6: Draw a mini-batch (x f i , y f i ) fromD N i=1 7: ? ? Beta(?, ?) 8:x i = M ? x b i + (1 ? M) ? x f i 9:? i = ?y b i + (1 ? ?)y f i 10: ? ? ? ? ??L((x i ,? i ); ?) 11:</formula><p>end for 12: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Minor-class-weighted Distribution Q</head><p>To sample the foreground image from minority classes, we design the minor-class-weighted distribution Q by uti-lizing the re-weighting methods. The re-weighting approach, dating back to the classical importance sampling method <ref type="bibr" target="#b23">[24]</ref>, provided a way to assign appropriate weights to samples. Commonly used sampling strategies include ones that assign a weight inversely proportional to the class frequency <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b51">51]</ref>, to the smoothed class frequency <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>, or to the effective number <ref type="bibr" target="#b11">[12]</ref>.</p><p>Let n k be the number of samples in the k-th class, then for the C classes, the total number of samples is N = C k=1 n k . Then, the generalized sampling probability for the k-th class can be defined by</p><formula xml:id="formula_3">q(r, k) = 1/n r k C k ? =1 1/n r k ? ,<label>(2)</label></formula><p>where the k-th class has a sampling weight inversely proportional to n r k . As r increases, the weight of the minor class becomes increasingly larger than that of the major class. By adjusting the value of r, we can examine diverse sampling strategies. Setting r = 1 uses the inverse class frequency <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b51">51]</ref> while setting r = 1/2 uses the smoothed inverse class frequency, as in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. We can also use the effective number <ref type="bibr" target="#b11">[12]</ref> instead of n r k , which is defined as</p><formula xml:id="formula_4">E(k) = (1 ? ? n k ) (1 ? ?) ,<label>(3)</label></formula><p>where ? = (N ? 1)/N . Since CMO is a new approach for long-tailed classification, it is hard to predict the performance of each sampling strategy for CMO. Therefore, we evaluate the different sampling strategies on the long-tailed CIFAR-100 <ref type="bibr" target="#b28">[29]</ref> and select the best strategy q(1, k) for the minor-class-weighted distribution Q. The experimental results are displayed in <ref type="table" target="#tab_0">Table 10</ref> of the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Regularization Effect of CMO</head><p>A recent study <ref type="bibr" target="#b56">[56]</ref> has reported that models trained on long-tailed datasets are more over-confident than the models trained on balanced data. In addition, the study reveals that the long-tailed classification accuracy can be improved by solving the over-confidence issue. Moreover, CMO can be interpreted as a way to mitigate over-confidence in longtailed classification. Inherited from CutMix, CMO uses a soft-target label?, as in Eq. 1. The soft-target label penalizes over-confident outputs, similarly to the label smoothing regularization <ref type="bibr" target="#b45">[45]</ref>. Therefore, we argue that CMO contributes not only to minority sample generation but also to mitigating the over-confidence, which both enable an impressive performance improvement in diverse long-tail settings. We will demonstrate the effectiveness of CMO through various experiments in the experimental section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We present experiments on and analyses of CMO in this section. We first describe our experimental settings and im-plementation details in Section 4.1. Next, we present the effectiveness of CMO using three long-tailed classification benchmarks: CIFAR-100-LT, ImageNet-LT, and iNaturalist. CMO consistently boosts the performance of these baselines with state-of-the-art accuracy (Section 4.2). In Section 4.3 we present in-depth analyses of CMO to study its inherent characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>Datasets. We validate CMO on the most commonly used long-tailed recognition benchmark datasets: CIFAR-100-LT <ref type="bibr" target="#b4">[5]</ref>, ImageNet-LT <ref type="bibr" target="#b33">[34]</ref>, and iNaturalist 2018 <ref type="bibr" target="#b20">[21]</ref> (see <ref type="table" target="#tab_0">Table 1</ref>). CIFAR-100-LT and ImageNet-LT are artificially made imbalanced from their balanced versions (CIFAR-100 <ref type="bibr" target="#b28">[29]</ref> and ImageNet-2012 <ref type="bibr" target="#b41">[41]</ref>). The iNaturalist 2018 dataset is a large-scale real-world dataset that exhibits longtailed imbalance. We used the official training and test splits in our experiments. Evaluation Metrics. Performances is mainly reported as the overall top-1 accuracy. Following <ref type="bibr" target="#b33">[34]</ref>, we also report the accuracy of three disjoint subsets: Many-shot classes (classes that contain more than 100 training samples), medium-shot classes (classes that contain 20 to 100 samples), and few-shot classes (classes that contain under 20 samples). Comparison methods. We compare CMO with the minority oversampling methods, the state-of-the-art long-tail recognition methods, and their combinations.</p><p>? Minority oversampling. (1) No oversampling (vanilla);</p><p>(2) Random oversampling (ROS) <ref type="bibr" target="#b47">[47]</ref>, that oversamples minority samples to balance the classes in the training data; (3) Remix <ref type="bibr" target="#b6">[7]</ref>, which oversamples minority classes by assigning higher weights to the minority labels when using Mixup <ref type="bibr" target="#b54">[54]</ref>; (4) Feature space augmentation (FSA) <ref type="bibr" target="#b7">[8]</ref>.</p><p>? Re-weighting. (5) label-distribution-aware margin (LDAM) loss <ref type="bibr" target="#b4">[5]</ref>, which regularizes the minority classes to increase margins to the decision boundary; <ref type="bibr" target="#b5">(6)</ref> influence-balanced (IB) loss <ref type="bibr" target="#b38">[38]</ref>, which re-weights samples by their influences; <ref type="bibr" target="#b6">(7)</ref> Balanced Softmax <ref type="bibr" target="#b40">[40]</ref>, an unbiased extension of Softmax; (8) LADE <ref type="bibr" target="#b19">[20]</ref>, which disentangles the source label distribution from the model prediction.</p><p>? Other state-of-the-art methods. (9) Deferred reweighting (DRW) <ref type="bibr" target="#b4">[5]</ref> and <ref type="formula" target="#formula_1">(10)</ref> Decouple <ref type="bibr" target="#b24">[25]</ref> are twostage algorithms that re-balance the classifiers during fine-tuning; (11) BBN <ref type="bibr" target="#b57">[57]</ref> and (12) RIDE <ref type="bibr" target="#b49">[49]</ref> use additional network branches to handle class imbalance; <ref type="bibr" target="#b12">(13)</ref> Causal Norm <ref type="bibr" target="#b46">[46]</ref>, which disentangles causal effects and adjusts the effects in training; <ref type="bibr" target="#b13">(14)</ref> MiSLAS <ref type="bibr" target="#b56">[56]</ref>, a twostage algorithm, enhances classifier learning and calibration with label-aware smoothing (LAS) in stage-2.</p><p>Implementation. We use PyTorch <ref type="bibr" target="#b39">[39]</ref> for all experiments.</p><p>For the CIFAR datasets, we use ResNet-32 <ref type="bibr" target="#b17">[18]</ref>. The networks are trained for 200 epochs following the training strategy in <ref type="bibr" target="#b4">[5]</ref>. For ImageNet-LT, we use ResNet-50 as the backbone network. The network is trained for 100 epochs using an initial learning rate of 0.1. The learning rate is decayed at the 60th and 80th epochs by 0.1. For iNaturalist 2018, we use ResNet-{50, 101, 152} and Wide ResNet-50 <ref type="bibr" target="#b53">[53]</ref>. We train the networks for 200 epochs using an initial learning rate of 0.1, and decay the learning rate at epochs 75 and 160 by 0.1. All experiments are trained with stochastic gradient descent (SGD) with a momentum of 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Long-tailed classification benchmarks 4.2.1 CIFAR-100-LT</head><p>We conduct experiments on CIFAR-100-LT using different imbalance ratios: 10, 50, 100. We apply CMO to various methods to verify its effectiveness on different algorithms: vanilla cross-entropy loss, class-reweighting loss (LDAM <ref type="bibr" target="#b4">[5]</ref>), a two-stage algorithm (DRW <ref type="bibr" target="#b4">[5]</ref>), and multibranch architecture (RIDE <ref type="bibr" target="#b49">[49]</ref>).</p><p>Comparison with state-of-the-art methods. The overall classification accuracies are displayed in <ref type="table" target="#tab_1">Table 2</ref>. It is surprising that CMO with basic cross-entropy (CE) loss shows comparable performance to that of complex long-tail recognition methods. Moreover, applying CMO to the state-ofthe-art model (i.e., RIDE) further boosts the performance markedly, especially when the imbalance ratios are high as 50 and 100.</p><p>Comparison with oversampling methods. We further compare the performance improvement of CMO with that of other oversampling techniques when combined with long-tailed recognition methods (see <ref type="table" target="#tab_2">Table 3</ref>). The results reveal that CMO consistently improves the performance of all long-tailed recognition methods. On the other hand, simply balancing the class distribution with ROS <ref type="bibr" target="#b47">[47]</ref> severely degrades performance. We speculate that this is because the naive balancing of the sampling distribution across classes hinders the model from learning generalized features for major classes and induces the model to memorize the minor class samples. Remix <ref type="bibr" target="#b6">[7]</ref> improves the performance of some methods but degrades the performance when combined with RIDE <ref type="bibr" target="#b49">[49]</ref>. This indicates that the simple labeling policy of  Remix may not be effective when the model complexity becomes large, as in RIDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">ImageNet-LT</head><p>Comparison with state-of-the-art methods. The results of our method and other long-tailed recognition methods are displayed in <ref type="table">Table 4</ref>. Applying CMO to the basic training with CE loss improves the performance by a significant margin, outperforming most of the recent baselines. The greater performance improvement on ImageNet-LT compared to CIFAR-100 indicates that our method benefits from the richer context information available in the major classes of ImageNet-LT. In addition, a consistent performance improvement by using CMO when combined with DRW or BS bolsters the efficacy of CMO, which can be easily integrated into modern state-of-the-art long-tailed recognition methods. It is noteworthy that as {CE-DRW + CMO } and {BS + CMO } especially achieve a much higher few-shot class accuracy than did the other methods, our method is useful for achieving consistent performance across classes. Lastly, applying CMO to RIDE further boosts performance, outperforming the results of RIDE with four experts. <ref type="table">Table 4</ref>. State-of-the-art comparison on ImageNet-LT. Classification accuracy (%) of ResNet-50 with state-of-the-art methods trained for 90 or 100 epochs. " * " and " ?" denote the results are from the original papers, and <ref type="bibr" target="#b24">[25]</ref>, respectively. The best results are marked in bold.  Comparison with oversampling methods. In <ref type="table" target="#tab_4">Table 5</ref>, we compare performance improvement using other oversampling techniques. While CMO consistently improves performance for all methods, Remix <ref type="bibr" target="#b6">[7]</ref> fails to improve the performance of the long-tailed recognition methods and barely improves the model trained with cross-entropy loss. This implies that the labeling strategy of Remix is not sufficient to compensate for the adverse effect of using the same original distribution as the two sampling distributions of the mixup method, especially when the imbalance ratio rises severly to 256, as with ImageNet-LT. In contrast, CMO generates more minority samples by using different distributions when selecting two images and produces much better classification accuracy on all tasks. Results on longer training epochs. Recently, PaCo <ref type="bibr" target="#b10">[11]</ref> performed impressively by using supervised contrastive learning. Since contrastive learning requires diverse augmentation strategies and longer training times, PaCo trained networks for 400 epochs using RandAugment <ref type="bibr" target="#b9">[10]</ref>. Since CMO should also improve using longer training epochs, we evaluate CMO using the same setting from PaCo (i.e., 400 epochs &amp; RandAug). <ref type="table">Table 6</ref> reveals that {BS + CMO } achieves a new state-of-the-art performance. It is note- <ref type="table">Table 6</ref>. Results on longer training epochs with Ran-dAugment <ref type="bibr" target="#b9">[10]</ref>. Classification accuracy (%) of ResNet-50 on ImageNet-LT. " * " denotes the results from <ref type="bibr" target="#b10">[11]</ref>.  worthy that applying CMO significantly surpasses the two baselines, especially in the few-shot classes. On top of its simplicity and much lower computational cost, the results demonstrate the effectiveness of the proposed method. The results on CIFAR-100-LT and iNaturalist 2018 are included in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">iNaturalist 2018</head><p>Comparison with state-of-the-art methods. <ref type="table" target="#tab_6">Table 7</ref> presents the classification results. On the naturally-skewed dataset, applying CMO to the simple training scheme of CE-DRW surpasses most of the state-of-the-arts. On iNaturalist 2018, as in ImageNet-LT, CMO dramatically improves the performance of the cross-entropy loss (CE) by 7.9%p (61.0% increased to 68.9%). This is because the sample generation by CMO fully utilizes the abundant context of training data. Again, it achieves a remarkable performance improvement in the few-shot classes. It is moreover noteworthy that when we apply the same stage-2 strategy, LAS, from <ref type="bibr" target="#b56">[56]</ref>, it further boosts performance. Lastly, applying CMO to RIDE achieves a new state-ofthe-art performance. <ref type="figure" target="#fig_0">Figure 2</ref>. A display of the minority images generated by CMO (minority classes: the snow goose and the Acmon blue (butterfly)). We randomly choose generated images for each original image. Our method is able to generate context-rich minority samples that have diverse contexts. For example, while the original 'snow goose' class contains only images of a 'snow goose' on grass, the generated images have various contexts such as the sky, the sea, the sand, and a flock of crows. These generated images enable the model to learn a robust representation of minority classes.</p><p>Results on large models. We investigate the performance of CMO and other oversampling methods using the large deep networks of Wide ResNet-50 <ref type="bibr" target="#b53">[53]</ref>, ResNet-101, and ResNet-152 <ref type="bibr" target="#b17">[18]</ref>. We compare CMO with the feature space augmentation method (FSA) <ref type="bibr" target="#b7">[8]</ref>. While both methods improve the results from vanilla training with cross-entropy loss, our method provides superior performance to that of FSA. This indicates that using the context-rich information from majority classes in the input space is simple but effective in improving the overall performance. Display of the generated images. We visualize the generated images for the minority classes in <ref type="figure" target="#fig_0">Figure 2</ref>. From the rarest minority classes, we randomly choose generated images for each original image. CMO produces diverse minority samples that have various contexts. For example, while the 'snow goose' class contains only images of geese on grass, the generated images have various contexts, such as the sky or sea. Likewise, the butterflies in the third row are newly created as diverse images that have various contexts, containing bees and flowers of various colors and shapes. We argue that various combinations of context and minority samples encourage the model to learn a robust representation of the minority classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>Is the distribution for augmenting images important? To justify the need for different distributions of background and foreground images, we compare CutMix and CMO.</p><p>As can be seen from <ref type="table" target="#tab_8">Table 9</ref>, CMO outperforms CutMix on long-tailed classification by a large margin. In particular, there is a remarkable performance improvement in the medium and few-shot classes. The performance gap is due to the absence of a minor-class-weighted distribution in CutMix augmentation. Although CutMix can generate informative mixed samples, its effect is limited when used with long-tailed distributions. Thus, we claim that the use of a minor-class-weighted distribution is a key-point in data augmentation in the long-tailed settings; this highlights the contribution and originality of CMO. How to choose the appropriate probability distribution Q. We evaluate different sampling strategies in Section 3.2 on CIFAR-100 with the imbalance ratio 100, The results are reported in <ref type="table" target="#tab_0">Table 10</ref>. q(1, k) displays the most balanced performance. This result is consistent with the common practice of balancing the dataset by assigning weights inversely proportional to the class frequency. While q(2, k), which imposes a higher probability on the minority class than does q(1, k), performs acceptably in the few-shot classes, the overall performance slightly deteriorates. We assume this is because we cannot sample more diverse images when imposing too high probabilities on the few-shot classes. Based on this result, we set Q as q <ref type="bibr">(1, k)</ref> in our all experiments. Why should we oversample only for the foreground samples? One may wonder why oversampling only for the foreground samples is better than oversampling both patches and background samples or oversampling only the backgrounds. To verify our design choice, we evaluate two variants of CMO. The first variant, CMO back , samples background images from a minor-class-weighted distribution and patches from the original distribution, which is exactly the opposite design of CMO, i.e., (</p><formula xml:id="formula_5">x b , y b ) ? Q, (x f , y f ) ? P .</formula><p>The second variant, CMO minor , samples both the background and the patches from a minor-class-weighted distribution, i.e., (x b , y b ), (x f , y f ) ? Q. We report the results of applying these variants of the CMO method to the model trained with CE loss and LDAM loss <ref type="bibr" target="#b4">[5]</ref> in <ref type="table" target="#tab_0">Table 11</ref>. CMO minor yields severe performance degradation using both methods. We suspect that this is because the rich context of the majority samples cannot be utilized. In contrast, CMO back produces acceptable performance improvements, but far less than did the original CMO. This is because, using the CutMix method, there is a high probability that the object in the foreground image overlaps the background image. Therefore, we can expect a loss of information about minority classes in the background image, resulting in a limited performance boost.</p><p>Comparison with other minority augmentations. To further verify our design choice, we analyze the effectiveness of using different augmentation methods, including Cut-Mix <ref type="bibr" target="#b52">[52]</ref>, Mixup <ref type="bibr" target="#b54">[54]</ref>, color jitter, and Gaussian blur. For Mixup, we use the same sampling strategy as for CMO. For color jitter and Gaussian blur, which do not interpolate two images, we apply augmentation only to the minority classes and oversample those classes. As evidenced in <ref type="table" target="#tab_0">Table 12</ref>, other augmentation methods provide little performance gain compared to the gains using CutMix. We suspect that this is because the pixel-level transformations (i.e., Gaussian blur and color jitter) are not effective in producing minority samples that have a rich context. Gaussian blur and color jitter do not combine two images; thus, it is hard to add a new context to minority samples. While Mixup combines two images, it does not distinguish the roles of the two samples, limiting the control of the source of the context and of the patch information. In contrast, CutMix can create diverse images with larger changes at pixel-level than can other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a novel context-rich oversampling method, CMO, to solve the data imbalance problem. We tackle the fundamental problem of previous oversampling methods that generate context-limited minority samples, which intensifies the overfitting problem. Our key idea is to transfer the rich contexts of majority samples to minority samples to augment minority samples. The implementation of CMO is simple and intuitive. Extensive experiments on various benchmark datasets demonstrate not only that our CMO significantly improves performance, but also that adding our oversampling method to the basic losses advances the state-of-the-art. Limitations. In some cases, the performance improvement for the minority classes occurs with the degraded performance of the majority classes. Future work should be designed to improve the performance of all classes without sacrificing the performance of the many-shot classes. Potential negative societal impact. Since our method creates new samples, it benefits more from longer training and deeper architectures. Thus, it may lead to more computations, which has a risk that the use of GPUs for machine learning could accelerate environmental degradation <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Results on longer training epochs</head><p>We evaluate CMO using the same setting from PaCo <ref type="bibr" target="#b10">[11]</ref>. That is, we train the network for 400 epochs and use AutoAugment <ref type="bibr" target="#b8">[9]</ref> on CIFAR-100-LT. For iNaturalist2018, RandAugment <ref type="bibr" target="#b9">[10]</ref> is applied.  <ref type="table" target="#tab_0">Table 15</ref>. Classification Accuracy on iNaturalist2018. We train ResNet-50 for 400 epochs with RandAugment <ref type="bibr" target="#b9">[10]</ref>. " * " indicates the results are from <ref type="bibr" target="#b10">[11]</ref>. The best results are marked in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Impact of ?</head><p>We evaluate the impact of the hyperparameter alpha in <ref type="figure">Figure 3</ref>. The classification accuracy according to different ? ? {0.1, 0.25, 0.5, 1.0, 2.0, 4.0} is plotted. CMO improves the baseline accuracy (38.6%) in all cases. The best performance is achieved when ? = 1.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Computational cost</head><p>One of the biggest advantages of our method is its low computational cost. CMO only requires to load an additional batch of data from the minor-class-weighted loader. We measure the training time per batch on ImageNet-LT (see <ref type="table" target="#tab_0">Table 16</ref>). While CE takes 0.355s, CE+CMO takes 0.369s, which is only an increase of 3.94%. <ref type="figure">Figure 3</ref>. Impact of ? on CIFAR-100-LT with an imbalance ratio of 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CE CE + CMO</head><p>Training Time (s) 0.355 0.369 <ref type="table" target="#tab_0">Table 16</ref>. Training time on ImageNet-LT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pseudo-code of Context-rich Minority Oversampling</head><p>We present the PyTorch-syle pseudo-code of CMO algorithm in Algorithm 2. Note that CMO is easy to implement with just a few lines that are easily applicable to any loss, networks, or algorithms. Thus, CMO can be a very practical and effective solution for handling imbalanced dataset. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 2</head><label>2</label><figDesc>PyTorch-style pseudo-code for CMO # original loader: data loader from original data distribution # weighted loader: data loader from minor-class-weighted distribution # model: any backbone network such as ResNet or multi-branch networks (RIDE) # loss: any loss such as CE, LDAM, balanced softmax, RIDE loss for epoch in Epochs: # load a batch for background images from original data dist. for x b, y b in original loader: # load a batch for foreground from minor-class-weighted dist. x f, y f = next(weighted loader) # get coordinate for random binary mask lambda = np.random.uniform(0,1) cx = np.random.randint(W) # W: width of images cy = np.random.randint(H) # H: height of images bbx1 = np.clip(cx -int(W * np.sqrt(1. -lambda))//2,0,W) bbx2 = np.clip(cx + int(W * np.sqrt(1. -lambda))//2,0,W) bby1 = np.clip(cy -int(H * np.sqrt(1. -lambda))//2,0,H) bby2 = np.clip(cy + int(H * np.sqrt(1. -lambda))//2,0,H) # get minor-oversampled images x b[:, :, bbx1:bbx2, bby1:bby2] = x f[:, :, bbx1:bbx2, bby1:bby2] lambda = 1 -((bbx2 -bbx1) * (bby2 -bby1) / (W * H))# adjust lambda # output (x f is attached to x b) output = model(x b) # loss losses = loss(output, y b) * lambda + loss(output, y f) * (1. -lambda) # optimization step losses.backward() optimizer.step()</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Summary of datasets. The imbalance ratio ? is defined by ? = max k {n k }/min k {n k }, where n k is the number of samples in the k-th class.</figDesc><table><row><cell>Dataset</cell><cell cols="3"># of classes # of training Imbalance ratio</cell></row><row><cell>CIFAR-100-LT</cell><cell>100</cell><cell>50K</cell><cell>{10, 50, 100}</cell></row><row><cell>ImageNet-LT</cell><cell>1,000</cell><cell>115.8K</cell><cell>256</cell></row><row><cell>iNaturalist 2018</cell><cell>8,142</cell><cell>437.5K</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>State-of-the-art comparison on CIFAR-100-LT dataset. Classification accuracy (%) for ResNet-32 architecture on CIFAR-100-LT with different imbalance ratios. * and ? are from the original paper and<ref type="bibr" target="#b19">[20]</ref>, respectively.</figDesc><table><row><cell>Imbalance ratio</cell><cell>100</cell><cell>50</cell><cell>10</cell></row><row><cell>Cross Entropy (CE)</cell><cell cols="3">38.6 44.0 56.4</cell></row><row><cell>CE-DRW</cell><cell cols="3">41.1 45.6 57.9</cell></row><row><cell>LDAM-DRW [5]</cell><cell cols="3">41.7 47.9 57.3</cell></row><row><cell>BBN [57]  ?</cell><cell cols="3">42.6 47.1 59.2</cell></row><row><cell>Causal Norm [46]  ?</cell><cell cols="3">44.1 50.3 59.6</cell></row><row><cell>IB Loss [38]  *</cell><cell cols="3">45.0 48.9 58.0</cell></row><row><cell>Balanced Softmax (BS) [40]  ?</cell><cell cols="3">45.1 49.9 61.6</cell></row><row><cell>LADE [20]  ?</cell><cell cols="3">45.4 50.5 61.7</cell></row><row><cell>Remix [7]</cell><cell cols="3">45.8 49.5 59.2</cell></row><row><cell>RIDE (3 experts) [49]</cell><cell cols="3">48.6 51.4 59.8</cell></row><row><cell>MiSLAS [56]  *</cell><cell cols="2">47.0 52.3</cell><cell>63.2</cell></row><row><cell>CE + CMO</cell><cell>43.9</cell><cell>48.3</cell><cell>59.5</cell></row><row><cell>CE-DRW + CMO</cell><cell>47.0</cell><cell>50.9</cell><cell>61.7</cell></row><row><cell>LDAM-DRW + CMO</cell><cell>47.2</cell><cell>51.7</cell><cell>58.4</cell></row><row><cell>BS + CMO</cell><cell>46.6</cell><cell>51.4</cell><cell>62.3</cell></row><row><cell>RIDE (3 experts) + CMO</cell><cell>50.0</cell><cell>53.0</cell><cell>60.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="4">Vanilla +ROS [47] +Remix [7] +CMO</cell></row><row><cell>CE</cell><cell>38.6 (+0.0)</cell><cell>32.3 (-5.3)</cell><cell>40.0 (+1.4)</cell><cell>43.9 (+5.3)</cell></row><row><cell>CE-DRW [5]</cell><cell>41.1 (+0.0)</cell><cell>35.9 (-5.2)</cell><cell>45.8 (+4.7)</cell><cell>47.0 (+5.9)</cell></row><row><cell>LDAM-DRW [5]</cell><cell>41.7 (+0.0)</cell><cell>32.6 (-9.1)</cell><cell>45.3 (+3.6)</cell><cell>47.2 (+5.5)</cell></row><row><cell>RIDE [49]</cell><cell>48.6 (+0.0)</cell><cell>22.6 (-26.0)</cell><cell>44.0 (-4.6)</cell><cell>50.0 (+1.4)</cell></row></table><note>Comparison against baselines on CIFAR-100-LT (Im- balance ratio = 100). Classification accuracy (%) of ResNet-32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Comparison against baselines on ImageNet-LT.</figDesc><table><row><cell>Clas-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 .</head><label>7</label><figDesc>State-of-the-art comparison on iNaturalist2018.</figDesc><table><row><cell>Clas-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Results on large architectures.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Classification accuracy</cell></row><row><cell cols="5">(%) of large backbone networks on iNaturalist 2018. The results</cell></row><row><cell cols="2">are copied from [8].</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Method ResNet-50 Wide ResNet-50 ResNet-101 ResNet-152</cell></row><row><cell>CE</cell><cell>61.0</cell><cell>-</cell><cell>65.2</cell><cell>66.2</cell></row><row><cell>FSA [8]</cell><cell>65.9</cell><cell>-</cell><cell>68.4</cell><cell>69.1</cell></row><row><cell>CMO</cell><cell>70.9</cell><cell>71.9</cell><cell>72.4</cell><cell>72.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc>Comparison with CutMix using cross-entropy loss.</figDesc><table><row><cell></cell><cell>All</cell><cell cols="2">Many Med</cell><cell>Few</cell></row><row><cell>CIFAR-100-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CutMix</cell><cell>35.6</cell><cell>71.0</cell><cell>37.9</cell><cell>4.9</cell></row><row><cell>CMO</cell><cell>43.9</cell><cell>70.4</cell><cell>42.5</cell><cell>14.4</cell></row><row><cell>ImageNet-LT</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>CutMix</cell><cell>45.5</cell><cell>68.6</cell><cell>38.1</cell><cell>8.1</cell></row><row><cell>CMO</cell><cell>49.1</cell><cell>67.0</cell><cell>42.3</cell><cell>20.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Impact of different Q sampling distributions. Results on CIFAR-100-LT (imbalance ratio=100) according to different Q sampling probabilities.</figDesc><table><row><cell></cell><cell>All</cell><cell cols="2">Many Med</cell><cell>Few</cell></row><row><cell>q(1/2, k)</cell><cell>42.6</cell><cell>71.6</cell><cell>42.1</cell><cell>9.5</cell></row><row><cell>q(1, k)</cell><cell>43.9</cell><cell>70.4</cell><cell>42.5</cell><cell>14.4</cell></row><row><cell>q(2, k)</cell><cell>40.1</cell><cell>67.2</cell><cell>36.7</cell><cell>12.3</cell></row><row><cell cols="2">E(k) [12] 39.5</cell><cell>70.4</cell><cell>38.0</cell><cell>4.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 .</head><label>11</label><figDesc>Ablation study. Results from variants of CMO with ResNet-32 on imbalanced CIFAR-100; imbalance ratio of 100.</figDesc><table><row><cell></cell><cell>All</cell><cell cols="2">Many Med</cell><cell>Few</cell></row><row><cell>Cross Entropy (CE)</cell><cell>38.6</cell><cell>65.3</cell><cell>37.6</cell><cell>8.7</cell></row><row><cell>CE + CMO minor</cell><cell>37.9</cell><cell>58.3</cell><cell>40.4</cell><cell>11.2</cell></row><row><cell>CE + CMO back</cell><cell>40.1</cell><cell>64.7</cell><cell>40.2</cell><cell>11.3</cell></row><row><cell>CE + CMO</cell><cell>43.9</cell><cell>70.4</cell><cell>42.5</cell><cell>14.4</cell></row><row><cell>LDAM [5]</cell><cell>41.7</cell><cell>61.4</cell><cell>42.2</cell><cell>18.0</cell></row><row><cell>LDAM + CMO minor</cell><cell>31.7</cell><cell>50.2</cell><cell>33.2</cell><cell>8.4</cell></row><row><cell>LDAM + CMO back</cell><cell>44.2</cell><cell>59.2</cell><cell>46.6</cell><cell>24.0</cell></row><row><cell>LDAM + CMO</cell><cell>47.2</cell><cell>61.5</cell><cell>48.6</cell><cell>28.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 12 .</head><label>12</label><figDesc>Data augmentation methods. Comparisons between augmentation methods for generating new minority samples on CIFAR-100-LT with an imbalance ratio of 100.</figDesc><table><row><cell></cell><cell>All</cell><cell cols="2">Many Med</cell><cell>Few</cell></row><row><cell cols="2">CMO w/ Gaussian Blur 31.1</cell><cell>54.7</cell><cell>28.8</cell><cell>6.2</cell></row><row><cell>CMO w/ Color Jitter</cell><cell>34.7</cell><cell>58.9</cell><cell>34.4</cell><cell>6.8</cell></row><row><cell>CMO w/ Mixup</cell><cell>38.0</cell><cell>54.8</cell><cell>40.2</cell><cell>15.9</cell></row><row><cell>CMO w/ CutMix</cell><cell>43.9</cell><cell>70.4</cell><cell>42.5</cell><cell>14.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 14 Table 14 .</head><label>1414</label><figDesc>, 15 reveals that {BS + CMO} surpasses PaCo in most cases, and achieves a new state-of-the-art performance. These results demonstrate the effectiveness of CMO, despite its simplicity. Classification Accuracy on CIFAR-100-LT with different imbalance ratios. We train ResNet-32 with Au-toAugment<ref type="bibr" target="#b8">[9]</ref> in 400 epochs. * is from<ref type="bibr" target="#b10">[11]</ref> The best results are marked in bold.</figDesc><table><row><cell cols="2">Imbalance ratio 100</cell><cell>50</cell><cell>10</cell><cell></cell><cell>All</cell><cell>Many Med Few</cell></row><row><cell>BS  *</cell><cell cols="3">50.8 54.2 63.0</cell><cell>BS  *</cell><cell>71.8</cell><cell>72.3</cell><cell>72.6 71.7</cell></row><row><cell>PaCo [11]  *</cell><cell cols="3">52.0 56.0 64.2</cell><cell cols="2">PaCo [11]  *  73.2</cell><cell>70.3</cell><cell>73.2 73.6</cell></row><row><cell>BS + CMO</cell><cell cols="3">51.7 56.7 65.3</cell><cell>BS + CMO</cell><cell>74.0</cell><cell>71.9</cell><cell>74.2 74.2</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep over-sampling framework for classifying imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Ando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun Yuan</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Seeing what a gan cannot generate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A systematic study of the class imbalance problem in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Buda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsuto</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><forename type="middle">A</forename><surname>Mazurowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Safe-level-smote: Safe-levelsynthetic minority over-sampling technique for handling the class imbalanced problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chumphol</forename><surname>Bunkhumpornpat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krung</forename><surname>Sinapiromsaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chidchanok</forename><surname>Lursinsap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<editor>Thanaruk Theeramunkong, Boonserm Kijsirikul, Nick Cercone, and Tu-Bao Ho</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with labeldistribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Smote: Synthetic minority oversampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Remix: Rebalanced mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ping</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chieh</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 Workshops</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Feature space augmentation for long-tailed data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Ekin Dogus Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Parametric contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classbalanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrance</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Borderlinesmote: A new over-sampling method in imbalanced data sets learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing-Huan</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">De-Shuang Huang, Xiao-Ping Zhang, and Guang-Bin Huang</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Advances in Intelligent Computing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Harnessing artificial intelligence for the earth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Herweijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Waughray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Fourth Industrial Revolution for the Earth Series</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Disentangling label distribution for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungju</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghee</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokjun</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beomsu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buru</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="6626" to="6636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The inaturalist species classification and detection dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Salt Lake City, UT, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-06-18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning deep representation for imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Survey on deep learning with class imbalance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taghi</forename><surname>Khoshgoftaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Methods of Reducing Sample Size in Monte Carlo Computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Marshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="263" to="278" />
			<date type="published" when="1953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heungseok</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soeun</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngil</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngkwan</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">M2m: Imbalanced classification via major-to-minor translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongheon</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Blt: Balancing long-tailed datasets with adversarially-perturbed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jedrzej</forename><surname>Kozerawski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fragoso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Karianakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Asian Conference on Computer Vision (ACCV)</title>
		<meeting>the Asian Conference on Computer Vision (ACCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Department of Computer Science, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Metasaug: Meta semantic augmentation for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixiong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Harold</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinjing</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="5212" to="5221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Piotr Dollar, and Larry Zitnick. Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="De" to=" cember" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Generative adversarial minority oversampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shounak</forename><surname>Sankha Subhra Mullick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swagatam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<meeting><address><addrLine>Seoul, Korea (South)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-11-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Influence-balanced loss for imbalanced visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seulki</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongin</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younghan</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Balanced meta-softmax for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cunjun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunan</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep imbalanced attribute classification using visual attention aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved mixedexample data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecilia</forename><surname>Summers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael J Dinneen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Longtailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Experimental perspectives on learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Van Hulse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taghi</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amri</forename><surname>Napolitano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Manifold mixup: Better representations by interpolating hidden states</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>International Conference on Machine Learning. PMLR</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Long-tailed recognition by routing diverse distribution-aware experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Implicit semantic data augmentation for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuran</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiji</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wu</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to model the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference (BMVC)</title>
		<meeting>the British Machine Vision Conference (BMVC)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">KNN Approach to Unbalanced Data Distributions: A Case Study Involving Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ICML&apos;2003 Workshop on Learning from Imbalanced Datasets</title>
		<meeting>the ICML&apos;2003 Workshop on Learning from Imbalanced Datasets</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving calibration for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiequan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">BBN: bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">and a weight decay of 2 ? 10 ?4 . As in [5], we used simple data augmentation [18] by padding 4 pixels on each side and applying horizontal flipping or random cropping to 32 ? 32 size. We trained for 200 epochs and used a linear warm-up of the learning rate [16] in the first five epochs. The learning rate was initialized as 0.1, and it was decayed at the 160th and 180th epoch by 0.01. The model was trained with a batch size of 128 on a single GTX 1080Ti. We turned off CMO for the last three epochs in order to finetune the model in the original input space. For experiments in Table 11, we use the same strategy as for {CMO w/ Mixup}. For {CMO w/ Gaussian Blur} and {CMO w/ Color Jitter}, which do not mix two images, we divided classes into two groups: the majority and the minority. Then, for the minority group, we augmented the data with color jitter and gaussian blur</title>
		<idno>5]. We trained ResNet-32 [18] by SGD optimizer with a momentum of 0.9</idno>
	</analytic>
	<monogr>
		<title level="m">To set up a fair comparison, we used the same random seed to make CIFAR-100-LT, and followed the implementation of</title>
		<imprint/>
	</monogr>
	<note>respectively. We set brightness to 0.5 and hue to 0.3 for color jitter, and set kernel size as (5, 7) and sigma as (0.1, 5) for Gaussian blur using the PyTorch [39] implemented functions</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">As in [49], we performed simple horizontal flips, color jittering, and took random crops 224 ? 224 in size. We used ResNet-50 as a backbone network. The networks were trained with a batch size of 256 on 4 GTX 1080Ti GPUs for 100 epochs using SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Lt. For</forename><surname>Imagenet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imagenet-Lt</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and an initial learning rate of 0.1</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">For iNaturalist 2018, we used the same data augmentation method as for ImageNet-LT. Multiple backbone networks were experimented on iNaturalist</title>
	</analytic>
	<monogr>
		<title level="j">ResNet</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>including ResNet-50. ResNet-152 [18], and Wide ResNet</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">All backbone networks were trained with a batch size of 512 on 8 Tesla V100 GPUs for 200 epochs using SGD at an initial learning rate of 0.1; this rate decayed by 0.1 at 75 epochs and 160 epochs. Experiments were implemented and evaluated on the NAVER Smart Machine Learning</title>
		<imprint/>
	</monogr>
	<note>NSML10 in Table 13. As in the imbalance ratio of 100, our method consistently improves performance in all long-tailed recognition methods</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Comparison against baselines on CIFAR-100-LT Results with classification accuracy (%) of ResNet-32. The best results are marked in bold</title>
		<idno>ratio 50 10 Method Vanilla +ROS [47] +Remix [7] +CMO Vanilla +ROS [47] +Remix [7</idno>
	</analytic>
	<monogr>
		<title level="m">Table 13</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
