<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NICE: NON-LINEAR INDEPENDENT COMPONENTS ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">D?partement d&apos;informatique</orgName>
								<orgName type="institution">Universit? de Montr?al Montr?al</orgName>
								<address>
									<postCode>H3C 3J7</postCode>
									<region>QC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">D?partement d&apos;informatique</orgName>
								<orgName type="institution">Universit? de Montr?al Montr?al</orgName>
								<address>
									<postCode>H3C 3J7</postCode>
									<region>QC</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">D?partement d&apos;informatique</orgName>
								<orgName type="institution">Universit? de Montr?al Montr?al</orgName>
								<address>
									<postCode>H3C 3J7</postCode>
									<region>QC</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NICE: NON-LINEAR INDEPENDENT COMPONENTS ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted as a workshop contribution at ICLR 2015</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep learning framework for modeling complex high-dimensional densities called Non-linear Independent Component Estimation (NICE). It is based on the idea that a good representation is one in which the data has a distribution that is easy to model. For this purpose, a non-linear deterministic transformation of the data is learned that maps it to a latent space so as to make the transformed data conform to a factorized distribution, i.e., resulting in independent latent variables. We parametrize this transformation so that computing the determinant of the Jacobian and inverse Jacobian is trivial, yet we maintain the ability to learn complex non-linear transformations, via a composition of simple building blocks, each based on a deep neural network. The training criterion is simply the exact log-likelihood, which is tractable. Unbiased ancestral sampling is also easy. We show that this approach yields good generative models on four image datasets and can be used for inpainting. * Yoshua Bengio is a CIFAR Senior Fellow.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>One of the central questions in unsupervised learning is how to capture complex data distributions that have unknown structure. Deep learning approaches <ref type="bibr" target="#b2">(Bengio, 2009</ref>) rely on the learning of a representation of the data that would capture its most important factors of variation. This raises the question: what is a good representation? Like in recent work <ref type="bibr" target="#b17">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b25">Rezende et al., 2014;</ref><ref type="bibr" target="#b24">Ozair and Bengio, 2014)</ref>, we take the view that a good representation is one in which the distribution of the data is easy to model. In this paper, we consider the special case where we ask the learner to find a transformation h = f (x) of the data into a new space such that the resulting distribution factorizes, i.e., the components h d are independent:</p><formula xml:id="formula_0">p H (h) = d p H d (h d ).</formula><p>The proposed training criterion is directly derived from the log-likelihood. More specifically, we consider a change of variables h = f (x), which assumes that f is invertible and the dimension of h is the same as the dimension of x, in order to fit a distribution p H . The change of variable rule gives us:</p><formula xml:id="formula_1">p X (x) = p H (f (x))|det ?f (x) ?x |.<label>(1)</label></formula><p>where ?f (x) ?x is the Jacobian matrix of function f at x. In this paper, we choose f such that the determinant of the Jacobian is trivially obtained. Moreover, its inverse f ?1 is also trivially obtained, allowing us to sample from p X (x) easily as follows:</p><formula xml:id="formula_2">h ? p H (h) x = f ?1 (h)<label>(2)</label></formula><p>A key novelty of this paper is the design of such a transformation f that yields these two properties of "easy determinant of the Jacobian" and "easy inverse", while allowing us to have as much capacity (a) Inference: log(p X (x)) = log(p H (f (x))) + log(|det( ?f (x) ?x )|)</p><formula xml:id="formula_3">(b) Sampling: h ? p H (h), x = f ?1 (h) (c) Inpainting: max x H log(p X ((x O , x H ))) = max x H log(p H (f ((x O , x H )))) + log(|det( ?f ((x O ,x H )) ?x )|)</formula><p>as needed in order to learn complex transformations. The core idea behind this is that we can split x into two blocks (x 1 , x 2 ) and apply as building block a transformation from (x 1 , x 2 ) to (y 1 , y 2 ) of the form:</p><formula xml:id="formula_4">y 1 = x 1 y 2 = x 2 + m(x 1 )<label>(3)</label></formula><p>where m is an arbitrarily complex function (a ReLU MLP in our experiments). This building block has a unit Jacobian determinant for any m and is trivially invertible since:</p><formula xml:id="formula_5">x 1 = y 1 x 2 = y 2 ? m(y 1 ).<label>(4)</label></formula><p>The details, surrounding discussion, and experimental results are developed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">LEARNING BIJECTIVE TRANSFORMATIONS OF CONTINUOUS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROBABILITIES</head><p>We consider the problem of learning a probability density from a parametric family of densities {p ? , ? ? ?} over finite dataset D of N examples, each living in a space X ; typically X = R D .</p><p>Our particular approach consists of learning a continuous, differentiable almost everywhere nonlinear transformation f of the data distribution into a simpler distribution via maximum likelihood using the following change of variables formula:</p><formula xml:id="formula_6">log(p X (x)) = log(p H (f (x))) + log(|det( ?f (x) ?x )|)</formula><p>where p H (h), the prior distribution, will be a predefined density function 1 , for example a standard isotropic Gaussian. If the prior distribution is factorial (i.e. with independent dimensions), then we obtain the following non-linear independent components estimation (NICE) criterion, which is simply maximum likelihood under our generative model of the data as a deterministic transform of a factorial distribution:</p><formula xml:id="formula_7">log(p X (x)) = D d=1 log(p H d (f d (x))) + log(|det( ?f (x) ?x )|) where f (x) = (f d (x)) d?D .</formula><p>We can view NICE as learning an invertible preprocessing transform of the dataset. Invertible preprocessings can increase likelihood arbitrarily simply by contracting the data. We use the change of = y key g y cipher m</p><p>x key x plain <ref type="figure">Figure 2</ref>: Computational graph of a coupling layer variables formula (Eq. 1) to exactly counteract this phenomenon and use the factorized structure of the prior p H to encourage the model to discover meaningful structures in the dataset. In this formula, the determinant of the Jacobian matrix of the transform f penalizes contraction and encourages expansion in regions of high density (i.e., at the data points), as desired. As discussed in <ref type="bibr" target="#b5">Bengio et al. (2013)</ref>, representation learning tends to expand the volume of representation space associated with more "interesting" regions of the input (e.g., high density regions, in the unsupervised learning case).</p><p>In line with previous work with auto-encoders and in particular the variational auto-encoder <ref type="bibr" target="#b17">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b25">Rezende et al., 2014;</ref><ref type="bibr" target="#b22">Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b12">Gregor et al., 2014)</ref>, we call f the encoder and its inverse f ?1 the decoder. With f ?1 given, sampling from the model can proceed very easily by ancestral sampling in the directed graphical model H ? X, i.e., as described in Eq. 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ARCHITECTURE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">TRIANGULAR STRUCTURE</head><p>The architecture of the model is crucial to obtain a family of bijections whose Jacobian determinant is tractable and whose computation is straightforward, both forwards (the encoder f ) and backwards (the decoder f ?1 ). If we use a layered or composed transformation f = f L ? . . . ? f 2 ? f 1 , the forward and backward computations are the composition of its layers' computations (in the suited order), and its Jacobian determinant is the product of its layers' Jacobian determinants. Therefore we will first aim at defining those more elementary components.</p><p>First we consider affine transformations. <ref type="bibr" target="#b25">(Rezende et al., 2014)</ref> and <ref type="bibr" target="#b17">(Kingma and Welling, 2014)</ref> provide formulas for the inverse and determinant when using diagonal matrices, or diagonal matrices with rank-1 correction, as transformation matrices. Another family of matrices with tractable determinant are triangular matrices, whose determinants are simply the product of their diagonal elements. Inverting triangular matrices at test time is reasonable in terms of computation. Many square matrices M can also be expressed as a product M = LU of upper and lower triangular matrices. Since such transformations can be composed, we see that useful components of these compositions include ones whose Jacobian is diagonal, lower triangular or upper triangular.</p><p>One way to use this observation would be to build a neural network with triangular weight matrices and bijective activation functions, but this highly constrains the architecture, limiting design choices to depth and selection of non-linearities. Alternatively, we can consider a family of functions with triangular Jacobian. By ensuring that the diagonal elements of the Jacobian are easy to compute, the determinant of the Jacobian is also made easy to compute.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">COUPLING LAYER</head><p>In this subsection we describe a family of bijective transformation with triangular Jacobian therefore tractable Jacobian determinant. That will serve a building block for the transformation f .</p><p>General coupling layer Let x ? X , I 1 , I 2 a partition of 1, D such that d = |I 1 | and m a function defined on R d , we can define y = (y I1 , y I2 ) where:</p><formula xml:id="formula_8">y I1 = x I1 y I2 = g(x I2 ; m(x I1 )) where g : R D?d ? m(R d ) ? R D?d</formula><p>is the coupling law, an invertible map with respect to its first argument given the second. The corresponding computational graph is shown <ref type="figure">Fig 2.</ref> If we consider I 1 = 1, d and I 2 = d, D , the Jacobian of this function is:</p><formula xml:id="formula_9">?y ?x = I d 0 ?y I 2 ?x I 1 ?y I 2 ?x I 2</formula><p>Where I d is the identity matrix of size d. That means that det ?y ?x = det</p><formula xml:id="formula_10">?y I 2 ?x I 2 .</formula><p>Also, we observe we can invert the mapping using:</p><p>x I1 = y I1</p><formula xml:id="formula_11">x I2 = g ?1 (y I2 ; m(y I1 ))</formula><p>We call such a transformation a coupling layer with coupling function m.</p><p>Additive coupling layer For simplicity, we choose as coupling law an additive coupling law g(a; b) = a + b so that by taking a = x I2 and b = m(x I1 ):</p><formula xml:id="formula_12">y I2 = x I2 + m(x I1 ) x I2 = y I2 ? m(y I1 )</formula><p>and thus computing the inverse of this transformation is only as expensive as computing the transformation itself. We emphasize that there is no restriction placed on the choice of coupling function m (besides having the proper domain and codomain). For example, m can be a neural network with d input units and D ? d output units.</p><p>Moreover, since det ?y I 2 ?x I 2 = 1, an additive coupling layer transformation has a unit Jacobian determinant in addition to its trivial inverse. One could also choose other types of coupling, such as a multiplicative coupling law g(a; b) = a b, b = 0 or an affine coupling law g(a;</p><formula xml:id="formula_13">b) = a b 1 +b 2 , b 1 = 0 if m : R d ? R D?d ? R D?d .</formula><p>We chose the additive coupling layer for numerical stability reason as the transformation become piece-wise linear when the coupling function, m, is a rectified neural network.</p><p>Combining coupling layers We can compose several coupling layers to obtain a more complex layered transformation. Since a coupling layer leaves part of its input unchanged, we need to exchange the role of the two subsets in the partition in alternating layers, so that the composition of two coupling layers modifies every dimension. Examining the Jacobian, we observe that at least three coupling layers are necessary to allow all dimensions to influence one another. We generally use four.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ALLOWING RESCALING</head><p>As each additive coupling layers has unit Jacobian determinant (i.e. is volume preserving), their composition will necessarily have unit Jacobian determinant too. In order to adress this issue, we include a diagonal scaling matrix S as the top layer, which multiplies the i-th ouput value by S ii :</p><formula xml:id="formula_14">(x i ) i?D ? (S ii x i ) i?D .</formula><p>This allows the learner to give more weight (i.e. model more variation) on some dimensions and less in others.</p><p>In the limit where S ii goes to +? for some i, the effective dimensionality of the data has been reduced by 1. This is possible so long as f remains invertible around the data point. With such a scaled diagonal last stage along with lower triangular or upper triangular stages for the rest (with the identity in their diagonal), the NICE criterion has the following form:</p><formula xml:id="formula_15">log(p X (x)) = D i=1 [log(p Hi (f i (x))) + log(|S ii |)].</formula><p>We can relate these scaling factors to the eigenspectrum of a PCA, showing how much variation is present in each of the latent dimensions (the larger S ii is, the less important the dimension i is). The important dimensions of the spectrum can be viewed as a manifold learned by the algorithm. The prior term encourages S ii to be small, while the determinant term log S ii prevents S ii from ever reaching 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">PRIOR DISTRIBUTION</head><p>As mentioned previously, we choose the prior distribution to be factorial, i.e.:</p><formula xml:id="formula_16">p H (h) = D d=1 p H d (h d )</formula><p>We generally pick this distribution in the family of standard distribution, e.g. gaussian distribution:</p><formula xml:id="formula_17">log(p H d ) = ? 1 2 (h 2 d + log(2?))</formula><p>or logistic distribution:</p><formula xml:id="formula_18">log(p H d ) = ? log(1 + exp(h d )) ? log(1 + exp(?h d ))</formula><p>We tend to use the logistic distribution as it tends to provide a better behaved gradient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RELATED METHODS</head><p>Significant advances have been made in generative models. Undirected graphical models like deep Boltzmann machines (DBM) <ref type="bibr" target="#b28">(Salakhutdinov and Hinton, 2009</ref>) have been very successful and an intense subject of research, due to efficient approximate inference and learning techniques that these models allowed. However, these models require Markov chain Monte Carlo (MCMC) sampling procedure for training and sampling and these chains are generally slowly mixing when the target distribution has sharp modes. In addition, the log-likelihood is intractable, and the best known estimation procedure, annealed importance sampling (AIS) <ref type="bibr" target="#b29">(Salakhutdinov and Murray, 2008)</ref>, might yield an overly optimistic evaluation <ref type="bibr" target="#b13">(Grosse et al., 2013)</ref>.</p><p>Directed graphical models lack the conditional independence structure that allows DBMs efficient inference. Recently, however, the development of variational auto-encoders (VAE) <ref type="bibr" target="#b17">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b25">Rezende et al., 2014;</ref><ref type="bibr" target="#b22">Mnih and Gregor, 2014;</ref><ref type="bibr" target="#b12">Gregor et al., 2014</ref>) -allowed effective approximate inference during training. In constrast with the NICE model, these approaches use a stochastic encoder q(h | x) and an imperfect decoder p(x | h), requiring a reconstruction term in the cost, ensuring that the decoder approximately inverts the encoder. This injects noise into the auto-encoder loop, since h is sampled from q(h | x), which is a variational approximation to the true posterior p(h | x). The resulting training criterion is the variational lower bound on the log-likelihood of the data. The generally fast ancestral sampling technique that directed graphical models provide make these models appealing. Moreover, the importance sampling estimator of the log-likelihood is guaranteed not to be optimistic in expectation. But using a lower bound criterion might yield a suboptimal solution with respect to the true log-likelihood. Such suboptimal solutions might for example inject a significant amount of unstructured noise in the generation process resulting in unnatural-looking samples. In practice, we can output a statistic of p(x | h), like the expectation or the median, instead of an actual sample. The use of a deterministic decoder can be motivated by the objective of eliminating low-level noise, which gets automatically added at the last stage of generation in models such as the VAE and Boltzmann machines (the visible are considered independent, given the hidden).</p><p>The NICE criterion is very similar to the criterion of the variational auto-encoder. More specifically, as the transformation and its inverse can be seen as a perfect auto-encoder pair <ref type="bibr" target="#b3">(Bengio, 2014)</ref>, the reconstruction term is a constant that can be ignored. This leaves the Kullback-Leibler divergence term of the variational criterion: log(p H (f (x))) can be seen as the prior term, which forces the code h = f (x) to be likely with respect to the prior distribution, and log(|det ?f (x) ?x |) can be seen as the entropy term. This entropy term reflects the local volume expansion around the data (for the encoder), which translates into contraction in the decoder f ?1 . In a similar fashion, the entropy term in the variational criterion encourages the approximate posterior distribution to occupy volume, which also translates into contraction from the decoder. The consequence of perfect reconstruction is that we also have to model the noise at the top level, h, whereas it is generally handled by the conditional model p(x | h) in these other graphical models.</p><p>We also observe that by combining the variational criterion with the reparametrization trick, <ref type="bibr" target="#b17">(Kingma and Welling, 2014)</ref> is effectively maximizing the joint log-likelihood of the pair (x, ) in a NICE model with two affine coupling layers (where is the auxiliary noise variable) and gaussian prior, see Appendix C.</p><p>The change of variable formula for probability density functions is prominently used in inverse transform sampling (which is effectively the procedure used for sampling here). Independent component analysis (ICA) <ref type="bibr" target="#b14">(Hyv?rinen and Oja, 2000)</ref>, and more specifically its maximum likelihood formulation, learns an orthogonal transformation of the data, requiring a costly orthogonalization procedure between parameter updates. Learning a richer family of transformations was proposed in <ref type="bibr" target="#b1">(Bengio, 1991)</ref>, but the proposed class of transformations, neural networks, lacks in general the structure to make the inference and optimization practical. <ref type="bibr" target="#b7">(Chen and Gopinath, 2000)</ref> learns a layered transformation into a gaussian distribution but in a greedy fashion and it fails to deliver a tractable sampling procedure. <ref type="bibr" target="#b26">(Rippel and Adams, 2013)</ref> reintroduces this idea of learning those transformations but is forced into a regularized auto-encoder setting as a proxy of log-likelihood maximization due to the lack of bijectivity constraint. A more principled proxy of log-likelihood, the variational lower bound, is used more successfully in nonlinear independent components analysis <ref type="bibr" target="#b15">(Hyv?rinen and Pajunen, 1999)</ref> via ensemble learning <ref type="bibr" target="#b27">(Roberts and Everson, 2001;</ref><ref type="bibr" target="#b19">Lappalainen et al., 2000)</ref> and in <ref type="bibr" target="#b17">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b25">Rezende et al., 2014)</ref> using a type of Helmholtz machine <ref type="bibr" target="#b9">(Dayan et al., 1995)</ref>. Generative adversarial networks (GAN) <ref type="bibr" target="#b10">(Goodfellow et al., 2014)</ref> also train a generative model to transform a simple (e.g. factorial) distribution into the data distribution, but do not require an encoder that goes in the other direction. GAN sidesteps the difficulties of inference by learning a secondary deep network that discriminates between GAN samples and data. This classifier network provides a training signal to the GAN generative model, telling it how to make its output less distinguishable from the training data.</p><p>Like the variational auto-encoders, the NICE model uses an encoder to avoid the difficulties of inference, but its encoding is deterministic. The log-likelihood is tractable and the training procedure does not require any sampling (apart from dequantizing the data). The triangular structure used in NICE to obtain tractability is also present in another family of tractable density models, the neural autoregressive networks <ref type="bibr" target="#b4">(Bengio and Bengio, 2000)</ref>, which include as a recent and succesful example the neural autoregressive density estimator (NADE) <ref type="bibr" target="#b20">(Larochelle and Murray, 2011)</ref>. Indeed, the adjacency matrix in the NADE directed graphical model is strictly triangular. However the element-by-element autoregressive schemes make the ancestral sampling procedure computationally expensive and unparallelizable for generative tasks on high-dimensional data, such as image data. A NICE model using one coupling layer can be seen as a block version of NADE with two blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">LOG-LIKELIHOOD AND GENERATION</head><p>We train NICE on <ref type="bibr">MNIST (LeCun and Cortes, 1998)</ref>, the Toronto Face Dataset 2 (TFD) <ref type="bibr" target="#b30">(Susskind et al., 2010)</ref>, the Street View House Numbers dataset (SVHN) <ref type="bibr" target="#b23">(Netzer et al., 2011) and</ref><ref type="bibr">CIFAR-10 (Krizhevsky, 2010)</ref>. As prescribed in <ref type="bibr" target="#b32">(Uria et al., 2013)</ref>, we use a dequantized version of the data: we add a uniform noise of 1 256 to the data and rescale it to be in [0, 1] D after dequantization. We add a uniform noise of 1 128 and rescale the data to be in [?1, 1] D for CIFAR-10. The architecture used is a stack of four coupling layers with a diagonal positive scaling (parametrized exponentially) exp(s) for the last stage, and with an approximate whitening for TFD and exact ZCA on SVHN and CIFAR-10. We partition the input space between by separating odd (I 1 ) and even (I 2 )  components, so the equation is:</p><formula xml:id="formula_19">h (1) I1 = x I1 h (1) I2 = x I2 + m (1) (x I1 ) h (2) I2 = h (1) I2 h (2) I1 = h (1) I1 + m (2) (x I2 ) h (3) I1 = h (2) I1 h (3) I2 = h (2) I2 + m (3) (x I1 ) h (4) I2 = h (3) I2 h (4) I1 = h (3) I1 + m (4) (x I2 ) h = exp(s) h (4)</formula><p>The coupling functions m (1) , m (2) , m (3) and m (4) used for the coupling layers are all deep rectified networks with linear output units. We use the same network architecture for each coupling function: five hidden layers of 1000 units for MNIST, four of 5000 for TFD, and four of 2000 for SVHN and CIFAR-10.</p><p>A standard logistic distribution is used as prior for MNIST, SVHN and CIFAR-10. A standard normal distribution is used as prior for TFD.</p><p>The models are trained by maximizing the log-likelihood log(p H (h)) + D i=1 s i with AdaM <ref type="bibr" target="#b16">(Kingma and Ba, 2014)</ref> with learning rate 10 ?3 , momentum 0.9, ? 2 = 0.01, ? = 1, and = 10 ?4 . We select the best model in terms of validation log-likelihood after 1500 epochs.</p><p>We obtained a test log-likelihood of 1980.50 on MNIST, 5514.71 on TFD, 11496.55 for SVHN and 5371.78 for CIFAR-10. This compares to the best results that we know of in terms of loglikelihood: 5250 on TFD and 3622 on CIFAR-10 with deep mixtures of factor analysers <ref type="bibr" target="#b31">(Tang et al., 2012</ref>) (although it is still a lower bound), see <ref type="table">Table 4</ref>. As generative models on continuous MNIST are generally evaluated with Parzen window estimation, no fair comparison can be made. Samples generated by the trained models are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>.   <ref type="figure">Figure 6</ref>: Inpainting on MNIST. We list below the type of the part of the image masked per line of the above middle figure, from top to bottom: top rows, bottom rows, odd pixels, even pixels, left side, right side, middle vertically, middle horizontally, 75% random, 90% random. We clamp the pixels that are not masked to their ground truth value and infer the state of the masked pixels by projected gradient ascent on the likelihood. Note that with middle masks, there is almost no information available about the digit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">INPAINTING</head><p>Here we consider a naive iterative procedure to implement inpainting with the trained generative models. For inpainting we clamp the observed dimensions (x O ) to their values and maximize loglikelihood with respect to the hidden dimensions (X H ) using projected gradient ascent (to keep the input in its original interval of values) with gaussian noise with step size ? i = 10 100+i , where i is the iteration, following the stochastic gradient update:</p><formula xml:id="formula_20">x H,i+1 = x H,i + ? i ( ? log(p X ((x O , x H,i ))) ?x H,i + ) ? N (0, I)</formula><p>where x H,i are the values of the hidden dimensions at iteration i. The result is shown on test examples of MNIST, in <ref type="figure">Fig 6.</ref> Although the model is not trained for this task, the inpainting procedure seems to yield reasonable qualitative performance, but note the occasional presence of spurious modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work we presented a new flexible architecture for learning a highly non-linear bijective transformation that maps the training data to a space where its distribution is factorized, and a framework to achieve this by directly maximizing log-likelihood. The NICE model features efficient unbiased ancestral sampling and achieves competitive results in terms of log-likelihood.</p><p>Note that the architecture of our model could be trained using other inductive principles capable of exploiting its advantages, like toroidal subspace analysis (TSA) <ref type="bibr" target="#b8">(Cohen and Welling, 2014)</ref>.</p><p>We also briefly made a connection with variational auto-encoders. We also note that NICE can enable more powerful approximate inference allowing a more complex family of approximate posterior distributions in those models, or a richer family of priors.</p><p>provided by Compute Canada and Calcul Qu?bec, and for the research funding provided by NSERC, CIFAR, and Canada Research Chairs. To illustrate the learned manifold, we also take a random rotation R of a 3D sphere S in latent space and transform it to data space, the result f ?1 (R(S)) is shown in <ref type="figure" target="#fig_4">Fig 7.</ref> A.2 SPECTRUM</p><p>We also examined the last diagonal scaling layer and looked at its coefficients (S dd ) d?D . If we consider jointly the prior distribution and the diagonal scaling layer, ? d = S ?1 dd can be considered as the scale parameter of each independent component. This shows us the importance that the model has given to each component and ultimately how successful the model was at learning manifolds. We sort (? d ) d?D and plot it in <ref type="figure" target="#fig_5">Fig 8.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B APPROXIMATE WHITENING</head><p>The procedure for learning the approximate whitening is using the NICE framework, with an affine function and a standard gaussian prior. We have:</p><formula xml:id="formula_21">z = Lx + b</formula><p>with L lower triangular and b a bias vector. This is equivalent to learning a gaussian distribution. The optimization procedure is the same as NICE: RMSProp with early stopping and momentum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C VARIATIONAL AUTO-ENCODER AS NICE</head><p>We assert here that the stochastic gradient variational Bayes (SGVB) algorithm maximizes the loglikelihood on the pair (x, ). <ref type="bibr" target="#b17">(Kingma and Welling, 2014)</ref>  with D X = dim(X). This is equivalent to: log(p (x, ),(?,?) (x, )) ? log(p ( )) = log(p H (h)) ? D X log(?) + log(|det ?g ? ? ( ; x)|) ? log(p ( )) = log(p H (h)) ? D X log(?) ? log(q Z|X;? (z)) = log(p ? (?)p Z (z)) ? D X log(?) ? log(q Z|X;? (z)) = log(p ? (?)) + log(p Z (z)) ? D X log(?) ? log(q Z|X;? (z)) = log(p ? (?)) ? D X log(?) + log(p Z (z)) ? log(q Z|X;? (z)) = log(p X|Z (x | z)) + log(p Z (z)) ? log(q Z|X;? (z)) This is the Monte Carlo estimate of the SGVB cost function proposed in <ref type="bibr" target="#b17">(Kingma and Welling, 2014)</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Computational graph of the probabilistic model, using the following formulas.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Architecture and results. # hidden units refer to the number of units per hidden layer. Log-likelihood results on TFD and CIFAR-10. Note that the Deep MFA number correspond to the best results obtained from<ref type="bibr" target="#b31">(Tang et al., 2012)</ref> but are actually variational lower bound.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Unbiased samples from a trained NICE model. We sample h ? p H (h) and we output x = f ?1 (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Sphere in the latent space. These figures show part of the manifold structure learned by the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>define a recognition network: z = g ? ( | x), ? N (0, I) For a standard gaussian prior p(z) and conditional p(x | z), we can define:? = x ? f ? (z) ?(a) Model trained on MNIST (b) Model trained on TFD (c) Model trained on SVHN (d) Model trained on CIFAR-10 Decay of ? d = S ?1 dd . The large values correspond to dimensions on which the model chooses to have larger variations, thus highlighting the learned manifold structure from the data. This is the non-linear equivalent of the eigenspectrum in the case of PCA. On the x axis are the components d sorted by ? d (on the y axis). If we define a standard gaussian prior on h = (z, ?). The resulting cost function is: log(p (x, ),(?,?) (x, )) = log(p H (h)) ? D X log(?) + log(|det ?g ? ? ( ; x)|)</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that this prior distribution does not need to be constant and could also be learned</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We train on unlabeled data for this dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>We would like to thank Yann Dauphin, Vincent Dumoulin, Aaron Courville, Kyle Kastner, Dustin Webb, Li Yao and Aaron Van den Oord for discussions and feedback. Vincent Dumoulin provided code for visualization. We are grateful towards the developers of Theano <ref type="bibr" target="#b6">(Bergstra et al., 2011;</ref><ref type="bibr" target="#b0">Bastien et al., 2012)</ref> and Pylearn2 <ref type="bibr" target="#b11">(Goodfellow et al., 2013)</ref>, and for the computational resources</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bouchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Artificial Neural Networks and their Application to Sequence Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<pubPlace>Montreal, Canada</pubPlace>
		</imprint>
		<respStmt>
			<orgName>McGill University, (Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning deep architectures for AI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How auto-encoders could provide credit assignment in deep networks via target propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1407.7906</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling high-dimensional discrete data with multi-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 12 (NIPS&apos;99)</title>
		<editor>Solla, S., Leen, T., and M?ller, K.-R.</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="400" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Better mixing via deep representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mesnil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rifai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML&apos;13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML&apos;13)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Theano: Deep learning on gpus with python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bergeron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Big Learn workshop, NIPS&apos;11</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gaussianization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Gopinath</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.4437</idno>
		<title level="m">Learning the irreducible representations of commutative lie groups</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The Helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2661</idno>
		<title level="m">Generative adversarial networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1308.4214</idno>
		<title level="m">Pylearn2: a machine learning research library</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Annealing between distributions by averaging moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis: Existence and uniqueness results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pajunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="429" to="439" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<ptr target="http://www.cs.utoronto.ca/kriz/conv-cifar10-aug2010.pdf" />
		<title level="m">Convolutional deep belief networks on CIFAR-10</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto. Unpublished Manuscript</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Nonlinear independent component analysis using ensemble learning: Experiments and discussion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lappalainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giannakopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Honkela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICA. Citeseer</title>
		<meeting>ICA. Citeseer</meeting>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The Neural Autoregressive Distribution Estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<publisher>AIS-TATS</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning and Unsupervised Feature Learning Workshop, NIPS</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep directed generative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0630</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>U. Montreal</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">High-dimensional probability estimation with deep density models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.5125</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Independent component analysis: principles and practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Everson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep Boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</title>
		<editor>Cohen, W. W., McCallum, A., and Roweis, S. T.</editor>
		<meeting>the Twenty-fifth International Conference on Machine Learning (ICML&apos;08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Toronto face dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>UTML TR 2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>U. Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.4635</idno>
		<title level="m">Deep mixtures of factor analysers</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rnade: The real-valued neural autoregressive density-estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Uria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
